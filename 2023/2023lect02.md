---
title: 第02回
author: 浅川 伸一
layout: default
---

<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+2" color="navy"><strong>2020年度開講 駒澤大学 心理学特講 IIIA, IIIB</strong></font><br/><br/>
<img src="/assets/header_logo.png" sytle="widht:09%">
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 15/Apr/2022<br/>
Appache 2.0 license<br/>
</div>

<center>
<div style="width:66%;text-align:left">

人間の感情と、他の生物のそれと、近代的な型の自動機械の反応との間に鋭い乗り越えられない区画線を引く心理学者は、私が私自身の主張に慎重でなければならないのと同様に、私の説を否定するのに慎重でなければならない <br/>

--- N. Wiener, The Human Use of Human Beings(人間機械論, みすず書房, p.73) ---
</div>
</center>

## 本日のメニュー

1. 注意事項
    * [出席確認](https://webclass.komazawa-u.ac.jp/webclass/course.php/2307440/manage/attendance?acs_=8460175d){:target="_blank"}
    * [画面共有 https://meet.google.com/oia-vgsd-cpb](https://meet.google.com/oia-vgsd-cpb){:target="_blank"} 後ろの席で，画面が見えにくい場合

## 先週の相談内容と復習

第二回となる今回は，言語モデルを題材に，人工知能，ニューラルネットワークの歴史と分類を概説します。
第一回にもお話したとり，この授業では chatGPT を始めとする，人工知能で使われている技術や考え方と，心理学における対応物や関係を考えることがテーマとなります。

先週の授業では，chatGPT の背後にある技術について，最初に説明することになりました。
シラバスで説明した内容とは取り上げる順番が異なります。
繰り返しになりますが，この授業では，chatGPT で用いられている技術と，心理学との関連を考えることを試みます。

具体的には以下の順で取り上げます:

1. 自然言語処理
2. 視覚認識
3. 強化学習

上記とシラバスとは 1 番目と 2 番目が入れ替わっています。
それは，chatGPT は自然言語処理技術分野のモデルに分類できるからです。

### chatGPT

<div class="figure figcenter">
<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" width="99%">
<div class="figcaption">

#### instructGPT の概要 [2022Quyang+](https://arxiv.org/abs/2203.02155) Fig.2 を改変
</div></div>

chatGPT の GPT とは **Genrative Pre-trained Transformer** の頭文字です。
**生成モデル (generative modeling)** と **事前学習 (pre-trained models)** と **トランスフォーマー (transformer)** についての理解が必要となります。

生成モデルと事前学習について触れる前に，最後のトランスフォーマーについて言及します。
トランスフォーマーは **言語モデル (Lanugage models)** です。
言語モデルによって，文章が処理され，適切な応答をするようになったモデルの代表が chatGPT となります。

そこで言語モデルを理解するために，その構成要素であるトランスフォーマーを取り上げる必要があります。
トランスフォーマーは，2017 年の論文 [Attention Is All You Need](https://arxiv.org/abs/1706.03762) で提案された，**ニューラルネットワーク neural network** モデルです。
トランスフォーマーはゲームチェンジャーとなりました。
最近の **大規模言語モデル (LLM: Large Language Model)** は，トランスフォーマーを基本構成要素とするモデルがほとんどです。
上記の論文のタイトルにあるとおり，トランスフォーマーは，**注意機構 attention mechanism** に基づいて，自然言語処理の諸課題を解くモデルです。

まず第一に，大規模ではない，言語モデルについて考えます。
言語モデルは，機械翻訳などでも使われる技術です。
ですから，DeepL や Google 翻訳で，使っている方もいることでしょう。

chatGPT を使える方は，上記太字のキーワードについて，chatGPT に質問してみることをお勧めします。
とりわけ 注意 については，認知，視覚，心理学との関連も深く，注意の障害は，臨床，教育，発達などの分野と関係するでしょう。

<div class="figure figcenter">
<img src="/assets/2017Vaswani_Fig2_1.svg" width="19%">
<div class="figcaption">

#### Transformer [2017Vaswani++](https://arxiv.org/abs/1706.03762) Fig.2 を改変
</div></div>


<!-- 本日は，機械学習と統計学との関係を取り上げ，ニューラルネットワークの導入と実習を行います。 -->

<!--
### 心理学における注意の専門用語の確認

* 外的注意 external と内的注意 internal
* 現外的注意 overt と内省的注意 covert : 眼球運動を伴う注意と，伴わない注意。
* 空間的注意
* 視覚探索: 特徴 features と対象 objects


先週は，心理学と人工知能との関係 -->


#### ASA アメリカ統計学会の声明再録

1. **P 値は，データが指定された統計モデルとどの程度相性が悪いかを示すことができる** P-values can indicate how incompatible the data are with a specified statistical model.
2. **P 値は，研究された仮説が真である確率を測定するものではない。そうではなく，データがランダムな偶然だけから，生成された確率を測定するものである** P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. **科学的な結論やビジネスや政策の決定は，p 値が特定の閾値を超えたかどうかだけに基づくべきではない** Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. **適切な推論を行うには，完全な報告と透明性が必要である** Proper inference requires full reporting and transparency.
5. **P 値や統計的有意性は，効果の大きさや結果の重要性を測定するものではない** A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. **それ自体では，p 値はモデルや仮説に関する証拠の良い尺度を提供しない。** By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

* [基礎と応用社会心理学 (BASP)  編集方針 (2014,2015)](/2023/2015Basic_and_Applied_Social_Psychology_ban_p_values_ja)
* [アメリカ統計学会の声明 2014, 2015](/2023/2015Basic_and_Applied_Social_Psychology_ban_p_values_ja)
* [統計学の誤り : 統計的妥当性の「ゴールドスタンダード」である P 値は多くの科学者が想定しているほど信頼できるものではない](/2023/2014Nuzzo_Statistical_errors_ja)
* [統計的有意性を引退させろ](/2023/2019Amrhein_Retire_statistical_significance_ja)

では，どうすればよいのか。

### Breiman によるデータサイエンスにおける 2 つの文化 <!-- あるいは，統計学と機械学習とニューラルネットワークの関係-->

<center>

<img src="/2023assets/2001Breiman_Two_Cultures_fig2.svg" width="39%"><br/>
<img src="/2023assets/2001Breiman_Two_Cultures_fig3_.svg" width="39%"><br/>
<!-- <img src="/2023assets/2001Breiman_cultures.svg" width="23%"><br/> -->

<!-- ![Breiman(2001)](/2023assets/2001Breiman_cultures.svg){#fig:2001breiman style="width:34%"} -->
From Leo Breiman, Statistical Modeling: The Two Cultures, _Statistical Science_, 2001, Vol. 16, No. 3, 199–231.
</center>


### 機械学習と心理統計学の違い

仮説検定とパラメータチューニングの差異は，母集団の相違に期すのか，それとも選択しているモデルによるものなのか。
心理統計では，データを説明する努力よりも，母集団の相違，すなわち，帰無仮説が棄却できるか採択されるかに興味がある。
ところが，帰無仮説が正しいかどうかは，選択する統計モデルに依存する。
このとき統計モデルの精度が正しいのかどうかを問題にすることは少ない。
だが，用いるモデルに依存して推論結果が変化するかも知れない。
そうするとモデルの優劣が問題になるであろう。

一方，機械学習では，心理統計の母集団に相当する概念が，汎化性能である。
所与のデータにだけ当てはまるモデルではなく，未知のデータにたいして性能の高いモデルが選択される。
未知のデータ，未学習のデータに対する性能と母集団の差異を，一概に比較することは難しいが，予測精度を高くすることが，現実には用いられる実用性が高い。
応用が可能で，実学として世の中の役に立つ成果を生み出すことができる。



## 3 つの分野の略史

|      |   人工知能        | ニューラルネットワーク|   心理学       |
|:-----|:-----------------|:------------------|:--------------|
|第一次 1950- |  記号処理         | パーセプトロン        | 認知革命 |
|      |  オモチャ問題     | ADALINE           |                     |
|      |                  | ネオコグニトロン     |                      |
|      |                  | アソシアトロン         |                      |
|第二次 1980- | Expert systems   | 誤差逆伝播法         | コネクショニスト     |
|      | Brooks           | リカレントニューラルネットワーク  | 脳画像研究       |
|      |                  | 強化学習  | 計算論的アプローチ |
|第三次 2010-|                  | ディープラーニング    |                      |
|      |                  | 畳み込みニューラルネットワーク       |               |



## おおまかな人工知能の歴史

* 1952-1956: 人工知能の誕生 <!-- The birth of artificial intelligence -->  [【AI研究者 浅川伸一先生②】AI＝人工知能の定義｜資格スクエア大学・独学部 vol.358](https://www.youtube.com/wath?v=uIi9pA5oRZA)
* 1956–1974: 第一次 AI ブーム <!-- The golden years -->
* 1974–1980: 第一次 AI の冬 <!-- The first AI winter-->
* 1980–1987: 第二次 AI ブーム <!--のBoom -->
* 1987–1993: 第二次 AI の冬 <!-- Bust: the second AI winter -->  [【AI研究者 浅川伸一先生③】第1次・第2次AIブーム｜資格スクエア大学・独学部 vol.359](https://www.youtube.om/watch?v=3TYPKGKhT-A)
* 1993–2001: AI の進展
* 2000–現在: 第三次 AI ブーム 深層学習，ビッグデータ，汎用人工知能 <!--Deep learning, big data and artificialgeneral intelligence: --> [【AI研究者 浅川伸一先生④】現在の第3次AIブーム｜資格スクエア大学・独学部 vol.360](https://www.youtube.co/watch?v=cofokoZJsA8)
* 現状 [【AI研究者 浅川伸一先生⑥】日本におけるAI研究の現状｜資格スクエア大学・独学部 vol.362](https://www.youtue.com/watch?v=Cra4wIqYcsA)
    <!--- [【AI研究者 浅川伸一先生⑦】ディープラーニング協会とは?｜資格スクエア大学・独学部 vol.363](https://ww.youtube.com/watch?v=1i05qTGRMYI&t=2s)-->
* 近未来: 第四次産業革命
* AI 脅威論 [【AI研究者 浅川伸一先生⑤】AIは人類の脅威か?｜資格スクエア大学・独学部 vol.361](https://www.youtube.com/atch?v=H0d_OnOOomE)
* [【AI研究者 浅川伸一先生⑧】AIの将来を語る!｜資格スクエア大学・独学部 vol.364](https://www.youtube.com/wach?v=g0zoL--iuM4)


### キーワード:
- GOFAI, symbolic AI, Enbodiment, Computational approach, neuveu AI, data science, web ontology
- 記号処理的 AI (Russel Novig, 2003)
- 計算論的モデル (Marr, 1980)

## 人工知能

何をもって知的であるとするかは，知能の定義が定まっていない以上決めようがありません。
知能検査で測定された数値が知能であるとする考え方もあります。
ですが，その考え方では，知能検査で測ることができない知能は存在しないかのような錯覚を引き起こしてしまいます。
このとき，もしある考えに基づいた知能が本当の知能を説明し尽くしているのであれば，その考えに沿って実際に作ってみることができるだろうという考え方があります。
実際に作ってみることで，その考えが正しいかどうかを試すことができるからです。このような方法を **構成論的アプローチ** と呼びます。

ある考えが間違っているのであれば正しく動作しないでしょうし，正しければ正しく動作するはずです。
構成論的アプローチの良いところは，枝葉末節の細かい枝葉にこだわらず，全体が俯瞰(ふかん)できることです。
断片的な知識 を積み重ねても全体像が見えないことがあります。
これに対して構成論的アプローチは動作する全体像を考える必要があるので全体像を見失うことが少ないと言えるでしょう。
一方で，未だよく分かっていない部分は類推して作る必要があるので，全体として動作するにしても細部は実体とかけ離れている場合もありえます。
構成論的アプローチの対極にある方法を分析的アプローチと呼び心理実験，生理学的実験の多くがこのアプローチに該当します。
どちらのアプローチがだけが正しいというわけでなくどちらのアプローチも必要なのだと理解して良いでしょう。
従って人工知能の研究とは知的な振る舞いを実際に作って実証してみるという意味で知能の構成論的アプローチであり，人間知能を扱う心理学で明らかとなった知見をコンピュータやロボットの上に実現する研究でもあります。
この意味では人工知能と知能の心理学とは重複する領域であるということができます。

人工知能の反対の概念を自然知能 (natural intelligence) と呼びます。
人間を含めて生物の持つ知的な振る舞いを自然知能と呼び，自然 知能を模倣しようとする試みを人工知能と呼びます。
人工知能を実現するためには コンピュータが必要です。
ですから人工知能研究の歴史は最長でもコンピュータの 歴史と同じことになります。
以下に，コンピュータを人間のような振る舞まわせる ことを考えた最初の研究者であるアラン・チューリング (Alan Turing)の考えたことを紹介します。

### 知的であるとはどういうことか -チューリングテスト-

ある機械が知的な振る舞いをすることを測る方法に，チューリングテストがあります。
アラン・チューリングは「機械は考えることができるか？」という問いを考えました。

<center>
<img src="/2023assets/Turing_test.png" width="33%">
<!-- [チューリングテストの模式図](/2023assets/Turing_test.png){#fig:TuringTest style="widht:39%"} -->
</center>

60 年以上前にチューリングが考えたチューリングテストは，現在でも形を変えて用いられることがあります。
人工知能の歴史は，チューリングテストに合格するコンピュータプログラムを作る努力であるとも言えます。
チューリングテストを人工知能最古の歴史だとすると，人間の脳を模倣する試みや，脳の働きを数式として表現する研究，人間の替わりをする江戸時代のからくり人形を含めたロボットを考えれば，これらの方が人工知能の研究より歴史が古いと言えます。
人間の脳，ロボット，人工知能の 3 者を結びつけて考える研究者もいます。
ロボットが自立して動くためには周囲の状況を正しく認識する必要がありますが，そのためには物体認識や音声認識などをロボットが行える必要があるからです。
物体認識や音声認識は人工知能の一分野とも考えられますが，そのためには人間の物体認識や音声認識の仕組みを知る必要があります。
このように人間の脳，ロボット，人工知能は関連します。本書は知能の本ですので，ロボットの話題は軽く触れるだけ止めます。


## 機械学習 <!--とは何か-->

### 機械学習の定義 <!--Definitions of machine learning: old and new,-->

* アーサー・サミュエル Arthur Samuel (1959): 機械学習とは，明示的にプログラムで指示せずにコンピュータに学習させる能力を研究する分野である。<br/>
“field of study that gives computers the ability to learn without being explicitly programmed”
* トム・ミッチェル Tom Mitchell (1999): ある課題 T とその成績 P の評価からなる経験 E をとおして学習するコンピュータプログラムを機械学習という。<br/>
"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”

<center>
<img src="/2023assets/SAM.JPG" width="13%">
<img src="/2023assets/tom_mitchell.jpg" width="15%"><br/>

左: Arthr L. Samual(1901-1990) from <http://www.i-programmer.info/history/people/669-a-l-samuel-ai-and-games-pioneer.html> <!-- (/2023assets/SAM.JPG) {#fig:SAM style="width39%"}--><br/>
右: Tom Mitchell from <http://wamc.org/post/dr-tom-mitchell-carnegie-mellon-university-language-learning-computer> <!--/2023assets/tom_mitchell.jpg){#fig:tom style="width:39%"}-->
</center>


### 機械学習の分類 <!-- ### Categories of Machine Learning-->
<!-- source: *This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*

*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*-->

機械学習は，実用上 **教師あり学習 supervised learning** と **教師なし学習 unsupervised learning** の 2 種類に大別されます。
<!--At the most fundamental level, machine learning can be categorized into two main types: supervised learning and unsupervised learning.-->

* **教師あり学習** では，データの特徴量とラベルの関係を何らかの方法でモデル化します。
モデルが決まれば，未知のデータにラベルを適用することができます。
教師あり学習は，**分類 classification** と **回帰 regression** に分けられます。
* 分類 では，ラベル，すなわち教師信号は離散的なカテゴリです。典型的には，真か偽かの 2 値です。
あたえられたデータが，基準に当てはまるか，当てはまらないか，という問題です。
2 値分類より分類するグループが多い分類問題を，多クラス分類 multi-class classification と呼びます。
* 回帰 では，ラベルは連続的量 となります。データとラベルが，それぞれ一つづつ，一対一対応であれば，$x\mapsto y: y=f(x)$ 一般には，単回帰と呼ばれます。

<!-- **Supervised learning** involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data.
This is further subdivided into *classification* tasks and *regression* tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities.
We will see examples of both types of supervised learning in the following section. -->

* **教師なし学習 unsupervised Learning** は，ラベルを参照せずにデータセットの特徴をモデル化する手法を指します。
`データセットに語らせる` と表現されることもあります。
教師なし学習のモデルには **クラスタリング clustering** や **次元削減 dimension reduction** などが含まれます。
クラスタリングアルゴリズムは，データの異なるグループを識別し，次元削減アルゴリズムでは，データのより単純な表現を探し出します。

<!-- *Unsupervised learning* involves modeling the features of a dataset without reference to any label, and is often described as "letting the dataset speak for itself."
These models include tasks such as *clustering* and *dimensionality reduction.*
Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data.
We will see examples of both types of unsupervised learning in the following section. -->

さらに，教師付き学習と教師なし学習の中間に位置する，いわゆる **半教師付き学習 semi-supervised learning** と呼ばれる手法もあります。
半教師付き学習法は，不完全なラベルしか得られない場合に有効な場合があります。
さらに，完全なラベルが与えられるデータであっても，半教師あり学習の手法を用いて訓練する場合もあります。
<!--In addition, there are so-called *semi-supervised learning* methods, which falls somewhere between supervised learning and unsupervised learning.
Semi-supervised learning methods are often useful when only incomplete labels are available.-->

<!-- To make these ideas more concrete, let's take a look at a few very simple examples of a machine learning task.
These examples are meant to give an intuitive, non-quantitative overview of the types of machine learning tasks we will be looking at in this chapter.
In later sections, we will go into more depth regarding the particular models and how they are used.
For a preview of these more technical aspects, you can find the Python source that generates the following figures in the [Appendix: Figure Code](06.00-Figure-Code.ipynb).-->

<center>

<img src="/assets/sklearn_map.svg" width="66%"><br/>
<!-- ![http://scikit-learn.org/stable/tutorial/machine_learning_map/](/assets/sklearn_map.svg){#fig:sklearn_map style="width:79%"}<br/> -->
出典: <http://scikit-learn.org/stable/tutorial/machine_learning_map/>
</center>

### 分類，相違，区分 SAS による

<center>
<img src="/2023assets/data-mining-Venn-diagram.png" width="49%"><br/>
<!-- ![SAS](./assets/data-mining-Venn-diagram.png){#fig:sas style="width:74%"} -->

出典:[https://blogs.sas.com/content/subconsciousmusings/2014/08/22/looking-backwards-looking-forwards-sas-data-mining-and-machine-learning/#prettyPhoto](https://blogs.sas.com/content/subconsciousmusings/2014/08/22/looking-backwards-looking-forwards-sas-data-mining-and-machine-learning/#prettyPhoto)
</center>


## 実習環境 google colablatory

この授業での実習環境について簡単に説明します

- [Google colaboratory](https://colab.research.google.com){:target="_blank"} とはブラウザ上 [jupyter notebook](https://jupyter.org/) ([jupyter lab](https://jupyterlab.readthedocs.io/en/stable/)) を実行するクラウド計算環境
- ブラウザ (Chrome 推奨) さえあれば，特別なインストール作業を必要ありません。
    - クラウドベースの実行系ですので，インストールの手間は不要です。
- [Jupyter](https://jupyter.org/){:target="_blank"} とは:
    - [ipython](https://ipython.org/notebook.html) をブラウザ上で実行する環境です。
    - 木星を表す ジュピター jupiter とは綴りが異なります。ですが由来は 木星 から来て言います。実際には，Julia, Python, R が実行可能な環境であることから命名。

- ipython とは [python](https://pytorch.org/) をインタラクティブに実行する環境です。
- [Python](https://www.python.org/){:target="_blank"} とは:
    * Python について詳しく知る必要はありません。
    * 機械学習やニューラルネットワークのコミュニティで使われるコンピュータ言語
    - Colab の使い方は jupyeter notebook をご存知であれば，ほぼ同じです。
    * AI や 機械学習 分野の共同体で使われることが多いコンピュータ言語のことです。下記に示すように高等学校の情報で採択されます。

    * [StackOverFlow におけるコンピュータ言語のトレンド](https://insights.stackoverflow.com/trends?tags=r%2Cpython%2Cjavascript%2Cjava%2Cc%2B%2B%2Cc%23){:target="_blank"}
    <!-- * [Stackoverflow の言語トレンド](https://insights.stackoverflow.com/trends?tags=python%2Cjavascript%2Cjava%2Cc%23%2Cphp%2Cc%2B%2B){:target="_blank"} -->
    * [TensorFlow と PyTorch の人気比較](http://horace.io/pytorch-vs-tensorflow/){:target="_blank"}

    <img src="https://thegradient.pub/content/images/size/w1600/2019/10/pasted-image-0-1.png" width="49%">

    * [Google トレンド](https://trends.google.com/trends/explore?q=pytorch,keras,tensorflow){:target="_blank"}

<!-- - [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb){:target="_blank"} って何？
    - Jupyter notebook をクラウド上で実行する環境です-->


### Python の特徴や注意点

1. 字下げによるブロック化
2. Jupyter notebook のセルには ２ 種類。
    1. `コードセル`: コード実行用
    2. `マークダウンセル`: 注釈，コメント用。mathjax による数式表記。マークダウン表記は R でも使われます。
3. 各セルは，コントロールキーを押しながらエンターキー (リターンキー) を押すか，または，セル左上の 黒地に白の三角形アイコンをクリックして実行。
3. `import` 文によるライブラリの輸入。機能拡張
4. `#` 以降はコメントとして扱われる
5. サンプルコードは自身の Google ドライブにコピーしてから実行。

* [高等学校 「情報I」研修教員教材<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0414high_school_information1_text.ipynb)
* [HAD と同じデータを機械学習で。ランダムフォレスト篇 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0417random_forest_HAD_iris.ipynb)
* [HAD と同じデータをニューラルネットワークで。ロジスティック回帰とパーセプトロン篇 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0422first_neural_networks.ipynb)

<!-- - [本日の課題 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0423_homework.ipynb) -->
<!-- ###  `TensorFlow` と `PyTorch` の関係と，授業ではなぜ `PyTorch` を用いるのかの理由 -->

## 実習

- [用語集](glossary)

* [文部科学省 高等学校情報科「情報Ⅰ」教員研修用教材（本編）](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/1416756.htm){:target="_blnak"}
    * [上サイト第 3 章コンピュータとプログラミング](https://www.mext.go.jp/component/a_menu/education/micro_detail/__icsFiles/afieldfile/2019/10/09/1416758_005.pdf){:target="_blank"} Python でサンプルコードが書かれている
    * [はじめての Colab 実習 HAD と同じデータを使う <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0422first_neural_networks.ipynb){:target="_blank"}

* [AI for everyone](https://www.jdla.org/certificate/everyone/) 全６回のビデオ講座。無料。
* [ニューラルネットワークで遊んでみよう](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"}
* [GLIDE <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/text2im.ipynb)

* [実習 PyTorch CNN <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_
0416PyTorch_CNN_demo.ipynb){:target="_blank"}

---

[心理実験で真実が分かるのか？ (Newell, 1973)](/2022/1973Newell_ja/)

<!--
## データサイエンス小史

- <https://enterprisetechnologyconsultant.wordpress.com/2013/03/10/data-science-and-the-definition-and-role-of
-a-data-scientist/>

- ウィリアム・クリーブランドの「データサイエンス:統計学の技術領野を拡張するための行動計画」が公刊されて以来データサイエンスという言葉が普及した。(W. S. Cleveland. Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics. ISI Review, 69:21–26, 2001.)

- <https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#123e7cc355cf> Gil Press, CONTRIBUTOR I write about technology, entrepreneurs and innovation.

* 1962 年，ジョン・チューキー (John W. Tukey) は「データ解析の未来 “The Future of Data Analysis”」を公刊した (1962 John W. Tukey writes in “The Future of Data Analysis”:)

そのイントロより

私は長い間、自分は統計学者であり、特殊なものから一般的なものへの推論に興味があると思っていました。
しかし、数理統計学の進化を見るにつけ、不思議に思い、疑問を抱くようになりました。
時系列のスペクトル解析のような手法がなぜこれほど有用なのか考えてみると、「ゆらぎに対処する」という側面は、ゆらぎがもはや問題ではない、より単純なケースである非常に広範なデータを効果的に扱うために必要だった側面よりも、多くの状況において重要性が低いことが明らかになった。
つまり、データを分析する手順、その結果を解釈する技術、分析をより簡単に、より正確にするためのデータ収集の計画方法、データ分析に適用される（数学的）統計のすべての機械と結果などである、と私は考えています。
<!-- For a long time I have thought I was a statistician, interestedin inferences from the particular to the general.
But as I have watched mathematicalstatistics evolve, I have had cause to wonder and to doubt.
And when Ihave pondered about why such techniques as the spectrum analysis of time series have proved so useful, it has become clear that their "dealing with fluctuations” aspects are, in many circumstances, of lesser importance than theaspects that would already have been required to deal effectively with the simpler case of very extensive data, where fluctuations would no longer be aproblem.
All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data,techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. -->

<!-- データ分析の大部分は，標本から集団への意味での推論であるが，これらは部分的なものであり，全体的なものではない。
データ解析の大部分は，生データを単純に直接調べるだけではわからない兆候を明らかにするものであるが，これも部分的なものであり，全体的なものではない。
データ解析のある部分は，この用語がその言語学的な範囲を超えて拡張されているように，観察，実験，または解析における努力の配分やその他の貴重な考察を導くという意味で，配分的である。
データ解析は，推論や切実な手続き，あるいは配分よりも，より大きく，より多様な分野である。 -->
<!-- Large parts of data analysis are inferential in the sample-to-population sense, but these are only parts, not the whole.
Large parts of data analysis are incisive, laying bare indications which we could not perceive by simple and direct examination of the raw data, but these too are only parts, not the whole.
Some parts of data analysis, as the term is here stretched beyond its philology, are allocation, in the sense that they guide us in the distribution of effort and other valuable considerations in observation, experimentation, or analysis.
Data analysis is a larger and more varied field than inference, or incisive procedures, or allocation. -->

<!--
- 1974 Peter Naur publishes ``Concise Survey of Computer Methods'' in Sweden and the United States.
- 1977 The International Association for Statistical Computing (IASC) is established as a Section of the ISI.
“It is the mission of the IASC to link traditional statistical methodology, modern computer technology, and th
e knowledge of domain experts in order to convert data into information and knowledge.”
- 1994 BusinessWeek publishes a cover story on “Database Marketing”:
- 1996 Members of the International Federation of Classification Societies (IFCS) meet in Kobe, Japan, for the
ir biennial conference. For the first time, the term “data science” is included in the title of the conference
- 1996 Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth publish “From Data Mining to Knowledge Disc
overy in Databases.”
- 2001 William S. Cleveland publishes “Data Science: An Action Plan for Expanding the Technical Areas of the F
ield of Statistics.”
-->

<!-- - CNN: 畳み込みニューラルネットワーク
- RNN: リカレントニューラルネットワーク
- RL: 強化学習-->

<!-- <center> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2008Fuster_Prefrontal_Cortex_fig8_4.svg" width="39%"> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2015Ronneberger_U-Net_Fig1_ja.svg" width="48%"> -->
<!-- </center> -->

<!-- - [2018Kriegeskorte](2018Kriegeskorte){:target="_blank"}
- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
- [G 検定](https://www.seshop.com/product/detail/23864?utm_source=seid_it_spot_20210412&utm_medium=email&utm_campaign=coupon){:target="_blank"}

### 2021年02月23日分
- [2020-0215](2020-0215abstract){:target="_blank"}
- [どうぶつの森モデル，動物の名前連想モデル](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0223word_associtaion.ipynb){:target="_blank"}
- [導入講義用 CCP ウィルス感染者予測モデルを題材に](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}
- [CNN の簡単なデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Keras_CNN_demo_with_wordnet_ja.ipynb){:target="_blank"}

# 統計学と機械学習の関係

母集団における差異の有無を問題にする心理統計学と機械学習との間には，決定的な差があります。

- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
-->

<!--
<br/>
1. [tSNE を用いた TLPA 200語の word2vec 視覚化](https://ShinAsakawa.github.io/2020cnps_tSNE_for_word2vec.ipynb)
2. [2020年2月24日資料1 tlpa 画像](https://ShinAsakawa.github.io/2020making_tlpa.html)
3. [2020年4月15日かじゅまるつがる松本先生のモデルの説明](https://shinasakawa.github.io/2020gajumarutugaru/2020-0415Friston_in_detail.html)
4. [2020年4月18日かじゅまるつがる投稿](https://shinasakawa.github.io/2020gajumarutugaru/2020-0418gajumarutugaru.html)

<br/>

1. [2020ccap 資料置き場](2020ccap)
2. [2020中央大学，緑川先生，重宗先生，研究会資料](2020chuo)
3. [2020 第2回 中央大学，緑川先生，重宗先生，研究会資料](2020chuo2)
4. [2020サイトビジット資料](2020sightvisit)

<a href="https://guides.github.com/features/pages/">Read this page to write this page.</a>-->
