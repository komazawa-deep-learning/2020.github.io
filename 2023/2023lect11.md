---
title: 第11回
author: 浅川 伸一
layout: default
---
<!-- コードはちゃんと書かないといかんな。 -->
<!-- <link href="/css/asamarkdown.css" rel="stylesheet"> -->

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 23/Jun/2023<br/>
Appache 2.0 license<br/>
</div>

* [PPO <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0623stable_baselines3_demo.ipynb)


## 本日の課題

- [LeCun, Bengio, Hinton (2015) ディープラーニング, Nature](2015LeCun_Nature_ja.pdf)

課題は，上記 ファイルを読んで，文章中の **太字** になっている単語の意味を書いてください。
分量は一行程度です。
調べる必要はありません。
自分なりの意味を類推して，おそらく，こういう意味だろうという自分の解釈を書いてください。
従って，その説明文が，正しいか誤っているかは，問題でありません。
あくまで，自分がどのように解釈したかを書いてください。

## イメージネットコンテストの結果

<!-- ## 本日のキーワード -->

# ニューラルネットワークの歴史

<center>
<img src='/assets/imagenet_result2017.png' style='width:74%'><br/>
画像認識の進歩
</center>

## 第 1 次ニューロブーム

### 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案
(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='/assets//mcculloch.jpg' style="width:38%">
<img src='/assets//pitts.jpg' style='width:50%'><br>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br> -->
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_{i}=\phi\left(\sum_jw_{ij}x_j\right),\tag{eq:formal_neuron}
$$

ここで $y_{i}$ は $i$ 番目のニューロンの出力，$x_{j}$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$ と $j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br> -->
形式ニューロン
</center>

---

## ローゼンブラット Rosenblatt のパーセプトロン

<center>
<img src='/assets/rosenblatt.jpg' style="width:49%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//rosenblatt.jpg' style="width:49%"><br> -->
フランク・ローゼンブラット
</center>

<!--
$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

<center>
<img src='/assets/perceptron.png' style="width:74%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//perceptron.png' style="width:74%"></br> -->
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center>


<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br> -->
ニューロンの模式図 wikipedia より
</center>

<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->

<!--
## パーセプトロンの学習

$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
パーセプトロン perceptron は 3 層の階層型ネットワークでそれぞれ
S(sensory layer), A(associative layer), R(response layer) と呼ぶ。
$S\rightarrow A \rightarrow R$ のうち パーセプトロンの本質的な部分は
$A\rightarrow R$ の間の学習にある。

入力パターンに $P^+$ と $P^-$ とがある。
パーセプトロンは $P^+$ が入力されたとき $1$, $P^-$ のとき $0$ を出力する
機械である。
出力層($R$) の $i$ 番目のニューロンへの入力(膜電位の変化) $u_i$は
\begin{equation}
 u_i = \sum_j w_{ij}x_j - \theta_i = \left(w\right)_i\cdot\left(x\right)_i-\theta_i.\label{eq1}
\end{equation}
ここで中間層($A$)の $j$ 番目のニューロンの出力 $y_i$とこのニューロンとの
結合係数を$w_{ij}$、しきい値を$\theta_i$ とした。
このニューロンの出力$y_i$(活動電位、スパイク)は、

\begin{equation}
y_i = \lceil u_i\rceil
\qquad\left\{
\begin{array}{ll}
 1 & \mbox{if $u_i \ge 0$,}\\
 0 & \mbox{otherwize}
\end{array} \right.
\end{equation}

と表される。
-->

<!--
式(\ref{eq1})の意味を理解するために以下の図を参照

%
\footnote{
Minsky and Papert はパーセプトロンのベクトル表示について
悲観的な考え方を持っているようですが、ここでは理解のしやすさを
優先します。}%
$$
\mathbf{w}\rightarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

---

- 1960 年，ミンスキーとパパートの批判
- 第一次氷河期の到来

---

## 第 2 次ニューロブーム
- 1986 年，PDP ブック，俗に言うバイブル，発表
- 1989 年，バプニック，サポートベクターマシン発表
- 第二次氷河期の到来


---

## 第 3 次ニューロブーム

<center>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>
<!-- <iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br> -->
</center>


<center>
<img src='/assets/2015Mnih_DQNFig.png' style='width:84%'><br/>
DQN の結果
</center>


- 2016 アルファ碁がイ・セドルを破る

<center>
<img src="/assets/2016AlphaGo_Fig1a.svg" style="width:84%"><br/>
アルファ碁 Natureより
</center>


### ボストン・ダイナミクス社ビデオ
<!--<iframe width="200" height="120" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<center>
<iframe width="480" height="300" src="https://www.youtube.com/embed/rVlhMGQgDkY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="480" height="300" src="https://www.youtube.com/embed/tf7IEVTDjng" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<div align="center">
<iframe width="480" height="300" src="https://www.youtube.com/embed/8vIT2da6N_o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
source: <https://www.youtube.com/embed/8vIT2da6N_o>
</div>

<div align="center">
<iframe width="480" height="300" src="https://www.youtube.com/embed/fRj34o4hN4I" <!--https://www.youtube.com/embed/WcbGRBPkrps"--> frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
source: <https://www.youtube.com/embed/fRj34o4hN4I>
</div>

<!-- <center>
<iframe width="635" height="358" src="../assets/A Style-Based Generator Architecture for Generative Adversarial Networks.mp4"  frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center> -->

<div align="center">
<!--<iframe width="805" height="453" src="https://www.youtube.com/embed/WcbGRBPkrps" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<iframe width="480" height="300" src="https://www.youtube.com/embed/WcbGRBPkrps" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

---

# CNN の特徴

- 次の 7 つを上げることができます
<!--[@2017Asakawa_deep_jps][^2017Aakawa\_deep\_jps\]-->。

1.  非線形活性化関数 (nonlinear activation functions)
2.  畳込み演算 (convolutional operation)
3.  プーリング処理 (pooling)
4.  データ拡張 (data augmentation)
5.  バッチ正規化 (batch normalization)
6.  ショートカット(shortcut)
7.  GPU の使用

<!-- 上記 7 つの特徴を説明するのは専門的になりすぎるので省略します。
一つだけ説明するとすれば最後のGPU とは高解像度でしかも処理速度を必要とするパソコンゲームで用いられるグラフィックボードのこと
です。
詳細な画像を高速に画面に表示する必要から開発されたグラフィックボードですが，大規模なニュールネットワークの計算でも用いられる
数学が同じです。
そのため，ゲーム用に開発されたグラフィックボードがニューラルネットワークにも用いられるようになりました。
-->

## ディープラーニングの特徴

from Democratize AI slides

- データハングリー data hungry
- 計算資源ハングリー resource hungry
- 理論欠如 theory lagging
- 不透明 opacity

- ニューラルネットワークは素人の統計学である, Anderson et. al (1993)

... But Neural networks are not alchemy.

## 生理学

<center>
<img src='/assets/1994vanEssen_gallant_Fig2.jpg' style='width:49%'>
<img src='/assets/1991Felleman_VanEssen_Fig4.svg' style='width:49%'><br/>

出典 左図 Van Essen & Gallant (1994)
右図  Felleman & Van Essen (1992)
</center>

<center>
<img src='/assets/thorpe_sj_01_260.jpg' style='width:44%'><br/>

Thorpe et al.(2001)
</center>

<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d2/Retinotopic_English.jpg" style="width:66%"><br/>

**出典: https://en.wikipedia.org/wiki/Retinotopy**
</center>


### ヒューベルとウィーゼル Hubel and Wiesel (1969)
<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>

出典: https://youtu.be/4nwpU7GFYe8
</center>

- トポグラフックマッピング topographic mappings (地形図):  網膜や皮膚などの体制感覚，筋肉系のような効果器系を、中枢神経系の
1 つ以上の構造物に整然と投影した地図。
地形図は、すべての感覚系と多くの運動系で観察される。
- トノトピー tonotopy（ギリシャ語のtono=周波数、topos=場所）とは、異なる周波数の音が脳内で処理される場所の空間的配置のこと。
周波数が近い音は、脳内の場所的に隣接する領域で表現される。トノトピック地図とも呼ばれる <!--は 視覚系のレチノトピーに似た地形
的な組織の特殊な例である。-->
- ソマトピー somatopy: 中枢神経系上の特定の点へ身体領域の照射，投影のこと。 身体領域は 第一次体性感覚皮質 (後腹回) に投影さ
れる。 典型的には 特定の身体部位と そのそれぞれの位置を ホムンクルス homunclus (小人)
 上に配置する 感覚ホムンクルスとして表現される。
 細かく制御されている部位 (指，舌) は体性感覚野の面積が大きい。一方 粗く制御されている部位 (体幹) は面積が小さい。内臓のよう
な領域は体性感覚野の位置を持たない。
- レティノトピー retinotopy: 網膜からの視覚入力を神経細胞 にマッピングすること。哺乳類の脳に多く見られる。
この地図の大きさ、数、空間的配置は種間で異なる。
視野の隣接する点 が 同じ領域 の 隣接する 領域で 表現される とは限らないという意味で、複雑な地図となる。
例えば 第 2 次視覚野 (V2) での視覚地図は 視野の上半分に応答する網膜の部分が 視野の下半分に応答する部分から分離されて表現され
ている。
第３次視覚野 (V3) や 第4次視覚野 (V4) では下位の視覚野に比べて より複雑な表現がなされている


---

## ブレイクモア と クーパー Blackmore and Cooper (1970)

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--<iframe width="845" height="676" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>--><br/>

source: https://youtu.be/QzkMo45pcUo
</center>

<center>
<img src='/assets/1970BlackmoreCooper_Fig1.svg' style='width:39%'>
<img src='/assets/1970BlackmoreCooper_Fig2.svg' style='width:49%'><br/>
</center>

---

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br/>source: https://youtu.be/RSNofraG8ZE
</center>


# 心理学
## セルフリッジ Selfridge のパンデモニウム pandemonium モデル

<center>
<img src='/assets/1958Selfridge_fig3.svg' style='width:37%'>
<img src='/assets/1958Selfridge_fig7.svg' style='width:38 %'><br/>

出典: Selfridge (1958) "Mechanisation of Thought Processes" より<br/>
</center>


<!-- <center>
<img src='/assets/1958Selfridge_fig4.svg' style='width:49%'><br/>
<img src='/assets/1958Selfridge_fig5.svg' style='width:74%'><br/>
<img src='/assets/1958Selfridge_fig6.svg' style='width:49%'><br/>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center>
-->

# 生理学，視覚心理学との対応

- Julesz(1981) Textons, the elements of texture perception, and their interactions, Nature

<center>
<img src="/assets/1981Julesz-texton-Fig2.svg" style="width:48%"></br>
Julesz (1981) Fig. 2 より
</center>


## 生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン)

- 層間の結合の仕方, アーキテクチャ
- forward/backward 役割，機能，実現方法
- 側抑制 lateral inhibition (これについては多層化して回避できる可能性あり)
- shape from X は正しかったのか？ ただし発達心理学におけるシェイプバアスは言語発達において重要な意味を持つはず
。だからと言って乳幼児はそのように強制(脅迫？)，矯正されて育つわけではないだろう。

  - Ritter (2017) Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study
  - Landau, Smith, Jones (1992) Syntactic Context and the Shape Bias in Childrens and Adults Lexical Learning
  - Yamins (2016) Using goal-driven deep learning models to understand sensory cortex

  - Julez のアプローチは視覚研究者 Haar, SIFT, DoG などのアルゴリズム開発者と対応
  - Poggio (1985) Computational Vision and Regularization Theory


## 現代的ニューラルネットワークモデルと生理学との対応

<center>
<img src='/assets/2012Zeiler_Deconvolution.png' style='width:47%'><br/>

出典: Zeiller (2012)
</center>

<center>
<img src="/assets/2010ZeilerKrishnanTaylorFerguss_fig.svg" style="width:66%"><br/>

Zeiler et. al. (2010) Fig. 7, and 8
</center>


<center>
<img src='/assets/2016Yamins_fig1s_ja.svg' style='width:77%'>
<!--<img src='../assets/2016Yamins_Fig1.svg' style='width:94%'>-->
<img src='/assets/2016Yamins_fig2ja.svg' style='width:77%'><br/>

出典: Yamins (2016) Fig.1，Fig. 2
</center>


<center>
<img src="/assets/2016Yamins_fig3s_ja.svg" width="48%"><br/>
<div style="text-align:left; width:77%;background-color:conrnsilk">

目標駆動型モデリングの構成要素。
内側の円は、与えられた数のレイヤーのモデルを含む完全なモデルクラスの部分空間を表す。
目標駆動モデルは、特に最適なモデルを発見するために、モデル・クラス内の軌道（実線）に沿ってシステムを駆動する学習アルゴリズム（黒の点線矢印）を使用して構築される。
各ゴールは、そのゴールを解くのに特に適したパラメータを含むモデルクラス（太い黒の等高線）内の引き寄せの盆地に対応していると考えることができる。
計算結果は、課題がモデルのパラメータ設定に強い制約を与えることを示す。
すなわち、与えられた課題の最適化パラメータのセットは、元の空間に比べて非常に小さいということである。
これらの目標駆動モデルは、与えられた課題領域での行動の根底にあると考えられている脳領域のニューロンの応答特性をどの程度予測できるかについて評価することができる。
例えば、単語認識のために最適化されたモデルのユニット、聴覚皮質の一次領域、ベルト領域、パラベルト領域の応答特性と比較することができる。
また、モデルを互いに比較して、異なるタイプの課題がどの程度の神経構造の共有につながるかを判断することもできる。
また、様々な構成要素のルール（教師あり、教師なし、半教師あり）を研究して、それらが生後の発達や専門知識学習の間にどのように異なるダイナミクスにつながるかを判断することもできる（緑の破線のパス）。
</div>
</center>


<center>
<!-- <img src='/assets/2014Yamins_fig2Aja.svg' style='width:74%'> -->
<!--<img src='../assets/2014Yamins_FigS2.svg' style='width:74%'>-->
<img src="/assets/2020Schrimpf_fig1.svg" style="width:84%"><br/>

出典: Schrimpf (2020) Fig. 1
</center>

### ニューラル インプラント (neural implant), あるいは，義神経回路

* 人工内耳のように単に脳の活動を刺激するだけのものとは異なり、シリコンチップ製のインプラントは、損傷した脳の一部と同じプロセスを実行します。
* この人工器官は ラットの脳の組織でテストされました。次第に，人間に近い動物でのテストがなされています。
* 問題がなければ 脳卒中やてんかん，アルツハイマー病などで脳に損傷を受けた人の補助器具として実験が行われるでしょう。
* ですが，脳を模倣したデバイスは **倫理的な問題** を引き起こします。困っている人を助ける器具としては，義肢は有効な道具でしょう。しかし，その延長線上に義脳を考えることには，慎重な議論が必要だと考えます。

<center>
<img src="/2022assets/dn3488-1_602.jpg" width="44%">
<!-- <img src="http://www.newscientist.com/wp-content/uploads/2003/03/dn3488-1_602.jpg" width="44%"> -->
<img src="/assets/2001Berger_fig1.jpg" width="44%"><br/>
<div style="text-align:left;width:88%;background-color:cornsilk">

図 1.
左図: ラット脳 (左下)，左脳の海馬形成の相対的位置 (白)，脳から分離後の左海馬の図解(中央)，海馬の縦軸に横切る断面のスライス。

右図: 内嗅皮質 (ENTO) からの線維は穿通路 (pp) を通って歯状回 (DG)へ，歯状回の顆粒細胞は CA3 領域へ，さらに CA1 領域へ投射，CA1 細胞は小柱 (SUB)へ，そして無傷脳では内嗅皮質へ投射される。
スライス標本を作製すると CA1 と小室からの帰還結合が切断され，海馬ニューロンの実験的研究のための開ループ状態が作り出される。
<!-- Fig. 1. Left panel: Diagrammatic representation of the rat brain (lower left), showing the relative location of the hippocampal formation on the left side of the brain (white); diagrammatic representation of the left hippocampus after isolation from the brain (center), and slices of the hippocampus for sections transverse to the longitudinal axis.
Right panel: Diagrammatic representation of one transverse slice of hippocampus, illustrating its intrinsic organization: fibers from the entorhinal cortex (ENTO) project through perforant path (pp) to the dentate gyrus (DG); granule cells of the dentate gyrus project to the CA3 region, which in turn projects to the CA1 region; CA1 cells project to the subiculum (SUB), which in the intact brain then projects back to the entorhinal cortex. In a slice preparation, return connections from CA1 and the subiculum are transected, creating an open-loop condition for experimental study of hippocampal neurons. -->
</div>
</center>

<center>
<img src="/assets/2001Berger_fig3upper.jpg" width="88%"><br/>
<img src="/assets/2001Berger_fig3lower.jpg" width="44%">
<img src="/assets/2001Berger_fig13.jpg" width="33%"><br/>

<div style="text-align:left; width:66%; background-color:cornsilk">

図 3 (a) 従来の人工ニューラルネットワークの処理要素の性質と，生物学的にリアルな Dynamic Synapse ニューラルネットワークの処理要素の性質。
(b）Dynamic Synapse ニューラルネットワークによる話者非依存型単語認識識別の概念的表現。
ネットワークへの入力は，同じ単語に対する異なる話者のデジタル音声波形であり，話者の発声の違いにより，類似性が低い（相互相関が低い）。
2 つのネットワークは，2 つの異なる訓練またはテスト試行における同じネットワークを表すためのものであり，実際の場合では，1 つのネットワークが両方 (またはそれ以上) の音声波形で訓練されます。
このとき，各音声波形は 第 1 層にある 5 つの入力ユニットすべての入力を構成する。
このネットワークの第 1 層の各ユニットは，音声波形の異なるパルストレイン符号化を生成する (異なるパラメータ値を持つ「積分発火ニューロン」）。
各シナプス (矢印) の第 2 層への出力は 4 つの動的過程［( a) 参照］に支配され，そのうちの 2 つの過程は 2 次非線形性を表す。
したがって，第 2 層のニューロンへの出力は以前の入力事象からの時間に依存する。
「動的学習規則」は，異なる入力音声信号に応答して出力ニューロンが共通の時間パターンに収束するまで (すなわち出力パターン間の高い相互相関)，各動的処理過程の相対的寄与を変更する。
<!-- Fig. 3. (a) Properties of a processing element of a traditional artificial neural network versus properties of a processing element of a biologically realistic Dynamic Synapse neural network.
(b) Conceptual representation of speaker-independent word recognition identification by a Dynamic Synapse neural network.
Inputs to the network are digitized speech waveforms from different speakers for the same word, which have little similarity (low cross-correlation) because of differences in speaker vocalization.
The two networks shown are intended to represent the same network on two different training or testing trials; in a real case, one network is trained with both (or more) speech waveforms.
On any given trial, each speech waveform constitutes the input for all five of the input units shown in the first layer.
Each unit in the first layer of the network generates a different pulse-train encoding of the speech waveform (“integrate and fire neurons” with different parameter values).
The output of each synapse (arrows) to the second layer of the network is governed by four dynamic processes [see (a)], with two of those processes representing second-order nonlinearities; thus, the output to the second layer neurons depends on the time since prior input events.
A “dynamic learning rule” modifies the relative contribution of each dynamic process until the output neurons converge on a common temporal pattern in response to different input speech signals (i.e., high cross-correlation between the output patterns).
-->

図 13 失われた高次皮質脳領域の認知機能を代替するための埋め込み型神経補綴の概念図。
ここでは，海馬の一部を補綴する具体的なケースを想定して，その概念を示している。
この義補系の 2 つの重要な構成要素は，機能不全または失われた海馬の領域の計算機能を実行する「ニューロコンピューティング」マルチチップモジュールと，ニューロコンピューティングマイクロチップが無傷の脳からの入力を受け取り，無傷の脳に出力を送るためのニューロン-シリコンインターフェースとして機能する「ニューロモーフィック」マルチサイト電極アレイである。
<!-- Fig. 13 Conceptual representation of an implantable neural prosthetic for replacing lost cognitive function of higher cortical brain regions.
The concept is illustrated here in the context of the specific case of a prosthetic substituting for a portion of the hippocampus.
The two essential components of the prosthetic system are a “neurocomputational” multichip module that performs the computational functions of the dysfunctional or lost region of hippocampus, and a “neuromorphic” multisite electrode array that acts as a neuron-silicon interface to allow the neurocomputational microchips to both receive input from, and send output to, the intact brain. -->

出典: Berger (2001) 図 3, 図 13
</div></center>


---

## 機械学習

### 機械学習の超簡単実習

今回取り上げる話題は以下のとおりです:


- 重回帰，逆行列，線形代数
- 主成分分析，固有値，変分法，
- tSNE
- ロジスティック回帰
- サポートベクトルマシン SVM


- [初めての画像認識 <img src="/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_ResNet50_demo.ipynb){:target="_blank"}
- [機械学習の超簡単デモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0507_3mnists_demo.ipynb){:target="_blank"}



### 1.1. <a name="mnist">3 つのデータセット: MNIST, Fashion MNIST, KMNIST</a>
- 機械学習分野で頻用されるデータセットとして，手書き数字認識データである MNIST があります。
- MNIST は FAIR (フェイスブック人工知能研究所) 現所長 の Yan LeCun によって作成されました。
NIST とは，アメリカ合衆国版の JIS です。すなわち，標準化機関の手書き数字認識用データセットを 修正した (modified) という意味から MNIST と呼ばれます。
- MNIST は データ数が ７万で，訓練データ数 6 万，テストデータ １ 万からなります。
データは，縦横それぞれ 28 画素からなっています。コンピュータで扱う際に，コンピュータにとってキリの良い 32 画素ではなく，
周囲を切り取ったために，28 画素になっています。
- Fasshion MNIST は， MNIST と同じ画像形式で，ファッション画像，具体的には 10 種類のアパレル画像データです。
- kmnist は日本語のくずし字データセットです。形式は MNIST, Fashion MNIST と同じです。

### 1.2. <a name="dataset">訓練データ，テストデータ，検証データ</a>
* 機械学習では，心理統計で用いられるような 仮説検定を行うこともありますが，むしろ，行わない場合も多いです。
* 理由としては，仮説検定を行うことによりも，モデルの性能を向上させることに主眼があるからという意味合いであろうと考えられます。
* ですが，考え方は母集団統計量の推定と同じような発想をします。すなわち，まだ見ぬ未知のデータに対して精度が良いモデルが優れているモデルと判断されます。
* 訓練データを使ってモデルを作成し，作成したモデルの評価をテストデータを使って評価します。
* このとき，テストデータは訓練には使いません。未知のデータに対しての精度でモデルの性能の優劣を競います。従って，モデルの精度の良いモデルが良いモデルであり，かつ，良いモデルとは，未知のデータに対してより精度が高く動作するモデルとなります。
* この点については，母集団の統計量の優劣を考える心理統計とは異なります。
* 真の母集団という，ありもしない曖昧 (かも知れない) 仮想集団について斟酌するよりも，実際のデータについて精度の優劣でモデルの性能を競うという意味では，実務的な発想と言えるでしょう。
* 機械学習におけるモデルの精度向上を目指したパラメータチューニングのことを **学習** と呼びます。

### 1.3. <a name="overfitting">過学習</a>

* モデルのパラメータを学習するときに，同じデータを用いて性能を検証することは，方法論的に間違っていると言えます。
* すでに見たことのある敵をたおせても，真の勇者とは言えません。何度でも生き返ることができる RPG とは違います。
* 見たことのあるデータ （遭遇した経験のあるモンスター）は倒せるでしょう。ですが，それでは 勇者 ではなく チキン です。
* 経験済のデータについては，完璧なスコアを示すことができるでしょう。ですが，まだ見ぬデータに対して有用な予測をすることはできません。
* このような状況を 過学習 (over-learning) あるいは オーバーフィッティング (over-fitting) といいます。
* これを避けるために、（教師あり）機械学習を行う際には，利用可能なデータの一部を テストデータセット `X_test`, `y_test` として用意しておくのが一般的です。
* 一般に k-hold out 法などと呼ばれる手法は，訓練データセットを ｋ 個に分割します。その上で，k 個に分割した 1 つのデータ群を除いた k-1 群の訓練データを用いてモデルの学習を行います。学習の都度，残しておいたデータを用いて性能を評価します。
* この方法により，最終評価に用いるテストデータを使うこと無くチューニングを行います。
* **なぜ全データを用いないで，データを分割するのか？**
  * 未知の母集団を仮定しないで，モデルの優劣を正当に評価するための方法であるとみなすことができます。

### 1.4. <a name="回帰と分類">回帰と分類</a>
* 機械学習で頻用される手法の分類に **回帰** と **分類** があります。
* 予測すべきデータが連続量の場合は，回帰
* 予測すべきデータが離散量の場合は，分類 と呼ばれます。
* 身長や体重，あるいは，明日の東京都における COVID-19 の感染者数を予測するのであれば 回帰 です。
* 一方，手書き数字認識は，予測すべきデータが 10 分類された各クラスですので 分類 と呼ばれます。
* $\mathbf{y} = \mathbf{Xw} +\mathbf{b}$ などは 線形回帰 と呼ばれます。これは中学校以来の 直線を表す 1次方程式 $y=ax+b$ と同じ形をしています。
* $y$ を予測すべき量，$x$ を与えられたデータと考えます。
* 傾き slope:$a$ と 切片 intercept:$b$ とを推定する問題が 回帰 です。
* 中学校までの数学の知識では，2 点 $(x_1, y_1)$, $(x_2, y_2)$ が与えられたとき，$a$ と $b$ とは計算して求めることが可能でした。
* では，N 個のデータ $(x_1,y_1),\cdots,(x_n,y_n)$ が与えられたとき，切片 と 傾き とはどう定めたら良いのでしょうか？

<center>
<img src="/assets/sklearn_map.svg" style="width:88%"><br/>
scikit-learn の カンペ (cheat sheet) を改変
</center>

### 1.5. <a name="precision">モデルの精度を測る指標</a>
* モデルの精度とは，何でしょうか。精度とは，正しく予測できることです。分類課題の場合，
* 正しい予測と誤った予測とには，詳細な検討が必要になります。
* ここでは，精度 とは，英語で precision と accuracy と ２ つあります。日本語ではどちらも精度です。

* **精度 precision**: This computes the proportion of instances predicted as positives that were correctly evaluated (it measures how right our classifier is when it
says that an instance is positive).
* **再現率 recall**: This counts the proportion of positive instances that were correctly evaluated (measuring how right our classifier is when faced with a positive instance).
* **F1 値 F1-score**: This is the harmonic mean of precision and recall, and tries to combine both in a single number

| | 予測: + | 予測: - |
|---|----|----|
|真の値: + | True Positive (ヒット Hit)| False  Negative (ミス Miss) |
|真の値: - | False Positive (虚報 False alarm)| True Negative (正しい棄却 Correect rejection) |


## 1.6 <a name="supervised_vs_unsupervised">教師あり学習と教師なし学習</a>
* 予測すべき数値に正解が与えられている場合，**教師あり学習 supervised learning** と呼びます。
* 一方，予測すべきデータが与えられていない場合を **教師なし学習 unsupervised learning** と呼びます。
* 手書き数字認識では，正解となるデータが与えられているので，教師あり学習となります。
* 一方で，正解データが与えられていない場合に，入力データを分類したりする場合を 教師なし学習と 呼びます。


以下はすぐに知る必要がない知識です

### 重回帰

中学校以来の直線の方程式 $y = ax + b$ を一般化します。
データ行列を $\mathbf{X}$，予測すべき値を $\mathbf{y}$ とし，推定すべきパラーメータを $\mathbf{W}$ で表します。
重回帰 multiple regression は次式で表されます:

$$
\mathbf{y}=\mathbf{Xw} +\mathbf{b}
$$
ここで $\mathbf{b}$ はバイアス項，中学数学で言えば切片にあたります。

### 主成分分析

データ $\mathbf{X}$ の次元圧縮 dimensionality reduction の方法です。
$\mathbf{X}$ を 係数行列 $\mathbf{w}$ によって変換したデータを $\mathbf{y}$ とします。
$\mathbf{y}$ の分散を最大化する方法として，次のような目的関数を最大化することを考えます:

$$
\mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} - \lambda\left(\mathbf{w}^\top\mathbf{w}-1\right)
$$

ここで $\lambda$ はラグランジェ Lagrange の未定定数 Lagrange's multiplier と呼ばれます。
すなわち，主成分分析とは，目的関数である $\mathbf{w}^\top\mathbf{w}$ を最小化する代わりに，制約付き最小化問題を解くことに相当します。
目的とする関数を最小化する代わりに，新たな目的関数を設定して，その新しい目的関数を最小化することで，制約付き最初化を実現する方法です。

この方法を一般化して **変分法** variational methods と呼びます。

また，上式を解くことは，$\left|\mathbf{X}-\lambda\mathbf{I}\right|=0$ なる固有方程式を解くことになります。
すなわち，主成分分析とは，データ行列の固有値問題を解くことと同義です。

固有値問題，および 変分法，変分問題は，古くは，オイラーやニュートンによって始められました。
すなわち，惑星の運行を記述する運動方程式の解法として考案されました。
この方法を洗練させたのが，ラグランジェ で解析力学として定式化されました。

### ロジスティック回帰

ロジスティック回帰とは 回帰の名前がついていますが，分類 問題を解くための手法です。
ある事象が生起する確率を $p$ とすれば，生起市内確率は $(1-p)$ と表せます。この確率比のことを **ロジット比** と呼びます。
ロジット比の対数が次式に従うことを仮定するのが，ロジスティック回帰です。

$$
\log\left(\frac{p}{1-p}\right) = e^{x}
$$

上式を解けば，

$$
p(x) = \frac{1}{1+e^{-x}}
$$

この式を **シグモイド関数** sigmoid function と呼びます。

<!-- #### 伏線回収

初回の授業で，COVID-19 の感染者数の変動を記述するモデルを紹介しました。
Kermack McKendrick モデルのポイントは 時刻 $t$ における感染者の増加率 $dp/dt$ は その時の感染者の比率と非感染者の比率 の積に比例する
と仮定することでした。 -->

上式を微分すると，次式を得ます:
$$
\frac{dp}{dt} = \beta p(t)\left(1-p(t)\right)
$$

上式を高等学校数学風味に書き換えると次式のようになります。

$$
y' = \beta y(1-y)
$$

ここでは $p$ を $y$ と書き換えました。
また微分を表す記号を プライム (') にしました。
この式は，高校学校 2 年生の知識で解くことができます。

あまり深入りする必要はありません。
ですが，$y$ を微分した右辺に，$x$ が入っていないことに注意です。

### 勾配降下法

重回帰では解析解が存在しました。一方，非線形問題は一般に解析解が存在しません。
その際に，目的関数を繰り返しによって求める方法があります。
**勾配降下法** gradient descent methods はその一つです。
任意の点 $x$ における関数 $f(x)$ の微分が定義されていれば，求める関数の最小値は次式:

$$
\Delta\theta = \eta\frac{\partial f}{\partial\theta}
$$

を逐次計算することで求めることができると仮定します。
ここで $\theta$  はモデルのパラメータ，$f$ は目的関数，$\eta$ は学習率，$\partial$ は **偏微分** partial differential を表します。

