---
title: 第11回
author: 浅川 伸一
layout: default
---
<!-- コードはちゃんと書かないといかんな。 -->
<!-- <link href="/css/asamarkdown.css" rel="stylesheet"> -->

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 23/Jun/2023<br/>
Appache 2.0 license<br/>
</div>

* [PPO <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0623stable_baselines3_demo.ipynb)


## 本日の課題

- [LeCun, Bengio, Hinton (2015) ディープラーニング, Nature](2015LeCun_Nature_ja.pdf)

課題は，上記 ファイルを読んで，文章中の **太字** になっている単語の意味を書いてください。
分量は一行程度です。
調べる必要はありません。
自分なりの意味を類推して，おそらく，こういう意味だろうという自分の解釈を書いてください。
従って，その説明文が，正しいか誤っているかは，問題でありません。
あくまで，自分がどのように解釈したかを書いてください。

## イメージネットコンテストの結果

<!-- ## 本日のキーワード -->

# ニューラルネットワークの歴史

<center>
<img src='/assets/imagenet_result2017.png' style='width:74%'><br/>
画像認識の進歩
</center>

## 第 1 次ニューロブーム

### 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案
(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='/assets//mcculloch.jpg' style="width:38%">
<img src='/assets//pitts.jpg' style='width:50%'><br>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br> -->
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_{i}=\phi\left(\sum_jw_{ij}x_j\right),\tag{eq:formal_neuron}
$$

ここで $y_{i}$ は $i$ 番目のニューロンの出力，$x_{j}$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$ と $j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br> -->
形式ニューロン
</center>

---

## ローゼンブラット Rosenblatt のパーセプトロン

<center>
<img src='/assets/rosenblatt.jpg' style="width:49%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//rosenblatt.jpg' style="width:49%"><br> -->
フランク・ローゼンブラット
</center>

<!--
$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

<center>
<img src='/assets/perceptron.png' style="width:74%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//perceptron.png' style="width:74%"></br> -->
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center>


<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br> -->
ニューロンの模式図 wikipedia より
</center>

<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->

<!--
## パーセプトロンの学習

$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
パーセプトロン perceptron は 3 層の階層型ネットワークでそれぞれ
S(sensory layer), A(associative layer), R(response layer) と呼ぶ。
$S\rightarrow A \rightarrow R$ のうち パーセプトロンの本質的な部分は
$A\rightarrow R$ の間の学習にある。

入力パターンに $P^+$ と $P^-$ とがある。
パーセプトロンは $P^+$ が入力されたとき $1$, $P^-$ のとき $0$ を出力する
機械である。
出力層($R$) の $i$ 番目のニューロンへの入力(膜電位の変化) $u_i$は
\begin{equation}
 u_i = \sum_j w_{ij}x_j - \theta_i = \left(w\right)_i\cdot\left(x\right)_i-\theta_i.\label{eq1}
\end{equation}
ここで中間層($A$)の $j$ 番目のニューロンの出力 $y_i$とこのニューロンとの
結合係数を$w_{ij}$、しきい値を$\theta_i$ とした。
このニューロンの出力$y_i$(活動電位、スパイク)は、

\begin{equation}
y_i = \lceil u_i\rceil
\qquad\left\{
\begin{array}{ll}
 1 & \mbox{if $u_i \ge 0$,}\\
 0 & \mbox{otherwize}
\end{array} \right.
\end{equation}

と表される。
-->

<!--
式(\ref{eq1})の意味を理解するために以下の図を参照

%
\footnote{
Minsky and Papert はパーセプトロンのベクトル表示について
悲観的な考え方を持っているようですが、ここでは理解のしやすさを
優先します。}%
$$
\mathbf{w}\rightarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

---

- 1960 年，ミンスキーとパパートの批判
- 第一次氷河期の到来

---

## 第 2 次ニューロブーム
- 1986 年，PDP ブック，俗に言うバイブル，発表
- 1989 年，バプニック，サポートベクターマシン発表
- 第二次氷河期の到来


---

## 第 3 次ニューロブーム

<center>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>
<!-- <iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br> -->
</center>


<center>
<img src='/assets/2015Mnih_DQNFig.png' style='width:84%'><br/>
DQN の結果
</center>


- 2016 アルファ碁がイ・セドルを破る

<center>
<img src="/assets/2016AlphaGo_Fig1a.svg" style="width:84%"><br/>
アルファ碁 Natureより
</center>


### ボストン・ダイナミクス社ビデオ
<!--<iframe width="200" height="120" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<center>
<iframe width="480" height="300" src="https://www.youtube.com/embed/rVlhMGQgDkY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="480" height="300" src="https://www.youtube.com/embed/tf7IEVTDjng" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<div align="center">
<iframe width="480" height="300" src="https://www.youtube.com/embed/8vIT2da6N_o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
source: <https://www.youtube.com/embed/8vIT2da6N_o>
</div>

<div align="center">
<iframe width="480" height="300" src="https://www.youtube.com/embed/fRj34o4hN4I" <!--https://www.youtube.com/embed/WcbGRBPkrps"--> frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
source: <https://www.youtube.com/embed/fRj34o4hN4I>
</div>

<!-- <center>
<iframe width="635" height="358" src="../assets/A Style-Based Generator Architecture for Generative Adversarial Networks.mp4"  frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center> -->

<div align="center">
<!--<iframe width="805" height="453" src="https://www.youtube.com/embed/WcbGRBPkrps" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<iframe width="480" height="300" src="https://www.youtube.com/embed/WcbGRBPkrps" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

---

## 機械学習

### 機械学習の超簡単実習

今回取り上げる話題は以下のとおりです:


- 重回帰，逆行列，線形代数
- 主成分分析，固有値，変分法，
- tSNE
- ロジスティック回帰
- サポートベクトルマシン SVM


- [初めての画像認識 <img src="/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_ResNet50_demo.ipynb){:target="_blank"}-->
- [機械学習の超簡単デモ<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0507_3mnists_demo.ipynb){:target="_blank"}



### 1.1. <a name="mnist">3 つのデータセット: MNIST, Fashion MNIST, KMNIST</a>
- 機械学習分野で頻用されるデータセットとして，手書き数字認識データである MNIST があります。
- MNIST は FAIR (フェイスブック人工知能研究所) 現所長 の Yan LeCun によって作成されました。
NIST とは，アメリカ合衆国版の JIS です。すなわち，標準化機関の手書き数字認識用データセットを 修正した (modified) という意味から MNIST と呼ばれます。
- MNIST は データ数が ７万で，訓練データ数 6 万，テストデータ １ 万からなります。
データは，縦横それぞれ 28 画素からなっています。コンピュータで扱う際に，コンピュータにとってキリの良い 32 画素ではなく，
周囲を切り取ったために，28 画素になっています。
- Fasshion MNIST は， MNIST と同じ画像形式で，ファッション画像，具体的には 10 種類のアパレル画像データです。
- kmnist は日本語のくずし字データセットです。形式は MNIST, Fashion MNIST と同じです。

### 1.2. <a name="dataset">訓練データ，テストデータ，検証データ</a>
* 機械学習では，心理統計で用いられるような 仮説検定を行うこともありますが，むしろ，行わない場合も多いです。
* 理由としては，仮説検定を行うことによりも，モデルの性能を向上させることに主眼があるからという意味合いであろうと考えられます。
* ですが，考え方は母集団統計量の推定と同じような発想をします。すなわち，まだ見ぬ未知のデータに対して精度が良いモデルが優れているモデルと判断されます。
* 訓練データを使ってモデルを作成し，作成したモデルの評価をテストデータを使って評価します。
* このとき，テストデータは訓練には使いません。未知のデータに対しての精度でモデルの性能の優劣を競います。従って，モデルの精度の良いモデルが良いモデルであり，かつ，良いモデルとは，未知のデータに対してより精度が高く動作するモデルとなります。
* この点については，母集団の統計量の優劣を考える心理統計とは異なります。
* 真の母集団という，ありもしない曖昧 (かも知れない) 仮想集団について斟酌するよりも，実際のデータについて精度の優劣でモデルの性能を競うという意味では，実務的な発想と言えるでしょう。
* 機械学習におけるモデルの精度向上を目指したパラメータチューニングのことを **学習** と呼びます。

### 1.3. <a name="overfitting">過学習</a>

* モデルのパラメータを学習するときに，同じデータを用いて性能を検証することは，方法論的に間違っていると言えます。
* すでに見たことのある敵をたおせても，真の勇者とは言えません。何度でも生き返ることができる RPG とは違います。
* 見たことのあるデータ （遭遇した経験のあるモンスター）は倒せるでしょう。ですが，それでは 勇者 ではなく チキン です。
* 経験済のデータについては，完璧なスコアを示すことができるでしょう。ですが，まだ見ぬデータに対して有用な予測をすることはできません。
* このような状況を 過学習 (over-learning) あるいは オーバーフィッティング (over-fitting) といいます。
* これを避けるために、（教師あり）機械学習を行う際には，利用可能なデータの一部を テストデータセット `X_test`, `y_test` として用意しておくのが一般的です。
* 一般に k-hold out 法などと呼ばれる手法は，訓練データセットを ｋ 個に分割します。その上で，k 個に分割した 1 つのデータ群を除いた k-1 群の訓練データを用いてモデルの学習を行います。学習の都度，残しておいたデータを用いて性能を評価します。
* この方法により，最終評価に用いるテストデータを使うこと無くチューニングを行います。
* **なぜ全データを用いないで，データを分割するのか？**
  * 未知の母集団を仮定しないで，モデルの優劣を正当に評価するための方法であるとみなすことができます。

### 1.4. <a name="回帰と分類">回帰と分類</a>
* 機械学習で頻用される手法の分類に **回帰** と **分類** があります。
* 予測すべきデータが連続量の場合は，回帰
* 予測すべきデータが離散量の場合は，分類 と呼ばれます。
* 身長や体重，あるいは，明日の東京都における COVID-19 の感染者数を予測するのであれば 回帰 です。
* 一方，手書き数字認識は，予測すべきデータが 10 分類された各クラスですので 分類 と呼ばれます。
* $\mathbf{y} = \mathbf{Xw} +\mathbf{b}$ などは 線形回帰 と呼ばれます。これは中学校以来の 直線を表す 1次方程式 $y=ax+b$ と同じ形をしています。
* $y$ を予測すべき量，$x$ を与えられたデータと考えます。
* 傾き slope:$a$ と 切片 intercept:$b$ とを推定する問題が 回帰 です。
* 中学校までの数学の知識では，2 点 $(x_1, y_1)$, $(x_2, y_2)$ が与えられたとき，$a$ と $b$ とは計算して求めることが可能でした。
* では，N 個のデータ $(x_1,y_1),\cdots,(x_n,y_n)$ が与えられたとき，切片 と 傾き とはどう定めたら良いのでしょうか？

<center>
<img src="/assets/sklearn_map.svg" style="width:88%"><br/>
scikit-learn の カンペ (cheat sheet) を改変
</center>

### 1.5. <a name="precision">モデルの精度を測る指標</a>
* モデルの精度とは，何でしょうか。精度とは，正しく予測できることです。分類課題の場合，
* 正しい予測と誤った予測とには，詳細な検討が必要になります。
* ここでは，精度 とは，英語で precision と accuracy と ２ つあります。日本語ではどちらも精度です。

* **精度 precision**: This computes the proportion of instances predicted as positives that were correctly evaluated (it measures how right our classifier is when it
says that an instance is positive).
* **再現率 recall**: This counts the proportion of positive instances that were correctly evaluated (measuring how right our classifier is when faced with a positive instance).
* **F1 値 F1-score**: This is the harmonic mean of precision and recall, and tries to combine both in a single number

| | 予測: + | 予測: - |
|---|----|----|
|真の値: + | True Positive (ヒット Hit)| False  Negative (ミス Miss) |
|真の値: - | False Positive (虚報 False alarm)| True Negative (正しい棄却 Correect rejection) |


## 1.6 <a name="supervised_vs_unsupervised">教師あり学習と教師なし学習</a>
* 予測すべき数値に正解が与えられている場合，**教師あり学習 supervised learning** と呼びます。
* 一方，予測すべきデータが与えられていない場合を **教師なし学習 unsupervised learning** と呼びます。
* 手書き数字認識では，正解となるデータが与えられているので，教師あり学習となります。
* 一方で，正解データが与えられていない場合に，入力データを分類したりする場合を 教師なし学習と 呼びます。


以下はすぐに知る必要がない知識です

### 重回帰

中学校以来の直線の方程式 $y = ax + b$ を一般化します。
データ行列を $\mathbf{X}$，予測すべき値を $\mathbf{y}$ とし，推定すべきパラーメータを $\mathbf{W}$ で表します。
重回帰 multiple regression は次式で表されます:

$$
\mathbf{y}=\mathbf{Xw} +\mathbf{b}
$$
ここで $\mathbf{b}$ はバイアス項，中学数学で言えば切片にあたります。

### 主成分分析

データ $\mathbf{X}$ の次元圧縮 dimensionality reduction の方法です。
$\mathbf{X}$ を 係数行列 $\mathbf{w}$ によって変換したデータを $\mathbf{y}$ とします。
$\mathbf{y}$ の分散を最大化する方法として，次のような目的関数を最大化することを考えます:

$$
\mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} - \lambda\left(\mathbf{w}^\top\mathbf{w}-1\right)
$$

ここで $\lambda$ はラグランジェ Lagrange の未定定数 Lagrange's multiplier と呼ばれます。
すなわち，主成分分析とは，目的関数である $\mathbf{w}^\top\mathbf{w}$ を最小化する代わりに，制約付き最小化問題を解くことに相当します。
目的とする関数を最小化する代わりに，新たな目的関数を設定して，その新しい目的関数を最小化することで，制約付き最初化を実現する方法です。

この方法を一般化して **変分法** variational methods と呼びます。

また，上式を解くことは，$\left|\mathbf{X}-\lambda\mathbf{I}\right|=0$ なる固有方程式を解くことになります。
すなわち，主成分分析とは，データ行列の固有値問題を解くことと同義です。

固有値問題，および 変分法，変分問題は，古くは，オイラーやニュートンによって始められました。
すなわち，惑星の運行を記述する運動方程式の解法として考案されました。
この方法を洗練させたのが，ラグランジェ で解析力学として定式化されました。

### ロジスティック回帰

ロジスティック回帰とは 回帰の名前がついていますが，分類 問題を解くための手法です。
ある事象が生起する確率を $p$ とすれば，生起市内確率は $(1-p)$ と表せます。この確率比のことを **ロジット比** と呼びます。
ロジット比の対数が次式に従うことを仮定するのが，ロジスティック回帰です。

$$
\log\left(\frac{p}{1-p}\right) = e^{x}
$$

上式を解けば，

$$
p(x) = \frac{1}{1+e^{-x}}
$$

この式を **シグモイド関数** sigmoid function と呼びます。

<!-- #### 伏線回収

初回の授業で，COVID-19 の感染者数の変動を記述するモデルを紹介しました。
Kermack McKendrick モデルのポイントは 時刻 $t$ における感染者の増加率 $dp/dt$ は その時の感染者の比率と非感染者の比率 の積に比例する
と仮定することでした。 -->

上式を微分すると，次式を得ます:
$$
\frac{dp}{dt} = \beta p(t)\left(1-p(t)\right)
$$

上式を高等学校数学風味に書き換えると次式のようになります。

$$
y' = \beta y(1-y)
$$

ここでは $p$ を $y$ と書き換えました。
また微分を表す記号を プライム (') にしました。
この式は，高校学校 2 年生の知識で解くことができます。

あまり深入りする必要はありません。
ですが，$y$ を微分した右辺に，$x$ が入っていないことに注意です。

### 勾配降下法

重回帰では解析解が存在しました。一方，非線形問題は一般に解析解が存在しません。
その際に，目的関数を繰り返しによって求める方法があります。
**勾配降下法** gradient descent methods はその一つです。
任意の点 $x$ における関数 $f(x)$ の微分が定義されていれば，求める関数の最小値は次式:

$$
\Delta\theta = \eta\frac{\partial f}{\partial\theta}
$$

を逐次計算することで求めることができると仮定します。
ここで $\theta$  はモデルのパラメータ，$f$ は目的関数，$\eta$ は学習率，$\partial$ は **偏微分** partial differential を表します。

