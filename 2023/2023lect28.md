---
title: 第28回
author: 浅川 伸一
layout: home
---
# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 22/Dec/2023<br/>
Appache 2.0 license<br/>
</div>

# 実習資料 (Colab ファイル など)

* [先週までの実習ファイル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1222stable_baselines3_demo_LunaLander_V2_etc.ipynb){:target="_blank"}
* [Q 学習 チュートリアル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1209Q_learning_tutorial%2BRendering_OpenAi_Gym_in_Colaboratory.ipynb){:target="_blank"}

<!--
* [TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb){:target="_blank"}
* アルファ碁, アルファ碁ゼロ, DQN, [Atari ゲーム (OpenAI Gym)](https://gym.openai.com/){:target="_blank"}
* [エージェント57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark){:target="_blank"}
* [ランダム探索 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb){:target="_blank"}
* [方策勾配法 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb){:target="_blank"}
- [SARSA <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb){:target="_blank"}
- [Q学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb){:target="_blank"} -->

<!--* [PyTorch チュートリアルによる DQN (2021_1105 現在未完成)](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105reinforcement_q_learning.ipynb) -->

<!-- (file:///Users/asakawa/study/2020personal/2020-1030deepmind_agent57.md)-->
<!-- 1. [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-pee
k-into-reinforcement-learning.html)
1. [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.htm
l) -->


<!-- * [Google Colab で OpenAI の Gym 環境を動かすための下準備](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1106Remote_rendering_OpenAI_Gym_envs_on_Colab.ipynb) -->

<!-- * [Annotated Transformers <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007Annotated_Transformer.ipynb)
* [BERT head visualization <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007BERT_head_view.ipynb)
- [日本語 BERT 2 つの文の距離を求める <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb) -->

<!-- * [PyTorch チュートリアルによる DQN (2021_1105 現在未完成)](https://colab.research.google.com/github/koma
zawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105reinforcement_q_learnin
g.ipynb) -->

## エージェント 57

Agent57 では，Atari ゲーム 57 ベンチマークの全ゲームで人間以上の性能を持つ，より一般的な知的エージェントの構築に成功した。

Agent57 は，以前のエージェント Never Give Up の上に構築され，エージェントがいつ探索し、いつ利用するか，また，どの時間地平で学習するのが有用かを知るのに役立つ適応的メタコントローラを実体化したものである。
幅広いゲーム課題では，当然これらのトレードオフの選択を変える必要があるため，メタコントローラはこのような選択を動的に適応させる方法を提供している。

Agent57 は，学習時間が長いほど得点が高くなり，計算量の増加に対応することができた。
これにより，Agent57 は強力な一般的成績を達成することができたが，多くの計算と時間がかかりるので，データ効率は確実に向上させることができる。
さらに，Agetn 57 は，Atari 57ゲームのセットでより良い 5 パーセンタイルの成績を示した。
これは，データ効率だけでなく，一般性能の点でも，決して Atari の研究の終わりを意味するものではない。
このことについて，2 つの見解を示す。

1. パーセンタイル間の成績を分析することで，Agent57 アルゴリズムがいかに一般的であるかについて新しい洞察を得ることができる。
Agent57 は 全 57 ゲームの最初のパーセンタイルで強力な結果を達成し，MuZero で示されるように NGU や R2D2 よりも良い平均値と中央値を保持しているが，それでもより高い平均性能を得ることができる。
2. 現在のすべてのアルゴリズムは，いくつかのゲームにおいて最適な性能を達成するには程遠いということである。
そのため，Agent57 が探索，計画，信用割当て問題に使用する表現を強化することが，使用する上で重要な改善点となるかもしれない。


### DQN からの改善
* 二重 DQN
* 優先度付き経験再生
* 決闘 (dueling)
* 分散化

#### 短期記憶
* LSTM, GRU $\rightarrow$ R2D2

#### エピソード記憶
* メモリーネットワーク
* ニューラルエピソディック制御
* トランスフォーマー

#### 探索
* 好奇心
* 内発的動機づけ

#### メタ制御

これらの説明は [先週の資料ページ](https://komazawa-deep-learning.github.io/2023/2023lect27/) を参照のこと

# 第 28 回 精神医学(統合失調症, 強迫神経症, 依存症, 幻覚幻聴) <!--, 神経心理学(意味痴呆, 相貌失認, 失語, 失行)-->

- [感情とはそもそも何なのか](https://www.amazon.co.jp/dp/4623083721)，乾 敏郎, ミネルヴァ書房，2018
- [計算論的精神医学](https://www.amazon.co.jp/dp/432625131X), 国里，片平，沖村，山下, 勁草書房, 2019

## 資料

- [真のAIへの鍵を握る天才神経科学者](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/)
- [YouTube](https://youtu.be/RXTizOtvsE8)

---

* (自由エネルギーの最小化) = (予測誤差を最小化するように信念を書き換え予測を最適化)+(予測誤差)を最小化するような行動をとる
* (自由エネルギー) = (内部エネルギー)-(エントロピー)

乾(2018, p. 134)

# 計算論的精神医学: 幻の器官としての脳 (Friston, 2014)<!-- Computational psychiatry: the brain as a phantastic organ-->

- 脳機能の理論的枠組みによる定式化: 主観的信念や行動を，形式的な計算論的枠組み，すなわち神経生理学に基づくシナプス機構のレベルで捉える
- アクティブ（ベイジアン）推論と予測コーディング
- 自閉症の偏った心や統合失調症の円滑追跡眼球運動異常

<center>
<img src="/assets/2014Friston_Fig1.svg" style="width:74%"></br>
<p align="center" style="width:74%">
<!--**Hierarchical neuronal message passing system that underlies predictive coding**-->
予測符号化を支える階層的ニューロン間のメッセージ送受信システム
</p>
<p align="left" style="width:74%">
<!-- Neuronal activity encodes expectations about the causes of sensory input, and these expectations minimise prediction error. Minimisation relies on recurrent neuronal interactions between different levels of the cortical hierarchy.
Within this model, the available evidence suggests that superficial pyramidal cells (red triangles) compare expectations (at each level) with top-down predictions from deep pyramidal cells (black triangles) at higher levels.-->
神経細胞の活動は，感覚入力の原因についての期待をコード化している。
この神経活動における期待は予測誤差を最小化しようとする。
この最小化は，皮質階層の異なるレベル間でのニューロンの再帰的な相互作用に依存している。
このモデルでは，表層の錐体細胞（赤い三角形）が，より高いレベルの深層の錐体細胞（黒い三角形）からのトップダウン予測と，各レベルでの予測を比較していることが示されている。
</p>

