---
title: 第06回
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 19/May/2023<br/>
Appache 2.0 license<br/>
</div>

<!-- <img src="/assets/2017Vaswani_Fig1.svg" width="19%"> 左の Multi-head Attention を拡大すると中
<img src="/assets/2017Vaswani_Fig2_2.svg" width="14%"> 中の Scaled Dot-Product Attention を拡大すると右
<img src="/assets/2017Vaswani_Fig2_1.svg" width="14%"> -->

## 実習

* [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb)
* [最小限の MeCab](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2022_0916mecab_test.ipynb)

<!-- - [リカレントニューラルネットワークによる文字ベース言語モデルデモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ みんなの日本語](https://komazawa-deep-learning.github.io/minnichi_char_rnn.html)
- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"} -->
<!-- - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"} -->
<!-- - [百人一首データ取得](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0917get_hyakunin_isshu.ipynb){:target="_blank"} -->
<!-- - [BERT の超簡単な使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"} -->

<center>
<img src="/2023assets/2023_0425linear_regression_NTTpsylex.png" width="66%">
<img src="/2023assets/2023_0426mlp_from_word2vec_to_NTTpsylex.png" width="66%">
</center>

<!-- ## 課題

以下の Mitchell (2008) 論文 [名詞の意味に関連した人間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}
を読み，要約しなさい。 -->

<!-- ## 資料と参考文献

- [感情とはそもそも何なのか](https://www.amazon.co.jp/dp/4623083721){:target="_blamk"}，乾 敏郎, ミネルヴァ書房，2018
- [計算論的精神医学](https://www.amazon.co.jp/dp/432625131X){:target="_blank"}, 国里，片平，沖村，山下, 勁草書房, 2019
- [真のAIへの鍵を握る天才神経科学者 Friston の紹介](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/){:target="_blank"}
- [Friston インタビュー YouTube 動画](https://youtu.be/RXTizOtvsE8){:target="_blank"} -->

## 先週の復習

* 符号化器-復号化器モデル
* ソフトマックス関数
* ワンホットベクトル，あるいは，ワンホット表現

## 本日のねらい

* 埋め込み表現，埋め込みベクトル
* コサイン類似度と相関係数

<div class="figure figcenter">
<img src="/2023assets/2000IttiKoch_fig1b.svg" width="33%">
<img src="/2023assets/2017Vaswani_Fig2_2ja.svg" width="28%">
<!-- <img src="figures/2017Vaswani_Fig2_1ja.svg" width="14%"> -->
<!-- <img src="figures/2017Vaswani_Fig2_2.svg" width="22%"> -->
<img src="/2023assets/2017Vaswani_Fig2_1ja.svg" width="24%">
</div>
<div class="figcaption">
心理と人工知能モデルにおける注意.
左図は Itti&Koch (2000) の視覚探索モデルの説明図。中央と右は transfomer の基本単位の概略図である。
</div>

左図では，情報は上から下へと流れる。左図最下部分に WTA (勝者専有回路 winner take all circuit) が見える。
心理学における注意とは，顕著性地図 (図では saliency map と表記されている) から，一つの勝者を選択することが注意
の役割であるとされる。

一方，上図中央と右では，情報は下から上へと流れる。また，左図が視覚に関するモデルであったが，中央と右とは，入力
情報は言語，具体的には，文である。
中央図の中程に 正規化ドット積注意 `scaled dot-product attention` と見える。この部分を詳しく描いたものが右図に
なる。
中央図も右図も，文を，クエリ，キー，バリューと 3 つのベクトルに変換し，その上で注目すべき項目を `ソフトマックス` によって選択することとなる。

本書の所要テーマである注意は，勝者専有回路 (WTA) でありソフトマックスである。
数式で書けば，実は WTA とソフトマックスは同じものである。

このことから，心理学における注意と，人工知能で用いられる注意とは，同じ機構を採用したものと言うことができる。


1. 注意機構: トランスフォーマー，BERT
- トランスフォーマー [Attention is all you needAttention is all you need](https://arxiv.org/abs/1706.03762)
- [BERT](https://arXiv.org/abs/1810.04805/), [BERTolgy](https://arxiv.org/abs/2002.12327)


## 1. Glaser (2019) の 教師あり機械学習の 4 つのレベル

<div class="figure figcenter">
<img src="/assets/2019Glaser_fig2.jpg" width="49%">
<div class="figcaption">

Glaser (2019) Fig. 2 より
</div></div>

1. 工学的な問題の解決
機械学習は， 医療診断， ブレインコンピュータインターフェース， 研究ツールなど， 神経科学者が使用する手法の予測性能を向上させることができる。
2. 予測可能な変数の特定
機械学習により， 脳や外界に関連する変数がお互いを予測しているかどうかをより正確に判断することができる。
3. 単純なモデルのベンチマーク。
解釈可能な簡易モデルと精度の高い ML モデルの性能を比較することで， 簡易モデルの良し悪しを判断するのに役立つ。
4. 脳のモデルとしての役割。
脳が機械学習システム， 例えばディープニューラルネットワークと同様の方法で問題を解決しているかどうかを論じることができる。


## 2. 機械学習と脳画像研究および心理モデル

### 2.1 言語と機能的脳画像研究を結びつけるために，単語の分散表現を機械学習的手法で表現する

- [名詞の意味に関連した人間の脳活動の予測, Mitchell, 2018, Predicting Human Brain Activity Associated with the  Meanings of Nouns](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}

<div class="figure figcenter">
<img src="/assets/2019mitchell-54_20.png" style="width:49%">
<img src="/assets/2008Mitchell_fig1ja.svg" style="width:49%">
<div class="figcaption">

Mitchell (2008) 図 1. 任意の名詞刺激に対する fMRI 活性化を予測するモデルの形式。
左のように 「セロリ」 から右の脳画像を予測するために，中間表現として， 兆 単位の言語コーパス (言語研究では訓練や検証に用いる言語データをコーパスと呼ぶ) から得られた **意味特徴** を用いる。

fMRI の活性化は 2 段階の処理から予測される。
第 1 段階では，入力刺激語の意味を，典型的な単語使用を示す大規模なテキストコーパスから値を抽出した中間的な意味的特徴の観点から符号化する。
第 2 段階では，これらの中間的な意味的特徴のそれぞれに関連する fMRIシグネチャ の線形結合として，fMRI 画像を予測する。
<!-- Form of the model for predicting fMRI activation for arbitrary noun stimuli.
fMRI activation is predicted in a two-step process.
The first step encodes the meaning of the input stimulus word in terms of intermediate semantic features whose values are extracted from a large corpus of text exhibiting typical word use.
The second step predicts the fMRI image as a linear combination of the fMRI signatures associated with each of these intermediate semantic features. -->
</div></div>

<br/><br/>

<div class="figure figcenter">
<img src="/assets/2008Mitchell_fig2.svg" style="width:88%">
<div class="figcaption">

Mitchell (2008) 図 2. 与えられた刺激語に対する fMRI 画像の予測。

他の単語 (下図左) eat, taset, fill などの単語から セロリ を予測する回帰モデルを使って予測する。
(A) 参加者 P1 が 「セロリ」刺激語に対して、他の 58 の単語で学習した後に予測を行う。
25 個の意味的特徴のうち 3 つの特徴量のベクトルを単位長にスケーリングすることである。
(食べる, 味わう, 満たす) について学習した $c_{vi}$ 係数は，パネル上部の 3 つの画像のボクセルの色で示されている。
刺激語「セロリ」に対する各特徴量の共起値は， それぞれの画像の左側に表示されている (例えば 「食べる（セロリ）」の 共起値は 0.84)。
刺激語の活性化予測値 ((A）の下部に表示)  は 25個 の意味的 fMRI シグネチャを線形結合し， その共起値で重み付けしたものである。
この図は 予測された三次元画像の1つの水平方向のスライス [z=-12 mm in Montreal Neurological Institute (MNI) space] を示している。
(B) 「セロリ」と「飛行機」について， 他の 58 個の単語を使った訓練後に予測された fMRI 画像と観察された fMRI 画像。
予測画像と観測画像の上部（後方領域）付近にある赤と青の 2本 の長い縦筋は、左右の楔状回である。
<!-- Predicting fMRI images for given stimulus words.
(A) Forming a prediction for participant P1 for the stimulus word “celery” after training on 58 other words.
Learned $c_{vi}$ coefficients for 3 of the 25 semantic features (“eat,” “taste,” and “fill”) are depicted by the voxel colors in the three images at the top of the panel.
The co-occurrence value for each of these features for the stimulus word “celery” is shown to the left of their respective images [e.g., the value for “eat (celery)” is 0.84].
The predicted activation for the stimulus word [shown at the bottom of (A)] is a linear combination of the 25 semantic fMRI signatures, weighted by their co-occurrence values.
This figure shows just one horizontal slice [z = –12 mm in Montreal Neurological Institute (MNI) space] of the predicted three-dimensional image.
(B) Predicted and observed fMRI images for “celery” and “airplane” after training that uses 58 other words.
The two long red and blue vertical streaks near the top (posterior region) of the predicted and observed images are the left and right fusiform gyri.-->
</div></div>

<br/><br/>

<div class="figure figcenter">
<img src="/assets/2008Mitchell_fig3.svg" style="width:49%"><br/>
<div class="figcaption">

Mitchell (2008) 図 3. 最も正確に予測されたボクセルの位置。

参加者 P5 の訓練セット以外の単語について、予測されたボクセルの活性化と実際のボクセルの活性化の相関を表面（A）とグラスブレイン（B）で表したもの。
これらのパネルは、少なくとも 10個 の連続したボクセルを含むクラスタを示しており、それぞれのボクセルの予測-実際の相関は少なくとも 0.28 である。
これらのボクセル・クラスターは、大脳皮質全体に分布しており、左右の後頭葉と頭頂葉、左右の豆状部、中央後葉、中央前葉に位置しています。
左右の後頭葉、頭頂葉、中前頭葉、左下前頭回、内側前頭回、前帯状回に分布している。
(C) 9人の参加者全員で平均化した予測-実測相関の表面表現。
このパネルは、平均相関が 0.14 以上の連続した10 個以上のボクセルを含むクラスターを示している。
<!-- Locations of most accurately predicted voxels.
Surface (A) and glass brain (B) rendering of the correlation between predicted and actual voxel activations for words outside the training set for participant P5.
These panels show clusters containing at least 10 contiguous voxels, each of whose predicted-actual correlation is at least 0.28.
These voxel clusters are distributed throughout the cortex and located in the left and right occipital and parietal lobes; left and right fusiform,
postcentral, and middle frontal gyri; left inferior frontal gyrus; medial frontal gyrus; and anterior cingulate.
(C) Surface rendering of the predicted-actual correlation averaged over all nine participants.
This panel represents clusters containing at least 10 contiguous voxels, each with average correlation of at least 0.14. -->
</div></div>


## 単語埋め込みモデル word2vec

<center>
<img src="/assets/2013Mikolov_KingQueenFig.svg" width="44%">
<img src="/assets/2013Mikolov_FigCountries.svg" width="44%"><br/>
<img src="/assets/2013Mikolov_skip-gram_cbow.svg" width="49%"><br/>
<div class="figcaption">
図 Mikolov+2013 より
</div></center>


* ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示した (Mikolov+2013)。
* Word2vec は実装に 2 種類ある。それぞれ **CBOW** と **skip-gram** と命名されている。
* "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝された。
* 下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目。
ベクトルは方向と大きさを持っている矢印で表現される。
このとき，矢印の原点を移動することを考える。
たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで持ってくると，その矢印は "QUEEN" を重なることが予想できる。
これがアナロジー問題の解放の直感的説明である。
* word2vec の実装には 2 種類あります。どちらを使っても同じような結果を得ることができます。
    1. CBOW: Continous Bog of Words 連続単語袋
    2. skip-gram: スキップグラム

両者は反対の関係になrる。下図を参照。

<center>
<img src="/assets/2013Mikolov_Fig1.svg" style="width:49%"><br>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あ
るいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなる。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決める。
- Mikolov が類推に用いたデータ例を下図に示した。
国名と対応する首都名，国名とその通貨名，などは意味的関係である。
一方罫線下方は文法関係である。形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などである。

### 文献

* [word2vec オリジナル論文, Mikolov2013](https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf)
* [fasttext高速版](https://fasttext.cc/)
* [その発展 浅川, 岡, 楠見 (2018)](../2018jsai.pdf)
* [日本語 Wikipedia の word2vec 表現と語彙特性の関係 近藤，浅川 (2017)](../2017jpa_word2vec_NTTdict.pdf)

<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

## 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティー
として与えることで精度が向上します。

<center>
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br>
From Milolov (2013) Tab. 1
</center>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，
異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画して
みれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば
良いのかについて示唆に富むと考えます。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%">
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かって
おおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると
大まかに我々の知識を表現できることを示唆していると考えます。

<!-- # 2. リカレントニューラルネットワーク

一時刻前の状態を保持して利用する SRN は下図左のように描くことができます。同時に時間発展を考慮すれば下図右のように描くことも可能です。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてください。あるいは同義ですが上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してください。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かります。

### エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示した。
図中の数字はニューロンの数を表す。
入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表す。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:33%"><br>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示し。
Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示された (Elman, 1990, 1991, 1993)。 -->

<!--
- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | live
s

これらの規則にはさらに 2 つの制約がある。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもと
らなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この
文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というも
のです。-->

<!-- ### LSTM

* LSTM (Long Short-Term Memory) は Schmithuber らによって開発されたリカレントネットワークモデルの改良版。
* 内部に 3 つのゲートを持つ。
* ゲートの出力関数は，シグモイド関数である。このため，信号の開閉をソフトに制御する機能を有する。
* 信号の取捨選択を，データから学習させることで，**長距離依存** に対応することを意図している。 -->

<!-- <center>
<img src="/assets/2015Greff_LSTM_ja.svg" style="width:34%">
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-19.png" style="width:16%"><br/>
左: LSTM (浅川, 2015) より，右: トランスフォーマー[@2017Vaswani_transformer]<br/>
入力ゲートと入力 は Q, K と同一視，出力ゲートと V とは同一視可能？
</center> -->

<!--
# Residual attention
<center>

![](assets/2017residual_attention.svg){style="width:33%"}
![](assets/2017residual_attention_motivation.svg){style="width:65%"}<br/>
![](assets/2017residual_attention_whole_net.svg){style="width:94%"}<br/>
[@2017Wang_residual_attention] Fig. 1, 2, 3
</center>
-->

<!--
# A2 net

<center>
![](assets/2018Chen_A2-Nets_fig1ja_a.svg){style="width:39%"}
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
![](assets/2018Chen_A2-Nets_fig1ja_b.svg){style="width:55%"}<br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>

# Relationship between self-attention and convolution

<center>
<img src="/assets/2019cordonnier_self_attention_convol.svg" style="width:66%"><br/>
<img src="/assets/2020Cordonnier_tab3.svg" style="width:66%"><br/>
From [@2020cordonnier_attention_and_convolution]
</center>

# まとめ

- MHSA は 畳み込み と同等の能力がありそうである。
- Reformer に見られるように position encodings を工夫する余地は残されているように思われる。
-->

# 3. 符号化器・復号化器モデル (encoder-decoder model), seq2seq model

* 中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば **機械翻訳** や **対話** のモデルになる。
* 初期の翻訳モデルである "seq2seq" の概念図を示した。
* "eos" は文末 end of sentence を表す。
* 中央の "eos" の前がソース言語であり，中央の "eos" の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。

* 注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がない。
* 逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
* この点を改善することを目指すことが 2014 年以降盛んになった。
* 顕著な例が後述する **双方向 RNN**, **LSTM** を採用したり，**注意** 機構を導入することであった。

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" width="49%"><br/>
From [2014Sutskever_Sequence_to_Sequence]
rom [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="/assets/2014Sutskever_Fig2left.svg" width="44%">
<img src="/assets/2014Sutskever_Fig2right.svg" style="width:44%"><br />
From [@2014Sutskever_Sequence_to_Sequence] Fig. 2, 3
</center>


### BERT


<center>
<img src="/assets/coco175469.jpg" width="44%"><br/>
</center>

1. 夕暮れのハーバーに汽船と複数の鳥が浮かんでいる
2. 水面に浮かぶ4羽の水鳥と、その向こうに停泊している2隻の船
3. 船着き場に2艘の船がとまっている
4. 朝焼けの中待機場所にある旅客船とマガモ
5. 停められた船の近くで水鳥が泳いでいる\
MS COCO データセットより: <http://farm5.staticflickr.com/4055/4704393899_a041476b4a_z.jpg>

上図は，MS COCO という画像データと画像に対応する脚注からなるデータセットからの一例である。
日本語文は，千葉工業大学 STAIRLABO が公開しているデータである。
人間が見れば，写真と文章とは結びいていることが分かる。
加えて，5 つの脚注も相互に似ていることが分かる。
MS COCO データセットでは，一枚の写真に 5 つの脚注が紐付けられている。

コンピュータにこれらの文章が似ていることを判断させようとすると，最近まで難しい仕事であった。
本章で紹介する，文の意味ベクトルを用いると，これらの文章が相互に似ていると判断させることが可能である。
下図は tSNE と呼ぶ手法を用いて，日本語の文章の類似度を センテンス BERT を用いて表現し，文章の類似度に従って地
図を描いたものである。
図では，同じ写真に紐付いている文章は同じ色で表現している。

<center>
<img src="/assets/2022_0915sbert_staircoco500.svg">
</center>

### コサイン類似度と相関係数の関係

諸島統計学で習う範囲に，ピアソンの積率相関係数と，ニューラルネットワークや機械学習で用いられるコサイン類似度の
関係について解説する。
ピアソンの積率相関係数を，相関係数と表記する。
相関係数の定義は以下の通りである：

$$
\begin{aligned}
r &= \frac{\text{x と y との共分散}}{\text{x の標準偏差$\times$ y の標準偏差}}\\
&= \frac{\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}
{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}}}\\
\end{aligned}
$$

ここで，$\bar{x},\bar{y}$ は，それぞれ $x$ と $y$ の平均である。
ここで，予め平均が引いてあるとしてみよう。$1/n$ は分子分母で共通であるから消去できる。
すると上式は，以下のようになる。

$$
r = \frac{\sum_{i=1}^{n}x_{i}y_{i}}
{\sqrt{\sum_{i=1}^{n}x_{i}^{2}}
 \sqrt{\sum_{i=1}^{n}y_{i}^{2}}} = \frac{\left(\mathbf{x\cdot y}\right)}{\left|\mathbf{x}\right|\,\left|\mathbf{y}\right|}
$$

上式，最右辺分子は，内積と呼ばれる。また分母は，原点，したがってこの場合平均からの距離である。
内積の定義は，高等学校数学の知識によれば，以下の通りである：

$$
\left(\mathbf{x\cdot y}\right) = \left|\mathbf{x}\right|\,\left|\mathbf{y}\right|\cos\theta,
$$

したがって，相関係数 $r$ とは，2 つのベクトルのなす角の余弦 $\cos$ であることがわかる。

このとき大事なことは，ベクトルの次元が n 次元であることである。
一般に，相関係数の計算では，散布図と x から y へ，あるいは，y から x への回帰式から説明される。
すなわち，散布図として説明される相関係数では，2 次元ベクトルが n 本あると考える。
これに対して，コサイン類似度との関連で言えば，n 次元ベクトルが 2 本と考えたときのベクトルのなす角の余弦のこと
を相関係数と呼ぶのである。

相関係数を算出する際に，平均を引くことはベクトルに対して，平行移動することを意味する。
従って，平行移動しても座標上の各点の関係は変化しないことから，，相関係数とコサイン類似度の本質は変化しないこと
が分かる。



## 3. 注意の近似表現としてのソフトマックス関数

<div class="figure figcenter">
<img src="/assets/1985KochUllman_Fig5.svg" width="33%">
<img src="/assets/1998IttiKoch_fig1.jpg" width="33%">
<div class="figcaption">
左: Koch & Ullman (1985) Fig. 5
右: Itti & Koch (1998) Fig. 1
</div></div>

$$
\text{Softmax}\left(x\right) = \frac{\exp(x)}{\sum_{y\in\mathcal{D}}\exp(y)}\tag{1}
$$

以降，ソフトマックス関数を $\sigma(x)$ と表記する。
ソフトマックス関数の本質は，以下のとおり

1. 全データを指数の肩に乗せる $\exp(x)=e^x$
2. 指数の肩に乗せることで，実数の範囲では，$\exp(x) \ge0$ が成り立つ。すなわち全ての値が 0 または 正 になる。
3. すべての値が 0 または 正であるので，全データ $\mathcal{D}$ に渡って全て足し合わた (総和 $\sum$) 値で割る (分母にする) ことで確率として表現できる。
確率であるためには，全ての要素が 0 または 正 であり，かつ，全て足し合わせると 1 となれば良い。
4. 各要素を分子に，全要素の総和を分母にすることで，どの要素がもっともらしいかの確率を得ることができる

以上がもっとも簡単なソフトマックス関数の説明である。

例えば，要素が 2 つしかない，単純な例を考える。
$x$ と $y$ という 2 つの値が得られたとき，この確率をソフトマックス関数を用いて考えてみる。
$e^x$ と $e^y$ とから，$x$ の確率 $p(x)$ は $\displaystyle P(x)=\frac{e^x}{e^x+e^y}$  となる。
要素が $x$ と $y$ と 2 つしかないのだから，2 つの要素を足し合わせると 1 となる，すなわち $P(x) + P(y) = 1$ が成り立つ。

$$
P(x) + P(y) = \frac{e^x}{e^x+e^y7} + \frac{e^y}{e^x+e^y} = \frac{e^x+e^y}{e^x+e^y} = 1\tag{2}
$$

上式の 2 つしか要素がない場合のソフトマックス関数のことを シグモイド関数 sigmoid function，あるいはロジスティックシグモイド関数 logistic sigmoid function と呼ぶ。
2 しか要素がない場合には，x の確率が決まれば，y の確率は $1-p(x)$ で定まる。
従って，$x$ と $y$ と 2 つの要素を考える必要はなく，一つだけ考えれば良い。
実際に以下のような計算が可能である:

$$
\begin{aligned}
\frac{e^x}{e^x+e^y} &= \frac{e^x/e^x}{e^x/e^x+e^y/e^v}\\
&= \frac{1}{1+e^{y-x}}\\
&= \frac{1}{1+e^{-(x-y)}}
\end{aligned}
$$

ここで $x-y$ を $z$ とすると，$\sigma(z)=[1+\exp(-z)]^{-1}$ となる。
$z$ は $x$ と $ｙ$ との差である。

$z$ が $0$ であれば $x=y$ を意味するので $e^{-z=0} = e^{-(x-y=0)} = z^{0} = 1 $ である。
従って $\sigma(z) = 1/(1+1) = 1/2 $ となる。

$z>0$ と $z$ が 正であれば，$z>0\rightarrow (x-y)>0 \rightarrow x>y$ であるので，$e^{-z} < 1$ となるから，$\sigma(z) > 1/2$ となる。

すなわち $x$ の方が $y$ より大きければ，$x$ である確率は 0.5 より大きくなる。
逆に $z<0$ すなわち $x$ の方が $y$ より小さければ $\sigma(z) < 0.5$ となる。
このことは $x$ と $y$ との大小関係により，その確率が 0.5 より大きくなるか小さくなるかが定まることを意味するので，我々の直感にも合致している。

$\sigma(x)$ の様子を描画する最小限の python プログラムを以下に示す


<div class="memo">

import numpy as npy<br/>
import matplotlib.pyplot as plt<br/>
<br/>
x = np.linspace(-5,5)<br/>
y = 1/(1+np.exp(-x))<br/>
plt.plot(x,y)
</div>

### 物理学メタファー

上記のソフトマックス関数は，熱力学，統計力学の分野との関係が指摘できる。
あるシステムには $N$ 個の粒子が存在するとする。
各粒子のエネルギー準位は $e^{\beta x_i}$ で表されるとする。
このシステムの全エネルギーは各粒子の総和であるので，$\sum_{i\in\mathcal{D}} e^{\beta x_i}$ である。
これは，上述のソフトマックス関数の分母に現れる式と同一である。
物理学では，この分母 $\sum_{i\in\mathcal{D}}e^{x_i}$ を **分配関数 partition function** と呼ぶ。

任意の粒子 $x_i$ のエネルギー分布は，この粒子のエネルギーを分子とし，直上の分配関数を分母として次式で表される。

$$
P(x_i) = \frac{e^{-\beta x_i}}{\sum_j e^{-\beta x_j}}\tag{3}
$$

$\beta$ は実際の温度の逆数であるので，逆温度 または 温度パラメータとも呼ばれる量である。
温度が高ければ，システム全体の挙動が不安定になる。
すなわち，各粒子のエネルギー準位をとる確率が不安定になる。

一方，温度が低いと，各粒子のエネルギー準位は決定論的に振る舞うことになる。



## 注意と出力関数

### シグモイド関数とソフトマックス関数の関係

ニューラルネットワークで用いられる出力関数には，シグモイド sigmoid 関数，ソフトマックス softmax 関数，がある。
本節では，この 2 つの関数に絞って，注意との関係を述べておくことにする。
ここで紹介する事項は，ほぼ高等学校初学年で程度の数学で理解できる事項であるため，冗長な記述である。
ソフトマックス関数と注意の関係について既知の読者は次節まで読み飛ばしていただきたい。

シグモイド関数，より正確にはロジスティック・シグモイド関数，の定義式は次式で与えられる:
$$
\sigma(x)=\left[1+\exp(-x)\right]^{-1}.\tag{eq:def_sigmoid}
$$

もちろん表記の簡便さのためであるが $\exp$ を $e$ と表記しても良い。
$e$ は自然対数の底であり，およそ 2.718 である。
シグモイド関数は，ニューラルネットワークの出力関数として非線形変換の一つとして導入されるこが多い。
ロジスティック回帰でも用いられるとおり，確率比 (ロジット比) でもある。
このため，ある事象 $x$ が生起する確率 $p(x)$ と生起しない確率 $1-p(x)$ との比 $p\left[1-p\right]^{-1}$ で定義される。
すなわち

$$
\begin{aligned}
p  &= \frac{1}{1+e^{-x}}\\
1-p &= \frac{1+e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}} = \frac{e^{x}}{1+e^{x}}\\
\end{aligned}\tag{1}
$$

すなわち，ある事象 $x$ が起こる確率 $p(x)$ と起こらない確率 $1-p(x)$ との比 $p/(1-p)$ は
$$
\frac{p}{1-p}=\frac{\frac{1}{1+e^{-x}}}{\frac{e^{-x}}{1+e^{-x}}}=\frac{1}{e^{-x}}=e^{x}
$$

あるいは，上式を $x$ について解いて，ニューラルネットワークの出力関数であるシグモイド関数は，
ネットワークの出力が起こる確率と起こらない確率の比の対数と考えても良い。
すなわち上式を $x$ について解けば次式を得る:

$$
x = \log\left|\frac{p(x)}{1-p(x)}\right|
$$

すなわち出力 $x$ は $x$ の起こる確率と起こらない確率 $1-p(x)$ の比の対数である。

以上を，ソフトマックス関数との関連で検討する。
ソフトマックス関数は，複数個の候補から，一つを選ぶ確率として定義される。
$x_{i}$ の選択確率 $p(x_{i})$ は次式で与えられる:

$$
p(x_i)=\frac{e^{x_{i}}}{\sum_{j\in N}e^{x_{j}}},
$$

ここで，$j$ は全ての項目についてである。
選択肢が 2 つ，$x$ と $y$ であれば，以下のようになる:

$$
p(x) = \frac{e^{x}}{e^{x}+e^{y}}
$$

分子，分母を $e^{x}$ で割れば，次式を得る:

$$
p(x) = \frac{1}{1+e^{-(x-y)}}
$$

すなわち，シグモイド関数の引数 $x$ を 2 つの出力の差 $x-y$ で置き換えたものが，選択肢数 2 のソフトマックス関数である。
このことを拡張すれば，選択肢数 N のソフトマックス関数は，上のシグモイド関数と同様に次式のように書き換える事が可能である。

$$
p(x_i) = \frac{e^{x_i}}{\sum_{j\in N}e^{x_j}} = \frac{e^{x_i}}{e^{x_i}+\sum_{j\ne i}e^{x_{j}}}=\frac{e^{\text{正事例}}}{e^{正事例}+\sum e^{負事例}}
$$

上式を，シグモイド関数と同様に，分子分母を $e^{\text{正事例}}$ で割ることにすれば，次式を得る:

$$
p(x_i) = \frac{1}{1+e^{-\text{正事例}}\sum e^{負事例}} = \frac{1}{1+\sum e^{-\left(\text{正事例}-\text{負事例}\right)}}
$$

上式の意味は，シグモイド関数を多項目に拡張し，他の選択項目に比べて，任意の項目がどれ程選択されやすいかを示す指標とみなすことができる。
また，この式は，正事例と負事例との差を強調することを意味する。
このため，sentenceBERT (arXiv:1908.10084) の微調整 (fine-tuning) や，word2vec (arXiv:1369.4168) の負事例サンプリングで用いられる，目的関数，すなわち対比学習 contrastive learninng の目的関数となっている。

シグモイド関数の微分は，$d\sigma(x)/dx = x (1-x)$ であることはよく知られている。
一方で，ソフトマックス関数の場合には，多変数関数であるから偏微分を用いて $\partial s(x_i)/\partial x_j$ を計算することになる。
具体的には，次式のとおりである:

$$
\frac{\partial s(x_i)}{\partial x_j}=\left\{
\begin{aligned}
x_{i} (1-x_{i})  & \text{  if $i=j$}\\
-x_{i} x_{j}     & \text{  otherwise}
\end{aligned} = x_{i}(\delta_{ij}-x_{j})
\right.
$$

ここに，$\delta_{ij}$ は Kronecker のデルタである。
上式の $i=j$ のとおり，ソフトマックス関数はシグモイド関数の多変数拡張であり，対角要素 $i=j$ ではシグモイド関数の微分に一致することがわかる。

シグモイド関数は，出力関数としての役割以外に，シナプス後接続による，神経活動の修飾を行う役割が考えられる。
すなわちシナプス結合による情報伝達のゲートとして役割が考えられる。

実際に，ニューロンへの入力信号ではなく，入力信号を修飾する結合が存在する。
下図は，ウミウシのエラ引っ込め反応時の模式図である。

<center>
<!-- <img src="figures/C87-fig2_24.jpg"> -->
<img src="/assets/shunting-inhibition.jpg" width="33%"><br/>

<http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html>
</center>

ゲート関数としての役割については，LSTM の項で取り上げる。

### ワンホットベクトルとソフトマックス

出力層のニューロンが複数個ある場合には，その一つを選択することが分類問題では求められる。
これは，ソフトマックス関数を用いて，最大値を与える選択肢の強調する。
すなわち，最大値をとる選択肢の選択確率を大きくすることを意味する。
逆に，最大値以外の値をとる他の選択肢の選択確率を低めることになる。

このような理由から，分類課題を解く場合，多層ニューラルネットワークの最終層では，ソフトマックス関数を用いることが多い。
多肢選択課題において，正解選択肢を選ぶ確率を与えると考えても良い。

出力層の出力において，最大値を与える選択肢の選択確率を高めることは，一つの選択肢の選択確率を 1 に近づけ，かつ，他の選択肢の選択確率を 0 に近づけることとなる。
この意味で，ワンホット表現 one-hot representation を得るために使われる。

著者の知る限り，ワンホット表現という用語が定着する以前では，one of k 表現と呼ばれることが多かった。
例えば Bishop の教科書 [@2006BishopPRML] (邦訳:パターン認識と機械学習) では one-of-k と記載されている。


### 双曲線正接関数 (ハイパータンジェント $\tanh$) との関係

シグモイド関数は，字義どおりの意味では，S 字状の曲線，という程度の意味である。
S 字状であるとは，単調非減少 $x< y\mapsto f(x)\le f(y)$ であることを意味する。
したがって，シグモイド関数と同じ定義域を持つ S 字状の曲線を持つ出力関数は，他にも考えられる。
例えば，正規分布の累積密度関数，整流線型化関数 ReLU, ハイパーボリックタンジェント $\tanh$ などが挙げられよう。

このうち，ハイパーボリックタンジェント，すなわち双曲線正接は $\tanh$ と表記されることが多い。
定義は次式のとおりである:

$$
\tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{x}}\tag{eq:def_tanh}
$$

$\lim_{x\rightarrow-\infty}\tanh(x)=-1$，$\lim_{x\rightarrow\infty}\tanh(x)=1$ であるので，シグモイド関数とは値域が異なる。

LeCun による指摘が最初だと思われるが，$\tanh(x)$ を出力関数として用いることで，シグモイド関数を出力関数としても用いた場合と比較して，圧倒的に収束が速くなる。
収束が速くなる理由は 2 つ挙げられる。

1. 勾配消失問題 gradient vanishing problem が生じ難い。
2. シグモイド関数が持つバイアスを回避できる

以下，上記 2 項を簡単に解説する。

勾配消失問題とは，多層ニューラルネットワークにおいて，層を重ねると学習時に学習が難しいという指摘である。
ニューラルネットワークの学習は，一般に次式で定義される:

$$
\Delta\theta = -\eta\frac{\partial\ell}{\partial\theta}
$$

ここで $\theta$ はネットワークを定義するパラメータ，すなわち結合係数やバイアス項を含めたパラメータ，$\eta$ は学習係数と呼ばれるハイパーパラメータであるとする。
$\ell$ は，自乗誤差を含む誤差関数，目的関数，損失関数，など分野や流儀によって様々に称されている関数である。
本稿では，特に区別せず損失関数 loss function と呼ぶこととする。

この損失関数を最小化することをニューラルネットワークの学習と呼ぶ。

ここで，多層ニューラルネットワークの場合，合成関数の微分則，あるいはチェインルール chain rule を用いて出力層から入力層に向かって誤差を計算していくことが必要となる。
これを，誤差逆伝播 back propagation と呼ぶ。
シュミットフーバー Schmithuber [@2015Schmidhuber_review] によれば，誤差逆伝播法の提案は 1950 年代あるいはそれ以前に遡ることができる。
チェインルールを言葉で書けば，$f$ が $x$ の関数であり，$x$ が $y$ の関数であれば，$f$ を $y$ で微分する際に，$f$ を $x$ で微分し，然る後に $x$ を $y$ で微分すれば良い，ということになる。
極めて単純な話だが，これが多層ニューラルネットワークの層ごとに行われると捉えれば良い。

勾配消失問題に戻ると，層を遡及する，すなわち逆伝播する際に，かならずある関数の微分が掛けられることを意味する。
したがって，損失関数の値が大きな値であっても，層を逆行するたびに，微係数が掛けられるので，出力関数の微係数が小さければ，入力層に近い下層では誤差が指数関数的に小さくなることを意味する。
微係数が小さいことは，収束が遅くなることを意味する。
ことのため，微係数が小さい出力関数では学習が遅くなる。
これを称して，勾配消失問題と言う。

実際，シグモイド関数の微係数は $\sigma'(x)=\sigma(x)(1 -\sigma(x))$ である。
したがって，微係数，すなわち接戦の傾きが最大となるのは $x=0$ のときであり，$\sigma'(x=0) = 0.5 (1-0.5)=0.25$ となる。
一方，ハイパータンジェントの場合には $\tanh'(x)= 1 - (\tanh(x))^2$ である。
$\tanh'(x)$ の最大値も $x=0$ のときであり，このときの値は 1 である。
層を重ねるに従って，この効果は拡大する。


## 分配関数との関係

ソフトマックス関数，あるいは，その特別な場合としてのシグモイド関数の分母は $\sum_{i\in\mathcal{X}}e^{x_{i}}$ は，統計力学の分配関数と数式の表記上は同一である。
より具体的にに，ある系のすべての状態の集合を $\Omega$ とし，系の状態を $\omega\in\Omega$ とすれば次式で与えられる:

$$
Z(\beta) = \sum_{\omega\in\Omega}e^{-\beta\mathcal{E}(\omega)}
$$

和の中の $\displaystyle e^{-\beta {\mathcal{E}}(\omega)}$ はボルツマン因子と呼ばれる。
カノニカルアンサンブルは熱浴と接触する閉鎖系を表現するアンサンブルである。
パラメータ $\beta$ は熱浴を特徴づける量で，熱浴の温度と解釈される。
熱力学温度 T とは $\beta=1/kT$ の関係にあり，逆温度と呼ばれる。
$k$ はボルツマン定数である。
分配関数に定数を乗じることはエネルギーの基準値をずらすことに等しい。
分配関数の大きさそのものには意味がない。

$\beta$ については，蒸留 distillation について説明する項で説明する。

## 交差エントロピー cross entropy との関係

交差エントロピー自体は，1980 年代から知られていた目的関数であり，2 値を取る教師信号に対して，収束が早い望ましい目的関数と考えれる。
教師信号を $t$，モデルからの出力を $p$ とすれば，次式:

$$
\ell_{CE}=-t\log(p) + (1-t)\log(1-p)
$$

で与えれる [@1989Hinton]。
上式右辺は，$t\in\{0,1\}$ の 2 値の教師信号を取るため，$t=1$ の場合には右辺第一項が選ばれる。
一方，このとき右辺第二項は 0 となるため考慮されない。
反対に $t=0$ であれば，右辺第一項は 0 となり無視され，反対に右辺第二項のみが有効となる。
このため交差エントロピー損失は，プログラミングコードにおいて `if` 文を使ったような効果が認められる。

平均自乗誤差に比して，教師信号 t と出力 p との乖離が多くくなるにつれて値が急激に大きくなるため，
学習が高速化される。
以下の Python コードを実行して交差エントロピー関数の意味を確認して欲しい。

<div class="quote">

```python
%config InlineBackend.figure_format = 'retina'   # Mac retina display でのおまじない
import matplotlib.pyplot as plt                  # グラフ描画ライブラリを輸入する
import numpy as np                               # Numpy の輸入

X = np.linspace(0.001, .999)                     # 無限大になることを避けるために [0,1] ではなく (0,1) の範囲にする
ce = lambda x: - np.log(x)                       # 交差エントロピーの定義

plt.plot(X, ce(X),   label='ce0')                # 実際の描画命令
plt.plot(X, ce(1-X), label='ce1')                # 同上
plt.legend()                                     # 凡例の描画
plt.title('Plot of cross-entropy loss function') # 図のタイトル
plt.show()                                       # 画面表示
```
</div>

交差エントロピーの定義式に若干の変形を加えれば，次式の如くなるため，シグモイド関数との関係は明白である。
$$
\ell_{CE} = -\log\frac{p^{t}}{(1-p)^{1-t}}
$$

したがって，シグモイド関数，あるいは，その拡張であるソフトマックス関数を用いる際に，交差エントロピーを用いることにすれば，
実際の収束が速いのみならず，情報量最大化，すなわちエントロピー最大化を満たす最尤解を得ることに相当する。

また，tSNE や VAE で，符号化器と復号化器との間での KL divergence との関係も指摘できる

エントロピーについては，熱力学と情報論とで用いられる概念であって，ニューラルネットワークでは，熱力学的にも情報論的にも用いられる。
両者とも $-p\log p$ で定義される。
情報論的には，エントロピーの最大化，あるいは情報量最大化という形で定義される。
すなわちニューラルネットワークの損失関数として用いられる。

したがって交差エントロピーとは，情報量最大化であり，ニューラルネットワークの出力がワンホットベクトルとして与えられる場合，
最終層をソフトマックス関数として定義し，かつ，損失関数を交差エントロピーで定義すれば，エントロピーの最大化，すなわち情報量最大化にもなっていることを意味する。


