---
title: "第 19 回"
author: "浅川 伸一"
layout: default
---
<link href="/css/asamarkdown.css" rel="stylesheet">

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 13/Oct/2023<br/>
Appache 2.0 license<br/>
</div>


先週の課題に使った colab ファイルは，修正しました。

<img src="/2023assets/alex_net_block_diagram.png"><br/>

<img src="/assets/ResNet_Fig2.svg" width="33%"><br/>
<img src="/assets/2015ResNet30.svg" width="94%"><br/>
<img src="/2023assets/2015Ren_Faster_R-CNN_fig2.svg">
<!--
<img src="/assets/2017He_MaskRCNN_41.svg">
<img src="/assets/2015Fast_R-CNN_Fig1.svg">
<img src="/assets/2015Faster_RCNN_RPN.svg">
<img src="/assets/1998LeCun_Fig2_CNN.svg">
<img src="/assets/2017He_MaskRCNN_02SemSeg.svg">
<img src="/assets/2017He_MaskRCNN_08.svg">
<img src="/assets/2017He_MaskRCNN_02Oject.svg">
<img src="/assets/2013Girshick_RCNN_Fig1.svg">
 -->

## 二つの文化

<!-- <img src="/2023assets/2001Breiman_cultures.svg"> -->
<!-- <img src="/2023assets/2001Breiman_Two_Cultures_fig1.svg"><br/> -->
<img src="/2023assets/2001Breiman_Two_Cultures_fig2.svg" width="44%"><br/>
従来の統計手法

<img src="/2023assets/2001Breiman_Two_Cultures_fig3.svg" width="44%"><br/>
機械学習による手法
<!-- <img src="/2023assets/2001Breiman_Two_Cultures_fig3_.svg" width="33%"> -->


## キーワード

* ResNet
    * スキップ結合 skip connection
    * バッチ正規化 batch normalization
* R-CNN
* 領域切り出し
    * バウンディングボックス bounding boxes
    * 意味的切り出し semantic segmentation
    * 実体切り出し instance segmentation
    * 汎光学的切り出し panoptic segmentation

## 実習

* [ResNet 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0603ResNet_with_Olivetti_faces_.ipynb){:target="_blank"}
<!-- * [ResNet 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0603ResNet_with_Olivetti_faces_.ipynb) -->
* [セグメンテーション実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0603Image_segmentation_demo.ipynb){:target="_blank"}
* [DETR  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0625DETR_demo.ipynb)

# 1. 経路仮説と残差ネット

* 腹側経路 ventral pathways ("what" 経路)
* 背側経路 dorsan pathways ("where" 経路)

<center>
<img src="/assets/1982Ungerleider_Mishkin.jpg" width="49%">
<div style="text-align:left;width:88%;color:teal">

* 下左: 物体弁別課題と下側頭回損傷。
* 下右: 目印課題と頭頂葉損傷。
Ungerleider&Mishkin1982 より。
</div></center>

<center>
<img src="/2023assets/2004Hickok_dorsal_ventral_language_fig1a.jpg" width="49%">
<img src="/2023assets/2004Hickok_dorsal_ventral_language_fig1b.jpg" width="49%">
<div style="text-align:left;width:94%;color:teal">
左: 言語の機能解剖学的枠組み。Hickok&Poeppel2000 より

右: 脳の側面図に示したモデル構成要素の一般的な場所。
モデル内のある機能に関連する皮質領域は，その機能に特化しているという仮説ではない。
調音に基づく音声符号を支援すると考えられる前頭葉領域の定義は，物品の命名と調音リハーサル処理の機能画像研究における活性化領域の一般的な分布から得られる (例えば Awh+1996, Hickok+2003, Indefrey&Levelt)。
帯状の領域 (上側頭溝) は，音素レベルの表現を支援すると思われる領域。
</div>
</center>

<center>
<img src="/2023assets/2004Hickok_dorsal_ventral_language_fig2ja.svg" width="77%">
<div style="text-align:left;width:88%;color:teal">

<!--図 2. -->
部分語彙分割能力を支える系と聴覚理解能力との関係の模式図。
これらの能力間の解離が観察されるのは，損傷や機能的脳画像研究が分岐点以降の系の一部に影響を与えた場合であり，損傷や機能的脳画像が系の初期の共有構成要素を対象とした場合は，これらの能力間にある程度の相関があることが予測される。
</div></center>

<center>
<img src="/assets/LNCS2766_Chapter_2_fig2_4.jpg" width="77%"><br/>
Behnke2003 より
</center>

> 同様の 2 経路による処理は 聴覚 (Romanski+1999) や 触覚 (Reed+2005) でも発見されている。

発展的な話題としては，このような 2 種類の処理経路は，処理される情報の種類の問題ではないくて，機能に関与した区別であるとの仮説もある。

* 腹側経路は物体に関する情報の知覚 (知覚のための視覚)
* 背側経路は行動を導くための情報処理 (行動のための視覚)

さらに，背側経路は 背外側経路 (dorsolateral) と背中側経路 (dorsomedial) に細分化できることが示唆されている (Binkofski&Buxbaum2013, Grafton2010, Rizzolatti&Matelli2003)。

* 背中側経路は V6A と内側頭頂内溝 を介して背側前頭前皮質（PMd）へ. 把持に関連する情報を統合する (Davare+2007, Davare+2010, Tunik+2005)

最近では，これら 2 つの 副回路が 行動によって要求されるオンライン制御の程度に応じて相互作用することも発見されている (Grol+2007, Verhagen+2013)。

<!--
## 実習

##  人工ニューロン

<center>
<img src="/assets/neuron.png" style="width:49%"><br>

<img src="/assets/neuron_model.jpeg" style="width:49%"><br/>
</center>



## パーセプトロンの学習

$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
パーセプトロン perceptron は 3 層の階層型ネットワークでそれぞれ
S(sensory layer), A(associative layer), R(response layer) と呼ぶ。
$S\rightarrow A \rightarrow R$ のうち パーセプトロンの本質的な部分は
$A\rightarrow R$ の間の学習にある。

入力パターンに $P^+$ と $P^-$ とがある。
パーセプトロンは $P^+$ が入力されたとき $1$, $P^-$ のとき $0$ を出力する
機械である。
出力層($R$) の $i$ 番目のニューロンへの入力(膜電位の変化) $u_i$は
\begin{equation}
 u_i = \sum_j w_{ij}x_j - \theta_i = \left(w\right)_i\cdot\left(x\right)_i-\theta_i.\label{eq1}
\end{equation}
ここで中間層($A$)の $j$ 番目のニューロンの出力 $y_i$とこのニューロンとの
結合係数を$w_{ij}$、しきい値を$\theta_i$ とした。
このニューロンの出力$y_i$(活動電位、スパイク)は、

\begin{equation}
y_i = \lceil u_i\rceil
\qquad\left\{
\begin{array}{ll}
 1 & \mbox{if $u_i \ge 0$,}\\
 0 & \mbox{otherwize}
\end{array} \right.
\end{equation}

と表される。

式(\ref{eq1})の意味を理解するために以下の図を参照 -->
