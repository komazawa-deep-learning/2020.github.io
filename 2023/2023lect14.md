---
title: "第 14 回"
author: "浅川 伸一"
layout: default
---
<link href="/css/asamarkdown.css" rel="stylesheet">

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 14/Jul/2023<br/>
Appache 2.0 license<br/>
</div>

<div class="figure figcenter">
<img src="/2023assets/generative-overview.png" width="66%">
<div class="figcaption" style="width:66%">
生成モデル
</div></div>

<!--
#### ローカルファイルのアップロード

自身の PC や タブレット端末にあるファイルを Colaboratory 環境にアップロードする際には，以下の処理を実行します。

<div class="code" style="width:49%">
from google.colab import files<br/>
uploaded = files.upload()
</div>

反対に結果を自身の PC やタブレット端末にダウンロードする場合には，以下のコマンドを実行してください。
<div class="code" style="width:49%">
from google.colab import files<br/>
files.download('ファイル名')
</div> -->

# キーワード

- EM アルゴリズム Estimation Maximization Algorithm
- 変分自己符号化器モデル VAE: Variational Auto-Encoders
- 情報量 Entropy
- カルバック・ライブラー・ダイバージェンス
- Wake-Sleep アルゴリズム，Helmholtz マシン
- 変分下限 (ELBO: Evidence Lower BOund) [ビデオ <img src="/2023assets/youtube-svgrepo-com.svg" width="3%">](https://www.youtube.com/watch?v=jugUBL4rEIM){:target="_blank"}
- 拡散モデル diffusion models

# 実習

* [Stable diffusion <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0707stable_diffusion.ipynb){:target="_blank"}
* [VAE <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_vae_demo.ipynb#scrollTo=GC8fpkk6cFbw){:target="_blank"}
* [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"}
<!-- - [DeepDream 実習 <img src="/assets/colab_icon.svg"> ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb){:target="_blank"} -->
<!-- - [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb) -->
<!-- - [CAM 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}-->
<!-- - [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}-->
<!-- - [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS)-->
<!--* [画像認識 PyTorch の基礎編  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb){:target="_blank"} -->
* [CNN 畳み込み層の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}
<!-- * [3 層パーセプトロンと確率的勾配降下法のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}-->
<!-- * [ニューラルネットワークモデルの定義 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1028komazawa_neural_networks_primer.ipynb)-->


## 転移学習 transfer learning

**転移学習** transfer learning は機械学習分野のみならず，ロボット工学や実応用の分野でも応用が進められている。
シミュレーションと現実との間隙をどのように埋めるのかという大きな問題に関連する。
一方で，転移学習と **ファインチューニング** や **領域適応** domain adaptation の区別がなされる。

転移学習とは 課題 A を用いて訓練したモデルに対して，別の課題 B に適用することを指す。
DNN では転移学習は頻用される。
イメージネットで画像分類を学習したネットワークに対して，例えば顔認識を学習させるような場合である。

PyTorch のチュートリアルなどでは，学習済のネットワークに対して，最終層 (全結合層) を入れ替えて別の課題を訓練することを転移学習と呼ぶ。
このとき，最終直下層と出力層との結合を学習させ，その他の下位層の結合は固定し，訓練しない。
一方で，下位層まで含めて全結合を訓練させる場合を，**微調整 (fine tuning ファインチューニング)** と呼び，区別している。

<div align="center" style="width:99%">
<img src="/assets/2019Ruder_hard_parameter_sharing_p48.jpg" style="width:44%">
<img src="/assets/2019Ruder_soft_parameter_sharing_p49.jpg" style="width:44%"><br/>
左: ハードパラメータ共有: 転移学習,  右: ソフトパラメータ共有: ファインチューニング
</div>

<div align="center" style="width:29%">
<img src="/assets/2017Li_Deeper_Broader_fig1ja.svg" style="width:84%"><br/>
</div>

# 3. 転移学習の定義

* 転移学習には，領域 (domain) と課題  (task) という概念がある。
* 領域は，特徴空間 $\mathcal{X}$ と特徴空間上の周辺確率分布 $P(X)$ からなり，$X = {x_{1}, \cdots, x_{n}} \in \mathcal{X}$ である。
* 文書分類課題では $\mathcal{X}$ は全ての文書表現の空間，$x_{i}$ はある文書に対応するベクトル，$X$ は学習に用いた文書サンプルなどとなる。
* 課題 $\mathcal{D} = {\mathcal{X},P(X)}$ は，ラベル空間 $\mathcal{Y}$ と条件付き確率分布 $P(Y\vert X)$ からなり，$x_{i}\in X$, $y_{i}\in Y$ の組からなる学習データを用いて学習される。

<!-- Given a domain, $\mathcal{D} = \left\{\mathcal{X},P(X)\right\}$, a task $\mathcal{T}$ consists of a label space $\mathcal{Y}$ and a conditional probability distribution $P(Y\vert X)$ that is typically learned from the training data consisting of pairs $x_{i}\in X$ and $y_{i}\in \mathcal{Y}$.
In our document classification example, $\mathcal{Y}$ is the set of all labels, i.e. *True*, *False* and $y_i$ is either *True* or *False*.-->

ソース領域 $\mathcal{D}_ {S}$ とそれに対応するソース課題 $\mathcal{T}_ {S}$，およびターゲット領域 $\mathcal{D}_ {T}$ とターゲット課題 $\mathcal{T}_ {T}$ が与えられたとき，
ソース領域 $\mathcal{D}_ {S}$ とターゲット課題 $\mathcal{T}_ {S}$ は，ターゲット課題とターゲット領域 $\mathcal{T}_ {T}$ に対応するソース課題とターゲット課題の両方を含む。
ここで，転移学習の目的は $\mathcal{D}_ {T}$ の条件付き確率分布 $P(Y_{T}\vert X_{T})$ を学習することである。
ここで $\mathcal{D}_ {S}$ と $\mathcal{T}_ {S}$ から得られる情報は $\mathcal{D}_ {S}\neq\mathcal{D}_ {T}$ または $\mathcal{T}_ {S}\neq \mathcal{T}_ {T}$ の場合である。
多くの場合，ラベル付けされた対象例は，ラベル付けされた元例より指数関数的に少ない限られた数しか利用できないと仮定する。
<!-- Given a source domain $\mathcal{D}_ {S}$, a corresponding source task $\mathcal{T}_ {S}$, as well as a target domain $\mathcal{D}_ {T}$ and a target task $\mathcal{T}_ {T}$, the objective of transfer learning now is to enable us to learn the target conditional probability distribution $P(Y_{T}\vert X_{T})$ in $\mathcal{D}_ {T}$ with the information gained from $\mathcal{D}_ {S}$ and $\mathcal{T}_ {S}$ where $\mathcal{D}_ {S}\neq \mathcal{D}_ {T}$ or $\mathcal{T}_ {S} \neq \mathcal{T}_ {T}$.
In most cases, a limited number of labeled target examples, which is exponentially smaller than the number of labeled source examples are assumed to be available. -->

<!-- 領域 $\mathcal{D}$ と課題 $\mathcal{T}$ は共にタプルとして定義されるため，これらの不等式は 4 つの転移学習シナリオを生じさせ，以下で議論する。 -->

1. $\mathcal{X}_ {S}\neq\mathcal{X}_ {T}$.
例えば，文書が 2 つの異なる言語で書かれているなど，ソース領域とターゲット領域の特徴空間が異なる場合。
自然言語処理の文脈では，一般に異言語間適応と呼ばれる。
<!-- The feature spaces of the source and target domain are different, e.g. the documents are written in two different languages.
In the context of natural language processing, this is generally referred to as cross-lingual adaptation. -->
2. $P(X_{S})\neq P(X_{T})$.
原文領域と訳文領域の周辺確率分布が異なる場合，例えば，異なる話題について議論している文書がある場合，原文領域と訳文領域の周辺確率分布は異なる。
このようなシナリオは一般にドメイン適応と呼ばれる。
<!-- The marginal probability distributions of source and target domain are different, e.g. the documents discuss different topics.
This scenario is generally known as domain adaptation. -->
3. $\mathcal{Y}_ {S}\neq\mathcal{Y}_ {T}$.
2 つの課題のラベル空間が異なる場合。例えば，ターゲット課題では文書に異なるラベルを割り当てる必要がある。
実際には 2 つの異なる課題が異なるラベル空間を持ちながら，全く同じ条件付き確率分布を持つことは極めて稀であるため，これは通常 4 で発生する。
<!-- The label spaces between the two tasks are different, e.g. documents need to be assigned different labels in the target task.
In practice, this scenario usually occurs with scenario 4, as it is extremely rare for two different tasks to have different label spaces, but exactly the same conditional probability distributions. -->
4. $P(Y_{S}\vert X_{S})\neq P(Y_{T}\vert X_{T})$.
原文と訳文の条件付き確率分布が異なる，例えば，原文と訳文がクラスに関してアンバランスである場合。
このようなシナリオは実際にはよくあることである<!-- ，オーバーサンプリング，アンダーサンプリング，あるいは SMOTE [7] などのアプローチが広く使われている。 -->


### 残差ネット (ResNet, He et. al, 2015)

<center>
<img src='/assets/ResNet_Fig2.svg' style='width:19%'>
<img src='/assets/2015ResNet30.svg' style='width:66%'><br>
He (2015) より
</center>

### Fast R-CNN と Faster R-CNN (2014)

- R-CNN によって，位置 where 情報と 物体 what 情報 とを多層畳み込みニューラルネットワークで表現する試みが，発展。実時間で物体の切り出しと認識とが行えるようになった。[Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf){:target="_blank"}, [YOLO](https://arxiv.org/pdf/1506.02640.pdf){:target="_blank"}, [SSD](https://arxiv.org/pdf/1512.02325.pdf){:target="_blank"},

<center>
<img src="/assets/2015Fast_R-CNN_Fig1.svg" width="49%">
<img src="/assets/2015Faster_RCNN_RPN.svg" width="44%"><br/>
左: Fast R-CNN の模式図。
右: Faster RCNN の領域提案ネットワーク
</center>

## エントロピー Entropy (あるいは，情報量 Information Measure)
<!-- Srihari slides https://cedar.buffalo.edu/~srihari/CSE574/ -->

* 離散的確率変数 $x$ の特定の値を観測したとき，どの程度の情報を受け取ることができるかを示す量。
* `驚き surprise` の程度を表す情報の量<!--Amount of information is degree of surprise-->
    * 確実性 certainity は，情報がないことを言う。
    * ある事象が起こる可能性が低い場合には，その情報が起こった時に，より多くの情報が得られると考える。
* 確率分布 $p(x)$ とそのときの量 $h(x)$ に依存する。
* 無関係な 2 つの事象 $x$ と $y$ があるとき，$h(x,y) = h(x)+h(y)$ としたい。
* したがって $h(x)=-\log_{2}p(x)$ を選ぶ。
    * 負は情報量が正であることを保証するために付与されている
* 平均的な情報伝達量は $p(x)$ に対する期待値であり，これがエントロピーと呼ばれる量である。


- 情報量: 確率変数 $x$ のサプライズ量
  - まれにしか起こらない事象が起こった場合には情報量は大きい。<strong>ニュースになる</strong>
  - 必ず起こることが起こっても情報量は小さい。<strong>ニュースにならない</strong>

$$
H(x)=-\sum_x p(x)\log_2p(x)
$$
- マイナスをつけるのは正の値にするため
サプライズ量の平均: 平均エントロピー


一方情報論的エントロピー $H$ の定義は事象 $A$ の起こる確率を $P(A)$ とすれば

$$
H(A) = - \sum_{A\in\Omega} P(A) \log P(A)
$$

確率の制約，及び，平均と分散に関する制約条件を以下のように記述:

- $\displaystyle\int p\left(x\right)\;dx =1$ : 確率
- $\displaystyle\int xp\left(x\right)\;dx =\mu$ : 平均
- $\displaystyle\int \left(x-\mu\right)^2p\left(x\right)\;dx=\sigma^2$ : 分散
- ラグランジェ乗数を使って制約条件下での最大化

$$
L(x,\lambda_1,\lambda_2,\lambda_3)=-\int p\left(x\right)\log p\left(x\right)\;dx + \lambda_1\left(\int p\left(x\right)\;dx-1\right) + \lambda_2\left(\int xp\left(x\right)\;dx-\mu\right)+\lambda_3\left(\int\left(x-\mu\right)^2p\left(x\right)\;dx-\sigma^2\right)
$$

各変数で微分して 0 と置き，整理:

$$
p\left(x\right)=\exp\left(-1+\lambda_1+\lambda_2x+\lambda_3\left(x-\mu\right)^2\right)
$$

- 以上より連続量の最大エントロピーを与える確率分布はガウス分布となる


* (自由エネルギーの最小化) = (予測誤差を最小化するように信念を書き換え予測を最適化)+(予測誤差)を最小化するような行動をとる)
* (自由エネルギー) = (内部エネルギー)-(エントロピー)

### カルバック・ライブラー・ダイバージェンス，(KL ダイバージェンス)

カルバック・ライブラーダイバージェンスは 2 つの確率密度 $p$, $q$ の乖離を表す指標である。
教科書によっては，カルバック・ライブラーの偽距離と記載されている場合もある。
また，表記として $KL\left(p\|\| q\right)$ あるいは $D_{KL}\left(p\|\| q\right)$ と表記する文献も存在する。

カルバック・ライブラーダイバージェンスは，非対称であることに注意が必要である。
すなわち $KL[p\|q] \ne KL[q\|p]$ である。
カルバック・ライブラーのダイバージェンスの非対称性は，定義を見れば納得できる。

$$ KL\left(p\|q\right)= - \int p \log\left(\frac{p}{q}\right)\;dp = -\left[\int p\log p\;dp + \int p\log q\;dp\right] $$

上式最右辺，第一項は，エントロピーの定義式であり，分布 $p$ の負の対数の平均である。
一方，上式最右辺第二項は，分布 $q$ を，$q$ の確率密度を使って平均を求めている。
このため，$\log q$ に大きな値を取る領域や部分があっても，$p$ が 0 に近ければ，両分布の乖離度合いとして計算されないことを意味している。
KL ダイバージェンスの非対称性を説明する下図に示す。

青い曲線は真の事後分布とする。
仮に双峰性の分布であると考える。
緑の分布は最適化を介して青い密度に適合させる変分近似による分布を表すものとする。
これは フォワード KL と呼ばれている。
図左のように，双峰性の真の分布を単峰性の分布で近似することを考える。
このとき，一方の峰に当てはまるように調整すると，もう一方の峰の値についての当てはまりが悪くなり結果として右下図のような裾野の広い分布を得ることになる。

反対に，緑の単峰性の分布を青の双峰性の分布で近似しようとする リバースKL を考える。
このとき基準となる真の分布である単峰性の分布の確率密度がほとんど 0 の領域では，推定する分布がどのような値を取ろうとも KL ダイバージェンス の値に影響を与えないため，いずれか一方の峰が真の分布と重なるような値を得ることになる。

<center>
<img src="/assets/forward-KL.png" width="44%">
<img src="/assets/reverse-KL.png" width="44%">
<div class="figcaption">
KL ダイバージェンスの非対称性。
フォーワード KL （左）とリバース KL (右)。
<!-- \url{https://blog.evjang.com/2016/08/variational-bayes.html} [A Beginner's Guide to Variational Methods: Mean-Field Approximation]より -->
</div></center>

大抵の場合，現実は複雑(怪奇) で（図中の青色で描かれた分布），モデル (図中の緑で描かれた分布) は素朴で単純であると考えられる。
図左は，現実が双峰性分布でモデルが単峰性分布のときに，現実(青色)からみたモデル(緑色)の乖離であるから，
現実（青）の存在する領域に，モデルが存在しない状況 （左上図）では KL ダイバージェンスは大きく，したがって，両分布の乖離は大きくなる。
したがって，現実を最もよく推定することを試みた場合 (左下図) モデル（の推定）は，裾野の広い分布と推定することとなる。

一方，単峰性分布モデル(緑)から，双峰性分布である現実（青）を見た場合，モデルと現実があっていない状況(右上図)になる。
この状態で，モデルから，無理やり現実を推定しようとする リバース KL (右下図) では，現実を最もよく推定すると，
双峰性分布の，どちらかのピークと重なる形で，モデル（緑色）は，現実 （青色）を推定してしまうこととなる。

# 物理学におけるエントロピー <!--Physics view of Entropy-->
- $N$ 個の物質が $i$ 個の状態，各状態には $n_i$ 個の物質
<!--- $N$ objects into bins so that $n_i$ are in $i^{th}$ bin where $\sum_in_i=N$-->
<!--- Number of different ways of allocating objects to bins-->
- $N$ 個の物質を全て並べる: $N\cdot(N-1)\cdots2\cdot1=N!$<!-- であればways to choose first, $N-1$ ways for second leads to $N\cdot(N-1)\cdots2\cdot1=N!$-->
- 各状態の中では物質の順序は問わないことにする<!--We don’t distinguish between rearrangements within each bin-->
<!--    - In $i^{th}$ bin there are $n_i!$ ways of reordering objects-->
- 総数 $N$ 個の物質を $n_i$ に分ける場合の組み合わせ: $W=\frac{N!}{\prod_{i}n_{i}!}$
<!--  - Total number of ways of allocating $N$ objects to bins is $W=\frac{N!}{\prod_{i}n_{i}!}$-->
<!--    - Called Multiplicity (also weight of macrostate)-->
<!-- Entropy: scaled log of multiplicity -->
- <strong>エントロピーの定義</strong>

$$
H=\frac{1}{N}\ln W=\frac{1}{N}\ln N!-\frac{1}{N}\sum_i\ln n_i!
$$

- スターリングの公式 $N! \approx N\log N-N$ を用いて
$$
H=-\lim_{N\rightarrow\infty}\sum_i\left(\frac{n_i}{N}\right)\ln\left(\frac{n_!}{N}\right)=-\sum_ip_i\ln p_i
$$
- 全体の分布 $\displaystyle\frac{n_i}{N}$ をマクロステート
- 各状態をミクロステート $n_i$ を

## 連続系のエントロピー <!--Entropy with Continuous Variable-->
<!-- - Divide $x$ into bins of width $\Delta$-->
<!-- - For each bin there must exist a value $x_i$ such that -->
<!-- - Gives a discrete distribution with probabilities $p(x_i)\Delta$ -->
- 離散量 $p(x_i)\Delta$ を考えて $\Delta\rightarrow0$ を考える:
  - $\displaystyle\int_{i\Delta}^{\left(i+1\right)\Delta}p(x)d(x)=p(x_i)\Delta$
- <strong>連続系のエントロピー</strong><!--Entropy -->
$$
H_{\Delta}=-\sum_ip(x_i)\Delta\log\left(p(x_i)\Delta\right)=-\sum_ip(x_i)\Delta\log(x_i)-\Delta\log\Delta
$$

<!-- - Omit the second term and consider the limit $\Delta\rightarrow0$-->
- $\Delta\rightarrow0$ の極限を考えれば:<!--第2項を無視すれば:-->
$$
H_\Delta=-\int p(x) \log p(x)\,dx
$$
<!-- - Known as Differential Entropy-->
- 連続系と離散系のエントロピーは $\Delta\log\Delta$ だけ異なる
<!-- - Discrete and Continuous forms of entropy differ by quantity $\log\Delta$ which diverges-->
<!-- - Reflects to specify continuous variable very precisely requires a large no of bits-->

## 連続系のエントロピーを最大化する分布
<!--
# Entropy with Multiple Continuous Variables
- Differential Entropy for multiple continuous variables
$$
H[x]=-\int p(x)\log p(x)\;dx
$$
- For what distribution is differential entropy maximized?
  - For discrete distribution, it is uniform
  - For continuous, it is Gaussian
-->
- どのような分布が連続系のエントロピーを最大化するか？
  - 離散系では一様分布
  - 連続系では?
$$
H[x]=-\int p(x)\log p(x)\;dx
$$


## 汎関数としてのエントロピー <!--Entropy as a Functional-->
- 通常の関数: 微分 := スカラを入力として，スカラを返す関数(演算子)
- 汎関数: 関数を入力としてスカラを返す関数(演算子)
- 機械学習における汎関数の例: スカラ値を返すエントロピー $H[p(x)]$ を最大化
  - <strong>変分原理</strong>あるいは<strong>変分推論</strong>

<!--
- Ordinary calculus deals with functions
- A functional is an operator that takes a function as input and returns a scalar
- A widely used functional in machine learning is entropy $H\BRc{p(x)}$ which is a scalar quantity
- We are interested in the maxima and minima of functionals analogous to those for functions
  - Called calculus of variations
-->

## Maximizing a Functional
- 汎関数: 関数からスカラへの写像
  - 最大値を与える関数を探す
  - 制約付の最大化(最小化)
  - <strong>ラグランジアン</strong>の利用

<!-- - 汎関数: mapping from set of functions to real value
- For what function is it maximized?
- Finding shortest curve length between two points on a sphere (geodesic)
  - With no constraints it is a straight line
  - When constrained to lie on a surface solution is less obvious– may be several
- Constraints incorporated using Lagrangian
-->

## エントロピーの最大化<!--Maximizing Differential Entropy-->
- 確率の制約，及び，平均と分散に関する制約条件を以下のように記述:<!--Assuming constraints on first and second moments of $p(x)$ as well as normalization-->
  - $\displaystyle\int p(x)\;dx =1$ : 確率
  - $\displaystyle\int xp(x)\;dx =\mu$ : 平均
  - $\displaystyle\int (x-\mu)^2 p(x)\;dx=\sigma^2$ : 分散
- ラグランジェ乗数を使って制約条件下での最大化<<!--Constrained maximization is performed using Lagrangian multipliers. Maximize following functional w.r.t $p(x)$:-->
<!--- Constrained maximization is performed using Lagrangian multipliers. Maximize following functional w.r.t $p(x)$:-->

$$
-\int p(x)\log p(x)\,dx + \lambda_1\left(\int p(x)\;dx-1\right) + \lambda_2\left(\int xp(x)\;dx-\mu\right) +\lambda_3\left(\int(x-\mu)^2p(x)\;dx-\sigma^2\right)
$$
<!--- Using the calculus of variations derivative of functional is set to zero giving-->
各変数で微分して0と置き，整理:
$$
p(x)=\exp\left(-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\right)
$$
  - 以上より連続量の最大エントロピーを与える確率分布はガウス分布となる
<!--  - Backsubstituting into three constraint equations leads to the result that distribution that maxi
mizes differential entropy is Gaussian-->

## Differential Entropy of Gaussian
<!--- Distribution that maximizes Differential Entropy is Gaussian-->
$$
p(x)=\frac{1}{\left(2\pi\sigma^2\right)^{1/2}} e^{-\left(\frac{x-\mu}{\sigma}\right)^2}
$$
- このとき最大エントロピーは以下:
<!-- - Value of maximum entropy is-->
$$
H[x]=\frac{1}{2}\left(1+\log\left(2\pi\sigma^2\right)\right)
$$

- 分散が大きくなればエントロピーは増大する
- 離散系のエントロピーとは異なり，連続系のエントロピーは $\sigma^2<\frac{1}{2}\pi e$ のとき，<strong>負</strong>となる

<!--
- Entropy increases as variance increases
- Differential entropy, unlike discrete entropy, can be negative for $\sigma^2<\frac{1}{2}\pi e$
-->

# 条件付きエントロピー Conditional Entropy
- 同時確率 $p(x,y)$ に対して
$$
H[x,y]=-\int\int p(x,y)\log p(x,y)\;dx\;dy
$$
<!--  - We draw pairs of values of $x$ and $y$-->
- $x$ が所与のとき<strong>条件付きエントロピー</strong>
$$
H[y\vert x]=-\int p(y\vert x)\log p(y\vert x)\;dy
$$

- さらに以下の関係がある
$$
H[x,y] = H[y\vert x] + H[x]
$$

<!--
  - If value of $x$ is already known, additional information to specify corresponding value of $y$ is $-\log p\of{y\given{x}}$
- Average additional information needed to specify $y$ is the conditional entropy
- By product rule $H\of{x,y} = H\of{y\given{x}} + H\of{x}$
-->
<!--
  - where $H\of{x,y}$ is differential entropy of $p\of{x,y}$
  - $H\of{x}$ is differential entropy of $p(x)$
  - Information needed to describe $x$ and $y$ is given by
  - information needed to describe $x$ plus additional information needed to specify $y$ given $x$
-->


<!--
- If we have joint distribution $p\of{x,y}$
  - We draw pairs of values of $x$ and $y$
  - If value of $x$ is already known, additional information to specify corresponding value of $y$ is $-\log p\of{y\given{x}}$
- Average additional information needed to specify $y$ is the conditional entropy
- By product rule $H\of{x,y} = H\of{y\given{x}} + H\of{x}$
  - where $H\of{x,y}$ is differential entropy of $p\of{x,y}$
  - $H\of{x}$ is differential entropy of $p\of{x}$
  - Information needed to describe $x$ and $y$ is given by
  - information needed to describe $x$ plus additional information needed to specify $y$ given $x$
-->

<!-- フリストンの言う自由エネルギーとは，ヘルムホルツの自由エネルギーを脳神経系に当てはめた仮説。

<p align="center">
力学的エネルギー = 運動エネルギー + 位置エネルギー(ポテンシャル)
</p>

$$
E = K + U\\
E = \frac{1}{2}mv^2 + mgh
$$ -->


- 統計物理学: 巨視的な物体，すなわち莫大な数の個別的な粒子，原子や分子，からなる物体のふるまいやっ性質を支配している特別な型の法則性を研究する学問分野
- [熱力学第一法則 エネルギー保存則](https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC%E4%BF%9D%E5%AD%98%E3%81%AE%E6%B3%95%E5%89%87)
- [熱力学第二法則 エントロピーは増大する](https://ja.wikipedia.org/wiki/%E7%86%B1%E5%8A%9B%E5%AD%A6%E7%AC%AC%E4%BA%8C%E6%B3%95%E5%89%87)

## エントロピー
熱力学的エントロピーと情報論的エントロピーが存在するが式は同じである。

## (熱)力学的エントロピー

ある位置 $i$ にある粒子があるとする。各位置にそれぞれ $n_i$ の粒子が存在するとする。
はおのおの区別できないものとすれば，全ての状態は何通りあるかを表す式は次式となる:

$$
W=\frac{N!}{\prod_i n_i!}
$$

エントロピーとはこの状態の数 $W$ の負の対数である.
$$
H=\frac{1}{N}\log W=\frac{1}{N}\log N!-\frac{1}{N}\sum_i\log n_i!
$$

以下のスターリングの近似公式 ($\log N!\approx N\log N - N$) を用いると以下の式を得る

$$
H=-\lim_{N\rightarrow\infty}\sum_i\left(\frac{n_i!}{N}\right)
\log\left(\frac{n_i!}{N}\right)=-\sum_i p_i\log p_i
$$

$$
S = k \ln W
$$

ここで，$k$ は[ボルツマン定数](https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AB%E3%83%84%E3%83%9E%E3%83%B3%E5%AE%9A%E6%95%B0)であり，$W$ は系の微視的な状
態を表す。
一方で統計力学におけるエントロピーの定義は以下の通り:

$$
S=k\left<\ln\frac{1}{p(\omega)}\right>=-k\sum_{\omega}p(\omega)\ln p(\omega)
$$

上式中 $\left<\;\right>$ は[アンサンブル平均](https://ja.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E9%9B%86%E5%9B%A3)と呼ばれ，巨視的に同条件下にある力学系が
系を構成する分子間に相関がなければ，系は微視的にはすべての状態をとりうることから，巨視的状態において統計的に系はすべての状態をとりうることが仮定される。
系の時間的平均と空間間的平均が同じであると仮定できるときその系は**エルゴード性**を有するという。
エルゴード性により時間平均と空間平均とを区別しないで(しばしば意図的に混乱させて)用いることが行われる。

<!--
## 情報量 Information Measure
Srihari slides https://cedar.buffalo.edu/~srihari/CSE574/

- 離散変数 $x How much information is received when we observe a specific value for a discrete random ariable $x$?
- Amount of information is degree of surprise
    - Certain means no information
    - More information when event is unlikely
- Depends on probability distribution $p\of{x}$, a quantity $h\of{x}$
- If there are two unrelated events $x$ and $y$ we want $h\of{x,y}=h\of{x}+h\of{y}$
- Thus we choose $h\of{x}=-\log_2p\of{x}$
    - Negative assures that information measure is positive
- Average amount of information transmitted is the expectation w.r.t $p\of{x}$ refered to as entropy

- 情報量: 確率変数 $x$ のサプライズ量
  - まれにしか起こらない事象が起こった場合には情報量は大きい。<strong>ニュースになる</strong>
  - 必ず起こることが起こっても情報量は小さい。<strong>ニュースにならない</strong>

$$
H\of{x}=-\sum_x p\of{x}\log_2p\of{x}
$$
- マイナスをつけるのは正の値にするため
サプライズ量の平均: 平均エントロピ


一方情報論的エントロピー $H$ の定義は事象 $A$ の起こる確率を $P(A)$ とすれば

$$
H(A) = - \sum_{A\in\Omega} P(A) \log P(A)
$$


確率の制約，及び，平均と分散に関する制約条件を以下のように記述:

- $\displaystyle\int p\left(x\right)\;dx =1$ : 確率
- $\displaystyle\int xp\left(x\right)\;dx =\mu$ : 平均
- $\displaystyle\int \left(x-\mu\right)^2p\left(x\right)\;dx=\sigma^2$ : 分散
- ラグランジェ乗数を使って制約条件下での最大化

$$
L(x,\lambda_1,\lambda_2,\lambda_3)=-\int p\left(x\right)\log p\left(x\right)\;dx + \lambda_1\left(\int p\left(x\right)\;dx-1\right) + \lambda_2\left(\int
 xp\left(x\right)\;dx-\mu\right)+\lambda_3\left(\int\left(x-\mu\right)^2p\left(x\right)\;dx-\sigma^2\right)
$$

各変数で微分して0と置き，整理:

$$
p\left(x\right)=\exp\left(-1+\lambda_1+\lambda_2x+\lambda_3\left(x-\mu\right)^2\right)
$$
- 以上より連続量の最大エントロピーを与える確率分布はガウス分布となる-->

<!-- - [自由エネルギー原理](./friston_FEP)<br> -->

ヘルムホルツの自由エネルギー: $F = U - TS $

$F$ はヘルムホルツの自由エネルギー，$T$ は温度，$S$ はエントロピー。<!-- <https://kotobank.jp/word/%E8%87%AA%E7%94%B1%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC-76745>-->

- 熱力学の第一法則 エネルギー保存則
- 熱力学の第二法則

ギブスの自由エネルギー: $ G = F + pV$

* [変分自己符号化モデル 説明文](/vae_from2020gtext.pdf)
* [https://cbmm.mit.edu/video/brain-object-recognition-high-performing-shallow-recurrent-anns](https://cbmm.mit.edu/video/brain-object-recognition-high-performing-shallow-recurrent-anns)


---


<!-- from `~/study/2020blog.evjang.com/2016/08/2016EricJang_vb_basic_ja.md`-->
変分ベイズ法 (VB: Variational Bayeisan Methods) は，統計的機械学習で非常によく使われる手法の一群である。
VB 法は，統計的推論問題 (別の確率変数の値から確率変数の値を推定する問題) を最適化問題(ある目的関数を最小化するパラメータ値を求める問題) として書き直すことができる。
<!-- Variational Bayeisan (VB) Methods are a family of techniques that are very popular in statistical Machine Learning.
VB methods allow us to re-write *statistical inference* problems (i.e. infer the value of a random variable given the value of another random variable) as *optimization *problems (i.e. find the parameter values that minimize some objective function). -->

この推論と最適化の二重性は，最新の最適化アルゴリズムを使って統計的機械学習の問題を解く (逆に，統計的手法を使って関数を最小化する) ことを可能にするため強力である。
<!-- This inference-optimization duality is powerful because it allows us to use the latest-and-greatest optimization algorithms to solve statistical Machine Learning problems (and vice versa, minimize functions using statistical techniques). -->

ここでは，変分法について，最適化の目的を導出する。
目的は **変分下限 Variational Lower Bound** としても知られ [Variational Autoencoders](https://arxiv.org/abs/1312.6114) で使われているものと全く同じものである。

系を確率変数の集まりとしてモデル化すると，ある観察変数 ($X$) と，観察不可能な潜在変数 ($Z$) を仮定する。

[ベイズの定理](https://en.wikipedia.org/wiki/Bayes%27_theorem) は，任意の確率変数の組の間の一般的な関係を与える。
<!-- [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) gives us a general relationship between any pair of random variables: -->

$$
p(Z|X) = \frac{p(X|Z)p(Z)}{p(X)}
$$

これの様々な断片に通称が関連付けられている。
<!-- The various pieces of this are associated with common names:-->

$p(Z|X)$ は **事後確率** である。
`ある画像が与えられたとき，それが猫の画像である確率は？`
もし $z\sim P(Z|X)$ から標本抽出できるなら，これを使って，与えられた画像が猫かどうか教えてくれる猫分類器を作ることができる。
<!-- $p(Z|X)$ is the **posterior probability**: "given the image, what is the probability that this is of a cat?"
If we can sample from $z\sim P(Z|X)$, we can use this to make a cat classifier that tells us whether a given image is a cat or not. -->

$p(X\vert Z)$ は **尤度** である。
$Z$ の値が与えられたとき，この画像 $X$ がそのカテゴリに属する「確率」がどのくらいかを計算する。
もし $x\sim P(X\vert Z)$ から標本抽出できるなら，乱数を生成するのと同じくらい簡単に猫の画像と猫以外の画像を生成することができる。
$p(Z)$ は**事前確率** と呼ばれる。

隠れ変数は [ベイズ統計学](https://en.wikipedia.org/wiki/Bayesian_statistics) の枠組みで，観測変数に付随する **事前確信** と解釈することができる。
例えば，$X$ が多変量ガウス分布であると信じる場合，隠れ変数 $Z$ はガウス分布の平均と分散を表すかもしれない。
このとき，パラメータに対する分布 $P(Z)$ は，$P(X)$ に対する **事前分布** となる。
<!-- Hidden variables can be interpreted from a [Bayesian Statistics](https://en.wikipedia.org/wiki/Bayesian_statistics) framework as *prior beliefs* attached to the observed variables.
For example, if we believe $X$ is a multivariate Gaussian, the hidden variable $Z$ might represent the mean and variance of the Gaussian distribution.
The distribution over parameters $P(Z)$ is then a *prior* distribution to $P(X)$. -->

また，$X$ と $Z$ とがどの値を表すかは，自由に選択することができる。
例えば，$Z$ の代わりに「平均，分散の立方根，あるいは  $X+Y$，ここで $Y \sim \mathcal{N}(0,1)$」などとすることも可能である。
これはやや不自然で変であるが，$P(X|Z)$ を適宜修正すれば，構造はそのまま有効である。
<!-- You are also free to choose which values $X$ and $Z$ represent.
For example, $Z$ could instead be "mean, cube root of variance, and $X+Y$ where $Y \sim \mathcal{N}(0,1)$".
This is somewhat unnatural and weird, but the structure is still valid, as long as $P(X|Z)$ is modified accordingly.-->

変数を「追加」することもできる。
$P(Z|\theta)$ は，それ自身の事前分布 $P(\theta)$ を持ち，それらもやはり事前分布を持つなど，事前分布自体が他の確率変数に依存するかもしれない。
どんなハイパーパラメータも事前分布と考えることができる。

## ELBO あるいは 変分下界 <!-- # Variational Lower Bound for Mean-field Approximation-->

変分推論の考え方は，事後推論の方法がわかっている簡単なパラメトリック分布 $Q_{\phi}\left(Z \vert X\right)$ (ガウス分布など) に対して推論を行い，パラメータ $\phi$ を調整して$P$ になるべく近づけようというものである。

<!-- The idea behind variational inference is this: let's just perform inference on an easy, parametric distribution $Q_\phi(Z | X)$ (like a Gaussian) for which we know how to do posterior inference, but adjust the parameters $\phi$ so that $Q_\phi$ is as close to $P$ as possible. -->

<!-- 青い曲線が本当の事後分布で，緑の分布が最適化によって青い密度に合わせた変分近似 (ガウシアン) である。 -->
<!-- This is visually illustrated below: the blue curve is the true posterior distribution, and the green distribution is the variational approximation (Gaussian) that we fit to the blue density via optimization. -->

<div class="figure figcenter">
<img src="/2023assets/Untitled+presentation+(2).png" width="44%">
</div>


分布が「近い」とはどういうことか？
平均場変分ベイズ (最も一般的な型) では，2 つの分布の間の距離指標として 逆 KL ダイバージェンスを用いる。
<!-- What does it mean for distributions to be "close"? Mean-field variational Bayes (the most common type) uses the Reverse KL Divergence to as the distance metric between two distributions. -->

$$
KL(Q_\phi(Z\vert X)\|P(Z\vert X)) = \sum_{z \in Z}{q_\phi(z \vert x)\log\frac{q_\phi(z \vert x)}{p(z \vert x)}}
$$

逆 KL ダイバージェンスは，$P(Z)$ を $Q_\phi(Z)$ に「歪める」ために必要な情報量 (つまり $\displaystyle\frac{1}{\log(2)}$ ビット) を測定する。
この量を $\phi$ に関して最小化したい。
<!-- Reverse KL divergence measures the amount of information (in nats, or units of $\frac{1}{\log(2)}$ bits) required to "distort" $P(Z)$ into $Q_\phi(Z)$.
We wish to minimize this quantity with respect to $\phi$.-->

条件付き分布の定義から $\displaystyle p(z\vert x) = \frac{p(x,z)}{p(x)}$ となる。
この式を，もとの $KL$ 式に代入して，分配してみよう。
<!-- By definition of a conditional distribution, $p(z|x) = \frac{p(x,z)}{p(x)}$.
Let's substitute this expression into our original $KL$ expression, and then distribute: -->

$$\begin{aligned}
KL(Q||P) & = \sum_{z \in Z}{q_\phi(z|x)\log\frac{q_\phi(z|x)p(x)}{p(z,x)}} \\
& = \sum_{z \in Z}{q_\phi(z|x)\big(\log{\frac{q_\phi(z|x)}{p(z,x)}} + \log{p(x)}\big)} \\
& = \left(\sum_{z}{q_\phi(z|x)\log{\frac{q_\phi(z|x)}{p(z,x)}}}\right) + \left(\sum_{z}{\log{p(x)}q_\phi(z|x)}\right) \\
& = \left(\sum_{z}{q_\phi(z|x)\log{\frac{q_\phi(z|x)}{p(z,x)}}}\right) + \left(\log{p(x)}\sum_{z}{q_\phi(z|x)}\right) \\
& = \log{p(x)} + \left(\sum_{z}{q_\phi(z|x)\log{\frac{q_\phi(z|x)}{p(z,x)}}}\right)
\end{aligned}\tag{1}$$

$KL(Q \| P)$ を変分パラメータ $\phi$ に関して最小化するには，$\log{p(x)}$ は $\phi$ に関して固定なので，$\displaystyle\sum_{z}{q_\phi(z \vert x)} \log{\frac{q_\phi(z \vert x)}{p(z,x)}}$ を最小化すればよい。
この量を分布 $Q_\phi(Z\vert X)$ に対する期待値として書き直そう。
<!-- To minimize $KL(Q||P)$ with respect to variational parameters $\phi$, we just have to minimize $\sum_{z}{q_\phi(z|x)\log{\frac{q_\phi(z|x)}{p(z,x)}}}$, since $\log{p(x)}$ is fixed with respect to $\phi$.
Let's re-write this quantity as an expectation over the distribution $Q_\phi(Z|X)$. -->

$$\begin{aligned}
\sum_{z}{q_\phi(z|x)\log{\frac{q_\phi(z\vert x)}{p(z,x)}}} & = \mathbb{E}_{z \sim Q_\phi(Z\vert X)}\left[\log{\frac{q_\phi(z\vert x)}{p(z,x)}}\right] \\
&= \mathbb{E}_Q\left[ \log{q_\phi(z|x)} - \log{p(x,z)} \right] \\
&= \mathbb{E}_Q\left[ \log{q_\phi(z|x)} - (\log{p(x|z)} + \log(p(z))) \right] && \\
& = \mathbb{E}_Q\left[ \log{q_\phi(z|x)} - \log{p(x|z)} - \log(p(z))) \right] \\
\end{aligned}$$

これを最小化することは，この関数の負の値を **最大化** することと等価である。
<!-- Minimizing this is equivalent to *maximizing* the negation of this function: -->

$$\begin{aligned}
\max\mathcal{L} &= -\sum_{z}{q_\phi(z\vert x)\log{\frac{q_\phi(z\vert x)}{p(z,x)}}} \\
& = \mathbb{E}_Q\left[ -\log{q_\phi(z|x)} + \log{p(x\vert z)} + \log(p(z))) \right] \\
& =  \mathbb{E}_Q\left[ \log{p(x\vert z)} + \log{\frac{p(z)}{ q_\phi(z\vert x)}} \right] \\
\end{aligned}\tag{2}$$

文献では，$\mathcal{L}$ は **変分下限 variational lower bound** と呼ばれ，$p(x|z), p(z), q(z|x)$ を評価できれば，計算上，扱いやすいとされている。
さらに直感的な式が得られるように項を並べ替えることができる。
<!-- In literature, $\mathcal{L}$ is known as the *variational lower bound*, and is computationally tractable if we can evaluate $p(x|z), p(z), q(z|x)$. We can further re-arrange terms in a way that yields an intuitive formula: -->

$$\begin{aligned}
\mathcal{L} &= \mathbb{E}_Q\left[ \log{p(x|z)} + \log{\frac{p(z)}{ q_\phi(z|x)}} \right] \\
            &= \mathbb{E}_Q\left[ \log{p(x|z)} \right] + \sum_{Q}{q(z|x)\log{\frac{p(z)}{ q_\phi(z|x)}}} && \\
&= \mathbb{E}_Q\left[ \log{p(x|z)} \right] - KL(Q(Z|X)||P(Z)) && \\
\end{aligned}\tag{3}$$

標本抽出 $z \sim Q(Z\vert X)$ が観測値 $x$ を潜在符号 $z$ に変換する「符号化」過程だとすると，標本抽出 $x \sim Q(X\vert Z)$ は観測値を $z$ から復元する「復号化」過程であることがわかる。
<!-- If sampling $z \sim Q(Z|X)$ is an "encoding" process that converts an observation $x$ to latent code $z$, then sampling $x \sim Q(X|Z)$ is a "decoding" process that reconstructs the observation from $z$. -->

このことから $\mathcal{L}$ は，期待「復号化」尤度 (変分分布が $Z$の標本を $X$ の標本に復号化できる程度) と，変分近似と $Z$ に関する事前分布の間の KL ダイバージェンスの和であることがわかる。
$Q(Z|X)$ が条件付きガウス分布であるとすると，$Z$ の事前分布は平均 0，標準偏差 1 の対角ガウス分布が選ばれることが多い。
<!-- It follows that $\mathcal{L}$ is the sum of the expected "decoding" likelihood (how good our variational distribution can decode a sample of $Z$ back to a sample of $X$), plus the KL divergence between the variational approximation and the prior on $Z$.
If we assume $Q(Z|X)$ is conditionally Gaussian, then prior $Z$ is often chosen to be a diagonal Gaussian distribution with mean 0 and standard deviation 1. -->

$\mathcal{L}$ が変分下界と呼ばれるのはなぜか？
式(1) に $\mathcal{L}$ を代入し直すと，以下のようになる:
<!-- Why is $\mathcal{L}$ called the variational lower bound?
Substituting $\mathcal{L}$ back into Eq. (1), we have: -->

$$\begin{aligned}
KL(Q||P) & = \log p(x) - \mathcal{L} \\
\log p(x) & = \mathcal{L} + KL(Q||P) \\
\end{aligned}\tag{4}$$

式 (4) の意味を平たく言うと，あるデータ点 $x$ の真の分布の下での対数尤度 $p(x)$ は$\mathcal{L}$ に，その特定の値 $x$ における $Q(Z\vert X=x)$ と $P(Z\vert X=x)$ 間の距離を表す誤差項 $KL(Q \vert\vert P)$ が加わるということである。
<!-- The meaning of Eq. (4), in plain language, is that $p(x)$, the log-likelihood of a data point $x$ under the true distribution, is $\mathcal{L}$, plus an error term $KL(Q||P)$ that captures the distance between $Q(Z|X=x)$ and $P(Z|X=x)$ at that particular value of $X$.-->

$KL(Q||P) \geq 0$ なので，$\log p(x)$ は $\mathcal{L}$ より大きい必要がある。
よって，$\mathcal{L}$ は $\log p(x)$ の **下限 lower bound** である。
また，$\mathcal{L}$ は別形式で証拠下界 (ELBO) とも呼ばれる。
<!-- Since $KL(Q||P) \geq 0$, $\log p(x)$ must be greater than $\mathcal{L}$.
Therefore $\mathcal{L}$ is a *lower bound* for $\log p(x)$.
$\mathcal{L}$ is also referred to as evidence lower bound (ELBO), via the alternate formulation: -->

$$\mathcal{L} = \log p(x) - KL(Q(Z|X)||P(Z|X)) = \mathbb{E}_Q\left[ \log{p(x|z)} \right] - KL(Q(Z|X)||P(Z))$$

なお，$\mathcal{L}$ 自体には，近似事後確率と事前確率の間の KL ダイバージェンス項が含まれているので，$\log p(x)$ には合計 2 つの KL 項が含まれることになる。
<!-- Note that $\mathcal{L}$ itself contains a KL divergence term between the approximate posterior and the prior, so there are two KL terms in total in $\log p(x)$. -->

# 順方向 KL と逆方向 KL
<!-- # Forward KL vs. Reverse KL-->

KL ダイバージェンスは対称距離関数ではなく，$KL(P||Q) \neq KL(Q||P)$ ($Q \equiv P$ である場合を除く) である。
前者は「順向 KL」、後者は「逆向 KL」と呼ばれる。
では、なぜ 逆向 KLを使うのだろうか？
それは，結果として得られる導出が $p(Z|X)$ の計算方法を知っている必要があるからである。
<!-- KL divergence is *not* a symmetric distance function, i.e. $KL(P||Q) \neq KL(Q||P)$ (except when $Q \equiv P$).
The first is known as the "forward KL", while the latter is "reverse KL".
So why do we use Reverse KL?
This is because the resulting derivation would require us to know how to compute $p(Z|X)$, which is what we'd like to do in the first place. -->

[PML 教科書](https://www.cs.ubc.ca/~murphyk/MLbook/) にある Kevin Murphy の説明がとても好みなので，ここで再掲してみることにする。
<!-- I really like Kevin Murphy's explanation in the [PML textbook](https://www.cs.ubc.ca/~murphyk/MLbook/), which I shall attempt to re-phrase here: -->

まず，順方向 KL を考える。
上の導出で見たように KL は重み関数 $p(z)$ に対する「罰則」関数 $\displaystyle\log \frac{p(z)}{q(z)}$ の期待値として書くことができる。
<!-- Let's consider the forward-KL first. As we saw from the above derivations, we can write KL as the expectation of a "penalty" function $\log \frac{p(z)}{q(z)}$ over a weighing function $p(z)$. -->

$$\begin{aligned}
KL(P||Q) & = \sum_z p(z) \log \frac{p(z)}{q(z)} \\
& = \mathbb{E}_{p(z)}{\left[\log \frac{p(z)}{q(z)}\right]}\\
\end{aligned}$$

ペナルティ関数は $p(Z) > 0$ のとき，全 KL に損失を寄与する。
$p(Z) > 0$ の場合，$\lim_{q(Z) \to 0}\log\frac{p(z)}{q(z)} \to \infty$ となる。
これは，$Q(Z)$ が $P(Z)$ を「カバー」できないところでは，順向 KL が大きくなることを意味する。
<!-- The penalty function contributes loss to the total KL wherever $p(Z) > 0$.
For $p(Z) > 0$, $\lim_{q(Z) \to 0} \log \frac{p(z)}{q(z)} \to \infty$.
This means that the forward-KL will be large wherever $Q(Z)$ fails to "cover up" $P(Z)$.-->

よって，$p(z)>0$ のところでは $q(z)>0$ を確保すれば，順向 KL は最小になることがわかる。
最適化された変分分布 $Q(Z)$ は「ゼロ回避 zero-avoiding」($p(Z)$ がゼロのとき，密度がゼロを回避する) として知られている。
<!-- Therefore, the forward-KL is minimized when we ensure that $q(z) > 0$ wherever $p(z)> 0$.
The optimized variational distribution $Q(Z)$ is known as "zero-avoiding" (density avoids zero when $p(Z)$ is zero). -->

<div class="figure figcenter">
<img src="/assets/forward-KL.png" width="44%">
<!-- ![image](../assets/forward-KL.png) -->
<!-- {width="49%"} -->
</div>

逆向 KL を最小化すると，全く逆の挙動になる:
<!-- Minimizing the Reverse-KL has exactly the opposite behavior: -->

$$\begin{aligned}
KL(Q||P) & = \sum_z q(z) \log \frac{q(z)}{p(z)} \\
& = \mathbb{E}_{p(z)}{\left[\log \frac{q(z)}{p(z)}\right]}\
\end{aligned}$$

$p(Z)=0$ ならば，分母 $p(Z)=0$ であればどこでも重み付け関数 $q(Z)=0$ を確保しなければ，KL は吹き飛んでしまう。
これは「ゼロ強制 zero-forcing」と呼ばれる。
<!-- If $p(Z)=0$, we must ensure that the weighting function $q(Z)=0$ wherever denominator $p(Z)=0$, otherwise the KL blows up.
This is known as "zero-forcing": -->

<div class="figure figcenter">
<img src="/assets/reverse-KL.png" width="44%">
</div>

つまり 順向 KL を最小化すると変分分布 $Q(Z)$ が $P(Z)$ 全体を **覆う** ように「伸び」，逆向 KL を最小化すると $P(Z)$ の **下に** $Q(z)$ が「しぼむ」。
<!-- So in summary, minimizing forward-KL "stretches" your variational distribution $Q(Z)$ to cover **over** the entire $P(Z)$ like a tarp, while minimizing reverse-KL "squeezes" the $Q(Z)$ **under** $P(Z)$.-->

機械学習の問題で平均場近似を使うとき，逆 KL を使うことの意味を覚えておくことが重要である。
単峰性の分布を多峰性の分布に当てはめると，偽陰性が多くなる ($Q(Z)$ は何もないと思っていたのに，$P(Z)$ は実際に確率的な量がある) ことになる。
<!-- It's important to keep in mind the implications of using reverse-KL when using the mean-field approximation in machine learning problems.
If we are fitting a unimodal distribution to a multi-modal one, we'll end up with more false negatives (there is actually probability mass in $P(Z)$ where we think there is none in $Q(Z)$). -->

<!-- # Connections to Deep Learning-->

変分法は Deep Learning にとって本当に重要である。
<!-- 詳しくは後日記事にしますが，ここでは簡単にネタバレします。 -->
<!-- Variational methods are really important for Deep Learning.
I will elaborate more in a later post, but here's a quick spoiler: -->

1. ディープラーニングは，たくさんのデータを使った非常に大きなパラメータ空間での最適化(具体的には勾配降下法) が得意だ。
2. 変分ベイズは，統計的推論問題を最適化問題として書き直すことができる枠組みを与えてくれる。

<!-- 1. Deep learning is really good at optimization (specifically, gradient descent) over very large parameter spaces using lots of data.
2. Variational Bayes give us a framework with which we can re-write statistical inference problems as optimization problems. -->

深層学習と変分ベイズの組み合わせにより，極めて複雑な事後分布の推論が可能になる。
結果的に，変分自己符号化器のような最新の技術は，この投稿で導かれたのと全く同じ平均場変分下界を最適化する
<!-- Combining Deep learning and VB Methods allow us to perform inference on *extremely* complex posterior distributions.
As it turns out, modern techniques like Variational Autoencoders optimize the exact same mean-field variational lower-bound derived in this post! -->


<!-- ## ある日のとある Slack team の会話より
from `2017rnn_book/bayes_learning.md`

「５回に１回の割合で帽子を忘れるくせのあるＫ君が，正月に Ａ，Ｂ，Ｃ ３軒を順に年始回りをして家に帰ったとき，帽子を忘れてきたことに気がついた。２軒目の家 Ｂ に忘れてきた確率を求めよ。」  早稲田大学（１９７６年文学部）で出題された入試問題  [http://www004.upp.so-net.ne.jp/s_honma/probability/bayes.htm](http://www004.upp.so-net.ne.jp/s_honma/probability/bayes.htm) より

新：同時確率を考えると（４/5）×（1/5）×（4/5）=16/125
  忘れた確定ならABCのどっかで忘れたんだから1/3 (自分はこの考えです)
  などいろいろある問題で
  ぶっちゃけ解がわからない

浅：同時確率を計算しただけでは，帽子を忘れたことに気が付いたという事実を考慮していないことになりますよね。帽子を忘れたという事象が生起する確率は 1/5 なのだから分母は全く忘れなかったを１から引いて 1-(4/5)^3 = 61/125. すなわち帽子を忘れたという事象が生起した確率。一方分子はA宅で忘れない(4/5)でかつB宅で忘れた(1/5)事象が起きたのでこれらの積で表される(4/5 * 1/5 = 4/5^2). 以上を計算すれば (4/25)/(61/125) = 20/61. ベイズ的には帽子を忘れたという事象が生起したという事実から考慮すべき確率空間，すなわち分母が変化するので新村さんの考えた同時確率 16/125 を帽子を忘れたという事実を知ったために調整しなければいけない，ということであります。

新：ぬぬ？abcのどこかに忘れたので1/3という解答はおかしいですかね？
  20/61だとしたら
  aに忘れてる確率も，bに忘れてる確率も同様になって
  あ，違うのか。
  順番に回ったから違うのですなぁ
  納得
  となると，よく忘れ物をする人は，最後に見たすぐ後に忘れてる可能性が最も高いのか
  確かにそんな気がします

---

## 解説
帽子を忘れた場所の確率は __負の二項分布__ Negative binomial distribution に従う。負の二項分布の確率密度は以下で与えられる：

$$
NB(X=x;r,p)=\left({x+r-1}\over{x}\right)\;p^{x}\left(1-p\right)^{r}
$$

負の二項分布を，上の例題に対応させて式中の文字の意味を記せば次のようになる。$p(=4/5)$ は帽子を忘れない確率，$1-p(=1/5)$ は反対に帽子を忘れる確率である。$p^x$ は $x$ 回帽子を忘れなかったことを意味し，$(1-p)^r$ は帽子を $r$ 回忘れなかったこととなる。今回は $r=1$ であり帽子を忘れた時点で次の家を訪問してもしなくても関係なくなる。負の二項分布とは $r$ 回失敗する前に成功回数が $x$ 回である確率を表している。[Wikiptediaを参照](https://en.wikipedia.org/wiki/Negative_binomial_distribution)。

1. 最初の訪問宅である A で帽子を忘れれば $p^{0}\;(1-p)^{1}$ で $p=\displaystyle\frac{4}{5}$ を代入すれば $Pr(A)=\displaystyle\frac{1}{5}=\frac{5}{25}=\frac{25}{125}$ となる。
2. B で帽子を忘れた場合は $Pr(B)=p^{1}\;(1-p)^{1}=\displaystyle\frac{4}{5}\;\left(1-\frac{4}{5}\right)=\frac{4}{25}\times\frac{1}{5}=\frac{20}{125}$ を得る。
3. C で帽子を忘れた場合は A, B で帽子を忘れず $x=2$ (成功２回)，C で忘れるのであるから  $p^{2}\;(1-p)^{1}=\displaystyle\left(\frac{4}{5}\right)^2\times\left(\frac{1}{5}\right)=\left(\frac{16}{25}\right)\times\left(\frac{1}{5}\right)=\frac{16}{125}$

比較が容易なように上では分母を $125$ に揃えた。上記のように A で帽子を忘れた確率が最も高くなる。負の二項分布で $r=1$ の場合は，初めて失敗する $r=1$ までに連続して成功した回数が $x$ となる。ベイズ確率を計算するには上の３つの起こった確率を足し合わせた値を分母にする。

$$
\frac{\frac{20}{125}}{\frac{25}{125}+\frac{20}{125}+\frac{16}{125}}=\frac{\frac{20}{125}}{\frac{61}{125}}=\frac{20}{61}
$$

このようにどこかの家で帽子を忘れたのだから等確率で $\displaystyle\frac{1}{3}$ という直観に反して B で忘れた確率は $\displaystyle\frac{1}{3}$ より少しだけ小さい $\displaystyle\frac{20}{61}$ となる。

---

### 繰り返し忘れるとどうなるか （ベイズ更新）

等確率の原理からどの家でも忘れる確率は等しく $\displaystyle\frac{1}{3}$ は事前知識あるいは事前分布を表している。そこで上式を事前知識を加味して書けば，

$$
\frac{\frac{20}{125}\times\frac{1}{3}}{\frac{25}{125}\times\frac{1}{3}+\frac{20}{125}\times\frac{1}{3}+\frac{16}{125}\times\frac{1}{3}}=\frac{\frac{20}{125}\times\frac{1}{3}}{\frac{61}{125}\times\frac{1}{3}}=\frac{20\times\frac{1}{3}}{61\times\frac{1}{3}}=\frac{20}{61}
$$

すなわち事前分布が等しく $\displaystyle\frac{1}{3}$ であったので省略してあったのである。ところが，一度帽子を忘れたという事実を観測した場合，上に従ってそれぞれの家で忘れる確率が $\displaystyle\frac{1}{3}$ ではなくなる。すなわち，

- $Pr(A)=\displaystyle\frac{25}{61}\doteq0.410$,
- $Pr(B)=\displaystyle\frac{20}{61}\doteq0.328$,
- $Pr(C)=\displaystyle\frac{16}{61}\doteq0.262$,

であるから，事前分布としてこの値を代入すれば，B に忘れた確率は $Pr(B)$ は

$$
\frac{\frac{20}{125}\times\frac{20}{61}}{\frac{25}{125}\times\frac{25}{61}+\frac{20}{125}\times\frac{20}{61}+\frac{16}{125}\times\frac{16}{61}}
=\frac{\frac{20^2}{125\times61}}{\frac{25^2+20^2+16^2}{125\times61}}
=\frac{20^2}{25^2+20^2+16^2}=\frac{400}{625+400+256}=\frac{400}{1281}\doteq0.312
$$

として求まる。同様の計算によって次式を得る：

$$
Pr(A)=\frac{25^2}{25^2+20^2+16^2}=\frac{625}{1281}\doteq0.487
$$

$$
Pr(C)=\frac{16^2}{25^2+20^2+16^2}=\frac{256}{1281}\doteq0.201
$$

A, B, C それぞれに帽子を忘れる確率は観測のたびに変化する。数式で表現すれば以下のようになる。
$$
P(\theta_{i}|D)=\frac{P(D|\theta_{i})P(\theta_{i})}{\sum_{j}P(D|\theta_{j})P(\theta_{j})}
$$
上式はベイズ更新と呼ばれる。ニューラルネットワークなどで $\theta$ を重み係数と考え $D$ を訓練データとみなせば上式は __ベイズ学習則の定義__ であり，重み係数の更新式である。ベイズ更新とは現在の重み係数と得られたデータから重み係数を更新する方法であると解釈可能である。 -->

