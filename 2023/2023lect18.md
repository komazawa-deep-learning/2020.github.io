---
title: "第 18 回"
author: "浅川 伸一"
layout: default
---
<link href="/css/asamarkdown.css" rel="stylesheet">

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 06/Oct/2023<br/>
Appache 2.0 license<br/>
</div>

## 実習

* [先週の最小ニューラルネットワーク](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0929miniam_XOR.ipynb)
* [畳み込みニューラルネットワークの事前訓練済モデルの中間表現を可視化する](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb)
- [実習 LeNet PyTorch 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528LeNet_pytorch.ipynb){:target="_blank"}
* [実習 いくつかの画像フィルタ 特徴点検出アルゴリズム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_visit_feature_extractions_demo.ipynb){:target="_blank"}
* [実習 DOG などのフィルタと Harr 特徴による顔検出 a.k.a ビオラ＝ジョーンズ アルゴリズム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528edge_and_face_detection_algorithm_not_cnn.ipynb){:target="_blank"}


## デモ

- [グーグルによるニューラルネットワークの遊び場 (プレイグランド)](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"}

---

先週の最小ネットワークの解
<center>
<img src="/2023assets/xor.svg">
<img src="/2023assets/xor-graph.svg">
</center>


# ニューラルネットワークの歴史

## 第 1 次ニューロブーム

### 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案
(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='/assets//mcculloch.jpg' style="width:38%">
<img src='/assets//pitts.jpg' style='width:50%'><br>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br> -->
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_{i}=\phi\left(\sum_jw_{ij}x_j\right),\tag{eq:formal_neuron}
$$

ここで $y_{i}$ は $i$ 番目のニューロンの出力，$x_{j}$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$
 と $j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
形式ニューロン
</center>

### ローゼンブラット Rosenblatt のパーセプトロン

<center>
<img src='/assets/rosenblatt.jpg' style="width:49%"><br/>
フランク・ローゼンブラット
</center>

<center>
<img src='/assets/perceptron.png' style="width:74%"><br/>
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center>


<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br>
 -->
ニューロンの模式図 wikipedia より
</center>

<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->

<!--
## パーセプトロンの学習

$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
パーセプトロン perceptron は 3 層の階層型ネットワークでそれぞれ
S(sensory layer), A(associative layer), R(response layer) と呼ぶ。
$S\rightarrow A \rightarrow R$ のうち パーセプトロンの本質的な部分は
$A\rightarrow R$ の間の学習にある。

入力パターンに $P^+$ と $P^-$ とがある。
パーセプトロンは $P^+$ が入力されたとき $1$, $P^-$ のとき $0$ を出力する
機械である。
出力層($R$) の $i$ 番目のニューロンへの入力(膜電位の変化) $u_i$は
\begin{equation}
 u_i = \sum_j w_{ij}x_j - \theta_i = \left(w\right)_i\cdot\left(x\right)_i-\theta_i.\label{eq1}
\end{equation}
ここで中間層($A$)の $j$ 番目のニューロンの出力 $y_i$とこのニューロンとの
結合係数を$w_{ij}$、しきい値を$\theta_i$ とした。
このニューロンの出力$y_i$(活動電位、スパイク)は、

\begin{equation}
y_i = \lceil u_i\rceil
\qquad\left\{
\begin{array}{ll}
 1 & \mbox{if $u_i \ge 0$,}\\
 0 & \mbox{otherwize}
\end{array} \right.
\end{equation}

と表される。
-->

<!--
式(\ref{eq1})の意味を理解するために以下の図を参照

%
\footnote{
Minsky and Papert はパーセプトロンのベクトル表示について
悲観的な考え方を持っているようですが、ここでは理解のしやすさを
優先します。}%
$$
\mathbf{w}\rightarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$-->


- 1960 年，ミンスキーとパパートの批判
- 第一次氷河期の到来

---

### 第 2 次ニューロブーム

<center>
<img src="/assets/Rumelhart_1.jpg" width="20%">
<img src="/assets/hinton.jpg" width="23%">
<img src="/assets/mcclelland.jpg" width="23%"><br/>
左: David Rumelhart, 央: Geoffrey Hinton, 右: James McClelland
</center>

- 1986 年，PDP ブック，俗に言うバイブル，発表
- 1989 年，バプニック，サポートベクターマシン発表
- 第二次氷河期の到来


### 第 3 次ニューロブーム

<!--![大規模画像認識チャレンジの結果](./assets/ilsvrc2015.svg){#fig:ilsvrc2015 style="width:49%"}-->

<center>
<img src='/assets/imagenet_result2017.png' style='width:74%'><br/>
画像認識の進歩
</center>

- 2013 ICLR スタート arXiv.org に予め論文を投稿，誰でも読める，誰でも批判できる。著者はそれに答えなければなら
ない。あっという間にトップカンファレンスとなる
- 2013 Mikolov word2vec を発表

<center>
<img src='/assets/Mikolov_analogy.png' style='width:94%'><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Mikolov_analogy.png' style='width:94%'><br> -->
Mikolov の類推課題
</center>

- 2013 DeepMind DQN を発表

<center>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>
<!-- <iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="https://komazawa-deep-learning.github.io/assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br> -->
</center>


<center>
<img src='/assets/2015Mnih_DQNFig.png' style='width:84%'><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/blob/master/assets/2015Mnih_DQNFig.png' style='width:84%'><br> -->
DQN の結果
</center>

# 生理学

## ヒューベルとウィーゼル Hubel and Wiesel による視覚野の生理学研究

<center>
<img src="/assets/1968Hubel_Wiesel_1.svg" style="width:74%"><br/>
Hubel と Wiesel (1959, 1962, 1968) の実験の模式図

<img src="/assets/1968Hubel_Wiesel_2.svg" style="width:74%"><br/>
Hubel と Wiesel の実験結果 (Hubel & Wiesel, 1968 の Fig.2.7をトレーシングしたもの
</center>

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br/>
出典: <https://youtu.be/4nwpU7GFYe8>
</center>

- トポグラフックマッピング topographic mappings (地形図):  網膜や皮膚などの体制感覚，筋肉系のような効果器系を、中枢神経系の 1 つ以上の構造物に整然と投影した地図。
地形図は、すべての感覚系と多くの運動系で観察される。
- トノトピー tonotopy（ギリシャ語のtono=周波数、topos=場所）とは、異なる周波数の音が脳内で処理される場所の空間的配置のこと。
周波数が近い音は、脳内の場所的に隣接する領域で表現される。トノトピック地図とも呼ばれる <!--は 視覚系のレチノトピーに似た地形的な組織の特殊な例である。-->
- ソマトピー somatopy: 中枢神経系上の特定の点へ身体領域の照射，投影のこと。 身体領域は 第一次体性感覚皮質 (後腹回) に投影される。
典型的には 特定の身体部位と そのそれぞれの位置を ホムンクルス homunclus (小人) 上に配置する 感覚ホムンクルスとして表現される。
細かく制御されている部位 (指，舌) は体性感覚野の面積が大きい。一方 粗く制御されている部位 (体幹) は面積が小さい。
内臓のような領域は体性感覚野の位置を持たない。
- レティノトピー retinotopy: 網膜からの視覚入力を神経細胞 にマッピングすること。哺乳類の脳に多く見られる。
この地図の大きさ、数、空間的配置は種間で異なる。
視野の隣接する点 が 同じ領域 の 隣接する 領域で 表現される とは限らないという意味で、複雑な地図となる。
例えば 第 2 次視覚野 (V2) での視覚地図は 視野の上半分に応答する網膜の部分が 視野の下半分に応答する部分から分離されて表現されている。
第３次視覚野 (V3) や 第4次視覚野 (V4) では下位の視覚野に比べて より複雑な表現がなされている


## ブレイクモア と クーパー Blackmore and Cooper (1970)

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--<iframe width="845" height="676" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>--><br/>

source: https://youtu.be/QzkMo45pcUo
</center>

<center>
<img src='/assets/1970BlackmoreCooper_Fig1.svg' style='width:39%'>
<img src='/assets/1970BlackmoreCooper_Fig2.svg' style='width:49%'><br/>
</center>

---

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br/>
出典: <https://youtu.be/RSNofraG8ZE>
</center>

<center>
<img src='/assets/1994vanEssen_gallant_Fig2.jpg' style='width:49%'>
<img src='/assets/1991Felleman_VanEssen_Fig4.svg' style='width:49%'><br/>

出典 左: Van Essen & Gallant (1994), 右: Felleman & Van Essen (1992)
</center>

<center>
<img src='/assets/thorpe_sj_01_260.jpg' style='width:44%'><br/>
Thorpe+2001
</center>

<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d2/Retinotopic_English.jpg" style="width:66%"><br/>

**出典: https://en.wikipedia.org/wiki/Retinotopy**
</center>


## ワン・アルゴリズム仮説 One algorithm hypothesis

Sur ら (1988) は，フェレット（西洋イタチ）の 聴覚信号と視覚信号との中継核，膝状体 で信号を入れ替える実験を行った。
すなわち，聴覚信号が視覚野へ入力され，逆に視覚信号が聴覚野への入力となるように外科手術を行った。
結果，本来聴覚信号を処理すべき聴覚野ニューロンでは，視覚刺激に応答する反応が観察され，
本来視覚信号を処理すべき視覚野ニューロンでは，聴覚刺激に応答する反応が観察された。

<center>
<img src="/assets/1988Sur_Fig1upper.svg" style="width:34%">
<img src="/assets/1988Sur_Fig1lower.svg" style="width:34%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

図 1. フェレットの聴覚系への視覚投射を誘導するための実験デザイン。
(左) 正常な動物における投射。
網膜は LGN と上丘 (superior colliculus: SC) に投射する。
LGN は 皮質 17 野 (一次視覚野すなわち線条体皮質) と 18野, さらに 19 野と外側上絨毛皮質などの外延野に投射している。
聴覚系では 下丘 (Inferior colliculus: IC) が MGN に投射している。
MGNの腹側および背側部は一次聴覚野 (A1) に，また大脳皮質の前聴野 (anterior auditory field:AAF) および後聴野(posterior auditory field:PAF) を含む他の皮質領域にも大きく投射している (29)。
(下) フェレット新生児で皮質 17 野と 18 野を切除すると，逆行性変性により LGN は著しく萎縮する。
上丘も切除し，下丘を切除するか，下丘から上行する線維を切断して MGN を遠ざけると，網膜は MGN へ，ひいては聴覚野へ投射されるようになる。
(Sur,1988, Fig. 1)
<!-- Fig. 1. The experimental design for induction of visual projections to the auditory systetn in ferrets.
(Top) Projections in normal animals.
The retina projects to LGN and superior colliculus (SC).
The LGN projects to cortical areas 17 (primary visual cortex or striate cortex) and 18 as well as to other extrastriatc areas including area 19 and the lateral suprasylvian (LS) cortex.
In the auditory system, the inferior colliculus (IC) projects to the MGN.
The ventral and the dorsal division of the MGN project heavily to primary auditory cortex (Al), as well as to other cortical areas including the anterior auditory field (AAF) and the posterior auditory field (PAF) in cortex (29).
(Bottom) If cortical areas 17 and 18 are ablated in neonatal ferrets, the LGN atrophies severely by retrograde degeneration.
Ablating the superior colliculus as well, and deafferenting the MGN by ablating the inferior colliculus or sectioning fibers ascending from it, causes the retina to project to the M GN and hence to auditory cortex. -->
</div>
</center>

<center>
<img src="/assets/1988Sur_fig2.jpg" style="width:66%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

図 2. 実験的に誘導された視床聴覚部への網膜投射 (網掛け部分) と視床聴覚部と大脳皮質聴覚野の接続。
手術した半球の反対側の眼は，生き残った背側 LGN (LGd) と腹側 LGN (LGv)，および MGN の背側部と腹側部内のパッチ （それぞれ MGd とMGv) に投射を，視床の傍矢状断面図 (番号付き) で示す。
同じ動物で，一次聴覚皮質 (A1) (注入部位を左上に示す) に HRP を注入すると，MGv, MGd および後方複合体の外側分割 (PO1) の細胞 (点で示す) が逆行性に満たされた。
MGd と MGv の多くの細胞は網膜投射を覆っている。
Sur (1988) Fig. 2
<!-- Fig. 2. Experimentally induced retinal projections (hatched areas) to the auditory thalamus and the connections of auditory thalamus with auditory cortex.
The eye contralateral to the operated hemisphere projects to the surviving dorsal LGN (LGd) and ventral LGN (LGv) as well as to patches within the dorsal and ventral divisions of the MGN (MGd and MG" respectively).
Numbered parasagittal sections of the thalamus are shown.
In the same animal, an injection of HRP in primary auditory cortex (A1) (the injection site is shown at top left) fills cells (indicated by dots) retrogradely in MGv,  MGd, and the lateral division of the posterior complex (PO1).
Many cells in MGd and MGv overlie the retinal projection zone. -->
</div>
</center>

<center>
<img src="/assets/1988Sur_Fig4.svg" style="width:77%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

図 3. 手術動物と正常動物の視床の電気生理学的結果と，これらの無感覚症の視床に入力を与える網膜神経節細胞の解剖学的標識。
(A) 正常動物の LGN における X, Y, W 細胞の視交叉電気刺激後の発火潜時の分布。
ヒストグラムは 5 匹の動物からプールされた 107 個の細胞を含んでいる。
X 細胞と Y 細胞は A 層に存在し，C 層には Y 細胞と W 細胞が存在する(11)。
(B) 手術した動物の LGN には A 層と C 層に見られる細胞と，C 層に見られる W 細胞があるが，X 細胞は非常に少ない。
5 匹から 81 個の細胞をプーリングしたデータ。
(c) 手術した動物 (5 匹 94個) の MGN 内の細胞は，正常な動物の LGN 細胞に比べ，視交叉刺激に対する潜時が長い(A と同じデータ)。
(D) 正常動物と手術した動物の視床に HRP を注入し，逆行性に充填した網膜神経節細胞の細胞体サイズを示すヒストグラム。
正常動物への注入は LGN を中心とし，手術動物への注入は MGN を中心とした。
ヒストグラムの各バーは，逆行充填された細胞集団の総数に対する所定の大きさの怒張の神経節細胞の割合を示す。
正常なフェレットの視床への網膜入力 (18) は，α すなわち Y 様細胞 （これらは一般に細胞体サイズが 400 $\mu m^{2}$ 以上の大型 (L 細胞)，β すなわち X 様細胞 （一般に細胞体サイズが 300 から 400 $\mu m^2$ の中型 (M サイズ細胞)，および W 様細胞の異種集団 （このクラスには中型細胞も含みうるが一般に細胞体サイズが 300 $\mu m^2$ 未満の小型 (S 細胞) から生起する。
操作されたフェレットでは MGN に投射する細胞は主に小サイズの範囲にある。
<!-- Fig. 3. Electrophysiological results from the thal­amus of operated and normal animals and anatomical labeling of retinal ganglion cells that provide input to the thalamus in these aninuls.
(A) The distribution of the latencies of firing, after electrical stimulation of the optic chiasm, of X, Y, and W cells in the LGN of normal animals.
The histogram include 107 cells pooled from five animals.
X and Y cells are found in the A laminae, whereas the C laminae contain Y and W cells (11).
(B) The LGN of operated animals contains cells (found in the A and C laminae), along with W cells (found in the C laminae), but very few X cells.
lData are from 81 cells pooled from five animals.
(C) Cells in the MGN of operated animals (94 celss in five animals) have long latencies
to optic chiasm stimulation compared to cells in the LGN of normal aninmals [same data as in (A)].
(D) Histogram of soma sizes of retinal ganglion cells filled retrogradely from an HRP injection in the thalamus of a norrnal animal and an operated aninmal.
The injection in the normal animal was centered on the LGN, and the injection in the operated animal was centered on the MGN.
Each bar in the histogram represents the ganglion cells in a given size rage as a percentage of the total population of backfilled cells.
Retinal input to the thalamus in normal ferrets (18) arises from alpha or Y-like cells (These are, in general, large (L) cells with soma size of 400 mum^2 and larger), beta or X-like cells (generally medium (M)-sized cells with soma sizes between 300 and 400 mum^2), and a heterogeneous population of W-like cells (generallly small (S) cells with soma sized smaller than 300 mum^2, although this class can include medium-sized cells as well).
In operated ferrets, the cells that project to the MGN lie mainly in the small size range. -->

結果: Sur (1988) Fig. 4
</div>
</center>

<center>
<img src="/assets/1988Sur_fig4new.png" style="width:66%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

図4. 聴覚系に視覚投射を誘導した手術動物の一次聴覚野の視覚細胞の受容野と、正常動物の一次視覚野の受容野との比較。
細胞は Hubel と Wiesel の基準に従って無指向性，方位選択性，単純型，複雑型に分類された(21)。
単純細胞はオン(+) とオフ (-) のゾーンを持つ配向野を持つが，複雑細胞は通常オンとオフのゾーンを併せ持つ方位選択野を持つ。
(A) 正常動物の 17 野領域で記録された細胞。
フェレットの第 17 野の視覚空間地図 (30) と同様に，17 野の背側から腹側へ記録位置が移動するにつれて，受容野の位置は視野の高い位置に漸次移動する。
十字は領域中枢の位置を示す。
受容野内の小さな矢印は，最大反応をもたらす刺激移動の方向を示している。
各受容野の端から伸びている線は，端が止まっていないことを示す。
受容野の端で終端する線は終端停止野を示す。
(B) 手術したフェレットの一次聴覚野では，視覚細胞は無方位性 (円形) または方位選択性 (長方形) のいずれかの受容野を有していた。
方位選択性野は複雑な形をしている。
視覚野では背側から腹側へ、聴覚野では後背側から前外側へ記録位置が移動するように，受容野が移動する。
(挿入）一次聴覚野の視覚細胞が，ヒストグラムの上に示した方向で受容野を横切るバーに対して反応したときの刺激周囲の時間ヒストグラム。
バー幅:1°, バー長:20°, 速度:5°/s, 50回 スイープ sps/s：spikes per second。
<!-- Fig. 4. Receptive fields of visual cells in primary auditory cortex of an operated animal with visual projections induced into the auditory system and co1nparison with receptive fields in primary visual cortex of a normal animal.
Cells were classified as nonoriented or oriented simple or complex according to the criteria of Hubel and Wiesel (21).
Simple cells have oriented fields with separate on (+) and off (-) zones, whereas complex cells have oriented fields usually with coextensive on and off zones.
(A) Cells recorded in area 17 of a normal animal.
Receptive field locations shifted progressively higher in the visual field as recording locations moved from dorsal to ventral in area 17, consistent with the map of visual spacee in area 17 in ferrets (3O).
The cross denotes the location of the area centrails.
Small arrows within the receptive field denote the direction of stimulus movement yielding maximal response.
Oriented line within each receptive field extending beyond receptive field edges denotes lack of end-stopping;
lines thar terminate at receptive field edges indicate end-stopped fields.
(B) In primary auditory cortex of an operated ferret, visual cells had either nonoricnted (circular) or oriented (rectangular) receprive fields.
The oriented fields were complex like.
Receptive fields moved from dorsal to ventral in the visual field as recording locations moved from posteromcdial to anterolateral in auditory cortex.
(Inset) Peristimulus time histogram of a visual cell in primary auditory cortex responding to a bar sweeping across the receptive field at the orientation and directions indicated above the histogram.
Bar width, 1°; bar length, 20°; velocity, 5°/s; 50 stimulus sweeps; sps/s, spikes per second. -->
</div>
</center>

#### 文献

- Metin and Frost (1989) Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus
- Roe et al. (1992) Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex


## 畳込みニューラルネット(CNN)

深層学習 (ディープラーニング) の中で **畳み込みニューラルネットワーク** CNN と呼ばれるニューラルネットワークについて解説します。

最初に画像処理の概略を述べる CNN が，それまで主流であった従来の手法の性能を凌駕したことはすでに述べました。
CNN の特徴の一つに **エンドツーエンド** と呼ばれる考え方があります。
エンドツーエンドとは，従来手法によるパターン認識システムでは，専門家による手の込んだ詳細な作り込みを必要としていたことと異なり，面倒な作り込みをせずとも性能が向上したことを指します。

エンドツーエンドなニューラルネットワークにより，次のことが実現しました。

- ニューラルネットワークの層ごとに，特徴抽出が行われ，抽出された特徴がより高次の層へと伝達される
- ニューラルネットワークの各層では，比較的単純な特徴から次第に複雑な特徴へと段階的に変化する
- 高次層にみられる特徴は低次層の特徴より大域的，普遍的である
- 高次層のニューロンは，低次層で抽出された特徴を共有している

このことを簡単に説明してみます。

我々人間は，外界を認識するために必要な計算を，生物種としての発生の過程と，個人の発達を通しての経験に基づく認識システムを保持していると見ることができます。
従って我々の視覚認識には化石時代に始まる光の受容器としての眼の進化の歴史と発達を通じた個人の視覚経験が反映された結果でもあります。
人工知能の目標は，この複雑な特徴検出過程をどうやったらコンピュータが獲得できるかということでもあります。
外界を認識するために今日まで考案されてきたモデル（例えば，ニューラルネットワークサポートベクターマシンなどは）は複雑です。
ですがモデルを訓練するための学習方法はそれほど難しくありません。
この意味で画像認識課題が正しく動作するためのポイントは，認識システムが問題を解く事が可能なほど複雑であるかどうかではなく，十分に複雑が視覚環境，すなわち画像認識の場合，外部の艦橋を反映するために十分な量の像データを容易すことができるか否かにあります。
今日の CNN による画像認識性能の向上は，簡単な計算方法を用いて複雑な外部環境に適応できる認識システムを構築する方法が確立したからであると言うことが可能です。

下図 <!--[fig:2012Ng_01](#fig:2012Ng_01){reference-type="ref"reference="fig:2012Ng_01"} -->
に画像処理の例を挙げました。

<center>
<img src="/assets/2012Ng_ML_and_AI_01.png" style="width:66%">
</center>

<!-- 図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"reference="fig:2012Ng_01"} -->

<center>
<img src='/assets/2013LeCun-tutorial-icml_15.svg' style="width:66%"><br/>

**LeCun (2013) より**
</center>



## ネオコグニトロン (Fukushima, 1980)

* S 細胞と C 細胞との繰り返し。最初の多層（深層）化された物体認識モデルととらえることが可能
    - S 細胞：生理学の単純細胞 simple cells に対応。受容野 receptive fileds の概念を実現。特徴抽出，特徴検出を行う。<br/>
    - C 細胞：複雑細胞 complex cells に対応。広い受容野。位置，回転，拡大縮小の差異を吸収<br>

<center>
<img src="/assets/Neocognitron.jpeg" width="64%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

ネオコグニトロンの模式図
</div>
</center>

---

# 3. LeNet5 (LeCun, 1998)
- **LeNet**. Yann LeCun (現 Facebook AI 研究所所長)による CNN 実装
 [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf){:target="_blank"} 手書き数字認識

<center>
<img src="/assets/1998LeNet5.png" width="84%"><br/>
<div style="text-align:left; width:77%;background-color:cornsilk">

LeNet5 の論文より改変
</div>
</center>

- 畳込層とプーリング層（発表当初はサブサンプリング）との繰り返し
    - 畳込とプーリングは<font color="green">局所結合</font>
- MNIST を用いた１０種類の手書き文字認識
- 最終２層は全結合層をつなげて最終層１０ニューロン，最終層の各ニューロンの出力がそれぞれの数字（０から９までの１０種）に対応する



<center>
<img src="/assets/dmoulin_gif/full_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%"><br/>
<div style="text-align=:left; width:66%; background-color:cornsilk">

左:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1.<br/>
右:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1. トランスポーズド畳み込み
</div>
<img src="/assets/dmoulin_gif/numerical_max_pooling.gif" style="width:33%">
<img src="/assets/dmoulin_gif/numerical_average_pooling.gif" style="width:33%"><br/>
<div style="text-align=:left; width:66%; background-color:cornsilk">

左: 最大値プーリング。
右: 平均値プーリング
</div>
<div style="text-align=:left; width:44%; background-color:cornsilk">
Dmoulin and Visin (2020) より
</div>

<img src="/assets/dmoulin_gif/padding_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/padding_strides_odd.gif" style="width:33%">
<img src="/assets/dmoulin_gif/padding_strides_odd_transposed.gif" style="width:33%"><br/>
<div style="text-align=:left; width:44%; background-color:cornsilk">

左: padding_strides, 中:padding_strides_odd, 右:padding_stride_transposed
</div>
<img src="/assets/dmoulin_gif/same_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%">
<div style="text-align=:left; width:44%; background-color:cornsilk">

右:same_padding_no_strides, 左: same_padding_no_strides_transposed
</div>
<img src="/assets/dmoulin_gif/arbitrary_padding_no_strides.gif" style="width:33%">
<img src="/assets/dmoulin_gif/arbitrary_padding_no_strides_transposed.gif" style="width:33%">
<div style="text-align=:left; width:44%; background-color:cornsilk">
右:arbitrary padding no strides, 左: artibtrary padding no stride transposed
</div>
</center>



## 現代的ニューラルネットワークモデルと生理学との対応

<center>
<img src='/assets/2012Zeiler_Deconvolution.png' style='width:47%'><br/>

出典: Zeiller (2012)
</center>

<center>
<img src="/assets/2010ZeilerKrishnanTaylorFerguss_fig.svg" style="width:66%"><br/>

Zeiler et. al. (2010) Fig. 7, and 8
</center>


<center>
<img src='/assets/2016Yamins_fig1s_ja.svg' style='width:77%'>
<!--<img src='../assets/2016Yamins_Fig1.svg' style='width:94%'>-->
<img src='/assets/2016Yamins_fig2ja.svg' style='width:77%'><br/>

出典: Yamins (2016) Fig.1，Fig. 2
</center>


<center>
<img src="/assets/2016Yamins_fig3s_ja.svg" width="48%"><br/>
<div style="text-align:left; width:77%;background-color:conrnsilk">

目標駆動型モデリングの構成要素。
内側の円は、与えられた数のレイヤーのモデルを含む完全なモデルクラスの部分空間を表す。
目標駆動モデルは、特に最適なモデルを発見するために、モデル・クラス内の軌道（実線）に沿ってシステムを駆動する学習アルゴリズム（黒の点線矢印）を使用して構築される。
各ゴールは、そのゴールを解くのに特に適したパラメータを含むモデルクラス（太い黒の等高線）内の引き寄せの盆地に対応していると考えることができる。
計算結果は、課題がモデルのパラメータ設定に強い制約を与えることを示す。
すなわち、与えられた課題の最適化パラメータのセットは、元の空間に比べて非常に小さいということである。
これらの目標駆動モデルは、与えられた課題領域での行動の根底にあると考えられている脳領域のニューロンの応答特性をどの程度予測できるかについて評価することができる。
例えば、単語認識のために最適化されたモデルのユニット、聴覚皮質の一次領域、ベルト領域、パラベルト領域の応答特性と比較することができる。
また、モデルを互いに比較して、異なるタイプの課題がどの程度の神経構造の共有につながるかを判断することもできる。
また、様々な構成要素のルール（教師あり、教師なし、半教師あり）を研究して、それらが生後の発達や専門知識学習の間にどのように異なるダイナミクスにつながるかを判断することもできる（緑の破線のパス）。
</div>
</center>


<center>
<!-- <img src='/assets/2014Yamins_fig2Aja.svg' style='width:74%'> -->
<!--<img src='../assets/2014Yamins_FigS2.svg' style='width:74%'>-->
<img src="/assets/2020Schrimpf_fig1.svg" style="width:84%"><br/>

出典: Schrimpf (2020) Fig. 1
</center>


## 畳み込み演算を利用したニューラルネットワーク

<div align="center">
<!--<img src='https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%">-->
<img src="/assets/Neocognitron.svg" style="width:74%">
<img src="/assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br>
</div>


# LeNet5 (LeCun, 1998)

- **LeNet**. Yann LeCun (現 Facebook AI 研究所所長)による CNN 実装
 [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf){:target="_blank"} 手書き数字認識

<center>
<img src="/assets/1998LeNet5.png" width="66%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
LeNet5 の論文より改変
</div></center>

- 畳込層とプーリング層（発表当初はサブサンプリング）との繰り返し
    - 畳込とプーリングは<font color="green">局所結合</font>
- MNIST を用いた１０種類の手書き文字認識
- 最終２層は全結合層をつなげて最終層１０ニューロン，最終層の各ニューロンの出力がそれぞれの数字（０から９までの１０種）に対応する


<center>
<img src="/assets/1999Riesenhuber_Poggio_fig2.svg" style="width:49%"><br/>
<!-- <img src="https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/cde8974e50a598aa8c2ff88760acc450fab3fbf8/assets/1999Riesenhuber_Poggio_fig2.svg"
 style="width:89%"><br/> -->
<div style="text-align: left;width:77%;background-color:cornsilk">
モデルのスケッチ。
このモデルは、単純な細胞から作られた複雑な細胞の古典的なモデル[4]を拡張したもので、線形演算（福島の表記法では「S」ユニット，テンプレート・マッチング 図中の実線）と非線形演算（「C」プーリングユニット，最大値 MAX 演算を行う 図中破線）を持つ層の階層で構成。
細胞入力の最大値を選択、その値を用いてセルを駆動する非線形の MAX演算は複雑細胞に対して，線形入力の合計とは異なり モデルの特性の鍵となる概念。
この 2 種類の操作は 異なる位置にチューニングされた求心性結合をプールすることでパターン特異性と並進不変性を，また異なるスケールにチューニングされた求心性結合をプールすることで、スケール不変性をもたらした(図示せず)。
</div></center>


<center>
<img src="/assets/1999Riesenhuber_Poggio_fig3a.svg" style="width:49%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
MAX 機構 高度に非線形な形状調整の特性。
「最適」特徴を決定するために考案された「単純化手順」を用いて得られた下側頭葉細胞の応答（選好刺激に対する反応が等しくなるように正規化された反応)。
この実験では、もともと細胞は「水のボトル」の画像（一番左の物体）に非常に強い反応を示した。
その後、刺激を単色の輪郭に単純化したところ、細胞の発火が増加し、さらに、楕円を支える棒からなるパドルのような物体に変化した。
この物体が強い反応を引き起こすのに対し、棒や楕円だけではほとんど反応しなかった。
</div></center>

<center>
<img src="/assets/1999Riesenhuber_Poggio_fig3b.svg" style="width:66%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
実験とモデルの比較。
白棒はの実験用ニューロンの反応を示す。
黒と灰色の棒は 選好刺激の 幹-楕円 の基部の遷移に合わせてチューニングしたモデル細胞の反応を示している。
モデル細胞は 直上図に示したモデルを簡略化したもの。
受容野の各位置に 2 種類の S1 特徴があり、それぞれが遷移領域の左側または右側にチューンしていて、その出力が C1 ユニットに入力され MAX 関数 (黒棒) または SUM 関数 (灰色棒) を用いてプールされている。
モデル細胞は 実験ニューロンの 選好刺激が受容野内にあるときに反応が最大になるよう、C1 ユニットに接続されていた。
</div></center>


* [デモ](/conv-demo/index.html){:target="_blank"}

<center>
<iframe src="/conv-demo/index.html" width="140%" height="640px;" style="border:none;"></iframe>

</center>

#### プーリング Pooling

ネットワークが、ある特定の場所のある特定の色合いのある特定の特徴を探してしまうのは、一番避けたいことです。
これでは良いCNNを作ることはできません。
画像は反転したり、回転したり、つぶれたりしているものがいい。
ネットワークがすべての画像の中からある物体（たとえばヒョウ）を認識できるように、同じものの写真がたくさん必要です。大きさや場所は関係ありません。
照明や斑点の数、そのヒョウが早く眠っているのか、獲物を潰しているのかなどは関係ありません。
空間的な変化が欲しい。柔軟性が必要です。
それがプーリングです。
<!-- The last thing you want is for your network to look for one specific feature in an exact shade in an exact location.
That’s useless for a good CNN!
You want images that are flipped, rotated, squashed, and so on.
You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location.
No matter what the lighting or the number of spots,or whether that leopard is fast asleep or crushing prey.
You want **spatial variance**! You want flexibility.
That’s what pooling is all about. -->

プーリングは，入力表現のサイズを徐々に小さくしていきます。
これにより，画像内のオブジェクトがどこにあっても検出できるようになります。
プーリングは、必要なパラメータの数を減らし、必要な計算量を減らすのに役立ちます。
また **オーバーフィッティング** の抑制にも役立ちます。
<!-- Pooling progressively reduces the size of the input representation.
It makes it possible to detect objects in an image no matter where they’re located.
Pooling helps to reduce the number of required parameters and the amount of computation required.
It also helps control **overfitting**.-->

オーバーフィッティングは，テスト前に情報を理解せずに超具体的な内容を暗記してしまうのと同じようなものです。
<!-- 細かいことを暗記するときは、家でフラッシュカードを使ってやるといいでしょう。 -->
<!-- しかし、実際のテストでは、新しい情報が提示されると失敗してしまいます。 -->
<!-- Overfitting can be kind of like when you memorize super specific details before a test without understanding the information.
When you memorize details, you can do a great job with your flashcards at home.
You’ll fail a real test, though, if you’re presented with new information.-->


訓練データに含まれるすべての犬に斑点と黒目がある場合，ネットワークは、犬に分類するためには画像に斑点と黒目がなければならないと考えるでしょう。
その学習データを使ってテストをすると、驚くほど正確に犬を分類することができます。
犬を正しく分類することができます。
しかし「犬」と「猫」しか出力されていないネットワークに，ダックスフンドとシベリアンハスキーが写っている画像を新たに入力した場合，ダックスフンドやシベリアンハスキーの画像を猫に分類してしまうことが起こりえます。
<!-- (Another example: if all of the dogs in your training data have spots and dark eyes, your network will believe that for an image to be classified as a dog, it must have spots and dark eyes.
If you test your data with that same training data, it will do an amazing job of
classifying dogs correctly! But if your outputs are only “dog” and “cat,” and your network is presented with new images containing, say, a Rottweiler and a Husky, it will probably wind up classifying both the Rottweiler and the Husky as cats. You can see the problem!)-->

分散がないと、ネットワークは訓練データと完全に一致しない画像では役に立たなくなります。
**訓練データと検証データは必ず別々にする** 必要があります。
訓練したデータでテストを行うと、ネットワークは情報を記憶してしまいます。
新しいデータを導入すると、ひどい結果になるでしょう。

<!-- Without variance, your network will be useless with images that don’t exactly match the training data.
**Always, always, always keep your training and testing data separate**!
If you test with the data you trained on, your network has the information memorized!
It will do a terrible job when it’s introduced to any new data.
-->

<!-- ### Overfitting is not cool.

つまり、このステップでは、**特徴地図** を取得し、**プーリング層** を適用して、**プール済特徴地図** を作成します。
So for this step, you take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.-->

プーリングの最も一般的な例は、**最大値プーリング**  (マックスプーリング) です。
最大値プーリングでは入力画像を重ならない領域のセットに分割します。
各エリアの出力は各エリアの最大値となります。
これにより，少ないパラメータで小さなサイズになります。
<!--The most common example of pooling is **max pooling**.
In max pooling, the input image is partitioned into a set of areas that don’t overlap.
The outputs of each area are the maximum value in each area.
This makes a smaller size with fewer parameters. -->


マックスプーリングとは、画像の各スポットで最大値を掴むことです。
これにより，特徴ではない 75％ の情報を取り除くことができます。
ピクセルの最大値を取ることで，歪みを考慮することができます。
特徴が左や右に少し回転しても，プールされた特徴は同じになります。サイズやパラメータを小さくしています。
これは，モデルがその情報に対してオーバーフィットしないことを意味します。
<!-- Max pooling is all about grabbing the maximum value at each spot in the image.
This gets rid of 75% of the information that is not the feature.
By taking the maximum value of the pixels, you’re accounting for distortion.
If the feature rotates a little to the left or right or whatever, the pooled feature will be the same. You’re reducing the size and parameters.
This is great because it means that the model won’t overfit on that information.-->

<!-- **平均プーリング** または **合計プーリング** を使用することもできますが、一般的な選択肢ではありません。
実際には、最大プーリングの方が両者よりも性能が良い傾向にあります。
最大プーリングでは、最大のピクセル値を取ることになります。
平均プーリングでは、画像のその場所にあるすべてのピクセル値の平均を取ります。
実際には、より小さなフィルターを使ったり、プーリングレイヤーを完全に破棄したりする傾向があります。
これは、積極的な表現サイズの縮小に対応したものです)。 -->
 <!-- You could use **average pooling or sum pooling**, but they aren’t common choices.
Max pooling tends to perform better than both in practice.
In max pooling, you’re taking the largest pixel value.
In average pooling, you take the average of all the pixel values at that spot in the image.
(Actually, there’s a trend now towards using smaller filters or discarding pooling layers entirely.
This is in response to an aggressive reduction in representation size.)-->

なぜ最大プーリングを選択するのか、もう少し詳しく見てみましょう。
を選択する理由と，ストライドを 2 画素にする理由をもう少し見てみたいと思いませんか？
[Dominik Scherer et. al, Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf){:target="_blank"} をご覧ください。
<!-- __Want to look a little more at why you might want to choose max pooling
and why you might prefer a stride of two pixels? Check out Dominik
Scherer et. al, [Evaluation of Pooling Operations in Convolutional
Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf).__-->

<!--
[ここ](http://scs.ryerson.ca/~aharley/vis/conv/flat.html){:target="_blank"} に行くと、畳み込み層の実に面白い 2D 視覚化をチェックすることができます。
画面の左端のボックスに数字を描き、出力を見てみましょう。
畳み込み層とプール層、そして推測を見ることができます。
1 つの画像の 上にカーソルを置いてみると、フィルターが適用された場所がわかります。 -->
<!-- If you go [here](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) you can check out a really interesting 2D visualization of a convolutional layer.
Draw a number in the box on the left-hand side of the screen and then really go through the output.
You can see the  convolved and pooled layers as well as the guesses.
Try hovering over a single pixel so you can see where the filter was applied.-->

<!-- So now we have an input image, an applied convolutional layer, and an applied pooling layer.

Let’s visualize the output of the pooling layer!

We were here:-->

<center>
<img src="/assets/output3.jpg" style="width:94%">
</center>

<!--
The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps.
It does this by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.

See how the image has changed size?-->

<center>
<img src="/assets/output4.jpg" style="width:94%">
</center>

<!--
## LeNet5 (LeCun, 1998)
<center>
<img src="/assets/1998LeCun_Fig2_CNN.svg" style='width:94%'><br>
LeCun (1998) より
</center>

## AlexNet (Krizensky, et al., 2012)

<center>
<img src="/assets/2012AlexNet.svg" style="width:94%"><br/>
Krzensky et al (2012) より
</center>

## GooLeNet (Inception) (Szegedy et. al, 2014)

<center>
<img src="/assets/2014Szegedy_GoogLeNet.svg" style='width:99%'><br/>
</center>
 -->
