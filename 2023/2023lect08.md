---
title: 第06回
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 03/Jun/2023<br/>
Appache 2.0 license<br/>
</div>

## おわび

先週は，突然の休講で，すみませんでした。

## 先週の復習

* 単語埋め込みモデル word2vec
* ワンホットベクトル，あるいは，ワンホット表現
* 単語ベクトルによる機能的脳画像データの予測

## 本日のキーワード

1. N-gram
2. one hot 表現，埋め込み表現，BoW (Bag of Words) 表現
3. 位置符号化器 (Position encoder)
4. 関数の直交
5. マスク化言語モデル

## 実習

- [ゼロからの Transformer <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602Transformer_from_scratch.ipynb)
- [SentenceBERT <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0602minimam_sentenceBERT.ipynb)
<!-- - [Transformer デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023Sharkar_Build_your_own_Transformer_from_scratch_using_Pytorch.ipynb) -->

<!-- - [BERT demo <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb)
- [BERT SNOW <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0628BERT_SNOW_no_pretrained.ipynb) -->

<!-- - [リカレントニューラルネットワークによる文字ベース言語モデルデモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ みんなの日本語](https://komazawa-deep-learning.github.io/minnichi_char_rnn.html)
- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"} -->
<!-- - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"} -->
<!-- - [百人一首データ取得](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0917get_hyakunin_isshu.ipynb){:target="_blank"} -->
<!-- - [BERT の超簡単な使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"} -->

<!-- ## 課題

以下の Mitchell (2008) 論文 [名詞の意味に関連した人間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}
を読み，要約しなさい。 -->

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"

[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"

[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"

[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"

[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"

[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"

を学習することとなる。

## 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティー
として与えることで精度が向上します。

<center>
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br>
From Milolov (2013) Tab. 1
</center>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%">
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かっておおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると大まかに我々の知識を表現できることを示唆していると考えます。

<!-- # 2. リカレントニューラルネットワーク

一時刻前の状態を保持して利用する SRN は下図左のように描くことができます。同時に時間発展を考慮すれば下図右のように描くことも可能です。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてください。あるいは同義ですが上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してください。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かります。

### エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示した。
図中の数字はニューロンの数を表す。
入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表す。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:33%"><br>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示し。
Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示された (Elman, 1990, 1991, 1993)。 -->

<!--
- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | live
s

これらの規則にはさらに 2 つの制約がある。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもと
らなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この
文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というも
のです。-->

<!-- ### LSTM

* LSTM (Long Short-Term Memory) は Schmithuber らによって開発されたリカレントネットワークモデルの改良版。
* 内部に 3 つのゲートを持つ。
* ゲートの出力関数は，シグモイド関数である。このため，信号の開閉をソフトに制御する機能を有する。
* 信号の取捨選択を，データから学習させることで，**長距離依存** に対応することを意図している。 -->

<!-- <center>
<img src="/assets/2015Greff_LSTM_ja.svg" style="width:34%">
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-19.png" style="width:16%"><br/>
左: LSTM (浅川, 2015) より，右: トランスフォーマー[@2017Vaswani_transformer]<br/>
入力ゲートと入力 は Q, K と同一視，出力ゲートと V とは同一視可能？
</center> -->

<!--
# Residual attention
<center>

![](assets/2017residual_attention.svg){style="width:33%"}
![](assets/2017residual_attention_motivation.svg){style="width:65%"}<br/>
![](assets/2017residual_attention_whole_net.svg){style="width:94%"}<br/>
[@2017Wang_residual_attention] Fig. 1, 2, 3
</center>
-->

<!--
# A2 net

<center>
![](assets/2018Chen_A2-Nets_fig1ja_a.svg){style="width:39%"}
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
![](assets/2018Chen_A2-Nets_fig1ja_b.svg){style="width:55%"}<br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>

# Relationship between self-attention and convolution

<center>
<img src="/assets/2019cordonnier_self_attention_convol.svg" style="width:66%"><br/>
<img src="/assets/2020Cordonnier_tab3.svg" style="width:66%"><br/>
From [@2020cordonnier_attention_and_convolution]
</center>

# まとめ

- MHSA は 畳み込み と同等の能力がありそうである。
- Reformer に見られるように position encodings を工夫する余地は残されているように思われる。
-->

# 3. 符号化器・復号化器モデル (encoder-decoder model), seq2seq model

* 中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば **機械翻訳** や **対話** のモデルになる。
* 初期の翻訳モデルである "seq2seq" の概念図を示した。
* "eos" は文末 end of sentence を表す。
* 中央の "eos" の前がソース言語であり，中央の "eos" の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。

* 注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がない。
* 逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
* この点を改善することを目指すことが 2014 年以降盛んになった。
* 顕著な例が後述する **双方向 RNN**, **LSTM** を採用したり，**注意** 機構を導入することであった。

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" width="49%"><br/>
From [2014Sutskever_Sequence_to_Sequence]
rom [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="/assets/2014Sutskever_Fig2left.svg" width="44%">
<img src="/assets/2014Sutskever_Fig2right.svg" style="width:44%"><br />
From [@2014Sutskever_Sequence_to_Sequence] Fig. 2, 3
</center>


### BERT

<div class="figure figcenter">
<img src="/assets/coco175469.jpg" width="44%"><br/>
</div>

1. 夕暮れのハーバーに汽船と複数の鳥が浮かんでいる
2. 水面に浮かぶ4羽の水鳥と、その向こうに停泊している2隻の船
3. 船着き場に2艘の船がとまっている
4. 朝焼けの中待機場所にある旅客船とマガモ
5. 停められた船の近くで水鳥が泳いでいる

MS COCO データセットより: <http://farm5.staticflickr.com/4055/4704393899_a041476b4a_z.jpg>

上図は，MS COCO という画像データと画像に対応する脚注からなるデータセットからの一例である。
日本語文は，千葉工業大学 STAIRLABO が公開しているデータである。
人間が見れば，写真と文章とは結びいていることが分かる。
加えて，5 つの脚注も相互に似ていることが分かる。
MS COCO データセットでは，一枚の写真に 5 つの脚注が紐付けられている。

コンピュータにこれらの文章が似ていることを判断させようとすると，最近まで難しい仕事であった。
本章で紹介する，文の意味ベクトルを用いると，これらの文章が相互に似ていると判断させることが可能である。
下図は tSNE と呼ぶ手法を用いて，日本語の文章の類似度を センテンス BERT を用いて表現し，文章の類似度に従って地
図を描いたものである。
図では，同じ写真に紐付いている文章は同じ色で表現している。

<center>
<img src="/assets/2022_0915sbert_staircoco500.svg">
</center>
