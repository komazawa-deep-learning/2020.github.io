---
title: 第25回
author: 浅川 伸一
layout: home
---

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 08/Dec/2023<br/>
Appache 2.0 license<br/>
</div>

# 第 25 回 と強化学習の導入



* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/)

<center>
<img src="/assets/2016AlphaGo_Fig1a.svg">
<img src="/assets/2016AlphaGo_Fig1b.svg"><br/>
AlphaGo の模式図，原著論文より

<img src="/assets/AlphaGoZeroFig2.png" width="66%"><br/>
AlphaGoZero のセルフプレイ，原著論文より
</center>

* <https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296>
* <https://gist.github.com/ruippeixotog/cde7cae770e72916e209b915521bb18f>

<!--
* [宇宙人の夢: アートシーンの出現](https://shinasakawa.github.io/2022/2021Snell_clip-art_ja)
* [CLIP のデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2020_1014CLIP_ja_basics.ipynb)
* アルファ碁, アルファ碁ゼロ, DQN, [Atari ゲーム (OpenAI Gym)](https://gym.openai.com/)
* [エージェント57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)

<!--* Mastering the game of Go with deep neural networks and tree search
* Mastering the game of Go without human knowledge-->

# 強化学習，条件付けの古典

- [パブロフ](https://en.wikipedia.org/wiki/Ivan_Pavlov) (Ivan Petrovich Pavlov; 1849/Sep/14-1936/Feb/27)古典的条件づけ 1904 年ノーベル医学生理学賞
- [スキナー](https://en.wikipedia.org/wiki/B._F._Skinner) (Burrhus Frederic Skinner; 1904/Mar/20-1990/Aug/18) 道具的条件付け， オペラント条件づけ，[スキナー箱, Skinner(1938) Fig.1, page 39 より](./assets/1938Skinner_Fig1_skinnerBOX.jpg)
- [Sutton](http://incompleteideas.net/) and [Barto](http://www-anw.cs.umass.edu/~barto/) の強化学習 [初版 1998年](http://incompleteideas.net/book/first/the-book.html), [第2版 2018年](http://incompleteideas.net/book/the-book-2nd.html), [初版は翻訳あり](https://www.amazon.co.jp/dp/4627826613/)，第2版は pdf ファイルで[ダウンロード可能](http://incompleteideas.net/book/bookdraft2017nov5.pdf)

<center>

<img src="https://www.nobelprize.org/images/pavlov-12840-content-portrait-mobile-tiny.jpg" width="24%">
<a href="https://www.nobelprize.org/prizes/medicine/1904/pavlov/biographical/">Ian Pavlov</a>&nbsp;&nbsp;
<img src="https://www.bfskinner.org/wp-content/gallery/1970s-1990/BFS-IN-THE-OFFICE.jpg" width="24%">
<a href="https://www.bfskinner.org/archives/photos/">Burrhus Frederic Skinner</a>&nbsp;&nbsp;
<!-- <img src="http://incompleteideas.net/sutton-head5.jpg" width="24%"> -->
<img src="https://cloudfront.ualberta.ca/-/media/science/people/rsutton/sutton.jpg" width="24%">
<a href="http://incompleteideas.net/">Richard S. Sutton,</a>&nbsp;&nbsp;
<img src="https://people.cs.umass.edu/~barto/barto2006-lowres.jpg" width="24%">
<a href="https://people.cs.umass.edu/~barto/">Andrew G. Barto</a>
</center>

# 実習資料 (Colab ファイル など)

* [TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb)
<!-- * [Google Colab で OpenAI の Gym 環境を動かすための下準備](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1106Remote_rendering_OpenAI_Gym_envs_on_Colab.ipynb) -->

<!-- * [PyTorch チュートリアルによる DQN (2021_1105 現在未完成)](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105reinforcement_q_learning.ipynb) -->

<!-- (file:///Users/asakawa/study/2020personal/2020-1030deepmind_agent57.md)-->
<!-- 1. [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
2. [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html) -->

<!-- 1 と 2 は，ローカルディスクに git clone あり。
強化学習の基礎概念である。
jekyell が動作しないので確認できないのだが。
# 昨年の 第8回の markdown file (lect08.md) より
-->


<center>
<div style="text-align: left;width: 88%;background-color: powderblue;">
人工知能の長年の目標は， 困難な領域でも超人的な能力をタブラ・ラサ方式で学習するアルゴリズムである。
最近では AlphaGo が囲碁の世界チャンピオンを破った初めてのプログラムとなった。
AlphaGo の木探索は， 深層ニューラルネットワークを用いて局面の評価と手の選択を行う。
このニューラルネットワークは，人間の熟練した手からの教師付き学習と， 自分自身の競技からの強化学習によって訓練されている。
ここでは， 人間のデータやガイダンス， ゲームルール以外の領域の知識を必要としない， 強化学習のみに基づいたアルゴリズムを導入する。
AlphaGo は自分自身の教師となり AlphaGo 自身の手の選択や AlphaGo のゲームの勝敗を予測するようにニューラルネットワークが学習される。
このニューラルネットワークは， 木探索の強度を向上させ， その結果， より質の高い手の選択が可能となり， 次の反復ではより強力な自己対戦が可能となる。
タブララサからスタートした私たちの新しいプログラム AlphaGo Zero は，既発表のチャンピオンに敗れた AlphaGo に対して 100-0 で勝利するという超人的な成績を達成した。
</div>
</center>

* [David Silver homepage](https://www.davidsilver.uk/)

<!-- ## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb) -->

<!-- 以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。 -->

## 基本概念
<!-- ### Key Concepts -->

<!-- RL の重要な概念を正式に定義しましょう。
Now Let's formally define a set of key concepts in RL. -->

* 動作主 (エージェントは) **環境** の中で行動する。
* 環境がある行動に対してどのように反応するかは，我々が知っているかどうかわからない **モデル** によって定義される。
* 動作主 (エージェント) は，環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ，多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができる。
* 動作主 (エージェント) がどの状態に到達するかは，状態間の遷移確率 ($P$) によって決定される。
* 行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与える。
<!-- The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.-->

* モデルは報酬関数と遷移確率を定義する。
* モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別される。
<!-- The model defines the reward function and transition probabilities.
We may or may not know how the model works and this differentiate two circumstances: -->

    * **モデルベース**
<!-- * **モデルを知る**：完全な情報で計画を立てる、モデルベースの RL を行う。-->
        環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP) によって最適解を求めることができます。
<!-- アルゴリズム 101 の授業で習った "longest increasing partialence" や "traveling salesmen problem" をまだ覚えていますか？笑
これはこの記事の焦点ではありませんが。-->

    * **モデルフリー**
<!-- * **モデルを知らない**：不完全な情報での学習；モデルフリー RL を行うか，-->
アルゴリズムの一部として明示的にモデルを学習しようとする。
<!-- 以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。 -->

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.-->

* 動作主 (エージェント)の **方策 (ポリシー)**  $\pi(s)$ は，**総報酬** $G$ を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
* 各状態には，その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数 $V(s)$ が関連付けられる。
* 言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化する。
* 強化学習で学習しようとするのは，方策関数 と 価値関数の両方。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.
-->

<center>
<img src="/assets/RL_algorithm_categorization.png" width="94%"><br/>
<div style="text-align: justify;width: 88%;background-color: cornsilk;">
価値，方針(ポリシー)，環境 のいずれをモデル化したいかに基づく強化学習アプローチのまとめ。
(画像出典：[David Silver の強化学習講座](https://youtu.be/2pWv7GOvuf0) より転載)
<!--
Fig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment.
(Image source: reproduced from David Silver's RL course [lecture 1](https://youtu.be/2pWv7GOvuf0).)
-->
</div>
</center>

* 動作主 (エージェント) と環境の相互作用には $t=1, 2, \dots,T$ 時間内の一連の行動と観測された報酬が含まれる。
* この過程で， エージェントは環境に関する知識を蓄積し，最適な政策を学習し，最適な政策を効率的に学習するために，次にどのような行動を取るべきかを決定する。
* 時間ステップ $t$ における状態，行動，報酬をそれぞれ $S_t$, $A_t$, $R_t$ とする。
<!-- このように， インタラクションシーケンスは 1 つの **エピソード** (「試行」または「軌跡」とも呼ばれる) で完全に記述され， 系列は末端の状態 $S_T$ で終了します。 -->
<!-- The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \dots, T$.
During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy.
Let's label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively.
Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $S_T$:-->

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

<!-- RL アルゴリズムの様々なカテゴリーを調べる際によく遭遇する用語。 -->
<!-- Terms you will encounter a lot when diving into different categories of RL algorithms: -->

- **モデルベース**: モデルが既知であるか、アルゴリズムがそれを明示的に学習する。
- **モデルフリー**: 学習時にモデルに依存しない。
- **オンポリシー**<!-- **On-policy** -->: アルゴリズムの学習にターゲットポリシーからの決定論的な結果やサンプルを使用する。
- **オフポリシー**<!-- **Off-policy** -->: ターゲット・方策(ポリシー)ではなく，異なる<!-- ビヘイビア・ -->方策(ポリシー)で生成された上他繊維やエピソード分布で学習する。

<!--
- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.-->

## モデル: 移行と報酬 transition and reward
<!-- #### Model: Transition and Reward -->

* モデルとは，環境を記述する実体。
* モデルを用いることで，環境がどのように動作主 (エージェント) と相互作用し，フィードバックを与えるかを学習または推論することができる。
* モデルには，遷移確率関数 $P$ と報酬関数 $R$ の 2 つの主要部分がある。
<!-- The model is a descriptor of the environment.
With the model, we can learn or infer how the environment would interact with and provide feedback to the agent.
The model has two major parts, transition probability function $$P$$ and reward function $R$. -->

* 状態 $s$ にいるとき，次の状態 $s’$ に到達して報酬 $r$ を得るために行動 $a$ をとることを決めたとする。
これは 1 つの **遷移** ステップと呼ばれ， タプル $(s, a, s', r)$ で表される。
<!-- Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r.
This is known as one **transition** step, represented by a tuple (s, a, s', r).-->

* 遷移関数 $P$ は，行動 $a$ を起こした後に報酬 $r$ を得て，状態 $s$ から $s’$ に遷移する確率を記録したものである。
ここでは $\mathbb{P}$ を「確率」の記号として用いる。
<!-- The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r.
We use $$\mathbb{P}$$ as a symbol of "probability".-->

$$
P(s', r \vert s, a)  = \mathbb{P} \left[S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a\right]
$$

したがって，状態遷移関数は $P(s', r \vert s, a)$ の関数として定義することができる。
<!-- Thus the state-transition function can be defined as a function of $P(s', r \vert s, a)$: -->

$$
P_{ss'}^a = P(s' \vert s, a) = \mathbb{P} \left[S_{t+1} = s' \vert S_t = s, A_t = a\right] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

報酬関数 $R$ は 1 つの行動 (アクション) によって引き起こされる次の報酬を予測する。
<!-- The reward function R predicts the next reward triggered by one action:-->

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$

## 方策 (ポリシー) policy
<!-- #### Policy -->

* 方策 (ポリシー) とは，動作主 (エージェント) の行動関数 $\pi$ として，状態 $s$ においてどのような行動を取るべきかを示すもの。
これは， 状態 $s$ から行動 $a$ への写像であり，決定論的なものと確率論的なものがある。
<!-- Policy, as the agent's behavior function $\pi$, tells us which action to take in state s.
It is a mapping from state s to action a and can be either deterministic or stochastic:-->

    * 決定論的: $\pi(s) = a$.
    * 確率論的: $\pi(a \vert s) = \mathbb{P}_ {\pi} \left[A=a \vert S=s\right]$.

## 価値関数 value function
<!-- #### Value Function -->

* 価値関数は，将来の報酬の予測によって，状態の良さや，状態や行動がどれだけ報われるかを測定するもの。
* 将来の報酬は **リターン** とも呼ばれ，今後の報酬を割引いたものの総和。
* 時刻 $t$ から始まるリターン $G_t$ を計算すると以下のようになる:

<!--Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward.
The future reward, also known as **return**, is a total sum of discounted rewards going forward.
Let's compute the return $G_t$ starting from time t:-->

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

$[0,1]$ の割引係数 $\gamma$  は，将来の報酬に罰則 (ペナルティ) を課すことになる。
<!-- The discounting factor $\gamma \in [0, 1]$ penalize the rewards in the future, because:-->

* 将来の報酬は不確実性が高い可能性がある。
例：株式市場。
* 将来の報酬はすぐに得られるものではない。
例：人間として 5 年後よりも今日楽しいことをしたいと思うかもしれない。
* 割引 (ディスカウント) は数学的な利便性をもたらす。
つまり，リターンを計算するために将来のステップを永遠に追跡する必要がないのです。
* 状態遷移グラフの無限ループを心配する必要はない

<!-- - The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.-->

* ある状態 $s$ の **状態値** は， 時間 $t$ でこの状態にある場合の期待リターンであり $S_{t}=s$ である。
<!-- The **state-value** of a state s is the expected return if we are in this state at time t, $S_t = s$: -->

$$
V_{\pi}(s) = \mathbb{E}_ {\pi}[G_t \vert S_t = s]
$$

* 同様に，状態と行動の対としての **行動価値**  $Q$ を以下のように定義する:
<!-- Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:-->

$$
Q_{\pi}(s, a) = \mathbb{E}_ {\pi}[G_t \vert S_t = s, A_t = a]
$$

* 目標方策 $pi$ に従うので，可能な行動に対する確率分布と Q 値を利用して，状態値を回復することができる。
<!-- Additionally, since we follow the target policy $\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: -->

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行動価値と状態価値の際は，**アドバンテージ (advantage)** と呼ばれる
<!-- The difference between action-value and state-value is the action **advantage** function ("A-value"): -->

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

### 最適価値と最適方策
<!-- ### Optimal Value and Policy -->

* 最適価値関数は，最大リターンを産む
<!-- The optimal value function produces the maximum return: -->

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

* 最適方策は，最適価値関数によって達成される:
<!-- The optimal policy achieves optimal value functions: -->

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

* $V_{\pi_{\star}}(s)=V_{\star}(s)$ かつ $Q_{\pi_{\star}}(s,a)=Q_{\star}(s,a)$ である
<!-- And of course, we have $V_{\pi_{\star}}(s)=V_{\star}(s)$ and $Q_{\pi_{\star}}(s, a) = Q_{\star}(s, a)$. -->


## マルコフ決定過程
<!-- ## Markov Decision Processes-->

* 広義には強化学習は，**マルコフ決定過程 (MDP: Markov Decision Process)** の一部である。
    * **マルコフ性 Markov property** とは，将来の状態が現在の状態にのみ依存すること。
    * とりわけ，不確かな状況でのマルコフ決定過程を **POMDP (Partially Observed Markov Decision Process)** と呼ぶ
<!-- In more formal terms, almost all the RL problems can be framed as **Markov Decision Processes** (MDPs).
All states in MDP has "Markov" property, referring to the fact that the future only depends on the current state, not the history: -->

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

逆に言えば，未来と過去とは **条件付き独立** である。
将来の意思決定は現在の状況によって定まる。
<!-- Or in other words, the future and the past are **conditionally independent** given the present, as the current state encapsulates all the statistics we need to decide the future.
 -->

<center>
<img src="/assets/agent_environment_MDP.png" width="49%"><br/>
Fig. 3. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).
</center>

* マルコフ決定過程 (Markov deicison process) は  $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, の 5 つの要素で構成される
<!-- A Markov deicison process consists of five elements $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where the symbols carry the same meanings as key concepts in the [previous](#key-concepts) section, well aligned with RL problem settings:-->

- $\mathcal{S}$: 状態集合<!-- - a set of states; -->
- $\mathcal{A}$: 行動集合 <!-- - a set of actions; -->
- $P$: 遷移関数 <!-- - transition probability function; -->
- $R$: 報酬関数<!-- - reward function; -->
- $\gamma$: 割引率<!--  - discounting factor for future rewards. -->

未知の環境では，$P$ と $R$ とは完全に知ることはできない
<!-- In an unknown environment, we do not have perfect knowledge about $P$ and $R$. -->

<center>
<img src="/assets/mdp_example.jpg" width="66%"><br/>
<div style="text-align: center;width:88%;background-color:cornsilk;">
マルコフ決定過程の例：典型的な仕事の一日
(画像出典: [randomant.net/reinforcement-learning-concepts](https://randomant.net/reinforcement-learning-concepts/))
<!--
Fig. 4. A fun example of Markov decision process: a typical work day.
![MDP example]({{ '/assets/images/mdp_example.jpg' | relative_url }}){: class="center"}
Fig. 4. A fun example of Markov decision process: a typical work day.
-->
</div>
</center>

## ベルマン方程式
<!-- ### Bellman Equations -->

ベルマン方程式とは， 価値関数を目先の報酬と割引された将来の価値に分解する一連の方程式を指します。
<!-- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. -->

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

Q 関数とは
<!-- Similarly for Q-value, -->

$$
\begin{aligned}
Q(s, a)
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$


### 期待ベルマン方程式
<!-- ### Bellman Expectation Equations -->

* 再帰的な更新過程は，さらに分解すると，状態値関数と行動値関数の両方で構成される方程式となる。
* 今後の行動ステップを進める際には $\pi$ の方針に沿って $V$ と $Q$ を交互に拡張していく。

<!-- The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.
As we go further in future action steps, we extend V and Q alternatively by following the policy $\pi$.-->

<center>
<img src="/assets/bellman_equation.png" width="60%"><br/>
<div style="text-align: center;width: 88%;background-color: powderblue;">
図 5. ベルマン期待値方程式がどのように状態値関数と行動値関数を更新するかを示す図。
<!--
Fig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.
![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }}){: style="width: 60%;" class="center"}
-->
</div>
</center>

$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi
} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi
} (s', a')
\end{aligned}
$$

### 最適ベルマン方程式
<!-- ### Bellman Optimality Equations -->

* 方策 (ポリシー) に従った期待値を計算するのではなく，最適値にしか興味がないのであれば，方策 (ポリシー) を使わずに代替更新中の最大リターンにすぐに飛びつくことができる。
<!-- RECAP:  最適値 $V_*$ と $Q_*$ は，得られる最高のリターンであり [ここ](#optimal-value-and-policy) で定義されています。-->
<!-- If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy.
RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined [here](#optimal-value-and-policy).-->

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

* 上式はベルマンの期待方程式と酷似している。
<!-- Unsurprisingly they look very similar to Bellman expectation equations. -->

* もし環境の完全な情報があれば，この問題は計画問題となり，動的計画法 (DP: dynamic programming) で解くことができる。
* 駄菓子菓子，ほとんどの場合 $P_{ss'}^a$ や $R(s, a)$ が不明であるため，ベルマン方程式を直接適用して MDP を解くことはできない。
* 駄菓子菓子，多くの RL アルゴリズムの理論的基礎を与えている。

<!--
If we have complete information of the environment, this turns into a planning problem, solvable by DP.
Unfortunately, in most scenarios, we do not know $P_{ss'}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. -->


<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
-->


# 強化学習とは何か？

<center>
<img src="/assets/2018Sutton_Fig3j.svg" style="width:74%"><br/>
<p align="center" style="width:74%">
Sutton & Barto (2018) Fig. 3.2 を改変
</p>
</center>

強化学習という言葉は古い言葉ですが機械学習の文脈では，環境とその環境におかれた動作主（エージェントと言ったり，ロボットシステムだったりします）が，環境と相互作用しながらより良い行動を形成するためのモデルです。
動作主は，環境から受け取った現在の状態を分析して，次にとるべき行動を選択します。このとき将来に渡って報酬が最大となるような行動を学習する手法の一つです。

2015 年には，Google 傘下のデープマインドというスタートアップチームが開発した囲碁プログラム AlphaGo がプロ棋士のイ・セドル氏に勝利し話題になりました。
AlphaGo は強化学習を基本技術の一つとして用いています。

1. [強化学習(1): 基礎](https://komazawa-deep-learning.github.io/rl01_elements.pdf)
2. [強化学習(2): エージェントと環境](https://komazawa-deep-learning.github.io/rl02_agentAndEnv.pdf)
3. [強化学習(3): 目標と報酬](https://komazawa-deep-learning.github.io/rl03_goalAndReward.pdf)
4. [強化学習(4): マルコフ決定過程](https://komazawa-deep-learning.github.io/rl04_mdp.pdf)
5. [強化学習(5): 価値反復，方策反復](https://komazawa-deep-learning.github.io/rl05_vi.pdf)
6. [強化学習(6): ](https://komazawa-deep-learning.github.io/rl06_advanced.pdf)
6. [強化学習(7): ](https://komazawa-deep-learning.github.io/rl07_robotics.pdf)

<!-- - エージェントと環境，マルコフ決定過程 MDP，POMDP，効用関数，ベルマン方程式，探索と利用のジレンマ，SARSA:
- 価値，方策，Q 学習，モデルベース対モデルフリー，アクター=クリティック:
- 深層 Q 学習:
- ゲーム AI へ (AlphaGo，AlphaGoZero，OpenAI five):
- セルフプレイ:
- 最近の発展 A3C，Rainbow，RDT，World model: -->

<!--
### 方策，報酬，価値観数，(環境)モデル

- $s,s'$: 状態 state
- $a$: 行動，行為 action
- $r$: 報酬 reward
- $t$: 時間 (離散時間 $t=1,2,\ldots,T$)
- $p(s',r\vert s, a)$: 状態 $s$ で行為 $a$ を行ったとき，報酬 $r$ を受け取って 状態 $s'$ に遷移する確率
- $p(s'\vert s, a)$: 状態 $s$ で行為 $a$ を行った場合，状態 $s'$ へ遷移する確率
- $r(s,a)$: 状態 $s$ で行為 $a$ を行った場合の即時報酬 immediate reward の期待値
- $r(s,a,s')$: 行為 $a$ を行った場合状態が $s$ から $s'$ へ変化したときの即時報酬の期待値
- $v_\pi(s)$: 方策 $\pi$ での状態 $s$ の価値 (期待報酬)
- $v_*(s)$: 最適方策化での状態 $s$ の価値
- $q_\pi(s,a)$: 方策 $\pi$ のもとで状態 $s$ で行為 $a$ をおこなた場合の価値
- $q_*(a)$: 行動 $a$ を行った場合の期待報酬(真の報酬) <!-- true value (expected reward) of action $a$-->

<!--
- $Q_t(a)$: 時刻 $t$ での $q_*(a)$ の期待値 qestimate at time $t$ of $q_*(a)$
- $N_t(a)$: 時刻 $t$ で行為 $a$ を行った回数 number of times action $a$ has been selected up prior to time $t$
- $H_t(a$) 時刻 $t$ で行為 $a$ を行う傾向(選好 preference) learned preference for selecting action a at time $t$
- $\pi_t(a)$: 時刻 $t$ で行為 $a$ を選択する確率 probability of selecting action a at time $t$
- $R_t$: $\pi_t$ が与えられた場合，時刻 $t$ における の期待報酬 estimate at time $t$ of the expected reward given $\pi_t$
--->


## 複雑な状況をどう理解して解決するのか？

- 強化学習というニューラルネットワークモデルがあるわけではない
- 動的で複雑な環境に対処 $\rightarrow$ **強化学習** + DL $\rightarrow$ 一般人工知能への礎

- DQN ATARIのビデオゲーム, [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)
- AlphaGo 囲碁, [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961)
- AlphaGoZero 囲碁, [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)

# Deep Q Network

<center>

<img src="/assets/2015DQNFig1.svg" width="66%"><br/>
DQNの模式図, 原著論文より
</center>

<!--
- [ギャラガ1](/assets/MOV_0013s.mp4)
- [ギャラガ2](/assets/MOV_0071s.mp4)
-->

- **Q 学習** Q learning に DNN を採用
- CNN が LeNet, [@1998LeCun] そうであったように，強化学習 RL も昔からの技術 [@Sutton_and_Barto1998]
- ではなぜ，今になって囲碁や自動運転に応用できるようになったのか？
  - $\Rightarrow$ コンピュータの能力, データ規模，アルゴリズムの改良, エコシステム(ArXiv, Linux, Git, ROS, AMT, TensorFlow)

<!--
# 強化学習

- 強化学習 $\Rightarrow$ 意思決定
  - **エージェント** agent が **行動**(行為) action をする
  - 行動によって **状態** が変化する
  - **環境** から与えられる **報酬** によって**目標**が決定
- 深層学習: $\Rightarrow$ 表現，表象
  - 教師信号として目標が与えられる
  - 目標を達成するために外部状況の **表現** を獲得

<center>
<font size="+2" color="Green">強化学習 + 深層学習 = 人工知能</font>
</center>

  - 強化学習 $\Rightarrow$ 目標の設定
  - 深層学習 $\Rightarrow$ 内部表象の獲得機構を提供

# 用語の整理

- 教師信号なし **報酬信号** reward signal
- 遅延フィードバック

- **価値** Value
- **行為** Action
- **状態** State
- TD 学習
  - **Sarsa**
  - **Q 学習**
  - **アクタークリティック**
- 報酬 $R_t$: **スカラ値**
  - 時刻 $t$ でエージェントのとった行動を評価する指標
  - エージェントは**累積報酬** cumulative reward の最大化する
  - 報酬仮説: **目標は累積期待報酬の最大化として記述可能**
-->

## DQN 結果

<center>

<img src="/assets/2015Mnih_DQNFig.png" style="width:39%"><br/>
</center>


## YouTube 上でのデモ動画

* ブロック崩し: [https://www.youtube.com/watch?v=V1eYniJ0Rnk](https://www.youtube.com/watch?v=V1eYniJ0Rnk)
* スペースインベーダー: [https://www.youtube.com/watch?v=W2CAghUiofY](https://www.youtube.com/watch?v=W2CAghUiofY)

<!--- packman: [https://www.youtube.com/watch?v=r3pb-ZDEKVg](https://www.youtube.com/watch?v=r3pb-ZDEKVg)
- OpenMind selfplay: [https://www.youtube.com/watch?v=OBcjhp4KSgQ](https://www.youtube.com/watch?v=OBcjhp4KSgQ)

-->
<!--
* DQN の動画 スペースインベーダー

<center>

<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video1.mp4" type="video/mp4" >
</center>

* DQN の動画 ブロック崩し

<center>

<div>
<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video2.mp4" type="video/mp4" >
</div>
</center>
-->

<!-- ## なぜ DQN には難しいのか？

<center>
<div>
<video style="width:74%" controls src="/assets/Montezuma.mp4" type="video/mp4" /></br>
**Montezuma**
</div>
</center>

<center>
<video style="width:74%" controls src="/assets/PrivateEye.mp4" type="video/mp4" /></br>
**Private Eye**
</center>

<center>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/Montezuma.mp4">
</video>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/privateEye.mp4">
</video>
 -->

## 人間にはできて強化学習には難しいこと

- Montenzuma's Revenge の動画 [https://www.youtube.com/watch?v=Klxxg9JM5tY](https://www.youtube.com/watch?v=Klxxg9JM5tY)
- Private Eys の動画 [https://www.youtube.com/watch?v=OfyS-Wj1M78](https://www.youtube.com/watch?v=OfyS-Wj1M78)

<!---
## エージェントと環境

- At each step $t$ the agent:
  - Executes action $A_t$
  - Receives observation $O_t$
  - Receives scalar reward $R_t$
- The environment:
  - Receives action $A_t$
  - Emits observation $O_{t+1}$
  - Emits scalar reward $R_{t+1}$
- $t$ increments at env. step
-->

<!--
- **エージェント**: 学習と意思決定を行う主体
  1. **行動** action **$A_t$** を行い
  1. 環境の **観察** observation **$O_t$** を行う
  1. 環境からスカラ値の **報酬** reward **$R_t$** を受け取る
- **環境**: エージェント外部の全て
  1. エージェントから **行為** $A_t$ を受け取り
  1. エージェントに **観察** $O_{t+1}$ を与え
  1. エージェントへ **報酬** $R_{t+1}$ を与える

## エージェントの要素

- **方策** Policy
- **価値関数** Value function
- **モデル** エージェントが持つ環境の表象

## 方策 policy

- **方策** : エージェントの行為
- 決定論的方策: $a=\pi(S)$
- 確率論的方策: $\pi(a|s)=p(A_{t=a}|S_{t=s})$

## 価値関数
- 将来の報酬予測
- 状態評価(良/悪)
- 行為の選択
$$
v_\pi(S)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+\ldots|S_{t=s}\right\}
$$

## 強化学習のモデル
- 価値ベース
  - 方策:なし
  - 価値関数:あり
- 方策ベース
  - 方策:あり
  - 価値関数:なし
- アクター=クリティック Actor Critic
  - 方策: あり
  - 価値関数: あり

- モデルフリー
  - 方策，価値関数: あり
  - モデル: なし
- モデルベース
  - 方策，価値関数: あり
  - モデル: あり

## 探索と利用のジレンマ Exploration and exploitaion dilemma
- 過去の経験から，一番良いと思う行動ばかりをしていると，さらに良い選択肢を見つけ出すことができない **探索不足**
- 更に良い選択肢ばかり探していると過去の経験が活かせない **過去の経験の利用不足**

## 目標，収益，報酬

- エージェントの目標は累積報酬を最大化すること (報酬仮説)
  - **報酬仮説** Reward Hypothesis
  - 目標: 期待報酬の最大化

- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価

## 逐次的意思決定 Sequential Decision Making
- 目標 Goal: 総収益を最大化する行動を選択すること
- 行為，行動 Actions は長期的結果
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある


## 収益 Return
- **収益** return $G_t$: 割引付き収益
$$
G_t=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
$$

- 割引率 The discount $\gamma\in\left\{0,1\right\}$ : 現時点から見た将来の報酬を計算するため

- **遅延報酬** delayed reward の評価
- $0$ に近ければ __近視眼的__ 評価
- $1$ に近ければ __将来を見通した__ 評価

## 価値関数 Value Function

- **状態価値関数 $v$ ** と **行動価値関数 $q$ **

- **価値関数** $v(s)$: gives the long-term value of state $s$
- **状態価値関数** $v(s)$ of an MRP is the expected return starting from state $s$
$$
v(s)=\mathbb{E}\left\{G_t|S_{t=2}\right\}
$$

- **状態価値関数** state-value function:
$$
v_\pi(s)=\mathbb{E}_\pi\left\{G_t|S_{t=s}\right\}
$$

# ベルマン期待期待 Bellman Expectation Equation
- **状態価値関数** : 即時報酬と後続状態の割引付き報酬の和に分解できる

$$
v_\pi(s)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma v_\pi(S_{t+1}|S_t=s)\right\}
$$

- **行動価値関数** action-value function:
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{G_t|S_{t}=s,A_{t}=a\right\}
$$

- **行動価値関数** 同じく分解可能 The action-value function can similarly be decomposed,
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a\right\}
$$


## 最適価値関数 Optimal Value Function
- 最適状態価値関数
$$
v_{*}(s) = \max_{\pi} v_{\pi}(s)
$$
- 最適行動価値関数
$$
q_{*}(s,a)=\max q_{\pi}(s,a)
$$

- ベルマン方程式 一般に非線形になるので難しい
-->

<!--
- No closed form solution (in general)
- Many iterative solution methods
- 幾つかの解法:
  - 価値反復
  - 方策反復
  - Q 学習
  - sarsa
-->

<!--
  - Value Iteration
  - Policy Iteration
  - Q-learning
  - Sarsa
-->

<!--
# Markov Reward Process
A Markov reward process is a Markov chain with values.

- A Markov Reward Process is a tuple $<S,P,R,\gamma>$
  - $S$ is a finite set of states
  - $P$ is a state transition probability matrix,
  - $P_{ss'}=P\of{S_{t+1}=s'\given{S_t=s}}$
  - $R$ is a reward function, $R_s=\mathbb{E}\BRc{R_{t+1}\given{S_t=s}}$
  - $\gamma$ is a discount factor

# Bellman Equation for MRPs
The value function can be decomposed into two parts:
  - immediate reward $R_{t+1}$
  - discounted value of successor state $\gamma v\of{S_{t+1}}$

$$
\begin{array}{lll}
v\of{s}&=&\mathbb{E}\BRc{G_t\given{S_t=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma\Brc{R_{t+2}+\gamma R_{t+3}+\ldots}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma G_{t+1}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}\given{S_{t}=s}}}
\end{array}
$$

# Bellman Equation for MRPs

$$
v\of{s}=\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}}\given{S_t=s}}
$$


$$
v\of{s}=R_s +\gamma\sum_{s'\in S} P_{ss'}v\of{s'}
$$

# Solving the Bellman Equation
- The Bellman equation is a linear equation
- It can be solved directly:
$$
\begin{array}{lll}
v &=& R +\gamma Pv\\
\Brc{I - \gamma P}v &=& R\\
v &=& \Brc{I-\gamma P}^{-1}R
\end{array}
$$

- Computational complexity is $O(n^3)$ for $n$ states
- Direct solution only possible for small MRPs
- There are many iterative methods for large MRPs, e.g.
  - Dynamic programming
  - Monte-Carlo evaluation
  - Temporal-Difference learning
-->


<!-- # 価値関数 Value Function -->
<!-- - 将来の報酬予測の関数 -->
<!--   - ある状態である行動を起こすとどれほどの報酬が得られるか -->
<!-- - **Q-値関数** Q-value function : 総期待報酬を得る関数<\!--gives expected total reward-\-> -->
<!--   - 方策 $\pi$ のもとで -->
<!--   - 状態 $s$ で行動 $a$ を行ったとき -->
<!-- $$ -->
<!--   Q^\pi\of{s,a}=\mathbb{E}\BRc{r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots\given{s,a}} -->
<!-- $$   -->

<!--
# 最適価値関数 Optimal Value Functions
- 最大の価値を与える関数
$$
Q^*(s,a)=\max_{\pi}Q^\pi(s,a)=Q^{\pi^*}(s,a)
$$
- 最適価値関数 $Q^*$ が得られれば最適方策 $\pi^*$ を求めることができる
$$
\pi^*(s)=\operatorname{argmax}_aQ^*(s,a)
$$

- 全ての意思決定における最適価値:
$$
\begin{array}{lll}
Q^*(s,a)&=&r_{t+1}+\gamma\max_{a_{t+1}}r_{t+2}+\gamma^2\max_{a_{t+2}}r_{t+3}+\ldots\\
           &=&r_{t+1}+\gamma\max_{a_{t+1}}Q^*(s_{t+1},a_{t+1})
\end{array}
$$
-->

<!-- from sliver (2016) icml lecture -->
<!--
- **ベルマン方程式** Bellman equation:
$$
Q^*(s,a)=\mathbb{E}_{s'}\left\{r+\gamma\max_{a'}Q^*(s',a')|s,a\right\}.
$$
-->

<!--
# 報酬(収益) Rewards
- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価Indicates how well agent is doing at step $t$

# Sequential Decision Making
- 目標 Goal: select actions to maximise total future reward
- 行為 Actions はmay have long term consequences
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある It may be better to sacrifice immediate reward to gain more long-term reward-
- Examples:
  - A financial investment (may take months to mature)
  - Refuelling a helicopter (might prevent a crash in several hours)
  - Blocking opponent moves (might help winning chances many moves from now)
-->

# RAINBOW, DQN 以降の発展

<center>
<img src="/assets/2017Hassel_RainbowFig1.svg" style="width:47%">
<img src="/assets/2017Hassel_RainbowRes1.svg" style="width:49%"><br/>

Rainbow の性能，
</center>

- カルパシーのブログ [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)


<center>
<img src="/assets/2017Hassel_RainbowFig1.svg" style="width:33%">

<img src="/assets/2017Hassel_Rainbow_fig3.svg" style="width:33%"><br/>
出典: @2018Hessel_Rainbow 左は各アルゴリズム単体の成績，右はアブレーション (ablation:廃止) 実験による性能低下
</center>

DDQN: 二重DQN，prioritized DQN 優先DQN，Dueling DQN 決闘DQN，A3C 非同期アドバンテージ，アクタークリティック, Distributional DQN 分散DQN，Noisy DQN

# 個別のゲームタイトル

<center>
<img src="/assets/Rainbow_appendix_Plots_GamesPub.svg" width="33%">
<img src="/assets/Rainbow_appendix_Plots_GamesAbl.svg" width="33%"><br/>

出典: Hassel+2018 <!--[@2018Hessel_Rainbow]--> の付録より
</center>


| Game               |  DQN|     A3C     |   DDQN | Prior. DDQN | Duel. DDQN | Distrib. DQN | Noisy DQN | Rainbow  |
|:------------------:|-----:|-----------:|-------:|------------:|-----------:|-------------:|----------:|----------:|
| ブロック崩し        |354.5 |**681.9**    |  368.9| 371.6       |      411.6 |        548.7 |   423.3   | 379.5    |
|モンテズーマの復習 | 47.0 |    67.0     |   42.0|        13.0 |       22.0 |        130.0 |     55.0  | **154.0**|
|プライベートアイ   |207.9 | 206.9       | -575.5|  179.0      |      292.6 |     5,717.5  |**5,955.4**| 1,704.4  |
|スペースインベーダー  |1293.8| **15,730.5**| 2628.7| 9,063.0     |    5,993.1 |      6,368.6 |   1,697.2 | 12,629.0 |

Human starts の評価: 訓練中に最高得点を獲得したエージェント・スナップショットから 200 回のテスト・エピソードで平均化した全ゲームの生得点。
DQN, A3C, DDQN, Dueling DDQN, Prioritized DDQN については，公表された得点を掲載。Distributional DQN と Rainbow については，著者自身によるエージェントの評価。
<!-- Human Starts evaluation regime: Raw scores across all games, averaged over 200 testing episodes, from the agent snapshot that obtained the highest score during training.
We report the published scores for DQN, A3C, DDQN, Dueling DDQN, and Prioritized DDQN. For Distributional DQN and Rainbow we report our own evaluations of the agents. -->


|             Game |        DQN  |   DDQN  | Prior. DDQN |    Duel. DDQN | Distrib. DQN |  Noisy DQN |  Rainbow |
|:----------------:|------------:|---------:|------------:|-------------:|-------------:|-----------:|---------:|
| ブロック崩し  |     385.5  |     418.5 |       381.5 |      345.3 |     **612.5**  |      459.1 |    417.5 |
| モンテズーマの復習 |       0.0 |       0.0 |         0.0 |        0.0 |          367.0 |        0.0 |   **384.0**|
| プライベートアイ |     146.7 |     129.7 |       200.0 |      103.0 |  **15,172.9**  |    3,966.0 |    4,234.0 |
| スペースインベーダー |    1692.3 |    2525.5 |     7,696.9 |    6,427.3 |        6,869.1 |    2,145.5 |  **18,789.0**|

No-op starts<br/>


No-op starts の評価: 訓練中に最高得点を得たエージェント・スナップショットから 200 回のテスト・エピソードで平均化した全ゲームの生得点。
DQN, DDQN, Dueling DDQN, Prioritized DDQN については，公表されている得点。Distributional DQN と Rainbow については，著者自身によるエージェントの評価。
A3C については，論文では no-ops 体制での得点が報告されていないため，掲載していない。
<!--
No-op starts evaluation regime: Raw scores across all games, averaged over 200 testing episodes, from the agent snapshot that obtained the highest score during training.
We report the published scores for DQN, DDQN, Dueling DDQN, and Prioritized DDQN. For Distributional DQN and Rainbow we report our own evaluations of the agents. A3C is not listed since the paper did not report the scores for the no-ops regime.-->

<!-- - 価値を $V$ で表せば， $V = \pi(s)$ あるいは $V = Q(s,a)$ である。あるいは $Q(a\vert s) = p(a\vert s)$ と書けば，条件付き確率の表記法ににて，状況 $s$ において，行為 $a$ を行った場合の価値のようにも表記することがある。
- $Q$ が $\theta$ をパラメータとするニューラルネットワークで表されているとすると，以下のように表記すれば，ニューラルネットワークにおける学習と同一の表記となる

$$
\Delta\theta = \alpha\nabla J(\theta) = \frac{\partial}{\partial \theta}J(\theta)
$$

$$
\Delta\theta = \alpha\nabla_\theta \pi(s) = \alpha\nabla \pi(s;\theta) = \frac{\partial}{\partial \theta}\pi(s;\theta)
$$-->

### エージェントと環境

- **環境** は **エージェント** に **観測** $S_t$ を与える。各離散時間ステップ $t=0,1,2\ldots$ において エージェントは **行動** $A_t$ を選択して応答
- 環境は次の報酬 $R_{t+1}$, 割引 $\gamma_{t+1}$, 状態 $S_{t+1}$ を与える。
この相互作用は，**マルコフ決定過程 (Markov Decision Process, or MDP)** $\left< \mathcal{S},\mathcal{A},T,r,\gamma \right>$ として定式化される。
$\mathcal{A}$ は有限の行動集合 $T(s,a,s')=P\left[S_{t+1}=s'\mid S_t=s,A_t=a\right]$ は(確率的な)遷移関数である。
- $R(s,a) = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]$ は報酬関数、$\gamma\in [0,1]$ は割引係数である。
- エピソード終了時に $\gamma_t=0$ となる場合を除き MDP は一定の エピソード的 であるが、アルゴリズムは一般的な形式で表現される。

* 各離散的な時間ステップ $t=0, 1, 2\ldots$ において，環境はエージェントに観測値 $S_{t}$ を与え， エージェントは行動 $A_{t}$ を選択して応答し，次に環境は次の報酬 $R_{t+1}$，割引 $\gamma_{t+1}$，状態$S_{t+1}$ を与える。
<!-- * この相互作用はマルコフ決定過程 (Markov Decision Process, MDP) として形式化され、$\left< \mathcal{S}, \mathcal{A}, T, r, \gamma \right>$ というタプルになる。
$T(s, a, s') = P[S_{t+1}=s'\mid S_{t}=s,A_{t}=a]$ は (確率的)遷移関数 $r(s,a)= \mathbb{E}\[R_{t+1} \mid S_{t}=s, A_{t}=a]$ は報酬関数，$\gamma \in \[0,1\]$ は割引率である。
実験では MDP はエピソード終了時に $\gamma_{t}=0$ となる以外は，定数 $\gamma_{t}=\gamma$ のエピソディックなものとしますが，アルゴリズムは一般的な形式で表されます。-->

<!-- At each discrete time step $t=0, 1, 2\ldots$, the environment provides the agent with an observation $S_t$, the agent responds by selecting an action $A_t$, and then the environment provides the next reward $R_{t+1}$, discount $\gamma_{t+1}$, and state $S_{t+1}$.
This interaction is formalized as a マルコフ決定過程 (Markov Decision Process, or MDP), which is a tuple $\left< \mathcal{S}, \mathcal{A}, T, r, \gamma \right>$, where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $T(s, a, s') = P[S_{t+1}=s'\mid S_t=s,A_t=a]$ is the (stochastic) transition function, $r(s,a) = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]$ is the reward function, and $\gamma \in [0,1]$ is a discount factor. In our experiments MDPs will be episodic with a constant $\gamma_{t}=\gamma$, except on episode termination where $\gamma_{t}=0$, but the algorithms are expressed in the general form. -->

- エージェントの 行動選択は 各状態に対する行動の確率分布を定義した方策 $\pi$ で与えられる。
- 時刻 $t$ で遭遇した状態 $S_t$ から 割引後のリターン $G_t=\sum_{k=0}^{\infty}\gamma_{t}^{(k)} R_{t+k+1}$ を定義する。エージェントが収集した将来の報酬の割引合計を表す。
ここで、将来の報酬 $k$ ステップの割引は、それまでの割引の積で与えられ $\gamma_t^{(k)} = \prod_{i=1}^{k}\gamma_{t+i}$  エージェントは、良い政策を見つけることによって、期待されるディスカウントされたリターンを最大化することを目的とする。

<!-- On the agent side, action selection is given by a policy $\pi$ that defines a probability distribution over actions for each state.
From the state $S_t$ encountered at time $t$, we define the discounted return $G_t =\sum_{k=0}^{\infty} {\gamma_t^{(k)} R_{t+k+1}}$ as the discounted sum of future rewards collected by the agent, where the discount for a reward $k$ steps in the future is given by the product of discounts before that time, $\gamma_t^{(k)} = \prod_{i=1}^{k} \gamma_{t+i}$.
An agent aims to maximize the expected discounted return by finding a good policy. -->

- **方針(ポリシー)** は 直接学習されてもよいし，他の学習された量の関数として構築されてもよい。
- 価値に基づく強化学習では，エージェントは，与えられた状態 $v^{\pi}(s)=E_{\pi}\left[G_t| S_t=s\right]$ または，
状態-行動のペア
$q^{\pi}(s,a)=E_{\pi} [G_t\mid S_{t}=s, A_{t} =a ]$ から，方針(ポリシー) $\pi$
に従うとき，期待される割引されたリターン，または，価値の推定値を学習する。
- 状態-行動関数から新しい方針(ポリシー) を導出する一般的な方法は，行動値に対して $\epsilon$-貪欲に行動することである。
これは，確率 $(1-\epsilon)$ で最も高い値の行動(Greedy) を取り，それ以外は確率 $\epsilon$ で一様にランダムに行動することに相当。
この種の方策は 「探索」を導入するために使用される：
- 現在の推定値に応じて，最適でない行動をランダムに選択することによって，エージェントは，適切なときに推定値を発見し，修正することができる。
- 主な制限は，将来に向けた代替行動コースを発見することが困難であることである。

<!-- The policy may be learned directly, or it may be constructed as a function of some other learned quantities.
In value-based reinforcement learning, the agent learns an estimate of the expected discounted return, or value, when following a policy $\pi$ starting from a given state, $v^\pi(s)=E_\pi[G_t| S_t =s]$, or state-action pair, $q^\pi(s,a) = E_\pi[G_t|S_t=s, A_t =a]$.
A common way of deriving a new policy from a state-action value function is to act $\epsilon$-greedily with respect to the action values.
This corresponds to taking the action with the highest value (the \textit{greedy} action) with probability $(1-\epsilon)$, and to otherwise act uniformly at random with probability $\epsilon$.
Policies of this kind are used to introduce a form of \textit{exploration}: by randomly selecting actions that are sub-optimal according to its current estimates, the agent can discover and correct its estimates when appropriate.
The main limitation is that it is difficult to discover alternative courses of action that extend far into the future; this has motivated research on more directed forms of exploration. -->

- **動作主** (エージェント) は，**環境** あるいは **外界**から，**状態** $s$ と**報酬** $r$ とを受け取る。逆に言えば，環境 は 動作主 に対して，状態 と 報酬 とを提供する
- 動作主 は 環境 に働きかけて，環境と相互作用して，動作主の報酬を最大化しようとする。
このときの報酬は，方向性を持たない値 スカラ である。
たとえば，犬など動物の躾，調教を行う場面を考える。飼い主は，動物に対して，褒めるか叱るか，のいずれかを行う。
これがスカラ値に相当する。すなわち，どの行動が，どのように良かったのか，あるいは，何がイケナイから罰を受けたのかは，教えない。
動物の場合，言葉が通じないので内容については伝達しようがない。
躾を受ける側の動物が，報酬（あるいは罰）を受け取ることで，行動を変容させる。

- 方策 (ポリシー) : $\pi(s)$  状況，あるいは 状態 を $s$ で表す。
$s$ における（あるいは取りうる）価値を表す ポリシーの頭文字 $p$ を使わず，対応するギリシャアルファベット $\pi$ を使うのは，確率を表す $p$ との混乱を避ける意味もある。
- 行動，あるいは 行為 を $a$ で表す。
- Q 行動価値関数: 状況 $s$ において，行為 $a$ を行う際に得られる価値を $Q(s,a)$ と表記する。

- 最適行動価値 optimal Q values $Q^{* }$ (キュースターと発音)

$$
Q^{* }(s,a)=\mathbb{E}_{s'}\left[r+\gamma\max_{a'} Q(s',a')^{* }\mid s,a\right]
$$

上式 RHS を <font color="red">$r+\gamma\max_{a'} Q(s',a')^{* }$</font> を教師信号として扱う

平均 2 乗誤差 (MSE)

$$
\ell =\left(r+\gamma\max_{a'} Q(s',a')^{* }- Q(s,a)\right)^{2}
$$

# REINFORCE: Monte Carlo Policy Gradient
<!-- @2018SuttonBartoRL chapt 13.3 -->

<!-- # アタリのゲームによるランキング

- [Atari Games on Atari 2600 Freeway](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)
-->

<center>
<img src="/2022assets/2020-1019paperswithcode_sota_atari_games_on_atari_2600_freeway.svg" style="width:49%"><br/>
</center>

ただし，Q 学習では不安定であった。DQN では， **経験再生** を用いることで，系列間相関と，教師信号の安定性を解決


# 二重 DQN (Double DQN)

[@2015Hasselt_doubleDQN]

最大の Q を求めるときに，自身の Q の計算が入っている，これを 2 つの Q 関数を並行して用いることで解消。

<!--
#### 分散優先体験リプレイ (Distributed Priooritized replay (APE-X))
@2016Schaul_prioritized_replay;@2018Horgan_APE-X-->

<!--
#### 逆強化学習 (Inverse RL)
@1998Russell_inverse_reinforcement_learning;@2000NgRussell_InverseRL

逆とは，価値の推定-->

Q 学習の際に，Q を最大にする行動を使って，Q の値を更新することが行われる。
具体的な更新式の核心部分は以下のとおり:

$$
R_{t+1} + \gamma Q\left(S_{t+1},\arg\max_a Q\left(S_{t+1},a\right)\right).
$$

Q を評価する際に，内部で Q を評価している。Q の評価が不当になる可能性があるので，Q の評価と更新とを別々の Q 関数を用意して，公平性を保証する

# 優先的経験再生 Prioritized replay.

<!-- DQN samples uniformly from the replay buffer. Ideally, we want to sample more frequently those transitions from which there is much to learn.
As a proxy for learning potential, prioritized experience replay (Schaul et al. 2015) samples transitions with probability ptrelative to the last encountered absolute TD error: -->

- DQN では 再生バッファから一様にサンプリングする。
- 理想的には 学習すべきことが多い遷移をより頻繁にサンプリングしたい
- そこで **優先的経験再生** (Schaul+2015) では 最後に遭遇した誤差が大きな系列を優先的にサンプリング

$$
p_t\propto \left|
R_{t+1}+\gamma_{t+1}\max_{a'} Q(s_{t+1},a') - Q(s_t,a_t)
\right|^\omega
$$

$\omega$ はハイパーパラメータで，$\omega=0.5$

# 決闘ネットワーク (Dueling Network)
@2016Wang_dueling。 Q 関数（⾏動価値）を価値関数と⾏動との差 (アドバンテージ) として分岐したニューラルネットワークで表現

<center>
<img src="/assets/2016duelingNet.svg" style="width:33%"><br/>
ドュエリング(決闘) ネットワークの模式図 (Wang+2016)
</center>

<!-- 決闘ネットワークは，価値ベースの RL のために設計されたニューラルネットワークアーキテクチャ。
これは，価値ストリームとアドバンテージストリームという 2 つの計算ストリームを特徴としており，畳み込み符号化器を共有し，特別な集計装置 (アグリゲータ)によってマージされます (Wang et al. 2016)。
これは，アクション値の以下の因数分解に対応しています。
The dueling network is a neural network architecture designed for value based RL.
It features two streams of computation, the value and advantage streams, sharing a convolutional encoder, and merged by a special aggregator (Wang et al. 2016).
This corresponds to the following factorization of action values:-->

$$
Q(s,a) = v(s) + a(s,a) - \frac{\sum_{a'}a(s,a')}{N_{\text{行動}}}
$$

# A3C

### アクタークリティック actor critic

- AC: アクター（行為者） Actor と クリティック (Critic) 批評家。アクターは方策（ポリシー）の改善を行い，クリティックは価値の更新を行う。Q 学習は，アクターとクリティックの両者を含む。
- アドバンテージ: $Q(s,a) - V(s)$ Q の引数は 状態と行為との２つから，報酬を定義，一方 価値関数とは 状況 から報酬を定義なので，この差をアドバンテージと呼ぶ

- Asyncronous Advantage Actor Critic

- A3C とは，アドバンテージ付きの DQN を非同期更新したもの

<!-- #### 逆強化学習 Inverse Reinforcement Learning

@2000NgRussell_InverseRL

観察された行動から報酬関数を推測する。-->

<!-- # マルチステップ強化学習 (Multi-step RL)

@Sutton_and_Barto1998 DQNでは1-stepの報酬を⽤いて、教師データを作成しているが、これをn-stepに拡張することで、学習が促進される場合がある -->

# 分散 DQN あるいはカテゴリカル DQN とも呼ばれる (Categorical DQN)

@2017Bellemare_C51

<!-- We can learn to approximate the distribution of returns instead of the expected return.

Recently Bellemare, Dabney, and Munos (2017) proposed to model such distributions with probability masses placed on a discrete support z, where z is a vector with $N_{\text{atoms}}\in\matcal{N}^{+}\text{atoms}$, defined by $z^i = v_{\min} + (i-1)\frac{v_{\max}-v_{\min}}{V_{\text{atom}}-1}$.
The approximating distribution dt attime t is defined on this support, with the probability mass pi(St;At) on each atom i, such that dt = (z; p(St;At)).
The goal is to update such that this distribution closely matches the actual distribution of returns. -->

期待報酬を離散値ではなく，N 分割したヒストグラムとして表現，


# ノイズネットワーク (Noisy networks)

@2018Fortunato_noisy_networks

<!-- #### 優先体験レプレイ (Prioritized Experience Replay)

- DQN では Experience Reply の活⽤が学習の効率化に寄与していた。
- それは，経験をランダムにサンプルすることで i.i.d. に近づく，および 学習に有⽤なレアな経験の再利⽤性を⾼める。
だが，経験をストアするためには⼤きな記憶領域が必要。
再⽣する記憶には、学習を促進するものとそうではないものがあるはずで，importance sampling によって悪影響を緩和 -->

<!-- The limitations of exploring using $\epsilon$-greedy policies are clear in games such as Montezuma’s Revenge, where many actions must be executed to collect the first reward.
Noisy Nets (Fortunato et al. 2017) propose a noisy linear layer that combines a deterministic and noisy stream, -->

イプシロン 貪欲 ($\epsilon$-グリーディ) な探索を用いる時の効率の悪さを乱数を加えることで軽減。
経験を保持するためには⼤きな記憶領域が必要。
<!--再⽣する記憶には、学習を促進するものとそうではないものがあるはずで，importance samplingによって悪影響を緩和-->
例えば，モンテズーマの復讐では，鍵を探し出して，その鍵を使って局面を打開するまでに要するエピソードの長さから，最初に報酬を得られるまでのエピソード系列が長いので，探索行動の効率が悪い。

# 非同期 強化学習

- 非同期更新については省略します。ですが，複数のエージェントを同時に実行し，その結果を束ねることで性能の改善が見込まれます。下図を見てください。

- A3C, [Gorila](Massively Parallel Methods for Deep Reinforcement Learning), [IMPALA](https://arxiv.org/abs/1802.01561), [APE-X](https://arxiv.org/abs/1803.00933), [NGU](https://arxiv.org/abs/2002.06038) (Never Give Up!)

<center>
<img src="/assets/2018Hogan_APE-X_fig2.svg" style="width:30%">
<img src="/assets/https___qiita-image-store.s3.amazonaws.com_0_144878_9297076e-65e3-9dac-763d-66d646969397.png" style="width:35%">
</center>

- [Never Give Up ICLR 2020 デモビデオ](https://sites.google.com/view/nguiclr2020)

<!-- - モンテズーマ・リベンジ オンライン版 <https://www.retrogames.cz/play_124-Atari2600.php>

<center>
<video width="15%" autoplay loop markdown="0" controls muted>
<source src="assets/openai_atari-reset_git_monte_video.mp4">
</video><br/>
出典: モンテズーマの復讐の解<https://github.com/openai/atari-reset>
</center> -->

# まとめ

- RAINBOW とは，複数の改善手法の全部乗せ。(RAINBOW = DDQN + Prioritized DDQN + Dueling DQN + A3C + Distributional DQN + Noisy DQN)
- アブレーションによる評価から，複数の手法が同時に性能向上に関与している
- 非同期更新 GORILA, APE-X, R2D2, NGU

<!--
## APE-X
@2018Horgan_APE-X

## TRPO (Trust Region Policy Optimization)
@2015Schulman_trpo

## IMPALA
<https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/>


#### TPRO と PPO
- source: <https://blog.syundo.org/post/20171204-reinforcement-learning-natural-policy-gradient-trpo-ppo/>
- さらに，上のトップ: <https://blog.syundo.org/post/20180115-reinforcement-learning/>

##### TRPO

最適化計算における更新ステップの計算に KL ダイバージェンスによる制約を加えたものが TRPO (Trust Region Policy Optimization) である。
この方法は、を KL ダイバージェンスで拘束しているため、近似的には自然勾配法と同様の手法となる。
TRPO は方策勾配法に限らず、モデルなし学習においても利用することができるが、以下では方策勾配法と組み合わせることを前提に述べる。

さて、$\theta$ でパラメタライズされた方策  $\pi_\theta(a\vert s)$ がある場合、方策勾配が

$$
\hat{g} = \mathbb{E}_\pi\left[ \nabla_\theta \log\pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

であった。これは、以下の値 $L_\theta$ の微分値である。

$$
L_\theta(\theta)=\mathbb{E}_\pi\left[\log \pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

このとき、更新のステップを制限するために、以下のようにKLダイバージェンスで制約を課して最大化を行う。

$$
\text{maximize}_{x} L_{\theta_{\text{old}}}(\theta)
$$

$$
\text{subject to } D_{KL}(\theta_{\text{old}},\theta)\le\delta
$$

ここで、 $\theta_{\text{old}}$ は $\pi_\theta$ におけるパラメタ $\theta$ の前の値である。
また、$D_{KL}$ は確率分布 $\pi_\theta$ と $\pi_{\theta_{\text{old}}}$ の間の KL ダイバージェンスであり、 $D_{KL}\max(\theta_{\text{old}},\theta)$ は任意のパラメタの組み合わせに対して、KLダイバージェンスを計算したときの最大値を表す。

実用的には組み合わせが膨大になり、最大値を求めるのは難しいため、制約はヒューリスティックに以下のように平均値で代用する。
$$
\text{maximize}_x L_{\theta_{\text{old}}}(\theta)$$
$$

$$
\text{subject to} D_{KL}^{\max}(\theta_{\text{old}},\theta) \le \delta
$$

ここで、 $\bar{D}_{KL}(\theta_{\text{old}},\theta)=\mathbb{E}_{s\sim p}\left[D_{KL}(\pi_\theta(\cdot\vert s),\pi_{\theta_2}(\cdot\vert s)\right]$ である。

ただし、制約において問題を解くのは簡単ではないので、以下のようにソフト制約を使う形に書き下す。

$$
\text{maximize}_x \mathbb{E}_\pi\left[L_{\theta_{\text{old}}}(\theta)-\beta\bar{D}_{KL}(\theta_{\text{old}},\theta)\right]
$$

以上が、TRPO の概要である。
-->

<!--
##### PPO

PPO (Proximal Policy Optimization) は方策の目標値をクリッピングすることで、おおまかに方策の更新を制約する方法である。
TRPO では KL ダイバージェンスを制約として利用していたが、PPO では、目的関数を以下の $L^{\text{clip}}$ として、勾配を求める。

$$
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min(r_t(\theta)\hat{A}_t,\text{clip}(r(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\right]
$$

ここで、$r_t(\theta)$ は確率の比率であり、

$$
r_t(\theta) = \frac{\pi_\theta(s_t,a_t)}{\pi_{\theta_{\text{old}}}(s_t,a_t)}
$$

である。また、$\text{clip}(r(\theta), 1−\epsilon, 1+\epsilon)$ は $r(\theta)$ が $1−\epsilon} あるいは $1+\epsilon$ を超過しないように制限する関数である。
$\text{clip}(r(\theta), 1−\epsilon , 1+\epsilon)^At$ の グラフと、$L^{\text{CLIP}}$ を以下に示す(John Schulmanらより引用)。
-->



# 強化学習

未知の環境にあるエージェントがいて，このエージェントは環境とインタラクトすることでいくつかの報酬を得ることができるとします．
エージェントは、累積報酬を最大化するように行動する必要があります。
現実的には、ゲームでハイスコアを出すロボットや、物理的なアイテムを使って物理的なタスクをこなすロボットなどが考えられますが、これらに限定されるものではありません。

<!--
Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment.
The agent ought to take actions so as to maximize cumulative rewards.
In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these.
-->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/)


## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb)

<!-- https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb)-->

以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/)
を用いています。

- [倒立振子 (cartpole) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0716Gym_cartpole_rendering.ipynb){target="_blank"}

Colaboratory 上で gym を 動作させるためには [StarAI の開発したレンダリング環境](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/) が必要です。

### キーコンセプト
<!-- ### Key Concepts -->

* エージェントは **環境** の中で行動する。
環境がある行動に対してどのように反応するかは， 我々が知っているかどうかわからない **モデル** によって定義されます。
エージェントは， 環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ， 多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができます。
エージェントがどの状態に到達するかは， 状態間の遷移確率 ($P$) によって決定される。
行動を起こすと、環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与えます。
<!--
The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.
-->

モデルは報酬関数と遷移確率を定義しています。
モデルがどのように動作するかを知っている場合と知らない場合があり、これにより 2 つの状況が区別されます。

- **モデルベース**：完全な情報で計画を立てて学習を行う。
環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP)によって最適解を求めることができます。
- **モデルフリー**：不完全な情報での学習；アルゴリズムの一部として明示的にモデルを学習しようとする。
以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.
-->

エージェントの **ポリシー**  $\pi(s)$ は、**総報酬** を最大化することを目的として、ある状態で取るべき最適な行動のガイドラインを提供する。
各状態には、その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数$V(s)$ が関連付けられています。
言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化します。
強化学習で学習しようとするのは，方策関数と価値関数の両方です。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.
-->


<!--
使うと環境構築は楽です。ただし -->

<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
## The Animal-AI Olympics

<http://animalaiolympics.com/>

<div>
<video style="width:84%" src='./assets/Animal-AI Olympics Preview.mp4' controls >
</div>

## Unity Obstacle Tower Challenge

<https://www.aicrowd.com/challenges/unity-obstacle-tower-challenge>

<div>
<video style="width:84%" src='./assets/Unity Obstacle Tower Challenge.mp4' controls >
</div>-->


# 強化学習

未知の環境にあるエージェントがいて，このエージェントは環境とインタラクトすることでいくつかの報酬を得ることができるとします。
動作主 (エージェント) は，累積報酬を最大化するように行動する必要があります。
現実的には，ゲームでハイスコアを出すロボットや，物理的なアイテムを使って物理的な課題をこなすロボットなどが考えられます。
ですが，これらに限定されるものではありません。
<!-- Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment.
The agent ought to take actions so as to maximize cumulative rewards.
In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these. -->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/)

<!-- ## 実習ファイル

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb) -->

<!-- 以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。 -->

## 基本概念
<!-- ### Key Concepts -->

<!-- RL の重要な概念を正式に定義しましょう。
Now Let's formally define a set of key concepts in RL. -->

* 動作主 (エージェントは) **環境** の中で行動する。
* 環境がある行動に対してどのように反応するかは，我々が知っているかどうかわからない **モデル** によって定義される。
* 動作主 (エージェント) は，環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ，多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができる。
* 動作主 (エージェント) がどの状態に到達するかは，状態間の遷移確率 ($P$) によって決定される。
* 行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与える。
<!-- The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.-->

* モデルは報酬関数と遷移確率を定義する。
* モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別される。
<!-- The model defines the reward function and transition probabilities.
We may or may not know how the model works and this differentiate two circumstances: -->

    * **モデルベース**
<!-- * **モデルを知る**：完全な情報で計画を立てる、モデルベースの RL を行う。-->
        環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP) によって最適解を求めることができます。
<!-- アルゴリズム 101 の授業で習った "longest increasing partialence" や "traveling salesmen problem" をまだ覚えていますか？笑
これはこの記事の焦点ではありませんが。-->

    * **モデルフリー**
<!-- * **モデルを知らない**：不完全な情報での学習；モデルフリー RL を行うか，-->
アルゴリズムの一部として明示的にモデルを学習しようとする。
<!-- 以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。 -->

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.-->

* 動作主 (エージェント)の **方策 (ポリシー)**  $\pi(s)$ は，**総報酬** $G$ を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
* 各状態には，その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数 $V(s)$ が関連付けられる。
* 言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化する。
* 強化学習で学習しようとするのは，方策関数 と 価値関数の両方。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.
-->

<center>
<img src="/assets/RL_algorithm_categorization.png" width="94%"><br/>
<div style="text-align: justify;width: 88%;background-color: cornsilk;">
価値，方針(ポリシー)，環境 のいずれをモデル化したいかに基づく強化学習アプローチのまとめ。
(画像出典：[David Silver の強化学習講座](https://youtu.be/2pWv7GOvuf0) より転載)
<!--
Fig. 2. Summary of approaches in RL based on whether we want to model the value, policy, or the environment.
(Image source: reproduced from David Silver's RL course [lecture 1](https://youtu.be/2pWv7GOvuf0).)
-->
</div>
</center>

* 動作主 (エージェント) と環境の相互作用には $t=1, 2, \dots,T$ 時間内の一連の行動と観測された報酬が含まれる。
* この過程で， エージェントは環境に関する知識を蓄積し，最適な政策を学習し，最適な政策を効率的に学習するために，次にどのような行動を取るべきかを決定する。
* 時間ステップ $t$ における状態，行動，報酬をそれぞれ $S_t$, $A_t$, $R_t$ とする。
<!-- このように， インタラクションシーケンスは 1 つの **エピソード** (「試行」または「軌跡」とも呼ばれる) で完全に記述され， 系列は末端の状態 $S_T$ で終了します。 -->
<!-- The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \dots, T$.
During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy.
Let's label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively.
Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $S_T$:-->

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

<!-- RL アルゴリズムの様々なカテゴリーを調べる際によく遭遇する用語。 -->
<!-- Terms you will encounter a lot when diving into different categories of RL algorithms: -->

- **モデルベース**: モデルが既知であるか、アルゴリズムがそれを明示的に学習する。
- **モデルフリー**: 学習時にモデルに依存しない。
- **オンポリシー**<!-- **On-policy** -->: アルゴリズムの学習にターゲットポリシーからの決定論的な結果やサンプルを使用する。
- **オフポリシー**<!-- **Off-policy** -->: ターゲット・方策(ポリシー)ではなく，異なる<!-- ビヘイビア・ -->方策(ポリシー)で生成された上他繊維やエピソード分布で学習する。

<!--
- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.-->

## モデル: 移行と報酬 transition and reward
<!-- #### Model: Transition and Reward -->

* モデルとは，環境を記述する実体。
* モデルを用いることで，環境がどのように動作主 (エージェント) と相互作用し，フィードバックを与えるかを学習または推論することができる。
* モデルには，遷移確率関数 $P$ と報酬関数 $R$ の 2 つの主要部分がある。
<!-- The model is a descriptor of the environment.
With the model, we can learn or infer how the environment would interact with and provide feedback to the agent.
The model has two major parts, transition probability function $$P$$ and reward function $R$. -->

* 状態 $s$ にいるとき，次の状態 $s’$ に到達して報酬 $r$ を得るために行動 $a$ をとることを決めたとする。
これは 1 つの **遷移** ステップと呼ばれ， タプル $(s, a, s', r)$ で表される。
<!-- Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r.
This is known as one **transition** step, represented by a tuple (s, a, s', r).-->

* 遷移関数 $P$ は，行動 $a$ を起こした後に報酬 $r$ を得て，状態 $s$ から $s’$ に遷移する確率を記録したものである。
ここでは $\mathbb{P}$ を「確率」の記号として用いる。
<!-- The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r.
We use $$\mathbb{P}$$ as a symbol of "probability".-->

$$
P(s', r \vert s, a)  = \mathbb{P} \left[S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a\right]
$$

したがって，状態遷移関数は $P(s', r \vert s, a)$ の関数として定義することができる。
<!-- Thus the state-transition function can be defined as a function of $P(s', r \vert s, a)$: -->

$$
P_{ss'}^a = P(s' \vert s, a) = \mathbb{P} \left[S_{t+1} = s' \vert S_t = s, A_t = a\right] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

報酬関数 $R$ は 1 つの行動 (アクション) によって引き起こされる次の報酬を予測する。
<!-- The reward function R predicts the next reward triggered by one action:-->

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$

## 方策 (ポリシー) policy
<!-- #### Policy -->

* 方策 (ポリシー) とは，動作主 (エージェント) の行動関数 $\pi$ として，状態 $s$ においてどのような行動を取るべきかを示すもの。
これは， 状態 $s$ から行動 $a$ への写像であり，決定論的なものと確率論的なものがある。
<!-- Policy, as the agent's behavior function $\pi$, tells us which action to take in state s.
It is a mapping from state s to action a and can be either deterministic or stochastic:-->

    * 決定論的: $\pi(s) = a$.
    * 確率論的: $\pi(a \vert s) = \mathbb{P}_ {\pi} \left[A=a \vert S=s\right]$.

## 価値関数 value function
<!-- #### Value Function -->

* 価値関数は，将来の報酬の予測によって，状態の良さや，状態や行動がどれだけ報われるかを測定するもの。
* 将来の報酬は **リターン** とも呼ばれ，今後の報酬を割引いたものの総和。
* 時刻 $t$ から始まるリターン $G_t$ を計算すると以下のようになる:

<!--Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward.
The future reward, also known as **return**, is a total sum of discounted rewards going forward.
Let's compute the return $G_t$ starting from time t:-->

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

$[0,1]$ の割引係数 $\gamma$  は，将来の報酬に罰則 (ペナルティ) を課すことになる。
<!-- The discounting factor $\gamma \in [0, 1]$ penalize the rewards in the future, because:-->

* 将来の報酬は不確実性が高い可能性がある。
例：株式市場。
* 将来の報酬はすぐに得られるものではない。
例：人間として 5 年後よりも今日楽しいことをしたいと思うかもしれない。
* 割引 (ディスカウント) は数学的な利便性をもたらす。
つまり，リターンを計算するために将来のステップを永遠に追跡する必要がないのです。
* 状態遷移グラフの無限ループを心配する必要はない

<!-- - The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.-->

* ある状態 $s$ の **状態値** は， 時間 $t$ でこの状態にある場合の期待リターンであり $S_{t}=s$ である。
<!-- The **state-value** of a state s is the expected return if we are in this state at time t, $S_t = s$: -->

$$
V_{\pi}(s) = \mathbb{E}_ {\pi}[G_t \vert S_t = s]
$$

* 同様に，状態と行動の対としての **行動価値**  $Q$ を以下のように定義する:
<!-- Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:-->

$$
Q_{\pi}(s, a) = \mathbb{E}_ {\pi}[G_t \vert S_t = s, A_t = a]
$$

* 目標方策 $pi$ に従うので，可能な行動に対する確率分布と Q 値を利用して，状態値を回復することができる。
<!-- Additionally, since we follow the target policy $\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: -->

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行動価値と状態価値の際は，**アドバンテージ (advantage)** と呼ばれる
<!-- The difference between action-value and state-value is the action **advantage** function ("A-value"): -->

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

### 最適価値と最適方策
<!-- ### Optimal Value and Policy -->

* 最適価値関数は，最大リターンを産む
<!-- The optimal value function produces the maximum return: -->

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

* 最適方策は，最適価値関数によって達成される:
<!-- The optimal policy achieves optimal value functions: -->

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

* $V_{\pi_{\star}}(s)=V_{\star}(s)$ かつ $Q_{\pi_{\star}}(s,a)=Q_{\star}(s,a)$ である
<!-- And of course, we have $V_{\pi_{\star}}(s)=V_{\star}(s)$ and $Q_{\pi_{\star}}(s, a) = Q_{\star}(s, a)$. -->


## マルコフ決定過程
<!-- ## Markov Decision Processes-->

* 広義には強化学習は，**マルコフ決定過程 (MDP: Markov Decision Process)** の一部である。
    * **マルコフ性 Markov property** とは，将来の状態が現在の状態にのみ依存すること。
    * とりわけ，不確かな状況でのマルコフ決定過程を **POMDP (Partially Observed Markov Decision Process)** と呼ぶ
<!-- In more formal terms, almost all the RL problems can be framed as **Markov Decision Processes** (MDPs).
All states in MDP has "Markov" property, referring to the fact that the future only depends on the current state, not the history: -->

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

逆に言えば，未来と過去とは **条件付き独立** である。
将来の意思決定は現在の状況によって定まる。
<!-- Or in other words, the future and the past are **conditionally independent** given the present, as the current state encapsulates all the statistics we need to decide the future.
 -->

<center>
<img src="/assets/agent_environment_MDP.png" width="49%"><br/>
Fig. 3. The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton & Barto (2017).
</center>

* マルコフ決定過程 (Markov deicison process) は  $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, の 5 つの要素で構成される
<!-- A Markov deicison process consists of five elements $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where the symbols carry the same meanings as key concepts in the [previous](#key-concepts) section, well aligned with RL problem settings:-->

- $\mathcal{S}$: 状態集合<!-- - a set of states; -->
- $\mathcal{A}$: 行動集合 <!-- - a set of actions; -->
- $P$: 遷移関数 <!-- - transition probability function; -->
- $R$: 報酬関数<!-- - reward function; -->
- $\gamma$: 割引率<!--  - discounting factor for future rewards. -->

未知の環境では，$P$ と $R$ とは完全に知ることはできない
<!-- In an unknown environment, we do not have perfect knowledge about $P$ and $R$. -->

<center>
<img src="/assets/mdp_example.jpg" width="66%"><br/>
<div style="text-align: center;width:88%;background-color:cornsilk;">
マルコフ決定過程の例：典型的な仕事の一日
(画像出典: [randomant.net/reinforcement-learning-concepts](https://randomant.net/reinforcement-learning-concepts/))
<!--
Fig. 4. A fun example of Markov decision process: a typical work day.
![MDP example]({{ '/assets/images/mdp_example.jpg' | relative_url }}){: class="center"}
Fig. 4. A fun example of Markov decision process: a typical work day.
-->
</div>
</center>

## ベルマン方程式
<!-- ### Bellman Equations -->

ベルマン方程式とは， 価値関数を目先の報酬と割引された将来の価値に分解する一連の方程式を指します。
<!-- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. -->

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

Q 関数とは
<!-- Similarly for Q-value, -->

$$
\begin{aligned}
Q(s, a)
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$


### 期待ベルマン方程式
<!-- ### Bellman Expectation Equations -->

* 再帰的な更新過程は，さらに分解すると，状態値関数と行動値関数の両方で構成される方程式となる。
* 今後の行動ステップを進める際には $\pi$ の方針に沿って $V$ と $Q$ を交互に拡張していく。

<!-- The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.
As we go further in future action steps, we extend V and Q alternatively by following the policy $\pi$.-->

<center>
<img src="/assets/bellman_equation.png" width="60%"><br/>
<div style="text-align: center;width: 88%;background-color: powderblue;">
図 5. ベルマン期待値方程式がどのように状態値関数と行動値関数を更新するかを示す図。
<!--
Fig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.
![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }}){: style="width: 60%;" class="center"}
-->
</div>
</center>

$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi
} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi
} (s', a')
\end{aligned}
$$

### 最適ベルマン方程式
<!-- ### Bellman Optimality Equations -->

* 方策 (ポリシー) に従った期待値を計算するのではなく，最適値にしか興味がないのであれば，方策 (ポリシー) を使わずに代替更新中の最大リターンにすぐに飛びつくことができる。
<!-- RECAP:  最適値 $V_*$ と $Q_*$ は，得られる最高のリターンであり [ここ](#optimal-value-and-policy) で定義されています。-->
<!-- If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy.
RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined [here](#optimal-value-and-policy).-->

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

* 上式はベルマンの期待方程式と酷似している。
<!-- Unsurprisingly they look very similar to Bellman expectation equations. -->

* もし環境の完全な情報があれば，この問題は計画問題となり，動的計画法 (DP: dynamic programming) で解くことができる。
* 駄菓子菓子，ほとんどの場合 $P_{ss'}^a$ や $R(s, a)$ が不明であるため，ベルマン方程式を直接適用して MDP を解くことはできない。
* 駄菓子菓子，多くの RL アルゴリズムの理論的基礎を与えている。

<!--
If we have complete information of the environment, this turns into a planning problem, solvable by DP.
Unfortunately, in most scenarios, we do not know $P_{ss'}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. -->


<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
-->
