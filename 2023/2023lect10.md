---
title: 第10回
author: 浅川 伸一
layout: home
---
<!-- コードはちゃんと書かないといかんな。 -->
<!-- <link href="/css/asamarkdown.css" rel="stylesheet"> -->

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 16/Jun/2023<br/>
Appache 2.0 license<br/>
</div>

## 本日のキーワード

1. 近位性方針最適化 (PPO: Proximal Policy Optimization)
2. GPT-4
<!-- 4. ファインチューニング -->

* [方針勾配法 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1126pg_pong_rendering.ipynb){:target="_blank"}
* 近位方針最適化 (PPO: Proximal Policy Optimization)。[OpenAI による PPO の説明](https://openai.com/research/openai-baselines-ppo)

    * [https://cdn.openai.com/openai-baselines-ppo/flagrun-no-bombardment-awesome-camera-work.mp4](https://cdn.openai.com/openai-baselines-ppo/flagrun-no-bombardment-awesome-camera-work.mp4){:target="_blank"} PPO の訓練を受けたエージェントは，目標地点に向かう際に，即座にターンやティルトを行うことができる柔軟な移動方針を身につけている。
	* [https://cdn.openai.com/openai-baselines-ppo/atlas.mp4](https://cdn.openai.com/openai-baselines-ppo/atlas.mp4){:target="_blank"}


* [miniGPT4 のデモ 3 日間のみ有効](https://ab39e15c4cba4d17de.gradio.live){:target="_blank"}
* [GPT-4 については，第 2 回講義資料](/2023/2023lect02) を見てください

<!-- <center>
<div>
<video controls src="https://www.youtube.com/watch?v=__tftoxpBAw" muted="false"></video>
<div class="figcaption">
</div></div>
</center> -->

## GPT-3

* **ファインチューニング fine-tuning**: 例文についての勾配の更新，すなわち，再訓練が行われる。
* **零撃 zero-shot**: 課題説明文だけが与えられ，モデルはその文に対する応答を予測する。勾配の更新は行われない。
* **一撃学習 one-shot**: 課題説明文に加えて，例文が一つだけ与えられる。勾配の更新は行われない。
* **少撃 few-shot**: 課題説明文に加えて，例文が数個与えられる。勾配の更新は行われない。

<!-- <img src="/2023assets/2021Brown_GPT3_fig2_1.jpg"> -->

### ニュース記事の生成 <!-- ### 3.9.4 News Article Generation-->

生成言語モデルに関する以前の研究では，ニュース記事のもっともらしい最初の文からなる人間が書いたプロンプトを与えられたモデルからの条件付きサンプリングによって，「ニュース記事」を生成する能力を定性的に検証した<!--[RWC+19]-->。
GPT-2 <!--[RWC+19]--> と比較すると，GPT-3 の学習に使用したデータセットはニュース記事へのウェイトが低いため，生の無条件サンプリングによってニュース記事を生成しようとすると，あまり効果がない。
例えば，GPT-3 はしばしば「ニュース記事」の最初の提案文をツイートとして解釈し，生成応答やフォローアップツイートを投稿する。
この問題を解決するために，我々は GPT-3 の少数撃学習能力を利用し，モデルの文脈にある 3 つの過去のニュース記事を条件として提供した。
提案された次の記事のタイトルとサブタイトルがあれば，モデルは「ニュース」ジャンルの短い記事を確実に生成することができるようになった。
<!-- Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story [RWC+19].
Relative to [RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a “news article” as a tweet and then posts synthetic responses or follow-up tweets.
To  solve this problem we employed GPT-3’s few-shot learning abilities by providing three previous news articles in the model’s context to condition it.
With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the “news” genre. -->

GPT-3 によるニュース記事生成の品質 (これは一般的な条件付きサンプル生成の品質と相関があると考えられる) を測定するために，GPT-3 が生成した記事を実際の記事と区別する人間の能力を測定することにした。
<!-- 同様の研究は，Kreps+[KMB20] と Zellers+[ZHR+19] によって実施されている。
生成言語モデルは，人間が生成したコンテンツの分布と一致するように訓練されるため，両者を区別する人間の (中略) 能力は，品質の重要な指標となる可能性がある。(脚注 3) -->
<!-- To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones.
Similar work has been carried out by Kreps+[KMB20] and Zellers+[ZHR+19].
Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality. (footnote 3) -->

<!-- * [KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s fit to fabricate: Ai-generated text as a tool of media misinformation, 2020.
* [ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019. -->

モデルで生成したテキストを人間がどの程度検出できるかを調べるため，ウェブサイトnewser.com から 25 個の記事タイトルと字幕を任意に選択した (平均長：215 単語)。
そして，125 M から 175 B (GPT-3) のパラメータを持つ 4 つの言語モデルから，これらのタイトルとサブタイトルの補完を生成した (平均長：200 単語)。
それぞれのモデルについて，米国在住の約 80 名の参加者に，これらの実際のタイトルとサブタトルの後に，人間が書いた記事とモデルによって生成された記事のどちらかを並べたクイズを提示した (脚注 4)。
参加者は，その記事が「人間が書いた可能性が非常に高い」「人間が書いた可能性が高い」「わからない」「機械が書いた可能性が高い」「機械が書いた可能性が非常に高い」のいずれであるかを選択するよう求められた。
<!-- In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words).
We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words).
For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model (footnote 4).
Participants were asked to select whether the article was “very likely written by a human”, “more likely written by a human”, “I don’t know”, “more likely written by a machine”, or “very likely written by a machine”. -->


選択した記事は，モデルの訓練データにはなく，モデルの出力は，人間のチェリーピッキングを防ぐために，プログラムによってフォーマットされ選択された。
すべてのモデルは，出力を条件付けるために同じ文脈を使用し，同じ文脈サイズで事前訓練され，各モデルのプロンプトとして同じ記事のタイトルとサブタイトルが使用された。
しかし，参加者の努力と注意をコントロールするために，同じフォーマットで，意図的に悪いモデルの記事を生成する実験も実施した。
これは「コントロールモデル」，すなわち，文脈がなく，出力のランダム性を高めた 160 Mパラメータのモデルから記事を生成することで行われた。
<!-- The articles we selected were not in the models’ training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking.
All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model.
However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles.
This was done by generating articles from a “control model”: a 160M parameter model with no context and increased output randomness. -->

意図的に悪い記事がモデルによって生成されたものであることを検出するための人間の平均精度 (参加者ごとの非中立的な割り当てに対する正しい割り当ての比率) は $\sim$86 % であった (50 % は偶然のレベルの性能)。
対照的に，175 B パラメータモデルによって生成された記事を検出するための人間の平均精度は，$\sim$52 % と偶然をほとんど上回らなかった
モデルによって生成されたテキストを検出する人間の能力は、モデルサイズが大きくなるにつれて低下するようである：
モデルサイズが大きくなるにつれて、偶然の精度が高くなる傾向があるようで、GPT-3 の人間の検出は偶然に近い。
これは，モデルサイズが大きくなるにつれて、参加者が各出力に費やす時間が長くなっているにもかかわらず、真実である。
<!-- GPT-3 からの合成記事の例を図 3.14 と図 3.15 に示す (脚注 7)。 -->
本文の多くは，評価で示されたように，人間が本物の人間のコンテンツと区別することが困難である。
事実の不正確さは，記事がモデルによって生成されたものであることを示す指標となり得る。
なぜなら，人間の著者とは異なり，モデルは記事のタイトルが言及している特定の事実や記事が書かれた時期にアクセスすることができないからである。
他の指標としては，繰り返し，非連続，変わった言い回しなどがあるが，これらは微妙なため気づかれないことが多い。
<!-- Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was -86% where 50% is chance level performance.
By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at -52% (see Table 3.11). (footnote 5)
Human abilities to detect model generated text appear to decrease as model size increases:
there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. (footnote 6)
This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).
Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. (footnote 7)
Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content.
Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written.
Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed. -->

<div class="figure figcenter">

<img src="/2023assets/2021Brown_GPT3_fig3_13.jpg" width="77%">
<div class="figcaption" style="width:99%">

図 ニュース記事がモデルによって生成されたものであるかどうかを識別する人間の能力 (正しい割り当てと中立でない割り当ての比率で測定) は，モデルサイズが大きくなるほど低下する。
意図的に悪い対照モデル (出力のランダム性が高い無条件 GPT-3 小型モデル) の出力に対する精度を上部の破線で示し，ランダムな確率 (50 %) を下部の破線で示す。ベストフィットの線は 95 %信頼区間を持つべき乗則である。
<!-- #### Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases.
Accuracy on the outputs on the deliberately bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best fit is a power law with 95% confidence intervals. -->
</div>

<div class="memo" style="width:77%">

| |平均精度 | 95% 信頼区間(低,高)| 対照モデルに対する t 値 (p 値)| “わからない” の割合|
|---|---|---|---|---|
|対照モデル (意図的な悪いモデル) |86% |83%–90% |- |3.6 %|
|GPT-3 Small |76% |72%–80% |3.9 (2e-4) |4.9%|
|GPT-3 Medium |61% |58%–65% |10.3 (7e-21) | 6.0%|
|GPT-3 Large  |68% |64%–72% |7.3 (3e-11) |8.7%|
|GPT-3 XL   |62%  |59%–65% |10.7 (1e-19) |7.5%|
|GPT-3 2.7B |62%  |58%–65% |10.4 (5e-19) |7.1%|
|GPT-3 6.7B |60%  |56%–63% |11.2 (3e-21) |6.2%|
|GPT-3 13B  |55%  |52%–58% |15.3 (1e-32) |7.1%|
|GPT-3 175B |52%  |49%–54% |16.9 (1e-34) |7.8%|
</div>
<div class="figcaption" style="width:88%">

#### 表 3.11: 短い (200 単語) ニュース記事がモデル生成されたものかどうかを識別する人間の精度。
人間の精度 (正しい割り当てと中立でない割り当ての比率で測定) は，対照モデルで 86 ％からGPT-3 175 B で 52 ％であることがわかる。
この表は，5 つの異なるモデル間の平均精度を比較し，各モデルと対照モデル (出力ランダム性を高めた無条件 GPT-3 Small モデル) の間の平均精度の差についての 2 標本 t 検定の結果を示す。
<!-- #### Table 3.11: Human accuracy in identifying whether short ( 200 word) news articles are model generated.
We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-3 175B.
This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness). -->
</div></div>

Ippolito+[IDCBE19] による言語モデル検出の関連研究は，GROVER[ZHR+19] や GLTR[GSR19] などの自動識別器が，人間の評価者よりもモデル生成テキストの検出で大きな成功を収めるかもしれないということを示している。
これらのモデルの自動検出は，今後の研究の有望な分野となるかもしれない。
<!-- Related work on language model detection by Ippolito+[IDCBE19] indicates that automatic discriminators like GROVER[ZHR+19] and GLTR[GSR19] may have greater success at detecting model generated text than human evaluators.
Automatic detection of these models may be a promising area of future research. -->

Ippolito+[IDCBE19] も，モデル生成テキストの検出における人間の精度は，人間がより多くのトークンを観察するほど高くなると述べている。
GPT-3 175 B が生成した長いニュース記事を人間がどの程度検出できるかを予備的に調査するために，Reuters から平均 569 語の世界のニュース記事 12 本を選び，GPT-3 から平均 498 語 (最初の実験より 298 語長い) の記事の補完を生成した。
上記の方法に従って，米国在住の約 80 名の参加者を対象に，GPT-3 と対照モデルで生成された記事を検出する人間の能力を比較する実験をそれぞれ 2 回実施した。
<!-- Ippolito+[IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens.
To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175 B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments).
Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model. -->

その結果，対照モデルの意図的に悪い長文記事を検出する人間の平均精度は $\sim$ 88 % であったが，GPT-3 175 B が生成した長文記事を検出する人間の平均精度は $\sim$ 52 % でまだ偶然をほとんど上回っていなかった (表 3.12 参照)。
このことは，500 語程度のニュース記事については，GPT-3 は人間が書いたニュース記事と区別するのが難しい記事を作り続けていることを示している。
<!-- We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was -88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at -52% (see Table 3.12).
This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles. -->

<div class="figure figcenter">
<center>
<div class="memo" style="width:77%">

| |平均精度 | 95% 信頼区間(低,高)| 対照モデルに対する t 値 (p 値)| “わからない” の割合|
|:---|:---:|:---:|:---:|:---:|
|Control    |88% |84%–91% | - | 2.7%|
|GPT-3 175B |52% |48%–57% |12.7 (3.2e-23)|10.6%|
</div>
</center>
<div class="figcaption" style="width:88%">

#### 表 3.12:
500 語の記事がモデル生成されたものかどうかを識別する人々の能力（正しい割り当てと中立でない割り当ての比率で測定）は，対照モデルで 88%，GPT-3 175B で 52% であった。
この表は，GPT-3 175B と対照モデル（出力ランダム性を高めた無条件 GPT-3 Small モデル）の間の平均精度の差に関する 2 標本 t 検定の結果を示している。
<!-- #### Table 3.12:
People’s ability to identify whether 500 word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B.
This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness). -->
</div>
</div>

<br/><br/>

<!-- <div class="figure figcenter"> -->
<div class="memo" style="width:94%">

タイトル: 連合メソジスト教団，歴史的な分裂に合意<br/>
サブタイトル: 同性婚に反対する人たちが自分たちの教派を結成することになる<br/>
記事内容:
<font style="color:blue">ワシントンポスト紙によると，2 日間にわたる激しい議論の末，連合メソジスト教会は歴史的な分裂に合意した。
この分裂は「神学的にも社会的にも保守的」な新教派の創設に終わると予想される。
5 月に開催された同教会の年次総会に出席した代議員の大多数は、LGBTQ 聖職者の叙任禁止を強化し，同性婚を司式した聖職者を「懲罰」する新しい規則を作成することに投票した。
しかし，これらの措置に反対した人たちは，新たな計画を立てている:
彼らは，2020 年までに別の教団を結成し，自分たちの教会を「クリスチャン・メソジスト教団」と呼ぶという。<br/>
ポスト紙は，1250 万人の会員を擁する同教団は，20 世紀初頭には「米国最大のプロテスタント教団」であったが，ここ数十年で縮小傾向にあると指摘している。
今回の分裂は，同教会の歴史上 2 回目の分裂となる。
最初の分裂は 1968 年で，教団の約 10 ％が離脱し，福音主義合同ブレザレン教会を設立した。
ポスト紙は，今回の分裂案について，"何年も会員が減り続けている教会にとって，重要な時期に来ている” と指摘している。
“教会における LGBTQ の役割をめぐって分裂の瀬戸際まで追い込まれている”。
教会を分裂させている問題は，同性婚だけではない。
2016 年には，トランスジェンダーの聖職者の叙任をめぐり，北太平洋地域会議が聖職者として奉仕することを禁止し，南太平洋地域会議が許可することを決議し，教団が分裂した。</font>
<!-- Title: United Methodists Agree to Historic Split
Subtitle: Those who oppose gay marriage will form their own denomination
Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be "theologically and socially conservative," according to The Washington Post.
The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will "discipline" clergy who officiate at same-sex weddings.
But those who opposed these measures have a new plan:
They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.
The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the "largest Protestant denomination in the U.S.," but that it has been shrinking in recent decades.
The new split will be the second in the church's history.
The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church.
The Post notes that the proposed split "comes at a critical time for the church, which has been losing members for years," which has been "pushed toward the brink of a schism over the role of LGBTQ people in the church." Gay marriage is not the only issue that has divided the church.
In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them. -->
<div class="figcaption" style="width:100%">

GPT-3 が生成したニュース記事で，人間が書いた記事と区別するのが最も困難だったもの (精度: 12％)。
<!-- ### Figure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy: 12%). -->
</div></div>
<!-- </div> -->

<br/><br/>

### 新しい単語の学習と使用 <!-- ### 3.9.5 Learning and Using Novel Words-->

発達言語学 [CB78] で研究されている課題として，新しい単語を学習し利用する能力がある。
例えば，ある単語が一度だけ定義されたのを見た後に文中で使用したり，逆にある単語の意味をたった一度の使用から推論することができる。
ここでは，GPT-3 の前者の能力を定性的に検証する。
具体的には，GPT-3 に `Gigamuru` のような実在しない単語の定義を与え，それを文章で使うように依頼する。
このとき，(別の) 非実在語を定義して文中で使用する例を過去に 1～5 例提示するので，大まかな課題の過去例としては数撃ちゃ当たる，特定の単語としては一撃必殺の課題となる。
表 3.16 は，我々が作成した 6 例である。
また，最初の回答は条件付けとして人間が生成し，それ以降の回答は GPT-3 で生成した。
これらの例は一度に連続して生成し，プロンプトを省略したり，繰り返し試したりすることはしなかった。
すべての場合において，生成された文は正しい，あるいは少なくとももっともらしい単語の使用であるように見える。
最後の文では `screeg` の活用として `screeghed` が生成されたが `screeghed at each other` と，おもちゃの剣戟を表現するという意味ではもっともらしいものの，言葉の使い方がややぎこちないことがわかる。
(In the final sentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of the word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy sword fight.)
(To "screeg" something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeghed at each other for several minutes and then we went outside and ate ice cream.)
全体として，GPT-3 は，文中に新しい単語を使うという課題に少なくとも熟達しているようだ。
<!-- A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word’s meaning from only one usage.
Here we qualitatively test GPT-3’s ability to do the former.
Specifically, we give GPT-3 the definition of a nonexistent word, such as “Gigamuru”, and then ask it to use it in a sentence.
We provide one to five previous examples of a (separate) nonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions
were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3.
These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts.
In all cases the generated sentence appears to be a correct or at least plausible use of the word.
In the final sentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of the word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy sword fight.
Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence. -->

<div class="memo" style="width:94%">

`whatpu` は，タンザニアに生息する毛むくじゃらの小動物です。<br/>
whatpu という単語を使った例文は:

<font style="color:blue">私たちはアフリカを旅行していて、とてもかわいい whatpu を見た。</font>

<hr>

`farduddle` とは、とても速く飛び上がることです。<br/>
farduddle という単語を使った文例は：

<font style="color:blue">ある日，妹と鬼ごっこをしていたら，妹がすごく興奮して，狂ったように farduddle をし始めた。</font>

<hr>

`yalubalu` は大きなカボチャのような野菜の一種である。<br/>
yalubalu という単語を使った文例は：

<font style="color:blue">アフリカに旅行したとき，そこの庭で栽培されていた `yalubalu` という野菜を試したことがある。おいしかったよ。</font>

<!-- A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:

We were traveling in Africa and we saw these very cute whatpus.
---
To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:

One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles.

A "yalubalu" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is:

I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there.
It was delicious. -->

<hr>

`Burringo` とは，非常に速い加速をする車のこと。Burringo という単語を使った例文は：<br/>
<font style="color:blue">我が家のガレージには，私の父が毎日通勤に使っている Burringo があります。</font>

<hr>
`Gigamuru` は日本の楽器の一種である。Gigamuru という単語を使った文例は：<br/>
<font style="color:blue">私は叔父がプレゼントしてくれた gigamuru を持っています。私は家でそれを弾くのが好きです。</font>

<hr>
`screeg` は、剣を振りかざすことです。screeg という単語を使った例文は：<br/>
<font style="color:blue">私たちは数分間お互いに screeg し合い，それから外に出てアイスクリームを食べた。</font>

<!-- A "Burringo" is a car with very fast acceleration. An example of a sentence that uses the word Burringo is:
In our garage we have a Burringo that my father drives to work every day.

A "Gigamuru" is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:
I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.

To "screeg" something is to swing a sword at it. An example of a sentence that uses the word screeg is:
We screeghed at each other for several minutes and then we went outside and ate ice cream. -->
</div>

### 英文法の修正 <!--Correcting English Grammar-->

また，数撃学習に向いている課題として，英文法添削がある。
GPT-3 では `Poor English Input:<sentence>\n Good English Output:<sentence>` というプロンプトを与え，数撃学習でこれを検査した。
GPT-3 に人間が作成した修正を 1 つ与え，さらに 5 つの修正を依頼する (ここでも省略や繰り返しはない)。
結果は図 3.17 に示すとおりである。
<!-- Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few shot setting by giving prompts of the form `Poor English Input: <sentence>\n Good English Output: <sentence>`.
We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats).
Results are shown in Figure 3.17. -->

<!-- <div class="figure figcenter"> -->
<div class="memo" style="width:94%">

Poor English input: I eated the purple berries.<br/>
Good English output: I ate the purple berries.<br/>
Poor English input: Thank you for picking me as your designer. I'd appreciate it.<br/>
Good English output: Thank you for choosing me as your designer. I appreciate it.<br/>
Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.<br/>
Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.<br/>
Poor English input: I'd be more than happy to work with you in another project.<br/>
Good English output: <font style="weighst:900;color:blue">I'd be more than happy to work with you on another project.</font>
<!-- **Good English output: I'd be more than happy to work with you on another project.** -->

<hr/>

Poor English input: Please provide me with a short brief of the design you're looking for and that'd be nice if you could share some examples or project you did before.<br/>
Good English output: <font style="color:blue">Please provide me with a brief description of the design you're looking for and that would be nice if you could share some examples or projects you have done before.</font>

<hr>

Poor English input: The patient was died.<br/>
Good English output: <font style="color:blue">The patient died.</font>

<hr>

Poor English input: We think that Leslie likes ourselves.<br/>
Good English output: <font style="color:blue">We think that Leslie likes us.</font>

<hr>

Poor English input: Janet broke Bill on the finger.<br/>
Good English output: <font style="color:blue">Janet broke Bill's finger.</font>

<hr>

Poor English input: Mary arranged for, in St. Louis, John to rent a house cheap.<br/>
Good English output: <font color="blue">Mary arranged for John to rent a house in St. Louis.</font>

<hr>

Poor English input: Today I have went to the store to to buys some many bottle of water.<br/>
Good English output: <font color="blue">Today I went to the store to buy some bottles of water.</font>

<hr>

Poor English input: I have tried to hit ball with bat, but my swing is has miss.<br/>
Good English output: <font style="color:blue">I tried to hit the ball with the bat, but my swing missed.</font>
<!-- </div> -->

<div class="figcaption" style="width:94%">

英文法添削の少数撃課題における GPT-3 の代表的な添削。<br/>
太字は GPT-3 による補完，平文は人間によるプロンプト。
最初の数例は，プロンプトと補完の両方が人間によって提供される。
これは，GPT-3 が連続して追加のプロンプトを受け取り，補完を提供する，その後の例の条件付けとして機能する。
GPT-3 には，最初の数例の条件付けと `Poor English input/Good English output` のフレーミング以外，課題に特化したものは何も提供されない。
しかし，「悪い英語」と「良い英語」の区別は，複雑で，文脈的で，争いのあるものである。
家のレンタルに関する例が示すように，「良い」とは何かという仮定が，モデルの誤りにつながることさえある (この例では，モデルは文法を調整するだけでなく，意味を変える形で「安い cheap」という単語を削除している)。
<!-- #### Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar.
Boldface is GPT-3’s completions, plain text is human prompts.
In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions.
Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the “Poor English input/Good English output” framing.
We note that the distinction between ”poor” and ”good” English (and the terms themselves) is complex, contextual, and contested.
As the example mentioning the rental of a house shows, assumptions that the model makes about what “good” is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word ”cheap” in a way that alters meaning). -->
</div></div>

## 4. ベンチマークの記憶化の測定と防止 <!--4. Measuring and Preventing Memorization Of Benchmarks-->

我々の訓練データセットはインターネットから提供されているため，我々のモデルはベンチマークテストセットの一部で訓練された可能性がある。
インターネット規模のデータセットからテストデータへ混入を正確に検出することは，ベストプラクティスが確立されていない新しい研究分野である。
汚染について調査することなく大規模なモデルを訓練することは一般的だが，事前訓練用データセットの規模が大きくなっていることから，この問題はますます重要視されるようになってきていると考えている。
この懸念は，単なる仮説ではない。
Common Crawl データで言語モデルを訓練した最初の論文の一つ[TL18] は，評価データセットの一つと重なる訓練文書を検出して削除した。
GPT-2[RWC+19] のような他の研究も，ポストホック重複分析を行った。
彼らの研究は比較的有望で，訓練データとテストデータが重複するデータではモデルの性能が中程度に向上するものの，汚染されたデータの割合が小さい (数パーセント程度) ため，報告された結果に大きな影響を与えないことを発見した。
<!-- Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets.
Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices.
While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.
This concern is not just hypothetical.
One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets.
Other work such as GPT-2 [RWC+19] also conducted post-hoc overlap analysis.
Their study was relatively encouraging, finding that although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent). -->

### BERT の事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力
* 入力: the man went to the store [SEP] he bought a gallon of milk.
* ラベル: IsNext
* 入力: the man went to the store [SEP] penguins are flightless  birds.
* ラベル: NotNext

#### BERT: ファインチューニング (1)

<img src="/assets/2018Devlin_BERT_Fig3.svg">
{(a), (b) は文レベル課題，(c),(d)はトークンレベル課題, E: 入力埋め込み表現,
$T_i$: トークン $i$ の文脈表象。[CLS]: 分類出力記号, [SEP]:文分離記号%% r task specific models are forme


### chatGPT

<div class="figure figcenter">
<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" width="99%">
<div class="figcaption">

### instructGPT の概要 [2022Quyang+](https://arxiv.org/abs/2203.02155) Fig.2 を改変

</div></div>

chatGPT の GPT とは **Genrative Pre-trained Transformer** の頭文字です。
**生成モデル (generative modeling)** と **事前学習 (pre-trained models)** と **トランスフォーマー (transformer
)** についての理解が必要となります。

生成モデルと事前学習について触れる前に，最後のトランスフォーマーについて言及します。
トランスフォーマーは **言語モデル (Lanugage models)** です。
言語モデルによって，文章が処理され，適切な応答をするようになったモデルの代表が chatGPT となります。

そこで言語モデルを理解するために，その構成要素であるトランスフォーマーを取り上げる必要があります。
トランスフォーマーは，2017 年の論文 [Attention Is All You Need](https://arxiv.org/abs/1706.03762) で提案された
，**ニューラルネットワーク neural network** モデルです。
トランスフォーマーはゲームチェンジャーとなりました。
最近の **大規模言語モデル (LLM: Large Language Model)** は，トランスフォーマーを基本構成要素とするモデルがほと
んどです。
上記の論文のタイトルにあるとおり，トランスフォーマーは，**注意機構 attention mechanism** に基づいて，自然言語
処理の諸課題を解くモデルです。

## GPT-4
加えて，chatGPT の後続モデルである GPT-4 では，マルチモーダル，すなわち，視覚と言語の統合が進みました。

<div class="figure figcenter">
<img src="/2023assets/2023kosmos_coverpage.png" width="77%"><br/>
<div class="figcaption">

[Kosmos-1 の概念図](https://arXiv.org/abs/2302.14045)
</div></div>
