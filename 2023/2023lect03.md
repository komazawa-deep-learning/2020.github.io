---
title: 第03回
author: 浅川 伸一
layout: default
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+2" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 28/Apr/2022<br/>
Appache 2.0 license<br/>
</div>


<strong> LLM は ハリーポッターにでてくる [みぞの鏡 erised mirror](/2023assets/HarryPotter_erised_mirror_chapt12_p207_.pdf){:target="_blank"} である</strong><br/>
--- [Sejnowski, 2022, 大規模言語モデルと逆チューリングテスト](https://direct.mit.edu/neco/article/35/3/309/114731/Large-Language-Models-and-the-Reverse-Turing-Test)

I show not your face but your hearts desire.


# 第 3 回

### 本日のメニュー

1. [MLP 実習](#実習)
2. [Softmax 解題](#ソフトマックス関数)
3. 人工知能の歴史
4. [告知 全脳アーキテクチャ勉強会 5 月 12 日 18:00-](#告知)

### 文献

* [心理学，神経科学，機械学習における注意, Lindsay, G. W. (2020) <img src="https://www.adobe.com/content/dam/cc/en/legal/images/badges/PDF_32.png">](/2023/2020Lindsay_attention_ja.pdf){:target="_blank"}
<!-- * [視覚系のモデルとしての畳み込みニューラルネットワーク: 過去，現在，そして未来 Lindsay (2020)](/2021cogpsy/2020Lindsay_ja.pdf){:target="_blank"} -->
* [深層学習 (ディープラーニング Deep Learning) LeCun, Bengio, and Hinton (2015)  <img src="https://www.adobe.com/content/dam/cc/en/legal/images/badges/PDF_32.png">](/2022/2015LeCun_Nature_ja.pdf){:target="_blank"}

### 実習

* [ニューラルネットワークで遊んでみよう](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"}
* [RNN 文字ベースの言語モデル 漱石「こころ」](/character_demo.html){:target="_blank"}
* [GLIDE <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/text2im.ipynb){:target="_blank"}
* [実習 PyTorch CNN <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_
0416PyTorch_CNN_demo.ipynb){:target="_blank"}

### トランスフォーマーと脳

<div class="figure figcenter">
<img src="/2023assets/2022Sejnowski_LLM_fig5.jpg" width="66%">
<div class="figcaption">

#### 図: トランスフォーマーのループと皮質-基底核のループの比較

(左) トランスフォーマーは，出力を入力とループさせて単語系列を生成するフィードフォワード自己回帰型アーキテクチャを持つ (Vaswani+2017)。
示された単一のエンコーダ/デコーダモジュールは N 層 (Nx) 積み重ねることができる。
(右) 位相的に写像された運動皮質は大脳基底核に投射し，大脳皮質にループバックして，話し言葉の単語系列ような一連の動作を生成する。
大脳皮質のすべての部分が大脳基底核に投影され，前頭前野と大脳基底核の間の同様のループによって，思考の系列が生成される。
[Large Language Models and the Reverse Turing Test](https://doi.org/10.1162/neco_a_01563) Figure 5
</div></div>

chatGPT は ハリーポッターにでてくる [みぞの鏡 erised mirror](/2023assets/HarryPotter_Erised_mirror_chpt12.svg) であるという風潮。

<!-- <img src="/2023assets/HarryPotter_Erised_mirror_chpt12.svg"> -->

ちょっとだけ人工知能の歴史を紐解くと，

初期の人工知能は，抽象的思考，記号操作，推論，などが知性だとされた。
いまでもそうであるが，いわゆる頭が良いとされる人は，抽象的な思考が論理的にできることである。
感情に流されたり，論理的に行動できない人は，知的でないとされる傾向があるだろう。
このような意味で，初期の人工知能が，論理，記号操作に重きを置いたのは理解できる。
逆に直感で判断されることは，誤りだとされてきた。

ところが 1970 年代から 1980 年代にかけて，感覚処理や運動制御への学習の応用に焦点を当てること多くのことが明らかになってきた。
むしろ，言葉や記号に置き換えられないことの方が複雑なのだ。

振り返ってみると，初期の AI で記号処理が魅力的な道であった理由がよくわかる。
デジタル・コンピュータは，記号を表現して論理を実行することに特に優れており，言語は記号処理の申し子だった。
しかし，言語であっても論理的なプログラムを書くのは手間がかかるし，次元の呪い (プログラマーが予測しなければならない，世界で起こりうる物事や状況の組み合わせの数が爆発的に増えること) に悩まされることになった。
例えば，私たちが物を認識し，それを手に取るのはとても簡単なことであるため，「見るだけ」「手を伸ばすだけ」の複雑さは非常に過小評価されていた。
我々は何も考えずにこの作業を行うので，感覚系や協調運動を担う運動系の部分の根底にあるすべての潜在意識的な処理に意識的にアクセスすることはできない。
また，ほとんどの意思決定がどのように行われるのかについても知ることができず，後になってから，もっともらしい合理的な理由をつけて説明することになる。
潜在意識の処理は，周辺機能だけでなく，芸術から数学まであらゆる分野の創造性の源でもある (Ritter&Dijksterhuis2014)。
<!-- Ritter S. M., & Dijksterhuis, A. (2014). Creativity—the unconscious foundations of the incubation period. Frontiers in Human Neuroscience, 8, 215. 10.3389/fnhum.2014.00215 -->
<!-- In retrospect, we can see why symbol processing was such an attractive road in early AI.
Digital computers are particularly efficient at representing symbols and performing logic, and language was the poster child for symbol processing.
But writing logical programs even for language proved to be labor intensive and suffered from the curse of dimensionality—an explosion in the number of possible combinations of things and situations that could occur in the world that programmers have to anticipate.
For example, the complexity of simply seeing and simply reaching was greatly underestimated because it is so effortless for us to recognize an object and pick it up.
We do this without thinking and do not have conscious access to all the subconscious processing that underlies sensory systems and the parts of the motor system that are responsible for coordinated movements.
Nor do we have access to how most decisions are made, and only later do we explain them with plausible rationalizations. Subconscious processing isn’t just for peripheral functions; it’s also a source of creativity in all fields, from art to mathematics (Ritter&Dijksterhuis2014). -->

知能のコンピュータメタファーは，魅力的な庭路だった。
私は以前，科学哲学者の Jerry Fodor の講演に参加したことがある。
彼は，人間の心は脳のハードウェア上で動くプログラムであると主張した。
Patricia Churchland  は 「猫についてはどうか？」 と質問した。
フォダーは自信たっぷりにこう答えた「猫の脳が猫のプログラムを動かしているのだ。」
この議論は，機能主義 (脳を単なるハードウェアとして無視し，プログラムに集中する考え方) を正当化するために用いられた。
デジタルコンピュータでは，同じハードウェアで異なるソフトウェアを動かすことができるが，脳のハードウェアはソフトウェアであり，同じ脳は 2 つとない。
新しいことを学ぶとき，私たちはハードウェアを修正しているのだ。
そのため，神経細胞同士がどのようにつながっているかを再構築し，その通信内容を記録することで，自然が発見したアルゴリズムを解明することができる。
脳は多くの相互作用するアルゴリズムから作られている ([Navlakha2018](https://www.wired.com/story/why-animal-extinction-is-crippling-computer-science/))，部分系を統合する問題は，互いに適応できるニューロンですべて作られているため，緩和されるのである。
これは，多様なネットワークアーキテクチャの構築と統合が急速に進んでいることに反映されている。
これに対して，異なる規則や記号で設計された視覚モジュール，運動制御モジュール，計画モジュールなどのモジュールを統合して大規模な系を構築することは問題であった。
<!-- Navlakha, S. (2018). Why animal extinction is crippling computer science. Wired (September 19). https://www.wired.com/story/why-animal-extinction-is-crippling-computer-science/ -->
<!-- The computer metaphor for intelligence was an attractive garden path.
I once was at a talk given by Jerry Fodor, a philosopher of science, who claimed that the human mind was a program that ran on the brain’s hardware.
Patricia Churchland asked, “What about cats?” “Yes,” Fodor replied confidently.
“The cat brain is running the cat program.”
This argument was used to justify functionalism—the view that we can ignore the brain as mere hardware and focus on the program.
Unlike digital computers, where the same hardware can run different software programs, the hardware in brains is the software, and no two brains are identical.
As we learn new things, we are modifying our hardware.
This is why the algorithms that nature discovered can be uncovered by reconstructing how neurons are connected with each other and recording what they communicate.
Although brains are built from many interacting algorithms (Navlakha2018), the problem of integrating subsystems is alleviated because they are all built with neurons that can adapt to each other.
This is reflected in the rapid progress that has been made in building and integrating diverse network architectures.
In contrast, integrating modules such as a vision module, a motor control module, and a planning module designed with different rules and symbols to build large-scale systems was problematic. -->

従来の AI で論理的推論を重視したのも見当違いだった。
数学者が習得している論理的なステップの系列をエミュレートする方法を学ぶには，多くの訓練が必要だ。
抽象的な推論課題に関する明示的な訓練がなければ，人間は身近な環境でより効果的に論理的な結論に達することができ，LLM でも同じバイアスが観察されている [Dasgupta+2022](https://arXiv.org/abs/2207.07051/)。
TD-ギャモンや AlphaGo が示した創造性は，深層学習皮質モデルだけで生じたものではなく，手続き学習の一種である強化学習と連動していた。
強化学習は，報酬予測誤差を表す神経調節物質であるドーパミンによって，我々の大脳基底核で実施される (図 5参照)。
我々はドーパミンの信号に意識的にアクセスすることはできないが，モチベーションに強力な影響を与え，すべての依存性薬物はドーパミンの活性を操作することで機能する(Sejnowski2019)。
<!-- The emphasis on logical reasoning in traditional AI was also misguided.
Learning how to emulate sequences of logical steps, which mathematicians have mastered, requires a lot of training.Without explicit training on abstract reasoning tasks, humans can more effectively reach logical conclusions in familiar settings, and the same bias has been observed in LLMs (Dasgupta+2022).
The creativity that TD-Gammon and AlphaGo exhibited did not arise from the deep learning cortical model alone, but in conjunction with reinforcement learning, a form of procedural learning.
Reinforcement learning is implemented in our basal ganglia by dopamine, a neuromodulator that represents reward prediction error (see Figure 5).
We do not have conscious access to dopamine signals, but they have a powerful impact on our motivation, and all addictive drugs work by manipulating dopamine activity (Sejnowski2019). -->

<div class="figure figcenter">
<img src="/2023assets/2022Sejnowski_LLM_fig5.jpg" width="66%">
<div class="figcaption">

#### 図 5: トランスフォーマーのループと皮質-基底核のループの比較。
(左) トランスフォーマーは，出力を入力とループさせて単語系列を生成するフィードフォワード自己回帰型アーキテクチャを持つ (Vaswani+2017)。
示された単一のエンコーダ/デコーダモジュールは N 層 深く (Nx) 積み重ねることができる。
(右) 位相的に写像された運動皮質は大脳基底核に投射し，大脳皮質にループバックして，話し言葉の単語列のような一連の動作を生成する。
大脳皮質のすべての部分が大脳基底核に投影され，前頭前野と大脳基底核の間の同様のループによって，思考の系列が生み出される。

thalamus: 視床，
putamen: 被殻
SMA: supplementary motor area 補足運動野
PM: premotor cortex 運動前野
STN: subthalamic nucleus 視床下核
GPi: internal segment of globus pallidus 淡蒼球内節
GP: globus pallidus	淡蒼球
M1: 一次運動野
<!-- #### Figure 5: Comparison between the transformer loop and the cortical-basal ganglia loop.
(Left) Transformers have a feedforward autoregressive architecture that loops the output with the input to produce a sequence of words (Vaswani et al., 2017).
The single encoder/decoder module shown can be stacked N layers deep (Nx).
(Right) The topographically mapped motor cortex projects to the basal ganglia, looping back to the cortex to produce a sequence of actions, such as a sequence of words in a spoken sentence.
All parts of the cortex project to the basal ganglia and similar loops between the prefrontal cortex and the basal ganglia produce sequences of thoughts. -->
</div></div>

現代の機械学習の種は AI の黎明期にはすでに蒔かれていた。
Frank Rosenblatt のパーセプトロン (1961) は，単層の重みの勾配降下を使って，例からの入力を分類することを学習した。
コネクショニストの研究コミュニティが，確率的な二値ニューロンの多層ネットワークの学習アルゴリズムを発明し(Ackley+1985)，決定論的な実数値ニューロンのバックプロパゲーションアルゴリズムを再発見してニューラルネットワークに適用するまで (Rumelhart+1986)，さらに 24 年かかる。
当時は，視覚や音声のような難しい問題の学習を進めるのに，どれだけのコンピュータパワーが必要なのかが分かっていなかったのだ。
それから 3 分の 1 世紀が経ち，今ではどれだけのコンピューティングパワーが必要なのかが分かっている。
驚いたのは，人間の知能の宝庫と考えられていた言語について，どれだけの進歩があったかということだ。
<!--The seeds of modern machine learning were already being sown at the dawn of AI.
Frank Rosenblatt’s perceptron (1961) learned to categorize inputs from examples using gradient descent on a single layer of weights.
It would take another 24 years before the connectionist research community invented a learning algorithm for multilayer networks of stochastic binary neurons (Ackley+1985) and rediscovered the backpropagation algorithm for deterministic, real-valued neurons and applied them to neural networks (Rumelhart+1986).
What was not known back then was how much computer power it would take to make progress with learning on difficult problems like vision and speech.
A third of a century later, we now know how much computing power is needed.
The surprise was how much progress has been made with language, thought to be the crown jewel of human intelligence.-->

## 7 ネットワークの規模が大きくなるに従い，性能は向上し続ける Performance Continues to Improve as Networks Scale in Size

特に視覚的物体認識や音声認識など，AI にアナログ世界とのインターフェースを与えるような，より大きなニューラルネットワークでの学習が進んできている。
LLM の学習による構文 syntax の出現など，ネットワークの規模が大きくなるにつれて，言語生成の重要な側面が現れてきたことは，我々が正しい道を歩んでいるという自信を与えてくれる。
言語が神経ネットワーク上で優雅に機能する可能性があるという証拠は，初期の言語モデルである NETtalk から生まれた (Rosenberg&Sejnowski1987)。
NETtalk は，英単語に含まれる文字の音をどのように発音するかを学習したが，不規則性の多い言語では容易ではない。
1980 年代の言語学は，記号と規則に支配されていた。
音韻論の本には，さまざまな単語の文字をどのように発音するかという規則が何百と書かれており，それぞれの規則には何百もの例外があり，しばしば同様の例外のための下位規則があった。
規則と例外の繰り返しだったのだ。
しかし，NETtalk はわずか数百台でありながら，英語の発音の規則と例外の両方を，同じ均一なアーキテクチャでマスターすることができた (図 3 参照)。
これは，英語の発音を記号や論理的な規則よりもネットワークで表現する方がはるかにコンパクトであること，文字と音の対応付けは学習可能であることを教えてくれた。
NETtalk が喃語の段階から発音のさまざまな側面を順次学習していくのを聞くのは，とても興味深い (NETtalk1986)。
<!-- We are making progress with learning in larger and larger neural networks, especially on visual object recognition and speech recognition, which is giving AI interfaces with the analog world.
The emergence of important aspects of language generation as networks scaled in size, such as the emergence of syntax from learning in LLMs, gives us confidence that we are on the right track.
Evidence that language might fall gracefully on neural networks emerged from NETtalk, an early language model (Rosenberg&Sejnowski1987).
NETtalk learned how to pronounce the sounds of letters in English words, which is not easy for a language riddled with irregularities.
Linguistics in the 1980s was dominated by symbols and rules.
Books on phonology were packed with hundreds of rules for how to pronounce letters in different words, each rule with hundreds of exceptions and often subrules for similar exceptions.
It was rules and exceptions all the way down.
What surprised us was that NETtalk, which had only a few hundred units, was able to master both the regularities and the exceptions of English pronunciation in the same uniform architecture (see Figure 3).
This taught us that networks are a much more compact representation of English pronunciation than symbols and logical rules and that the mapping of letters to sounds can be learned. It is fascinating to listen to NETtalk sequentially learning different aspects of pronunciation starting with a babbling phase (NETtalk1986). -->

<div class="figure figcenter">
<img src="/2023assets/2022Sejnowski_fig3.jpg" width="33%">
<div class="figcaption">

#### 図 3: NETtalk はテキストを音声に変換する隠れユニットを 1 層持つフィードフォワードニューラルネットワーク。
ネットワークの 200 のユニットと 18,000 の重みは，誤差逆伝播法で訓練された。
各単語は 7 文字の窓を一度に 1 文字ずつ移動し，NETtalk は中央の文字に正しい音素や音を割り当てるように訓練された (Rosenberg&Sejnowski1987)。
<!-- #### Figure 3: NETtalk is a feedforward neural network with one layer of hidden units that transformed text to speech.
The 200 units and 18,000 weights in the network were trained with backpropagation of errors.
Each word moved one letter at a time through a seven-letter window, and NETtalk was trained to assign the correct phoneme or sound to the center letter (Rosenberg&Sejnowski1987). -->
</div></div>

言葉には，意味上の仲間，関連，関係があり，それは生態系と考えることができる。
単語が保つ仲間や，それらが出会う場所によって，単語の意味を知ることができる。
記号表現では，すべての単語対が等しく似ているため，単語から意味性を奪ってしまう。
LLM では，単語はあらかじめ訓練された埋め込みによって，すでに意味情報が豊富な大きなベクトルで表現される(Morin&Bengio2005)。
LLM はこの処理を継続し，文脈を利用して，語順や，単語と単語群の関係を示す構文マーカーによって，句レベルの追加情報を抽出する。
単語は象徴的なさなぎから脱出すると，蝶のように，その意味を理解するための目もくらむようなマーカーや連想の数々を見せるようになる。
そして，これらの意味は学習することができる。
<!-- Words have semantic friends, associations, and relationships that can be thought of as an ecosystem.
You know the meaning of a word by the company it keeps and where they meet.
In a symbolic representation, all pairs of words are equally similar, which strips words of their semantics.
In LLMs, words are represented by pretrained embeddings in large vectors that are already rich in semantic information (Morin&Bengio2005).
LLMs continue this process by using the context to extract additional information afforded by word order and syntactical markers indicating relationships between words and word groups on the clause level.
Once words escape from their symbolic chrysalis, they display, like butterflies, a dazzling array of markers and associations to help the mind make sense of their meaning.
And these meanings can be learned. -->

我々は爆発的な計算量の増加期にあり，特に GPU (Graphics Processing Unit) が活用された過去 10 年間は，2012 年に 6 倍の倍加時間の変曲点をもたらした (Mehonic&Kenyon2022; 図 4 参照)。
コンピューティングパワーが指数関数的に増加し続ける中，ネットワークの規模が拡大し，LLM の性能も加速してきた。
2020 年，GTP-3 は 1986 年の NETtalk の $10^{12}$ - 100万倍 - の計算能力を訓練に必要とする。
LLM の重みは数千億個で，これは大脳皮質の 1 平方センチメートルの下にあるシナプスの数とほぼ同じである。
ニューラルネットワークモデルの推論は 1 つのプロセッサでは重みの数でスケールしますが，脳のような超並列アーキテクチャでは，キロヘルツプロセッサでリアルタイムに処理が進む。
このように問題の大きさに応じてスケールするアルゴリズムはほとんどない。
我々の大脳皮質の面積は約 1200 $cm^{2}$ である。
過去 70 年間と同様に計算能力が指数関数的に向上し続ければ，チップあたりのコア数を増やすことで，遠くない将来，人間の脳の計算能力 (推定値) に到達することになる。
<!-- We are in a period of explosive computational growth, especially over the past decade when graphics processing units (GPUs) were harnessed, leading to a six-fold inflection point in the doubling time in 2012 (Mehonic&Kenyon2022; see Figure 4).
As computing power has continued to increase exponentially, networks have grown in size, and the performance of LLMs has accelerated.
In 2020, GTP-3 required $10^{12}$ — a million million — times more computing power to train than NETtalk in 1986.
LLMs have hundreds of billions of weights, about the same as the number of synapses under a square centimeter of cortex.
Inference in neural network models scales with the number of weights for a single processor but on massively parallel architectures like brains, processing proceeds in real time on kilohertz processors.
Very few algorithms scale this well with the size of the problem.
Our cerebral cortex has an area of approximately 1200 cm2.
If computing power continues to increase exponentially, as it has for the past 70 years, it will reach the estimated computing power of human brains at some point in the not-too-distant future by increasing the number of cores per chip. -->

<div class="figure figcenter">
<img src="/2023assets/2022Sejnowski_fig4.jpg" width="66%">
<div class="figcaption">

#### 図 4: ネットワークモデルの学習に使用されるペタ浮動小数点演算 ($10^{15}$) の推定計算日数を，その発表日の関数として示したもの (Mehonic&Kenyon2022)。
<!-- #### Figure 4: Estimated computation in days of peta floating point operations ($10^{15}$) used to train network models as a function of their date of publication (from Mehonic&Kenyon2022). -->
(Data sources: Sevilla+2022, Amodei&Hernandez2018)</div>
</div>

また，AI ネットワークのアーキテクチャも急速に進化している。
アルゴリズムの進歩は，ハードウェアやデータとともに，過去 10 年間の AI の進歩に等しく寄与してきた。
2012 年，ディープラーニングのフィードフォワード畳み込みニューラルネットワークである Alexnet は，画像中の物体認識に大きな進歩をもたらした。
2016 年には，リカレントニューラルネットワーク (RNN) が系列処理に使われ，言語翻訳の性能を高めた。
自然言語処理の代表的なネットワークモデルであるトランスフォーマーからの双方向符号化器表現 (BERT; Devlin+2018)，そして今日の LLM はトランスフォーマーを使用している(Vaswani+2017)。
トランスフォーマーはサイズと機能を増やし続けている。
パスウェイ言語モデル (PaLM) は 5400 億の接続を持つ最近の LLM で，多くの言語課題において規模に応じた不連続な改善を示す (Chowdhery+2022)。
<!-- The architectures of AI networks have also been rapidly evolving.
Advances in algorithms have contributed equally with hardware and data to advances in AI over the past decade.
In 2012, Alexnet, a deep learning feedforward convolutional neural network, made a major advance in object recognition in images.
By 2016, recurrent neural networks (RNNs) were being used to process sequences and boost the performance of language translation.
The bidirectional encoder representations from transformers (BERT), a seminal network model for natural language processing (Devlin+2018), and today’s LLMs use transformers (Vaswani+2017).
Transformers continue to increase in size and capabilities.
Pathways language model (PaLM) is a recent LLM with 540 billion connections that exhibits discontinuous improvements in many language tasks with scale (Chowdhery+2022). -->

トランスフォーマーには，前世代の言語モデル用フィードフォワードニューラルネットワークやリカレントニューラルネットワークと比較して，いくつかの利点がある (図 5 参照)。
まず，トランスフォーマーへの入力は，一度に 1 つの単語ではなく，文全体である。
このため，他の多くの単語で区切られた単語を簡単につなげることができる。
第二に，トランスフォーマーは新しい自己注意を導入し，文中の単語の対の関連性の高さに応じて，入力表現を乗算的に強調することで修正する。
第三に，トランスフォーマーには外側のループがあり，出力を一度に一語ずつ入力にフィードバックし，一連の単語を生成する。
LLM の学習に必要なデータ量は，重みの数に対して線形に増加するだけであり ([Hoffman+2022](https://arXiv.org/abs/2203.15556/))，古典的な推定値から予想されるよりもはるかに少ない。
最後に，トランスフォーマーはフィードフォワードモデルであり，高度に並列化されたハードウェアで効率的に実装することができる。
LLM の容量と能力は，規模が大きくなるにつれて大きくなり，自然界がより大きく優れた脳を進化させたのと同じ道を歩むことになった (Allman1999, Hoffmann+2022)。
<!-- Transformers have several advantages over the previous generation of feedforward and recurrent neural networks for language models (see Figure 5).
First, the inputs to a transformer are whole sentences rather than one word at a time.
This makes it easier to connect words that are separated by many other words.
Second, transformers introduce a new form of selfattention that modifies the input representation by multiplicatively enhancing pairs of words in the sentence according to how highly they are related.
Third, transformers have an outer loop that feeds back the output, one word at a time, to the input, producing a sequence of words.
The amount of data needed to train LLMs increases only linearly with the number of weights (Hoffman+2022), far less than expected from classical estimates.
Finally, transformers are feedforward models, which can be implemented efficiently on highly parallel hardware.
The capacity and capability of LLMs greatly increased with scale, taking the same path that nature took by evolving bigger and better brains (Allman1999, Hoffmann+2022). -->

このループは，脳の大脳皮質と大脳基底核のループを彷彿とさせ，運動皮質とのループでは運動動作の系列を，前頭前野とのループでは思考系列の学習と生成に重要であることが知られている (Graybiel1997)。
大脳基底核は，頻繁に練習する系列を自動化し，意識制御に関わる皮質層を他の課題に解放する役割も担っている。
また，自動化された系が異常な状況や稀な状況に遭遇して失敗した場合，大脳皮質が介入することができる。
大脳基底核がループ内にあることのもう一つの利点は，複数の皮質領域からの入力が収束することで，次の行動や思考を決定するためのより広い文脈が得られることである。
トランスフォーマーの強力な多頭注意機構を大脳基底核に実装することができる。
ループアーキテクチャでは，ループ内のどの領域も意思決定に貢献することができる。
強化学習における行為者として，大脳基底核はまた，次の行為の学習価値を考慮し，将来の報酬や目標を達成するために行動や発話を偏らせる。
<!-- The outer loop of the transformer is reminiscent of the loop between the cortex and the basal ganglia in brains, known to be important for learning and generating sequences of motor actions in the loop with the motor cortex (see Figure 5) and sequences of thoughts in the loop with the prefrontal cortex (Graybiel1997).
The basal ganglia is also responsible for automatizing frequently practiced sequences, freeing up cortical layers involved in conscious control for other tasks.
And when the automatic system fails upon encountering an unusual or rare circumstances, the cortex can intervene.
Another advantage of having the basal ganglia in the loop is that convergence of inputs from multiple cortical areas provides a broader context for deciding on the next action or thought.
The powerful multiheaded attention mechanism in transformers could be implemented in the basal ganglia.
In a loop architecture, any region in the loop can contribute to making a decision.
As the actor in reinforcement learning, the basal ganglia also takes into account the learned value of the next action, biasing actions and speech toward achieving future rewards and goals. -->
<!-- Graybiel, A. M. (1997). The basal ganglia and cognitive pattern generators. Schizophrenia Bulletin, 23, 459–469. 10.1093/schbul/23.3.459 -->

### ソフトマックス関数

<img src="/2023assets/2006Oreilly_wm_fig2.svg">
<div class="figcaption">

図 ゲートが開いているときは，感覚入力は作業記憶を更新できるが，閉じているときはそれができない。
このため，妨害刺激など無関連情報が以前に記憶した情報の維持を妨げるのを防ぐことができる。
</div>

#### ニューロンのモデル

ニューロンが振動数符号化法のみを利用している --- すなわちニューラルネットワークにおけるすべての情報はニューロンの発火頻度によって伝達される --- ことを仮定する。
すなわち，このニューロンに到達する信号を単位時間あたりで等しく貢献するものと考える。
このような記述の仕方は integrate--and--fire (積分発火) モデルと呼ばれる。

一つのニューロンの振る舞いは，n 個のニューロンから入力を受け取って出力を計算する多入力，1 出力の情報処理素子である (下図 1)。

$n$ 個の入力信号を $x_1, x_2, \cdots, x_n$ とし、$i$ 番目の軸索に信号が与えられたとき、この信号 1 単位によって変化する膜電位の量をシナプス荷重(または結合荷重, 結合強度とも呼ばれる) といい $w_i$ と表記する。
抑制性のシナプス結合については $w_i < 0$, 興奮性の結合については $w_i > 0$ である。
このニューロンのしきい値を $h$, 膜電位の変化を $u$, 出力信号を $z$ とする。

<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br>
 -->
ニューロンの模式図 wikipedia より
</center>


<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br> -->
形式ニューロン
</center>


<center>
<img src="/assets/formal_neuron.svg"><br/>
図 1. ニューロンの模式図
</center>

出力信号 $z$ は次式で表される:

$$ z=f(\mu)=f\left(\sum_{i=1}^{n}w_{i}x_{i}-h\right).\tag{1} $$

$f(\mu)$ は出力関数であり、$0\le f(\mu)\le1$ の連続量を許す場合や、0 または 1 の値しかとらない場合などがある。
連想記憶などを扱うときなどは $-1\le f(\mu)\le 1$ とする場合もある。

- 上図で，$f$ がなければ，$z=\sum wx - h$ となり，回帰と同じである。
- すなわちニューラルネットワークのもっとも簡単な形は，統計学の用語では，**回帰 regression** である。
- 統計学においては，回帰，この場合，線形回帰になる，にかかわる変数 $x_{i}$
- 回帰の中を複雑にしていくことで，データに適合させようとする努力が，データサイエンスであり，機械学習であり，ニューラルネットワークであると言える。


## マッカロック・ピッツの形式ニューロン

信号入力の荷重和
$$ \mu = \sum_{i=1}^{n}w_{i}x_{i},\tag{2} $$
に対して、出力 $z$ は $u$ がしきい値 $h$ を越えたときに $1$, そうでなければ 0 を出力するモデルのことをマッカロックとピッツの形式ニューロン (formal neuron) と呼ぶ。
マッカロック・ピッツの形式ニューロンは神経細胞の振る舞いを記述するもっとも古く、単純な神経細胞のモデルであるが、現在でも用いられることがある。

$$
z = \left\{ \begin{array}{ll}
 1, & \mbox{$u > 0$ のとき} \\
 0, & \mbox{それ以外}
 \end{array}
\right.
$$

とすればマッカロック・ピッツのモデルは次式で表すことができる

$$ z=\mathbb{1}\left(\sum_{i=1}^nw_ix_i -h\right)\tag{3} $$

式中の $\mathbb{1}$ は数字ではなく 上式で表される関数の意味である。
このモデルは、単一ニューロンのモデルとしてではなく、ひとまとまりのニューロン群の動作を示すモデルとしても用いることがある。

### 出力が連続関数の場合

時間 $t$ における入力信号は $x_i(t)$ は $i$ 番目のシナプスの興奮伝達の時間 $t$ 付近での平均ととらえることができる。
すると、最高頻度の出力を 1, 最低(興奮無し)を 0 と規格化できると考えて $0\le f(\mu) \le1$ とする。
入出力関係は $f$ を用いて

$$ z = f(\mu) = f\left(\sum w_ix_i(t)-h\right)\tag{4} $$

のように表現される。
このモデルは、ニューロン集団の平均活動率ととらえることもできる。

良く用いられる出力関数 $f$ の形としては、$\mu = \sum w_ix_i -h$ として、

$$ f(\mu) = \frac{1}{1+e^{-\mu}},\tag{5} $$

や

$$ f(\mu) = \tanh(\mu)\tag{6}, $$

ただし $\mu = \sum w_{i}x_{i} -h$ などが使われることが多い。
(5) 式および (6) 式は、入力信号の重みつき荷重和 $\mu$ としてニューロンの活動が定まることを示している。
後述するバックプロパゲーション則で必要となるので、$\mu$ の微小な変化がニューロンの活動どのような影響を与えるか調べるために (5) 式 および (6) 式 を $\mu$ で微分することを考える。

$$ f(x) = \frac{1}{1+e^{-x}}, $$

を $x$ について微分すると

$$ \frac{df}{dx} = f(x)\left(1- f(x)\right) $$

$$ f(x) = \tanh{x} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$

を $x$ について微分すると

$$ \frac{df}{dx} =
1 - \tanh^2x = 1 - \left(f(x)\right)^2 $$

$\tanh$ は双曲線関数である。


以降では表記を簡単にするために線形数学の表記、すなわちベクトルと行列による
表記方法を導入する。$n$ 個の入力信号の組 $\left(x_1, x_2, \cdots, x_n\right)$ をまと
めて $\mathbf{x}$ のようにボールド体で表す。本章では一貫してベクトル表記には小
文字のボールド体を、行列には大文字のボールド体を用いることにする。例えば
$n$ 個の入力信号の組 $(x_1, x_2, \cdots, x_n)=\mathbf{x}$ に対して、同数の結合
荷重 $\left(w_1, w_2, \cdots, w_n\right)=\mathbf{w}$ が存在するので、加算記号 $\sum$ を使っ
て表現していた任意のニューロンへの全入力 $\mu=\sum w_{i}x_{i}$ はベクトルの内
積を用いて $\mu=(\mathbf{w}\cdot\mathbf{x})$ と表現される。

なお、横一行のベクトルを行ベクトル、縦一列のベクトルを列ベクトルと呼ぶことがある。
ここでは行ベクトルと混乱しないように、必要に応じて列ベクトルを表現する際には $\{x_1, x_2, \cdots, x_n\}^{\top}=\mathbf{x}$ とベクトルの肩に $\top$ を使って表現することもある。

そして、これらをベクトル表現や行列表現で表せば、表記も簡単になり、行列の演算法則を活用することもできる。
そのため、ニューラルネットワークに関する文献でも行列表現が用いられることが多い。

図のような単純な 2  層の回路を例に説明する。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/matrix-notation.svg"><br/>
<!-- <img src="figures/matrix-notation.svg"><br/> -->
ネットワークの行列表現
</center>

わかりやすいように図と対応させながら、対応する行列表現とシグマ記号による表記を併記するので、よく理解した上で先に読み進んでいただきたい。
なお、どうしても行列表現にはなじめないという読者は、行列表現の箇所だけをとばして読んでもある程度はわかるよう記述するつもりである。

それでは、まず図 4 のような単純な 2 層のネットワークを例に説明しよう。
図には、３つの入力素子 (ユニットと呼ばれることもある)
と２つの出力素子の活性値（ニューロンの膜電位に相当する）

$x_{1}, x_{2}, x_{3}$ と $y_{1}, y_{2}$，および入力素子と出力素子の結合強度を表す $w_{11}, w_{12}, \cdots, w_{32}$ が示されている。
これらの記号をベクトル $\mathbf{x}$, $\mathbf{y}$ と行列 $\mathbf{w}$ を使って表すと $\mathbf{y}=\mathbf{Wx}$ となる。

図\ref{fig:matrix-notation.eps}の場合、ベクトルと行列の各要素を書き下せば、
$$
\left(\begin{array}{l}y_{1}\\
y_2\\
\end{array}\right)
=\left(
\begin{array}{lll}
w_{11}&w_{12}&w_{13}\\
w_{21}&w_{22}&w_{23}
\end{array}
\right)
\left(
\begin{array}{l}
x_1\\
x_2\\
x_3\\
\end{array}
\right)
$$
のようになる。　　

---

行列の積は、左側の行列の $i$ 行目の各要素と右側の行列（ベクトルは１列の行列でもある）の $i$ 列目の各要素とを掛け合わせて合計することなので、以下のような、加算記号を用いた表記と同じである。

$$
\begin{array}{lllll}
y_1 &=& w_{11}x_1 + w_{12}x_2 + w_{13}x_3 &=&\sum_i w_{1i} x_i\\
y_2 &=& w_{21}x_1 + w_{22}x_2 + w_{23}x_3 &=&\sum_i w_{2i} x_i\\
\end{array}
$$

これを、$m$ 個の入力ユニットと $n$ 個の出力ユニットの場合に一般化すれば、
以下のようになる。


単純な 2 層のネットワークを考える。
$i$ 番目の出力層の各ニューロンの膜電位 $y_i,(i=1,2,\cdots,n)$ をまとめて $\mathbf{y}$ とベクトル表現し、同様に入力層も $\mathbf{x}$ とする。
ニューロン $x_{j}$ から ニューロン $y_{i}$ へのシナプス結合強度を $w_{ij}$ と表現すれば、入力層から出力層への関係は

$$
\left(\begin{array}{l}
y_1 \\ y_2 \\ \vdots \\ y_n \end{array}\right)  =
\left(\begin{array}{llll}
w_{11} & w_{12} & \cdots & w_{1m} \\
w_{21} & w_{22} & \cdots & w_{2m} \\
\vdots &        & \ddots & \vdots \\
w_{n1} & w_{n2} & \cdots & w_{nm}
\end{array}\right)
\left(\begin{array}{l}
x_1 \\ x_2 \\ \vdots \\ x_m \end{array}\right) \\
\mathbf{y} = \mathbf{Wx}
$$

と表現できる。
しきい値の扱いについては、常に 1 を出力する仮想的なニューロン$x_0=1$を
考えて $W$ に組み込むことも可能である。


実際の出力は $\mathbf{y}$ の各要素に対して

$$ f(y) = \frac{1}{1+e^{-y}}, $$

のような非線型変換を施すことがある。

階層型のネットワークにとっては上式，非線型変換が本質的な役割を果たす。
なぜならば、こうした非線形変換がなされない場合には、ネットワークの構造が何層になったとしても、この単純なシナプス結合強度を表す行列を $\mathbf{W}_{i}$
($i=1,\cdots, p$) としたとき、$\mathbf{W} = \prod_{i=1}^p \mathbf{W}_{i}$ と置くことによって本質的には 1 層のネットワークと等価になるからである。

$$ \mathbf{y} = \mathbf{W}_{p}\mathbf{W}_{p-1}\cdots\mathbf{W}_{1}\mathbf{y}=\left(\prod_{i=1}^p\mathbf{W}_{i}\right)\mathbf{y}. $$

<img src="/assets/xor.svg"><br/>

<img src="/assets/xor-graph.svg"><br/>


## 告知

* [第 38 回 前脳アーキテクチャ勉強会 「神経科学における chatGPT 等の活用」<img src="https://wba-initiative.org/wp-content/uploads/2015/05/logo.png" width="19%">](https://wba-initiative.org/21787/){:target="_blank"}

<!-- ### [第95回WBAI運営委員会 (第106回 WBAP研究打ち合わせ) アジェンダ兼議事録](https://docs.google.com/document/d/13MlfLMbp8PQ-3EclgZG5XQNHx_brWfipPatIftQO0yk/edit#)
### [第38回WBA勉強会企画書](https://docs.google.com/document/d/10Xs2SEZOpUkXbZg3chpUpnDLZhH-Xy1bedpvGHUemzk/edit#) -->

* テーマ：「神経科学における ChatGPT 等の活用」
* 日程：2023年5月11日（木）
* 時間：18:00〜21:00頃
* 会場：オンライン（Zoom）
* 演者１：中江 健（生命創成探究センター）ken.nakae@gmail.com
* 演者２：芦原佑太（WBAI）y.ashi.data@gmail.com
* 演者３：小島 武（東京大学）t.kojima@weblab.t.u-tokyo.ac.jp
* 参加費用：1,000円 （学生無料）
* 参加人数：一般：150人、学生：50人
* 実施体制
* 主催： NPO法人全脳アーキテクチャ・イニシアティブ
* 共催： 学術変革「行動変容を創発する脳ダイナミクスの解読と操作が拓く多元生物学」
* 開催趣旨
ChatGPT などの大規模言語モデルの実用化にともない、自然言語で記述された論文内などから目的に応じて神経科学の知見を大量の公開情報から検索することが現実的となってきた。
これは、神経科学の促進や、脳型ソフトウエアの設計データの蓄積に役立つ。
しかしながら、実際にそれをおこなうためには、段階的にプロンプトをあたえるなどのノウハウが必要である。
こうした取り組みは、神経科学研究だけでなく、特定分野における大規模言語モデルを活用する実例として広く参考になるであろう。
そこで今回は、WBAIおよび学術変革「行動変容生物学」で進められている神経科学分野での大規模言語モデル適用について紹介するための勉強会を行う。


# 2. 再帰型ニューラルネットワーク

## 2.1. NETtalk
系列情報処理を扱った初期のニューラルネットワーク例として NETTalk が挙げられます。
NETTalk[^NETTalk] は文字を音読するネットワークです。下図のような構成になっています。
下図のようにアルファベット 7 文字を入力して，空白はアンダーラインで表現されています，中央の文字の発音を学習する 3 層のニューラルネットワークです。NETTalk は 7 文字幅の窓を移動させながら
逐次中央の文字の発音を学習しました。たとえば /I ate the apple/ という文章では
"the" を "ザ" ではなく "ジ" と発音することになります。

印刷単語の読字過程のニューラルネットワークモデルである SM89[^SM89], PMSP96[^PMSP96] で用いられた発音表現は <a target="_blank" href="https://en.wikipedia.org/wiki/ARPABET">ARPABET</a> の亜種です。Python では `nltk` ライブラリを使うと ARPABET の発音を得ることができます(<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb">ARPABET のデモ<img src="/assets/colab_icon.svg"></a>)。

[^NETTalk]: Sejnowski, T.J. and Rosenberg, C. R. (1987) Parallel Networks that Learn to Pronounce English Text, Complex Systems 1, 145-168.

[^SM89]: Seidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmetal model of word recognition and naming. Psychological Review, 96(4), 523–568.

[^PMSP96]: Plaut, D. C., McClelland, J. L., Seidenberg, M. S. & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56–115.

<center>

<img src="/assets/1986Sejnowski_NETtalkFig2.svg" style="width:47%"><br/>
Sejnowski (1986) Fig. 2
</center>

## 2.2. 単純再帰型ニューラルネットワーク

NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案されました。
発案者の名前で **Jordan ネット**[^JordanNet]，**Elman ネット**[^ElmanNet] と呼ばれます。

[^JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.

[^ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211.

Jordan ネットも Elman ネットも上位層からの **帰還信号** を持ちます。これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われます。Jordan ネットでは一時刻前の出力層の情報が用いられます(下図)。

<center>

<img src="/assets/SRN_J.svg" style="width:47%"><br/>
<p style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</p>
</center>

- 駄菓子菓子 <a target="_blank" href="/assets/MJ_air.jpg">彼（マイケル・ジェフェリー(エアー)・ジョーダン）</a> ではない :)
- <a target="_blank" href="/assets/c3-s4-jordan.jpg">マイケル・アーウィン・ジョーダン。ミスター機械学習[^jordan_ai_revolution_not_yet]</a>

[^jordan_ai_revolution_not_yet]: 彼は(も？)神様です。多くの機械学習アルゴリズムを提案し続けている影響力のある人です。長らく機械学習の国際雑誌の編集長でした。2018年 <a target="_blank" href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">AI 革命は未だ起こっていない</a> と言い出して議論を呼びました。


一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられます。

<center>

<img src="/assets/SRN_E.svg" style="width:47%"><br/>
<p style="align:center; width:47%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</p>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられます。
上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数です。
Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かります。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきました。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味です。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられます。従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるでしょう。

## 2.3. リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができます。同時に時間発展を考慮すれば下図右のように描くことも可能です。

<center>

<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてください。あるいは同義ですが上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してください。このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かります。

## 2.4. エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示しました。
図中の数字はニューロンの数を表します。入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表します。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:47%"><br/>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示しました。Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示されました(Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives

これらの規則にはさらに 2 つの制約があります。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものです。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示しました。"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示しています。
同じ文型の文章は同じような状態遷移を辿ることが分かります。

<center>
<img src="/assets/1991Elman_Fig3.jpg" style="width:49%"><br/>
<p align="left" style="width:47%">
<!-- Trajectories through state space for sentences boy chases boy, boy sees boy, boy walks.
Principal component 1 is plotted along the abscissa; principal component 3 is plotted along the ordinate.
These two PC’s together encode differences in verb-argument expectations. -->
</p>
</center>

<!-- <img src="/assets/1991Elman_Fig4a.jpg" style="width:84%"><br> -->

下図は文 "boy chases boy who chases boy" を入力した場合の遷移図です。この文章には単語 "boy" が 3 度出てきます。それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかります。
同様に 'chases" が 2 度出てきますが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されています。<br/>

<center>
<img src="/assets/1991Elman_Fig4b.jpg" style="width:49%"><br/>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示しました。<br/>
<center>

<img src="/assets/1991Elman_Fig4c.jpg" style="width:48%"><br/>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に島します。</br>
<center>

<img src="/assets/1991Elman_Fig4d.jpg" style="width:48%"><br/>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば， 同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが示唆されます。
このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上)

<!--
<p align="left" style="width:74%">
Movement through state space for sentences with relative clauses. Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. These two PC’s encode depth of embedding in relative clauses.
</p>
</center>
-->

## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がないことです。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことです。
この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。

<!--
<center>

<img src="/assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>

<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>

$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$

## 2.6. 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログ[^karpathy] から下図に引用します。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示しています。

[^karpathy]: 去年までスタンフォード大学の大学院生。現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www

<center>

<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきました。
ですがが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は
1024 程度まで用いられていることには注意してください。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現[^onehot]

[^onehot]: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。一つだけが "熱い" あるいは "辛い" ベクトルと呼びます。以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたのですが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多いです。ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じます。典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新します。従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになります。そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行いました。上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当します。単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展します。

<center>
<img src="/assets/charseq.jpeg" style="width:66%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

[@1991Siegelmann_RNN_universal] said Turing completeness of RNN.


現在までニューラルネットワーク系も記号処理系の人工知能も 3 回の流行がありました。
人工知能研究の歴史はこの 3  回の流行でどのようなことが起こったのかを知ることが重要になります。

人工知能の教科書を紐解くと，1956 年のダートマス会議(Dartmouth 大学，ニューハンプシャー州，アメリカ合衆国)であると書かれています。
人工知能という言葉が初めて用いられたとも書かれています。
ですがそれ以前にもチューリングの研究もありましたし，日本のからくり人形まで含めれば機械や人形に知的な振る舞いをさせる試みは以前から存在しました。

ダートマス会議では人工知能が取り組むべきつの問題を列挙しています5。

1. コンピュータの自動化 (Automatic Computers)
2. 日常言語を用いたコンピュータプログラミング (How Can a Computer be Programmed to Use a Language)
3. ニューラルネットワーク (Neuron Nets)
4. 計算規模の理論 (Theory of the Size of a Calculation)
5. 自己改善 (Self-lmprovement)
6. 抽象化 (Abstractions)
7. 乱雑さと創造性 (Randomness and Creativity)

上記の解くべき問題のリストは，あらかじめ定められた課題だけしか扱うことができない融通の効かない機械を越えて人間のように柔軟に状況に対応することが人工知能に求められた課題であると言うことができます。

人工知能研究には
2 つの大きな流れがあります。
一つは脳がニューロンを基本単位としたネットワークであることから，ニューロンのネットワークを模倣しようとする試みで ニューラルネットワーク 研究です。
もう一つは，他の種と異なり我々人間だけが，言語や数学など高度な記号処理体系を持っていることから，これらの記号処理体系をコンピュータに実現しようとする 記号処理 に基づく人工知能研究です。

---

- [用語集](/2023/2023lect03_glossary){:target="_blank"}

<!-- ## 実習 -->

<!--     - [実習 画像認識 Keras 版<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"}
- [実習 Kermack McKendric model<img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"}
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->

<!-- - [実習 画像認識 Keras 版](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kera_CNN_demo_with_wordnet_ja.ipynb) -->
<!-- - [実習 Keras CNN](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb){:target="_blank"} -->
<!-- - [実習 PyTorch CNN](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0416PyTorch_CNN_demo.ipynb){:target="_blank"} -->
<!-- - [実習 Kermack McKendric model](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}-->
<!--- [実習](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb#scrollTo=oD497lby40Fp)-->


<!-- - CNN: 畳み込みニューラルネットワーク
- RNN: リカレントニューラルネットワーク
- RL: 強化学習-->

<!-- <center> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2008Fuster_Prefrontal_Cortex_fig8_4.svg" width="39%"> -->
<!--  <img src="https://komazawa-deep-learning.github.io/assets/2015Ronneberger_U-Net_Fig1_ja.svg" width="48%"> -->
<!-- </center> -->

<!-- - [2018Kriegeskorte](2018Kriegeskorte){:target="_blank"}
- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
- [G 検定](https://www.seshop.com/product/detail/23864?utm_source=seid_it_spot_20210412&utm_medium=email&utm_campaign=coupon){:target="_blank"}

### 2021年02月23日分
- [2020-0215](2020-0215abstract){:target="_blank"}
- [どうぶつの森モデル，動物の名前連想モデル](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0223word_associtaion.ipynb){:target="_blank"}
- [導入講義用 CCP ウィルス感染者予測モデルを題材に](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Kermack_McKendrick_model.ipynb){:target="_blank"}
- [CNN の簡単なデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021Keras_CNN_demo_with_wordnet_ja.ipynb){:target="_blank"}

# 統計学と機械学習の関係

母集団における差異の有無を問題にする心理統計学と機械学習との間には，決定的な差があります。

- [1970Newell](1970Newell){:target="_blank"}
- [2019Glaser](2019Glaser){:target="_blank"}
- [2020Lindsay](2020Lindsay){:target="_blank"}
-->

<!--
<br/>
1. [tSNE を用いた TLPA 200語の word2vec 視覚化](https://ShinAsakawa.github.io/2020cnps_tSNE_for_word2vec.ipynb)
2. [2020年2月24日資料1 tlpa 画像](https://ShinAsakawa.github.io/2020making_tlpa.html)
3. [2020年4月15日かじゅまるつがる松本先生のモデルの説明](https://shinasakawa.github.io/2020gajumarutugaru/2020-0415Friston_in_detail.html)
4. [2020年4月18日かじゅまるつがる投稿](https://shinasakawa.github.io/2020gajumarutugaru/2020-0418gajumarutugaru.html)

<br/>

1. [2020ccap 資料置き場](2020ccap)
2. [2020中央大学，緑川先生，重宗先生，研究会資料](2020chuo)
3. [2020 第2回 中央大学，緑川先生，重宗先生，研究会資料](2020chuo2)
4. [2020サイトビジット資料](2020sightvisit)

<a href="https://guides.github.com/features/pages/">Read this page to write this page.</a>-->
