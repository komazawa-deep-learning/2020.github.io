---
title: "How ChatGPT works"
layout: default
---

# How ChatGPT Works: The Model Behind The Bot

##### Molly Ruby (2022)

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_1.png" width="77%">
<!-- <img src="figures/2023Ruby_fig0.jpg" width="55%"> -->
</div>

ChatGPT を支える機械学習モデルについてご紹介する。
<!-- A brief introduction to the intuition and methodology behind the chat bot you can’t stop hearing about.-->

ChatGPT を支える機械学習モデルをやさしく紹介する本稿は，大規模言語モデルの導入から始まり，GPT-3 の学習を可能にした画期的な自己学習機構，そして ChatGPT を特別なものにした新しい手法である人間のフィードバックからの強化学習に入っていく。
<!-- This gentle introduction to the machine learning models that power ChatGPT, will start at the introduction of Large Language Models, dive into the revolutionary self-attention mechanism that enabled GPT-3 to be trained, and then burrow into Reinforcement Learning From Human Feedback, the novel technique that made ChatGPT exceptional. -->

### 大規模言語モデル <!-- ### Large Language Models-->

ChatGPTは、Large Language Model (LLM) と呼ばれる機械学習の自然言語処理モデルの一種を外挿したものである。
LLM は，膨大な量のテキストデータを消化し，テキスト内の単語間の関係を推論するものである。
このモデルは，ここ数年，計算機の性能の向上とともに成長してきた。
LLM は，入力データセットとパラメータ空間のサイズが大きくなるにつれて，その能力を高めていく。
<!-- ChatGPT is an extrapolation of a class of machine learning Natural Language Processing models known as LargeLanguage Model (LLMs).
LLMs digest huge quantities of text data and infer relationships between words within the text.
These models have grown over the last few years as we’ve seen advancements in computational power.
LLMs increase their capability as the size of their input datasets and parameter space increase. -->

言語モデルの最も基本的な学習は，単語列の中の単語を予測することである。
最も一般的には，次トークン予測とマスク化言語モデルのいずれかとして観察される。
<!-- The most basic training of language models involves predicting a word in a sequence of words.
Most commonly, this is observed as either next-token-prediction and masked-language-modeling. -->

<div class="figure figcenter">
<img src="/2023assets/2023Ruby_fig1.jpg">
<div class="figcaption">
Arbitrary example of next-token-prediction and masked-language-modeling generated by the author.
</div></div>

この基本的な逐次処理技術は，LSTM (Long-Short-Term-Memory) モデルによって展開されることが多く，モデルは，周囲の文脈から統計的に最も可能性の高い単語で空白を埋めていくことになる。
この逐次モデリングの構造には，2 つの大きな限界がある。
<!-- In this basic sequencing technique, often deployed through a Long-Short-Term-Memory (LSTM) model, the model is filling in the blank with the most statistically probable word given the surrounding context.
There are two major limitations with this sequential modeling structure.-->

1. このモデルでは，周囲の単語を他の単語よりも高く評価することができない。
上記の例では reading は hates と結びつくことが多いが，データベースでは Jacob は熱心な読書家であり，モデルは reading よりも Jacob に重きを置き，hates ではなく love を選択すべきかもしれない。
<!-- The model is unable to value some of the surrounding words more than others.
In the above example, while‘reading’ may most often associate with ‘hates’, in the database ‘Jacob’ may be such an avid reader that the model should give more weight to ‘Jacob’ than to ‘reading’ and choose ‘love’ instead of ‘hates’. -->

2. 入力データは，コーパス全体としてではなく，個別に順次処理される。
つまり，LSTM を学習する際，文脈の窓は固定され，個々の入力を超えて，系列のいくつかの段階にのみ拡張されます。
このため，単語間の関係や導き出される意味の複雑さに限界があります。
<!-- The input data is processed individually and sequentially rather than as a whole corpus.
This means that when an LSTM is trained, the window of context is fixed, extending only beyond an individual input for several steps in the sequence.
This limits the complexity of the relationships between words and the meanings that can be derived. -->

この問題に対応するため，2017 年に Google Brain のチームがトランスフォーマーを導入した。
LSTM とは異なり，トランスフォーマーはすべての入力データを同時に処理することができる。
自己注意機構を使用して，モデルは，言語系列の任意の位置に関連して，入力データのさまざまな部分にさまざまな重みを与えることができる。
この機能により，LLM への意味の付与が大幅に改善され，非常に大きなデータセットの処理が可能になった。
<!-- In response to this issue, in 2017 a team at Google Brain introduced transformers.
Unlike LSTMs, transformers can process all input data simultaneously.
Using a self-attention mechanism, the model can give varying weight to different parts of the input data in relation to any position of the language sequence.
This feature enabled massive improvements in infusing meaning into LLMs and enables processing of significantly larger datasets. -->

## GPT と自己注意 <!-- ## GPT and Self-Attention-->

Generative Pre-training Transformer (GPT) モデルは，2018 年に GPT-1 として openAI が初めて発表した。
このモデルは，2019 年に GPT-2，2020 年に GPT-3，そして直近では 2022 年に InstructGPT と ChatGPT で進化し続けた。
人間のフィードバックを系に統合する前に，GPT モード革命の最大の進歩は，計算効率の達成によってもたらされ，GPT-3 は GPT-2 よりも大幅に多くのデータで学習することができ，より多様な知識ベースと幅広い課題を実行する能力を得ることができた。
<!-- Generative Pre-training Transformer (GPT) models were first launched in 2018 by openAI as GPT-1.
The models continued to evolve over 2019 with GPT-2, 2020 with GPT-3, and most recently in 2022 with InstructGPT and ChatGPT.
Prior to integrating human feedback into the system, the greatest advancement in the GPT model evolution was driven by achievements in computational efficiency, which enabled GPT-3 to be trained on significantly more data than GPT-2, giving it a more diverse knowledge base and the capability to perform a wider range of tasks. -->

<div class="figure figcenter">
<img src="/2023assets/2023Ruby_fig2.jpg" width="66%">
<div class="figcaption">

Comparison of GPT-2 (left) and GPT-3 (right). Generated by the author.
</div></div>

GPT は，入力配列を処理する符号化器と出力配列を生成する復号化器を持つ，トランスフォーマーアーキテクチャを採用している。
符号化器と復号化器の両方が，マルチヘッド自己注視機構を備えており，モデルが系列の一部に異なる重み付けをして，意味と文脈を推論することを可能にする。
また，符号化器は，単語間の関係を理解し，より理解しやすい応答を生成するために，マスク化言語モデルを活用する。
<!-- All GPT models leverage the transformer architecture, which means they have an encoder to process the input sequence and a decoder to generate the output sequence.
Both the encoder and decoder have a multi-head self-attention mechanism that allows the model to differentially weight parts of the sequence to infer meaning and context.
In addition, the encoder leverages masked-language-modeling to understand the relationship between words and produce more comprehensible responses.-->

GPT を駆動する自己注意機構は，トークン (単語，文，その他のグループ化されたテキストの断片) を，入力系列におけるトークンの重要度を表すベクトルに変換することで機能する。
これを実現するために，モデルは以下のような:
<!-- The self-attention mechanism that drives GPT works by converting tokens (pieces of text, which can be a word, sentence, or other grouping of text) into vectors that represent the importance of the token in the input sequence.
To do this, the model, -->

1. 入力系列の各トークンについて，クエリ，キー，およびバリュー，ベクトルを作成
2. ステップ 1 のクエリベクトルと他のトークンのキーベクトルとの間の類似度を，2 つのベクトルのドット積を取ることによって計算
3. ステップ 2 の出力をソフトマックス関数に入力し，正規化された重みを生成
4. ステップ 3 で生成された重みに各トークンの値ベクトルを乗じることにより，系列内のトークンの重要度を表す最終ベクトルを生成

<!-- 1. Creates a query, key, and value vector for each token in the input sequence.
2. Calculates the similarity between the query vector from step one and the key vector of every other token by taking the dot product of the two vectors.
3. Generates normalized weights by feeding the output of step 2 into a softmax function.
4. Generates a final vector, representing the importance of the token within the sequence by multiplying the weights generated in step 3 by the value vectors of each token. -->

GPT が採用している「マルチヘッド」注意機構は，自己注意を発展させたものである。
1-4 ステップを 1 回行うのではなく，並行してこの機構を数回繰り返し，その都度，クエリ，キー，バリューベクトルの新しい線形射影を生成する。
このように自己注意を拡張することで，モデルは入力データ内の部分的な意味やより複雑な関係を把握することができるようになる。
<!-- The ‘multi-head’ attention mechanism that GPT uses is an evolution of self-attention.
Rather than performing steps 1–4 once, in parallel the model iterates this mechanism several times, each time generating a new linear projection of the query, key, and value vectors.
By expanding self-attention in this way, the model is capable of grasping sub-meanings and more complex relationships within the input data. -->

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_2.png" width="77%">
<!-- <img src="figures/2023Ruby_fig3.jpg" width="55%"> -->
</div>

GPT-3 は自然言語処理において目覚ましい進歩を遂げたが，ユーザの意図に沿うような出力には限界がある。
例えば，GPT-3 は以下のような出力をすることがある:
<!-- Although GPT-3 introduced remarkable advancements in natural language processing, it is limited in its ability to align with user intentions.
For example, GPT-3 may produce outputs that:-->

* ユーザの明示的な指示に従わない，**親切さに欠ける** 出力が得られる
* 存在しない，あるいは誤った事実を反映した **幻想** が含まれる
* モデルがどのように特定の決定や予測に至ったかを人間が **理解することが困難** な解釈可能性を欠く
* 有害または攻撃的で，誤った情報を広める **有害または偏ったコンテンツを含む**

<!-- * Lack of helpfulness meaning they do not follow the user’s explicit instructions.
* Contain hallucinations that reflect non-existing or incorrect facts.
* Lack interpretability making it difficult for humans to understand how the model arrived at a particular decision or prediction.
* Include toxic or biased content that is harmful or offensive and spreads misinformation. -->

ChatGPT では，標準的な LLM のこれらの固有の問題を解決するために，革新的な訓練手法が導入された。
<!-- Innovative training methodologies were introduced in ChatGPT to counteract some of these inherent issues of standard LLMs. -->

## ChatGPT

ChatGPT は InstructGPT のスピンオフで，人間のフィードバックを学習処理過程に取り入れることで，モデルの出力をよりユーザーの意図に沿うようにする新しいアプローチを導入した。
人間のフィードバックからの強化学習 (RLHF) については，openAI の 2022 年の論文 Training language models to follow instructions with human feedback で詳しく説明されているが，以下では簡略化して説明する。
<!-- ChatGPT is a spinoff of InstructGPT, which introduced a novel approach to incorporating human feedback into the training process to better align the model outputs with user intent.
Reinforcement Learning from Human Feedback (RLHF) is described in depth in openAI’s 2022 paper Training language models to follow instructions with human feedback and is simplified below. -->

### ステップ 1：教師あり微調整 (SFT: Supervised Fine Tuning) モデル <!-- ### Step 1: Supervised Fine Tuning (SFT) Model-->

最初の開発では，GPT-3 モデルの微調整を行うため，40 人の契約社員を雇い，モデルが学習するための既知の出力を持つ入力の教師付き学習データセットを作成した。
入力 (プロンプト) は，実際に Open API に入力されたユーザから収集されたものである。
ラベラはプロンプトに対して適切な応答を記述することで，各入力に対して既知の出力を作成する。
この新しい教師ありデータセットを用いて GPT-3 モデルを微調整し，GPT-3.5 (SFT モデルとも呼ばれる) を作成した。
<!-- The first development involved fine-tuning the GPT-3 model by hiring 40 contractors to create a supervised training dataset, in which the input has a known output for the model to learn from.
Inputs, or prompts, were collected from actual user entries into the Open API.
The labelers then wrote an appropriate response to the prompt thus creating a known output for each input.
The GPT-3 model was then fine-tuned using this new,supervised dataset, to create GPT-3.5, also called the SFT model. -->

プロンプトデータセットの多様性を最大化するため，任意のユーザー ID から得られるプロンプトは 200 件までとし，共通の長い接頭辞 shared long common prefixes を持つプロンプトは削除された。
最後に，特定可能な個人情報 (PII) を含むプロンプトはすべて削除された。
<!-- In order to maximize diversity in the prompts dataset, only 200 prompts could come from any given user ID and any prompts that shared long common prefixes were removed.
Finally, all prompts containing personally identifiable information (PII) were removed. -->

OpenAI API からプロンプトを集計した後，ラベラーには，実際のサンプルデータが少ないカテゴリーを埋めるためのサンプルプロンプトの作成も依頼した。
対象となったカテゴリは以下の通り：
<!-- After aggregating prompts from OpenAI API, labelers were also asked to create sample prompts to fill-out categories in which there was only minimal real sample data.
The categories of interest included:-->

* 平易なプロンプト: 任意の質問
* 少数撃プロンプト: 複数のクエリとレスポンスのペアを含む指示
* ユーザーベースのプロンプト: OpenAI API に要求された特定のユースケースに対応

<!-- * Plain prompts: any arbitrary ask.
* Few-shot prompts: instructions that contain multiple query/response pairs.
* User-based prompts: correspond to a specific use-case that was requested for the OpenAI API. -->

応答を生成する際，ラベラはユーザからの指示が何であるかを推察することに全力を尽くすよう求められた。
本稿では，プロンプトが情報を要求する主な 3 つの方法について説明する。
<!-- When generating responses, labelers were asked to do their best to infer what the instruction from the user was.
The paper describes the main three ways that prompts request information.-->

1. 直接: "Tell me about… " について教えてください。
2. Few-shot: この 2 つのストーリーの例をもとに，同じテーマで別のストーリーを書いてみてください。
3. 継続:  物語の始まりが与えられたら，それを完成させなさい。

<!-- 1. Direct: “Tell me about…”
2. Few-shot: Given these two examples of a story, write another story about the same topic.
3. Continuation: Given the start of a story, finish it. -->

OpenAI API からのプロンプトとラベラによる手書きのプロンプトをまとめた結果，教師ありモデルに活用できる 13,000 の入力/出力サンプルが得られた。
<!-- The compilation of prompts from the OpenAI API and hand-written by labelers resulted in 13,000 input / output samples to leverage for the supervised model. -->

<div class="figure figcenter">
<img src="/2023assets/ChatGPT_Diagram.svg" width="77%">
<!-- <img src="figures/2023Ruby_fig4.jpg"> -->
</div>

### ステップ 2: 報酬モデル <!-- ### Step 2: Reward Model-->

ステップ 1 で SFT モデルを訓練した後，このモデルはユーザのプロンプトに対してより適切な応答を生成する。
このモデルの入力は一連のプロンプトと応答であり，出力は報酬と呼ばれるスカラ値である。
報酬モデルは，モデルが報酬を最大化するように出力を生成することを学習する強化学習 (ステップ 3 参照) を活用するために必要なものである。
報酬モデルを訓練するために，ラベラには 1 つの入力プロンプトに対して 4～9 個の SFT モデル出力が提示される。
そして，これらの出力をベストからワーストにランク付けするよう求められ，以下のような出力ランクの組み合わせが作成される。
<!-- After the SFT model is trained in step 1, the model generates better aligned responses to user prompts.
The next refinement comes in the form of training a reward model in which a model input is a series of prompts and responses, and the output is a scaler value, called a reward.
The reward model is required in order to leverageReinforcement Learning in which a model learns to produce outputs to maximize its reward (see step 3).
To train the reward model, labelers are presented with 4 to 9 SFT model outputs for a single input prompt.
They are asked to rank these outputs from best to worst, creating combinations of output ranking as follows. -->

<div class="figure figcenter">
<img src="figures/2023Ruby_fig5.jpg">
</div>

各組み合わせを個別のデータ点としてモデルに含めると，過学習 (見たデータ以上の外挿ができない) が発生した。
そこで，各順位群を 1 つのデータ点として活用し，モデルを構築した。
<!-- Including each combination in the model as a separate data point led to overfitting (failure to extrapolate beyondseen data).
To solve, the model was built leveraging each group of rankings as a single batch data point. -->

### ステップ 3: 強化学習モデル <!-- ### Step 3: Reinforcement Learning Model-->

最終段階では，モデルにランダムなプロンプトを提示し，応答を返す。
応答は，モデルがステップ 2 で学習した「方針」を用いて生成される。
方針は，機械が目標を達成するために学習した戦略であり，この場合，報酬を最大化することである。
ステップ 2 で開発された報酬モデルに基づいて，プロンプトと応答の対に対してスカラ報酬値が決定される。
この報酬は，方針を進化させるためにモデルにフィードバックされる。
<!-- In the final stage, the model is presented with a random prompt and returns a response.
The response is generated using the ‘policy’ that the model has learned in step 2. The policy represents a strategy that the machine has learned to use to achieve its goal; in this case, maximizing its reward.
Based on the reward model developed instep 2, a scaler reward value is then determined for the prompt and response pair.
The reward then feeds back into the model to evolve the policy. -->

2017 年 Schulman+ は，各応答が生成されるたびにモデルの方針を更新する際に使用される手法である Proximal Policy Optimization (PPO) を発表した。
PPO では，SFT モデルからトークンごとの KL (Kullback-Leibler) ペナルティを組み込んでいる。
KL ダイバージェンスは，2 つの分布関数の類似性を測定し，極端な距離にはペナルティを与える。
この場合，KL ペナルティを使用することで，ステップ 1 で学習した SFT モデル出力から応答が離れる距離を減らし，報酬モデルを最適化しすぎて人間の意図データセットから大きく逸脱するのを防ぐ。
<!-- In 2017, Schulman+ introduced Proximal Policy Optimization (PPO), the methodology that is used in updating the model’s policy as each response is generated.
PPO incorporates a per-token Kullback–Leibler (KL) penalty from the SFT model.
The KL divergence measures the similarity of two distribution functions and penalizes extreme distances.
In this case, using a KL penalty reduces the distance that the responses can be from the SFTmodel outputs trained in step 1 to avoid over-optimizing the reward model and deviating too drastically from the human intention dataset. -->

<div class="figure figcenter">
<img src="/2023assets/2023Ruby_fig6.jpg" width="44%">
</div>

ステップ 2 とステップ 3 は，繰り返し行うことができるが，実際にはあまり行われていない。
<!-- Steps 2 and 3 of the process can be iterated through repeatedly though in practice this has not been done extensively. -->

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_3.png" width="77%">
<!-- <img src="figures/2023Ruby_fig7.jpg" width="55%"> -->
</div>

## モデルの評価 <!-- ## Evaluation of the Model-->

モデルの評価は，訓練中に，モデルが見たことのない検証セットを用意することで行われる。
検証セットでは，モデルが前身である GPT-3 よりも優れているかどうかを判断するために，一連の評価が行われる。
<!-- Evaluation of the model is performed by setting aside a test set during training that the model has not seen.
On thetest set, a series of evaluations are conducted to determine if the model is better aligned than its predecessor, GPT-3. -->

* **有益性 helpfulness**: モデルの出力は，ラベラにとって好ましいものであった。
ラベラは GPT-3 よりも InstructGPT の出力を 85±3 %の確率で好んだ。
* **真実性 truthfulness**: モデルの偽りの傾向。PPO モデルは TruthfulQA データセットを用いて評価した場合，真実性と情報性がわずかに増加する出力を生成した。
* **無害性 harmlessness**: 不適切な内容、軽蔑的な内容、否定的な内容を回避する能力。無害性は，RealToxicityPrompts データセットを用いて検証された。テストは 3 つの条件下で行われた。

<!-- * **Helpfulness**: the model’s ability to infer and follow user instructions. Labelers preferred outputs from InstructGPTover GPT-3 85 ± 3% of the time.
* **Truthfulness**: the model’s tendency for hallucinations. The PPO model produced outputs that showed minor increases in truthfulness and informativeness when assessed using the TruthfulQA dataset.
* **Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset. The test was performed under three conditions. -->

1. 尊敬に値する回答をするよう指示された場合: 有害な回答が有意に減少した。
2. 敬意を設定せずに応答を行うように指示した場合: 毒性に大きな変化はない。
3. 有害な反応をするように指示した場合: GPT-3 モデルよりも有意に有害な反応をするようになった。

<!-- 1. Instructed to provide respectful responses: resulted in a significant decrease in toxic responses.
2. Instructed to provide responses, without any setting for respectfulness: no significant change in toxicity.
3. Instructed to provide toxic response: responses were in fact significantly more toxic than the GPT-3 model. -->

ChatGPT と InstructGPT の作成に用いた手法の詳細については OpenAI が発表したオリジナル論文 Training language models to follow instructions with human feedback, 2022 https://arxiv.org/pdf/2203.02155.pdf をお読みください。
<!-- For more information on the methodologies used in creating ChatGPT and InstructGPT, read the original paper published by OpenAI Training language models to follow instructions with human feedback, 2022 https://arxiv.org/pdf/2203.02155.pdf. -->

<!-- Happy learning! -->
<!-- Sources -->

* https://openai.com/blog/chatgpt/
* https://arxiv.org/pdf/2203.02155.pdf
* https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer
* https://www.assemblyai.com/blog/how-chatgpt-actually-works/
* https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b

<!-- ChatgptMachine LearningData ScienceNLPEditors Pick

Sign up for The Variable
By Towards Data Science
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. 
Take a look.
By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices. -->
