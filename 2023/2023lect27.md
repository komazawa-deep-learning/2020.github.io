---
title: 第27回
author: 浅川 伸一
layout: home
---
# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align="right">
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 15/Dec/2023<br/>
Appache 2.0 license<br/>
</div>


## ベルマン方程式

ベルマン方程式とは， 価値関数を目先の報酬と割引された将来の価値に分解する一連の方程式を指します。
<!-- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. -->

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

Q 関数とは
<!-- Similarly for Q-value, -->

$$
\begin{aligned}
Q(s, a)
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \\
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$


### 期待ベルマン方程式
<!-- ### Bellman Expectation Equations -->

* 再帰的な更新過程は，さらに分解すると，状態値関数と行動値関数の両方で構成される方程式となる。
* 今後の行動ステップを進める際には $\pi$ の方針に沿って $V$ と $Q$ を交互に拡張していく。

<!-- The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.
As we go further in future action steps, we extend V and Q alternatively by following the policy $\pi$.-->

<center>
<img src="/assets/bellman_equation.png" width="60%"><br/>
<div style="text-align: center;width: 88%;background-color: powderblue;">
図 5. ベルマン期待値方程式がどのように状態値関数と行動値関数を更新するかを示す図。
<!--
Fig. 5. Illustration of how Bellman expection equations update state-value and action-value functions.
![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }}){: style="width: 60%;" class="center"}
-->
</div>
</center>

$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi
} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi
} (s', a')
\end{aligned}
$$

### 最適ベルマン方程式
<!-- ### Bellman Optimality Equations -->

* 方策 (ポリシー) に従った期待値を計算するのではなく，最適値にしか興味がないのであれば，方策 (ポリシー) を使わずに代替更新中の最大リターンにすぐに飛びつくことができる。
<!-- RECAP:  最適値 $V_*$ と $Q_*$ は，得られる最高のリターンであり [ここ](#optimal-value-and-policy) で定義されています。-->
<!-- If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy.
RECAP: the optimal values $V_*$ and $Q_*$ are the best returns we can obtain, defined [here](#optimal-value-and-policy).-->

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

* 上式はベルマンの期待方程式と酷似している。
<!-- Unsurprisingly they look very similar to Bellman expectation equations. -->

* もし環境の完全な情報があれば，この問題は計画問題となり，動的計画法 (DP: dynamic programming) で解くことができる。
* 駄菓子菓子，ほとんどの場合 $P_{ss'}^a$ や $R(s, a)$ が不明であるため，ベルマン方程式を直接適用して MDP を解くことはできない。
* 駄菓子菓子，多くの RL アルゴリズムの理論的基礎を与えている。

<!--
If we have complete information of the environment, this turns into a planning problem, solvable by DP.
Unfortunately, in most scenarios, we do not know $P_{ss'}^a$ or $R(s, a)$, so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. -->


<!--
```python
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
```

[cartpole 問題](https://www.youtube.com/watch?v=J7E6_my3CHk)を解いてみました

```bash
cd ~/study/2019tensorflow_models.git/research/a3c_blogpost
# python a3c_cartpole.py --train
python a3c_cartpole.py --algorithm=random --max-eps=4000
```
-->


## 基本概念
<!-- ### Key Concepts -->

<!-- RL の重要な概念を正式に定義しましょう。
Now Let's formally define a set of key concepts in RL. -->

* 動作主 (エージェントは) **環境** の中で行動する。
* 環境がある行動に対してどのように反応するかは，我々が知っているかどうかわからない **モデル** によって定義される。
* 動作主 (エージェント) は，環境の多くの **状態**（$s\in\mathcal{S}$）のうちの 1 つに留まることができ，多くの **行動**（$a\in\mathcal{A}$）のうちの 1 つを選択して， ある状態から別の状態に切り替えることができる。
* 動作主 (エージェント) がどの状態に到達するかは，状態間の遷移確率 ($P$) によって決定される。
* 行動を起こすと，環境はフィードバックとして **報酬** ($r\in\mathcal{R}$) を与える。
<!-- The agent is acting in an **environment**. How the environment reacts to certain actions is defined by a **model** which we may or may not know.
The agent can stay in one of many **states** ($s \in \mathcal{S}$) of the environment, and choose to take one of many **actions** ($a \in \mathcal{A}$) to switch from one state to another.
Which state the agent will arrive in is decided by transition probabilities between states ($P$).
Once an action is taken, the environment delivers a **reward** ($r \in \mathcal{R}$) as feedback.-->

* モデルは報酬関数と遷移確率を定義する。
* モデルがどのように動作するかを知っている場合と知らない場合があり，これにより 2 つの状況が区別される。
<!-- The model defines the reward function and transition probabilities.
We may or may not know how the model works and this differentiate two circumstances: -->

    * **モデルベース**
<!-- * **モデルを知る**：完全な情報で計画を立てる、モデルベースの RL を行う。-->
        環境が完全にわかっている場合 [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP) によって最適解を求めることができます。
<!-- アルゴリズム 101 の授業で習った "longest increasing partialence" や "traveling salesmen problem" をまだ覚えていますか？笑
これはこの記事の焦点ではありませんが。-->

    * **モデルフリー**
<!-- * **モデルを知らない**：不完全な情報での学習；モデルフリー RL を行うか，-->
アルゴリズムの一部として明示的にモデルを学習しようとする。
<!-- 以下の内容のほとんどは、モデルがわからない場合のシナリオに対応しています。 -->

<!-- The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:
- **Know the model**: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP).
Do you still remember "longest increasing subsequence" or "traveling salesmen problem" from your Algorithms 101 class? LOL.
This is not the focus of this post though.
- **Does not know the model**: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm.
Most of the following content serves the scenarios when the model is unknown.-->

* 動作主 (エージェント)の **方策 (ポリシー)**  $\pi(s)$ は，**総報酬** $G$ を最大化することを目的として，ある状態で取るべき最適な行動のガイドラインを提供する。
* 各状態には，その状態で対応するポリシーを実行することで得られる将来の報酬の期待値を予測する **価値** 関数 $V(s)$ が関連付けられる。
* 言い換えれば，価値関数は，ある状態がどれだけ良いかを定量化する。
* 強化学習で学習しようとするのは，方針関数 と 価値関数の両方。
<!-- The agent's **policy** $$\pi(s)$$ provides the guideline on what is the optimal action to take in a certain state with **the goal to maximize the total rewards**.
Each state is associated with a **value** function $$V(s)$$ predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy.
In other words, the value function quantifies how good a state is.
Both policy and value functions are what we try to learn in reinforcement learning.
-->

* 動作主 (エージェント) と環境の相互作用には $t=1, 2, \dots,T$ 時間内の一連の行動と観測された報酬が含まれる。
* この過程で， エージェントは環境に関する知識を蓄積し，最適な政策を学習し，最適な政策を効率的に学習するために，次にどのような行動を取るべきかを決定する。
* 時間ステップ $t$ における状態，行動，報酬をそれぞれ $S_t$, $A_t$, $R_t$ とする。
<!-- このように， インタラクションシーケンスは 1 つの **エピソード** (「試行」または「軌跡」とも呼ばれる) で完全に記述され， 系列は末端の状態 $S_T$ で終了します。 -->
<!-- The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, $t=1, 2, \dots, T$.
During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy.
Let's label the state, action, and reward at time step t as $S_t$, $A_t$, and $R_t$, respectively.
Thus the interaction sequence is fully described by one **episode** (also known as "trial" or "trajectory") and the sequence ends at the terminal state $S_T$:-->

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$

<!-- RL アルゴリズムの様々なカテゴリーを調べる際によく遭遇する用語。 -->
<!-- Terms you will encounter a lot when diving into different categories of RL algorithms: -->

- **モデルベース**: モデルが既知であるか、アルゴリズムがそれを明示的に学習する。
- **モデルフリー**: 学習時にモデルに依存しない。
- **オンポリシー**<!-- **On-policy** -->: アルゴリズムの学習にターゲットポリシーからの決定論的な結果やサンプルを使用する。
- **オフポリシー**<!-- **Off-policy** -->: ターゲット・方策(ポリシー)ではなく，異なる<!-- ビヘイビア・ -->方策(ポリシー)で生成された上他繊維やエピソード分布で学習する。

<!--
- **Model-based**: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
- **Model-free**: No dependency on the model during learning.
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.-->

## モデル: 移行と報酬 transition and reward
<!-- #### Model: Transition and Reward -->

* モデルとは，環境を記述する実体。
* モデルを用いることで，環境がどのように動作主 (エージェント) と相互作用し，フィードバックを与えるかを学習または推論することができる。
* モデルには，遷移確率関数 $P$ と報酬関数 $R$ の 2 つの主要部分がある。
<!-- The model is a descriptor of the environment.
With the model, we can learn or infer how the environment would interact with and provide feedback to the agent.
The model has two major parts, transition probability function $$P$$ and reward function $R$. -->

* 状態 $s$ にいるとき，次の状態 $s’$ に到達して報酬 $r$ を得るために行動 $a$ をとることを決めたとする。
これは 1 つの **遷移** ステップと呼ばれ， タプル $(s, a, s', r)$ で表される。
<!-- Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r.
This is known as one **transition** step, represented by a tuple (s, a, s', r).-->

* 遷移関数 $P$ は，行動 $a$ を起こした後に報酬 $r$ を得て，状態 $s$ から $s’$ に遷移する確率を記録したものである。
ここでは $\mathbb{P}$ を「確率」の記号として用いる。
<!-- The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r.
We use $$\mathbb{P}$$ as a symbol of "probability".-->

$$
P(s', r \vert s, a)  = \mathbb{P} \left[S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a\right]
$$

したがって，状態遷移関数は $P(s', r \vert s, a)$ の関数として定義することができる。
<!-- Thus the state-transition function can be defined as a function of $P(s', r \vert s, a)$: -->

$$
P_{ss'}^a = P(s' \vert s, a) = \mathbb{P} \left[S_{t+1} = s' \vert S_t = s, A_t = a\right] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

報酬関数 $R$ は 1 つの行動 (アクション) によって引き起こされる次の報酬を予測する。
<!-- The reward function R predicts the next reward triggered by one action:-->

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$

## 方針 (ポリシー) policy

* 方針 (ポリシー) とは，動作主 (エージェント) の行動関数 $\pi$ として，状態 $s$ においてどのような行動を取るべきかを示すもの。
これは， 状態 $s$ から行動 $a$ への写像であり，決定論的なものと確率論的なものがある。

    * 決定論的: $\pi(s) = a$.
    * 確率論的: $\pi(a \vert s) = \mathbb{P}_ {\pi} \left[A=a \vert S=s\right]$.

## 価値関数 value function

* 価値関数は，将来の報酬の予測によって，状態の良さや，状態や行動がどれだけ報われるかを測定するもの。
* 将来の報酬は **リターン** とも呼ばれ，今後の報酬を割引いたものの総和。
* 時刻 $t$ から始まるリターン $G_t$ を計算すると以下のようになる:

<!--Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward.
The future reward, also known as **return**, is a total sum of discounted rewards going forward.
Let's compute the return $G_t$ starting from time t:-->

$$
G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

$[0,1]$ の割引係数 $\gamma$  は，将来の報酬に罰則 (ペナルティ) を課すことになる。
<!-- The discounting factor $\gamma \in [0, 1]$ penalize the rewards in the future, because:-->

* 将来の報酬は不確実性が高い可能性がある。
例：株式市場。
* 将来の報酬はすぐに得られるものではない。
例：人間として 5 年後よりも今日楽しいことをしたいと思うかもしれない。
* 割引 (ディスカウント) は数学的な利便性をもたらす。
つまり，リターンを計算するために将来のステップを永遠に追跡する必要がないのです。
* 状態遷移グラフの無限ループを心配する必要はない

<!-- - The future rewards may have higher uncertainty; i.e. stock market.
- The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;).
- Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return.
- We don't need to worry about the infinite loops in the state transition graph.-->

* ある状態 $s$ の **状態値** は， 時間 $t$ でこの状態にある場合の期待リターンであり $S_{t}=s$ である。
<!-- The **state-value** of a state s is the expected return if we are in this state at time t, $S_t = s$: -->

$$
V_{\pi}(s) = \mathbb{E}_ {\pi}[G_t \vert S_t = s]
$$

* 同様に，状態と行動の対としての **行動価値**  $Q$ を以下のように定義する:
<!-- Similarly, we define the **action-value** ("Q-value"; Q as "Quality" I believe?) of a state-action pair as:-->

$$
Q_{\pi}(s, a) = \mathbb{E}_ {\pi}[G_t \vert S_t = s, A_t = a]
$$

* 目標方策 $pi$ に従うので，可能な行動に対する確率分布と Q 値を利用して，状態値を回復することができる。
<!-- Additionally, since we follow the target policy $\pi$, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value: -->

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

行動価値と状態価値の際は，**アドバンテージ (advantage)** と呼ばれる
<!-- The difference between action-value and state-value is the action **advantage** function ("A-value"): -->

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$

### 最適価値と最適方針

* 最適価値関数は，最大リターンを産む
<!-- The optimal value function produces the maximum return: -->

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

* 最適方策は，最適価値関数によって達成される:
<!-- The optimal policy achieves optimal value functions: -->

$$
\pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

* $V_{\pi_{\star}}(s)=V_{\star}(s)$ かつ $Q_{\pi_{\star}}(s,a)=Q_{\star}(s,a)$ である
<!-- And of course, we have $V_{\pi_{\star}}(s)=V_{\star}(s)$ and $Q_{\pi_{\star}}(s, a) = Q_{\star}(s, a)$. -->
