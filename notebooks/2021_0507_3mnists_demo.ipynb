{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習実習用ファイル\n",
    "\n",
    "* [線形回帰](#linreg)\n",
    "* [主成分分析](#pca)\n",
    "* [tSNE](#tSNE)\n",
    "* [ロジスティック回帰](#logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qOuXszUyp3ya"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "mnist_urls = {\n",
    "    #http://yann.lecun.com/exdb/mnist/\n",
    "    'Xtrain': 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "    'Ytrain': 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "    'Xtest': 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "    'Ytest':'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "fashionmnist_urls = {\n",
    "    #https://github.com/zalandoresearch/fashion-mnist\n",
    "    'Xtest': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "    'Ytest': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
    "    'Xtrain': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "    'Ytrain': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "kmnist_urls = {\n",
    "    #http://codh.rois.ac.jp/kmnist/\n",
    "    'Xtrain': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
    "    'Ytrain': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
    "    'Xtest': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
    "    'Ytest': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    " \n",
    "\n",
    "def download_mnist(dataset):\n",
    "    #上で定義したデータセットの情報を元にデータをダウンロードする\n",
    "    for name, url in dataset.items():\n",
    "        fname = url.split('/')[-1]\n",
    "        print(url, fname)\n",
    "        r = requests.get(url, timeout=35) #timeout=None はサーバからの応答が遅い場合永遠に待ち続ける\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q652FbHrYSFj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: *.gz*\n",
      "http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz train-images-idx3-ubyte.gz\n",
      "http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz train-labels-idx1-ubyte.gz\n",
      "http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz t10k-images-idx3-ubyte.gz\n",
      "http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#データを変更して，繰り返し実行する際には，次行行頭の # を削除して，上のセルを再実行する必要があります\n",
    "!rm *.gz\n",
    "!ls -l *.gz*\n",
    "\n",
    "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "fashionmnist_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat' , \\\n",
    "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "kmnist_labels = ['お', 'き', 'す', 'つ', 'な', 'は', 'ま', 'や', 'れ', 'を']\n",
    "# '0,U+304A,お', '1,U+304D,き', '2,U+3059,す', '3,U+3064,つ', '4,U+306A,な', \n",
    "# '5,U+306F,は', '6,U+307E,ま', '7,U+3084,や', '8,U+308C,れ', '9,U+3092,を'\n",
    "\n",
    "labels = mnist_labels\n",
    "labels = fashionmnist_labels\n",
    "labels = kmnist_labels\n",
    "\n",
    "#以下の 3 つのデータセットのうち 1 つを選んで実習してみましょう\n",
    "dataset = mnist_urls\n",
    "#dataset = fashionmnist_urls\n",
    "dataset = kmnist_urls\n",
    "\n",
    "labels = mnist_labels\n",
    "#labels = fashionmnist_labels\n",
    "labels = kmnist_labels\n",
    "\n",
    "download_mnist(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 asakawa staff     5120 May  9 00:16 t10k-labels-idx1-ubyte.gz\n",
      "-rw-r--r-- 1 asakawa staff  3041136 May  9 00:16 t10k-images-idx3-ubyte.gz\n",
      "-rw-r--r-- 1 asakawa staff    29497 May  9 00:16 train-labels-idx1-ubyte.gz\n",
      "-rw-r--r-- 1 asakawa staff 18165135 May  9 00:16 train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "!gls -lt *.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WxyjhUROVQis"
   },
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"ダウンロードしたデータを読み込む関数\"\"\"\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgAYom64_-dG"
   },
   "outputs": [],
   "source": [
    "!pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq5euU6kW4fE"
   },
   "outputs": [],
   "source": [
    "#データの表示\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "X_train, Y_train = load_mnist('.', kind='train')\n",
    "X_test, Y_test = load_mnist('.', kind='t10k')\n",
    "\n",
    "#次行の数字を変更して実施してください。ただし数字の範囲は 0 から 59999 までです\n",
    "No = 5001\n",
    "plt.figure(figsize=(2,2))    #表示する縦横の大きさ，単位はインチ\n",
    "plt.title('label:{}'.format(labels[Y_train[No]]))\n",
    "plt.axis(False)\n",
    "plt.imshow(X_train[No].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a name=\"linreg\">線形回帰</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.zeros_like(X_train)\n",
    "X = X_train/255\n",
    "X_t = np.zeros_like(X_test)\n",
    "X_t = X_test/255\n",
    "\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5vlNoT5hbf9"
   },
   "outputs": [],
   "source": [
    "#ここから先は単純な重回帰の実行です\n",
    "import numpy as np\n",
    "\n",
    "#簡単のためデータ数を最初の N 個に制限します。\n",
    "N = 12000\n",
    "x = X_train[:N]\n",
    "y = Y_train[:N]\n",
    "x = X[:N]\n",
    "\n",
    "y_ = np.zeros((N,10))\n",
    "y_[np.arange(len(y)),y] = 1             # 教師データ\n",
    "bias = np.ones((x.shape[0],1))          # 線形回帰 y = w x + b の b すなわちバイアス項の定義\n",
    "X1 = np.concatenate((x, bias), axis=1)  # x と b とを連接して一つの行列にする\n",
    "XtX = np.dot(X1.T, X1)                  # $X^t X$\n",
    "XtX_inv = np.linalg.inv(XtX)            # 逆行列 $(X^t X)^{-1}$ の計算\n",
    "w = np.dot(np.dot(XtX_inv, X1.T), y_)\n",
    "print(f'定義した重み係数行列のサイズ: {w.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O1fWk02llcn"
   },
   "outputs": [],
   "source": [
    "y_ = np.zeros((y.shape[0],10))  #教師データの作成の準備\n",
    "y_[np.arange(len(y)),y] = 1     #教師データ\n",
    "\n",
    "#y_hat = np.dot(w, y_)          #線形回帰の実施\n",
    "y_hat = np.dot(X1, w)           #線形回帰の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaHACk1OpSIY"
   },
   "outputs": [],
   "source": [
    "#回帰分析の結果，精度の印字\n",
    "teach = np.argmax(y_,axis=1)    #教師信号を teach とする。この処理は冗長です\n",
    "pred  = np.argmax(y_hat,axis=1) #予測値を pred とする\n",
    "print(f'訓練データの精度: {((teach == pred) * 1).sum() / N * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "td81nz6myoyF"
   },
   "outputs": [],
   "source": [
    "#検証データによる評価\n",
    "\n",
    "y = Y_test\n",
    "y_ = np.zeros((y.shape[0],10))  #教師データの作成の準備\n",
    "y_[np.arange(len(y_)),Y_test] = 1     #教師データ\n",
    "\n",
    "x = X_test\n",
    "bias = np.ones((x.shape[0],1))          # 線形回帰 y = w x + b の b すなわちバイアス項の定義\n",
    "X1 = np.concatenate((x, bias), axis=1)  # x と b とを連接して一つの行列にする\n",
    "\n",
    "#print(w.shape)\n",
    "#sys.exit() gb \n",
    "y_hat = np.dot(X1, w)\n",
    "teach = np.argmax(y_, axis=1)\n",
    "pred = np.argmax(y_hat, axis=1)\n",
    "print(f'テストデータの精度: {((teach == pred) * 1).sum() / len(Y_test) * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a name='pca'>主成分分析</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-psINn_oauWG"
   },
   "outputs": [],
   "source": [
    "#視覚化のためのライブラリを読み込む\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIyxHEKTaxS1"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "N = 500\n",
    "x = X_train[:N]\n",
    "y = Y_train[:N]\n",
    "\n",
    "label = [labels[y[i]] for i in range(len(y))]\n",
    "pca_results = pca.fit_transform(x)\n",
    "pca1, pca2 = pca_results[:,0], pca_results[:,1] \n",
    "fig, ax = plt.subplots(figsize=(13,14))  \n",
    "ax.scatter(pca1, pca2, s=20, color='cyan')\n",
    "for i, l in enumerate(label):\n",
    "    ax.annotate(l, (pca1[i], pca2[i]), fontsize=12)\n",
    "ax.set_xlabel('第一主成分')\n",
    "ax.set_ylabel('第二主成分')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a name=\"tSNE\">tSNE</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaRuXOnPKXVu"
   },
   "outputs": [],
   "source": [
    "tsne_results = TSNE(n_components=2).fit_transform(x)\n",
    "tsne1, tsne2 = tsne_results[:,0], tsne_results[:,1]\n",
    "label = [labels[y[i]] for i in range(len(y))]\n",
    "fig, ax = plt.subplots(figsize=(13,14))\n",
    "ax.scatter(tsne1, tsne2, s=20, color='cyan')\n",
    "for i, l in enumerate(label):\n",
    "    ax.annotate(l, (tsne1[i], tsne2[i]),fontsize=12)\n",
    "ax.set_xlabel('tSNE 1')\n",
    "ax.set_ylabel('tSNE 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"logistic\">4. ロジスティック回帰</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TV-ZDplfyoyG"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "N=10000\n",
    "X = X_train[:N]\n",
    "Y = np.copy(Y_train[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keOd7dHNyoyG"
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5, verbose=10, max_iter=1e+3)\n",
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JH3wsnoyoyG"
   },
   "outputs": [],
   "source": [
    "pred = logreg.predict(X_test)\n",
    "print(f'テストデータの精度: {((pred == Y_test) * 1).sum() / pred.shape[0] * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMhQTmiZyoyG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "SVC = LinearSVC()\n",
    "SVC.fit(X,Y)\n",
    "pred = SVC.predict(X_test)\n",
    "print(f'テストデータの精度: {((teach == pred) * 1).sum() / N * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここから先は単純なロジスティック回帰\n",
    "\n",
    "# ハイパーパラメータの定義\n",
    "N = 2000                  #簡単のためデータ数を最初の N 個に制限します。\n",
    "lr = 1e-3                 #学習率の設定\n",
    "max_iter = 10 ** 3        #最大学習回数 \n",
    "interval = max_iter >> 2  #途中経過の表示間隔\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"シグモイド関数\"\"\"\n",
    "    ret = np.array([np.exp(xi - xi.mean())/np.exp(xi - xi.mean()).sum() for xi in x])\n",
    "    return ret\n",
    "\n",
    "def Dsigmoid(x):\n",
    "    \"\"\"シグモイド関数の微分\"\"\"\n",
    "    return np.array([[xij * (1-xij) for xij in xi] for xi in x])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"ソフトマックス関数\"\"\"\n",
    "    mx = [xx - xx.mean() for xx in x]\n",
    "    return [np.exp(xx) / np.exp(xx).sum() for xx in mx]\n",
    "\n",
    "def Dsoftmax(x):\n",
    "    \"\"\"ソフトマックス関数の微分\"\"\"\n",
    "    return x\n",
    "\n",
    "X = X_train / X_train.max()\n",
    "x = np.copy(X[:N])\n",
    "y = Y_train[:N]\n",
    "y_ = np.zeros((len(y),10), dtype=np.float)\n",
    "y_[np.arange(len(y)),y] = 1\n",
    "one = np.ones((x.shape[0],1))\n",
    "x1 = np.concatenate((x, one),axis=1)\n",
    "\n",
    "#学習すべきパラメータの初期化\n",
    "w = (np.random.random((x.shape[1]+1, 10)) - 0.5) / np.sqrt(x.shape[1])\n",
    "\n",
    "for t in range(max_iter):\n",
    "    y_hat = x1 @ w\n",
    "\n",
    "    #p = sigmoid(y_hat)\n",
    "    p = softmax(y_hat)\n",
    "\n",
    "    #delta = (y_ - p) * Dsigmoid(p)\n",
    "    delta = (y_ - p) * Dsoftmax(p)\n",
    "    \n",
    "    dw = np.dot(x1.T, delta)\n",
    "    w += lr * dw\n",
    "\n",
    "    if t % interval == 0:\n",
    "        print(f't:{t:>4d} ', end='')\n",
    "        print(f'訓練データの精度: {((y == np.argmax(p, axis=1)) * 1).sum() / N * 100:.3f} %')\n",
    "\n",
    "print(f't:{t+1:>4d} ', end='')\n",
    "print(f'訓練データの精度: {((y == np.argmax(p, axis=1)) * 1).sum() / N * 100:.3f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = np.ones((X_test.shape[0],1))\n",
    "x1 = np.concatenate((X_test, one), axis=1)\n",
    "y_ = x1 @ w\n",
    "#pred = sigmoid(y_)\n",
    "pred = softmax(y_)\n",
    "print(f'テストデータの精度: {((Y_test == np.argmax(pred, axis=1)) * 1).sum() / len(Y_test) * 100:.3f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EFwp2ntqZc2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hZMrBkkqGIm"
   },
   "outputs": [],
   "source": [
    "!ls -lt /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 版ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "lr_rate = 0.001\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print(\"Iteration: {:5d} Loss:{}. Accuracy: {:.3f}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2021_0507_3mnists_demo.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
