{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2020-0515komazawa_step-by-step-CNN-Pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pys6dGWIpGGt",
        "colab_type": "text"
      },
      "source": [
        "# ステップ・バイ・ステップで画像認識の基礎\n",
        "\n",
        "<font color=\"teal\" size=\"+3\"><strong>PyTorch 編</strong></font>\n",
        "\n",
        "- Date: 2020-0320\n",
        "- Author: Shin Asakawa <asakawa@ieee.org>\n",
        "- Filename: 2020-0515komazawa_step-by-step-CNN-PyTorch.ipynb\n",
        "<!--- note: Original がどこかにあるはずなのだが，忘れた。見つからない 2020-0513-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qn8Mq1lpGGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4ZZoJ-JpGGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bc771d5-513c-4011-a9c8-22a26330afb8"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__, torchvision.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101 0.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJfEEwVGpGGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help(torchvision.models._utils)\n",
        "#help(torchvision.models.alexnet)\n",
        "alex = torchvision.models.alexnet(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dO-WbSMpGG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display\n",
        "filename=\"https://miro.medium.com/max/2812/1*bD_DMBtKwveuzIkQTwjKQQ.png\"\n",
        "IPython.display.Image(url=filename)\n",
        "# from https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr43vwuCpGG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filename=\"https://miro.medium.com/max/3684/1*vXBvV_Unz3JAxytc5iSeoQ.png\"\n",
        "#IPython.display.Image(url=filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNr4ODTBpGG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help(alex)\n",
        "print(alex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFUSF4ULpGG8",
        "colab_type": "text"
      },
      "source": [
        "- 下から上に昇って見ていくと Linear は通常の全結合数であるので in_features=4096 out_features=100 だから，4096 ニューロンを 1000 ニューロンに変換している\n",
        "- その上は 4096 から 4096 に変換\n",
        "- 次の (avgpool) のところで出力サイズが (6,6) になっているので penultimate 層では x=6, y=6, チャンネル数（特徴数）= 256 である。\n",
        "したがって $256\\times 6 \\times 6 = 9126$ となる。ただし原著論文では平均プーリングではなく最大値プーリングである。\n",
        "\n",
        "np.sqrt(9216 / 256)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2idv8UvpGG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchsummary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ljmBY3pGHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torchsummary.summary(alex,input_size=(3,224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpXjwRGipT6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-8fDuVvpGHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchviz\n",
        "\n",
        "#help(torchviz.make_dot)\n",
        "x = torch.rand([1,3,255,255])\n",
        "y = alex.forward(x)\n",
        "torchviz.make_dot(y.mean(), params=dict(alex.named_parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiic1IyRpGHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1mr3jKfpGHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "gpu = False\n",
        "alex.cpu()\n",
        "torch.no_grad()\n",
        "alex.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOGPOrgjpGHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help(torch.rand)\n",
        "#help(torch.random)\n",
        "#alex.eval()\n",
        "#alex.parameters\n",
        "#alex.modules\n",
        "#print(dir(alex))\n",
        "#help(alex.eval())  # alex.eval() でモードをセットする\n",
        "#help(alex.forward(torch.rand([1,3,255,255])))\n",
        "out = alex.forward(torch.rand([1,3,255,255]))  \n",
        "# ということなので，[チャンネル数，x, y] という tensor を nn.Module.forward() の引数として渡せば良い\n",
        "print(type(out), out.size())\n",
        "print(np.argmax(out.detach().numpy()))\n",
        "#torch.rand([1,3,255,255])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgYEwgmJpdJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVSWpMBopGHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls *.jpg *.png\n",
        "from PIL import Image\n",
        "img = Image.open('cat.jpg')\n",
        "print(img)\n",
        "np.asarray(img).shape  # (534,600,3) のように，(y, x, チャンネル) となっている。\n",
        "print(torch.from_numpy(np.asarray(img)).size())\n",
        "torch.from_numpy(np.asarray(img)).view(-1,862,1298,3).transpose(1,3).size()\n",
        "# troch.view() や np.reshape() では転置できないのかしら？という疑問が湧く\n",
        "# 答え：torch.view() は見かけだけを変更するので不可。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRO68KZUpGHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pytorch の tensor における view と size の使い方\n",
        "#a = torch.range(1, 16)\n",
        "a = torch.arange(1,17)\n",
        "print(a)\n",
        "print(a.view(-1,4,4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SADK7rRRpGHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# すなわち PyTorch の tensor における view は numpy における reshape と同等\n",
        "np.random.rand(16).reshape(-1,4,4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA90ozBlpGHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help(a.view)\n",
        "x = torch.randn(4, 4)  # 乱数行列の生成\n",
        "x.size() # torch.Size([4, 4])  size() 関数は行列の次数を返す関数\n",
        "y = x.view(16)  # view() 関数は numpy.reshape() 関数と同等\n",
        "y\n",
        "y.size()  # torch.Size([16])\n",
        "z = x.view(-1, 8)  # -1 は与えられたデータから類推せよという意味\n",
        "z\n",
        "z.size()  # torch.Size([2,8])  \n",
        "\n",
        "a = torch.randn(1,2,3,4)  # 1 行，2 列, 3 カラム, 4 カラム＋1次 の乱数で初期化された tensor \n",
        "#a\n",
        "a.size()  # torch.Size([1,2,3,4])\n",
        "b = a.transpose(1,2)  # 2 番目と 3 番目を転置 transpose\n",
        "#b\n",
        "b.size()  # torch.Size([1,3,2,4])\n",
        "c = a.view(1,3,2,4)  # c は見え方を変えるだけで内部は変化しない\n",
        "print(b); print(c)\n",
        "torch.equal(b,c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpcRZkA-pGHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls *.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI8DpaHGpGHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jpg_file = 'cat.jpg'\n",
        "#IPython.display.Image(jpg_file)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.axis(False)\n",
        "plt.imshow(plt.imread(jpg_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAGI8gflpGHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = Image.open(jpg_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dyV8MpGHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvK_53YppGHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseTransform():\n",
        "    \"\"\"\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    sizes : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sizes, mean, std):\n",
        "        self.base_transform = transforms.Compose([\n",
        "            transforms.Resize(sizes),  # 短い辺の長さがresizeの大きさになる\n",
        "            transforms.CenterCrop(sizes),  # 画像中央をresize × resizeで切り取り\n",
        "            transforms.ToTensor(),  # Torchテンソルに変換\n",
        "            transforms.Normalize(mean, std)  # 色情報の標準化\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.base_transform(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyIKdfa-pGHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. 画像の前処理と処理済み画像の表示\n",
        "resize = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "transform = BaseTransform(resize, mean, std)\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvRx3P0pGHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPython.display.Image(img_transformed.numpy())\n",
        "type(img_transformed)\n",
        "img_transformed.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDtDl1CIpGHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = img_transformed.numpy().transpose((1,2,0))\n",
        "a = np.clip(a, 0, 1)\n",
        "plt.axis(False)\n",
        "plt.imshow(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2McY1MVmpGHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPython.display.display(PIL.Image.fromarray(a.transpose(2,0,1)))\n",
        "#IPython.display.display(a.transpose((2,0,1)))\n",
        "#from ipython_display import imshow\n",
        "#imshow(a)\n",
        "#a.transpose((2,0,1)).shape\n",
        "#a.shape\n",
        "#IPython.display.display(IPython.display.Image(data=a))  # ValueError: ndarray is not C-contiguous\n",
        "#IPython.display.display(a)  # nG\n",
        "#IPython.display.display(PIL.Image.fromarray(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQPs7680pGHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.axis(False)\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMh3JrCNpGHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 前処理の後、バッチサイズの次元を追加する\n",
        "transform = BaseTransform(resize, mean, std)  # 前処理クラス作成\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVYVF1ZgpGHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルに入力し、モデル出力をラベルに変換する\n",
        "out = alex(inputs)  # torch.Size([1, 1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8T2vIPIpGHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.detach().numpy().argmax()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5dPHGx7qowQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "!wget https://gist.githubusercontent.com/PonDad/4dcb4b242b9358e524b4ddecbee385e9/raw/dda9454f74aa4fafee991ca8b848c9ab6ae0e732/imagenet_class_index.json\n",
        "\n",
        "# ImageNet のラベル一覧の読み込み\n",
        "with open('imagenet_class_index.json') as f:\n",
        "    data = json.load(f)\n",
        "    class_names = np.array([row['ja'] for row in data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9oy_10ppGHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1f9f39d-f6ce-433f-9221-c4d9de943376"
      },
      "source": [
        "# ILSVRCのラベル情報をロードし辞意書型変数を生成します\n",
        "import json\n",
        "ILSVRC_class_index = json.load(open('imagenet_class_index.json', 'r'))\n",
        "#print(ILSVRC_class_index)\n",
        "len(ILSVRC_class_index)\n",
        "ILSVRC_class_index[285]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Egyptian_cat', 'ja': 'エジプトの猫', 'num': 'n02124075'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH91hIyqpGHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img = np.random.random((224,224,3))\n",
        "#test_img = PIL.Image.fromarray(test_img)\n",
        "#plt.imshow(test_img)\n",
        "#img_transformed2 = transform(test_img)\n",
        "#inputs = img_transformed.unsqueeze_(0)\n",
        "inputs = torch.from_numpy(test_img).unsqueeze_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X84mZFbGpGHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inputs.size()\n",
        "#out = alex.forward(inputs)\n",
        "a = torch.rand([1,3,224,224])\n",
        "out = alex.forward(a)\n",
        "#alex.forward(torch.from_numpy(np.random.random((1,224,224,3))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErcHiWR-pGHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ILSVRC_class_index[out.detach().numpy().argmax()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPrDo1-SpGHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://blog.counselor.or.jp/wp-content/uploads/2019/06/image001-20.jpg\n",
        "!mv image001-20.jpg cat.jpg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqlckl5fpGHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jpg_file = 'cat.jpg'\n",
        "#IPython.display.Image(jpg_file)\n",
        "img = Image.open(jpg_file)\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
        "out = alex(inputs)  # torch.Size([1, 1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP37kfzMpGH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ILSVRC_class_index[out.detach().numpy().argmax()]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}