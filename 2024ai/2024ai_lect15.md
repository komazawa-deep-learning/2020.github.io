---
title: 第15回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>


$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/>
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 12/Jul/2024<br/>
Appache 2.0 license<br/>
</div>

# 連絡事項

* 病気，インターン，公式行事への出席，卒論実験などでやむを得ない事情で欠席する場合には，事前に連絡してください。連絡があれば考慮します。
  * 欠席に対する救済措置を希望する場合は，事前に連絡し，後日欠席が証明できる資料を提示して，相談してください。
* 交通機関の遅延などによる遅刻，欠席の場合には，当日で状況を連絡してください。
* [課題提出用 GDrive](https://drive.google.com/drive/folders/1raA3F2KUOiOKWO1XxeKgnrVysJKLhXoS?usp=drive_link)

<div class="figcenter">
<img src="/2024assets/2024Novel_Research_Ideas_fig1_2_.svg" style="width:77%">
<div class="figcaption" style="width:66%">

研究のアイデア出し。LLM によるアイデアと専門家のアイデアを自然言語処理分野の 7 つの話題について比較。
条件１は人間の専門家，条件２は LLM によるアイデア，条件３は LLM のアイデア出しを人間の専門家がランク付け。
結果を専門家が評価５項目に渡って評価した結果。赤アスタリスクは統計的に有意であることを示す。
下図縦軸は評定値 10 点満点。
[Chenglei Si+2024](https://arXiv.org/2409.04109)
</div></div>

<br/><br/><br/>

<div class="figcenter">
<img src="/2024assets/2022Wei_fig11.svg" style="width:77%">
<div class="figcaption">

課題毎のモデルの性能。
図の縦軸は精度。
横軸は，モデルの持つパラメータ数。M は 100 万，B は１億，B は 1000 億。
パラメータが多いと大規模言語モデルと呼ばれる。モデルの持つパラメータが多くなると，突然，性能が向上する点が存在する。
大規模言語モデル (LLM) の知能創発。[Wei+2022](https://arXiv.org/2206.07682/) Fig.11
</div></div>

<br/><br/><br/>

<div class="figcenter">
<img src="/2024assets/2024Superintelligence_fig1.jpg" style="width:77%">
<div class="figcaption" style="width:66%">

知能爆発のシナリオ。[SITUATIONAL AWARENESS](https://situational-awareness.ai/from-agi-to-superintelligence/) <br/>
自動化された AI 研究は，アルゴリズムの進歩を加速させ，1 年で 5OOM 以上の計算能力を向上させることができる。
知能爆発的時，AI システムは人間よりもはるかに賢いものになっているかも知れない。
<!-- Automated AI research could accelerate algorithmic progress, leading to 5+ OOMs of effective compute gains ina year.
The AI systems we’d have by the end of an intelligence explosion would be vastly smarter than humans. -->
</div></div>

# 復習

<img src="/assets/1986pdp_chap8_Fig1.svg" style="width:39%">

1. [ディープラーニング概説, 2015, LeCun, Bengio, Hinton, Nature](https://komazawa-deep-learning.github.io/2021/2015LeCun_Bengio_Hinton_NatureDeepReview.pdf){:target="_blank"}
1. [ゴール駆動型深層学習モデルを用いた感覚皮質の理解 Yamins(2016) Nature](https://project-ccap.github.io/2016YaminsDiCarlo_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex.pdf){:target="_blank"}
1. [ディープラーニングレビュー Storrs ら, 2019, Neural Network Models and Deep Learning, 2019](https://komazawa-deep-learning.github.io/2021/2019Storrs_Golan_Kriegeskorte_Neural_network_models_and_deep_learning.pdf){:target="_blank"}
1. [深層学習と脳の情報処理レビュー Kriegestorte, 2015, Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing](2015Kriegeskorte_Deep_Neural_Networks-A_New_Framework_for_Modeling_Biological_Vision_and_Brain_Information_Processing.pdf){:target="_blank"}
1. [生物の視覚と脳の情報処理をモデル化する新しい枠組み Kriegeskorte, Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing, 2015](https://project-ccap.github.io/2015Kriegeskorte_Deep_Neural_Networks-A_New_Framework_for_Modeling_Biological_Vision_and_Brain_Information_Processing.pdf){:target="_blank"}
1. [計算論的認知神経科学 Kriegeskorte and Douglas, 2018, Cognitive computational neuroscience](https://project-ccap.github.io/2018Kriegeskorte_Douglas_Cognitive_Computational_Neuroscience.pdf){:target="_blank"}
1. [視覚系の畳み込みニューラルネットワークモデル，過去現在未来 Lindsay, 2020, Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future](https://project-ccap.github.io/2020Lindsay_Convolutional_Neural_Networks_as_a_Model_of_the_Visual_System_Past_Present_and_Future.pdf){:target="_blank"}

### 神経細胞 (neuron)

脳は莫大な数($10^{10}$ 個以上ともいわれる)の神経単位(ニューロン neuron)から成り立っている。
このニューロンが脳の情報処理における基本単位である。
複数のニューロンが結合してニューラルネットワークが形成されている。

個々のニューロンは、単純な処理しか行なわないが、脳はこのニューロンが相互に結合された並列処理システムであると捕えることができる。

<!-- The human brain consists of a large number (more than a billion) of neural cells that process informations.
Each cell works like a simple processor and only the massive interaction between all cells and their parallel processing makes the brain's abilities possible.

Below you see a sketch of such a neural cell, called a neuron: -->

神経回路網を構成しているのはこのニューロン (神経細胞) であり，ニューロンは，細胞体，樹状突起，軸索，とよばれる部分からなる。
樹状突起はアンテナ(入力)，軸索は送電線 (出力) と考えれば分かりやすい。

ニューロンの内部と外部とでは $Na^+$ , $K^+$ イオンなどの働きにより電位差がある。
通常，内部電位は外部よりも低い。
外部を 0 としたときの内部の電位を膜電位という。
入力信号が無いときの膜電位を静止膜電位という (約 $-70 mV$ ぐらいである)。

情報は樹状突起から電気信号の形でニューロンに伝達され，すべての樹状突起からの電気信号が加え合わされる。
樹状突起からやってくる外部電気信号の影響で膜電位が一定の値 (しきい値 約 $-55mV$) を越えると約 1 msec の間膜電位が急激に高くなる。
このことをニューロンが興奮した(あるいは発火した)という。
ニューロンの興奮は、軸索をとおって別のニューロンに伝達される。

この電気信号が一定の値を超えると(しきい値)そのニューロンが発火する。

ニューロンは膜電位が $-55mV$ より高くなれば興奮し，そうでなければ興奮しない。
この意味で $-55mV$ 付近を閾値という。
また，一旦興奮したニューロンはしばらくは興奮することができない。
これを不応期という。

<!--% 電位変化に現われる興奮(パルス)，不応期などの概念を図\ref{GENESIS-Neuron} によって確認されたい。-->
<img src="/2024assets/hh.svg">

ニューロンの興奮(1 msec だけなのでパルス pulse と呼ぶことがある)は軸索をとおって他のニューロンに伝達される。
軸索を通る興奮の伝達速度は 100 m/s くらいである。
たとえば，文字を見て音声を発声するまでの応答時間は，たかだか，1 秒程度で、ニューロンの応答時間を 10 数ミリ秒とすると多めに見積もっても 100 程度のニューロンしか通過していないことになる。
このことは「100 step のプログラムの制約」と呼ばれる。

### 神経細胞の結合様式

ニューロンからニューロンへ情報が伝達される部分をシナプス (synapse) と呼ぶ。
シナプスに興奮が到達するたびに送り手側 (シナプス前ニューロン) のニューロンからある種の化学物質が放出される。
この化学物質は受け手側 (シナプス後ニューロン) の膜電位をわずかに変化させる。

化学物質の種類によって，膜電位を高めるように作用する場合 (興奮性のシナプス結合) と逆に低めるように作用する場合 (抑制性のシナプス結合) とがある。
この他のシナプス結合としては，別の興奮性のシナプス結合の伝達効率を抑制するように働くシナプス結合 (シナプス前抑制) が存在することが知られている。
多くの研究者の努力にもかかわらず，今のところシナプス結合の種類はこの 3 種類しか発見されていない。
我々は上記 3 種類のシナプスだけを考えればよさそうである。

送り手のシナプスの興奮が興奮的に働くか抑制的に働くかは，送り手の側の細胞の種類によって異なることが知られている (Dale の法則)。

一つのニューロンには多いもので数万個のシナプス結合が存在する。
多数の軸索にシナプス結合を通して興奮(あるいは抑制)が伝えられると細胞体を伝わる途中で重なり合う。
すべての膜電位の変化の総和によってニューロンの膜電位の変化が決定される。
すべてのシナプス結合の和のことを空間加算という。
あるシナプスによって膜電位が変化し，その変化が減衰する前に次の興奮が伝達されれば，まだ残っている直前の電位変化に加え合わされて膜電位の変化が起きる。
このことを時間加算という。

シナプスに興奮が到達すると 0.3 msec 程度の時間遅れの後シナプス結合部の膜電位がわずかに変化する。
１つのシナプスが生成する膜電位の変化は 0.1 mV から 30 mV ぐらいのものまで様々なシナプス結合が存在する。

樹状突起を介したニューロン間の結合の強さは，しばしば変化することが知られている。
これを学習という。

### STDP

Spike Timing Dependent Plasticity（STDP）は，シナプス前後のニューロンのスパイクの時間的相関によって誘発される，時間的に非対称なヘブ型学習である。
他の形態のシナプス可塑性と同様に，脳における学習と情報記憶，および脳の発達過程における神経回路の発達と洗練の根底にある，と広く信じられている (Bi&Poo2001, Sjöström+)
STDP では，シナプス後活動電位の数ミリ秒前にシナプス前スパイクが繰り返し到達することで，多くのシナプスでシナプスの長期増強 (LTP) が起こり，シナプス後スパイクの後にスパイクが繰り返し到達することで，同じシナプスで長期抑圧 (LTD) が起こる。
シナプス前後の活動電位の相対的なタイミングの関数としてプロットされたシナプスの変化は STDP 関数または学習窓と呼ばれ，シナプスのタイプによって異なる。
スパイクの相対的なタイミングによる STDP 関数の急激な変化は，ミリ秒単位の時間スケールの時間コーディングスキームの可能性を示唆している。
<!-- Spike Timing Dependent Plasticity (STDP) is a temporally asymmetric form of Hebbian learning induced by tight temporal correlations between the spikes of pre- and post-synaptic neurons.
As with other forms of synaptic plasticity, it is widely believed that it underlies learning and information storage in the brain, as well as the development and refinement of neuronal circuits during brain development (e.g. Bi and Poo, 2001; Sjöström et al., 2008).
With STDP, repeated presynaptic spike arrival a few milliseconds before post-synaptic action potentials leads in many synapse types to Long-Term Potentiation (LTP) of the synapses, whereas repeated spike arrival after post-synaptic spikes leads to Long-Term Depression (LTD) of the same synapse.
The change of the synapse plotted as a function of the relative timing of pre- and post-synaptic action potentials is called the STDP function or learning window and varies between synapse types.
The rapid change of the STDP function with the relative timing of spikes suggests the possibility of temporal coding schemes on a millisecond time scale. -->

#### Basic STDP Model

シナプス前ニューロン j からのシナプス重みの変化 $\Delta w_{j}$ はシナプス前スパイクの到着とシナプ ス後スパイクの相対的なタイミングに依存する。
シナプス $j$ のシナプス前スパイク到着時間を $t_{fj}$ とし，$f=1,2,3,\ldots$ はシナプス前スパイクを数える。
同様に，$n=1,2,3,\ldots$ の $t_{ni}$ はシナプス後ニューロンの発火時間を示す。
シナプス前スパイクとシナプス後スパイクのペアによる刺激プロトコルによって誘発される総重み変化 $\Delta w_{j}$ は，次のようになる (Gerstner+1996, Kempter+1999)。
<!-- The weight change Δwj of a synapse from a presynaptic neuron j depends on the relative timing between presynaptic spike arrivals and postsynaptic spikes.
Let us name the presynaptic spike arrival times at synapse j by tfj where f=1,2,3,... counts the presynaptic spikes.
Similarly, tni with n=1,2,3,... labels the firing times of the postsynaptic neuron.
The total weight change Δwj induced by a stimulation protocol with pairs of pre- and postsynaptic spikes is then (Gerstner+1996, Kempter+1999)  -->

$$\tag{1}
\Delta w_{j}=\sum_{t=1}^{N}\sum_{n=1}^{N}w\left(t_{i}^{n}-t_{j}^{f}\right),
$$

where W(x) denotes one of the STDP functions (also called learning window) illustrated in Figure 1.
A popular choice for the STDP function W(x)

$$\begin{aligned}
W(x) &= A_{+}\exp(-x/\tau_{+}) & \text{ for $x>0$,}\\
W(x) &= A_{-}\exp(-x/\tau_{-}) & \text{ for $x<0$.}\\
\end{aligned}$$


### Hebbian rule

細胞 A の軸索が細胞 B を興奮させるのに十分なほど近くにあるか，繰り返し，あるいは一貫して細胞 B の発火に関与している場合，細胞 B を発火させる細胞の一つとしての A の効率が高まるように，細胞の一方または両方に何らかの成長や代謝の変化が起こる。<!-- When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased. -->

$$
I=C_{0}\frac{dV}{dt}+\bar{g}_{Na}m^{3}h(V-V_{Na})+\bar{g}_{K}n^{4}\left(V-V_{k}\right)
$$

$$\begin{aligned}
\frac{dm}{dt}&+\left\{\alpha_{m}(V)+\beta_{m}(V)\right\}m &=& \alpha_{m}(V),\\
\frac{dh}{dt}&+\left\{\alpha_{h}(V)+\beta_{h}(V)\right\}h &=& \alpha_{h}(V),\\
\frac{dn}{dt}&+\left\{\alpha_{n}(V)+\beta_{n}(V)\right\}h &=& \alpha_{n}(V),\\
\end{aligned}$$


#### 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='/assets//mcculloch.jpg' style="width:38%">
<img src='/assets//pitts.jpg' style='width:50%'><br>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br> -->
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_{i}=\phi\left(\sum_jw_{ij}x_j\right),\tag{eq:formal_neuron}
$$

ここで $y_{i}$ は $i$ 番目のニューロンの出力，$x_{j}$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$ と $j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br> -->
形式ニューロン
</center>

---

#### ローゼンブラット Rosenblatt のパーセプトロン

<center>
<img src='/assets/rosenblatt.jpg' style="width:49%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//rosenblatt.jpg' style="width:49%"><br> -->
フランク・ローゼンブラット
</center>

<!--
$$ \mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x} $$
-->

<center>
<img src='/assets/perceptron.png' style="width:74%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//perceptron.png' style="width:74%"></br> -->
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center>

<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br>
 -->
ニューロンの模式図 wikipedia より
</center>

<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->



# 生成 AI

* GAN
* Diffusion

## 心理学的注意

<img src="/assets/1988Treisman_Fig1.svg" width="24%">
<img src="/assets/1994Wolfe_GS2Fig2.jpg" width="39%">
<img src="/assets/1998IttiKoch_fig1.jpg" width="29%"><br/>

<p style="text-align: left;width:88%;background-color: cornsilk;">
左: Treisman1988 Fig.1，特徴統合理論の概念図。ボトムアップ注意
中: Wolfe1994 Fig.2 ガイド付き探索 バージョン 2 モデル。トップダウン注意
右: Itti and Koch (1998) Fig. 1 計算モデルの実装例
</p>


### 正則化

データ $y$ から $z$ を見つけ出す不良設定問題の正則化 $Az = y$ では，正則化項 $\left\|\cdot\right\|$ の選択と汎関数の安定化項 $\left\|Pz\right\|$ が必要となる。
標準正則化理論においては，$A$ は線形演算子，ノルムは 2 次，$P$ は線形である。
2 種類の方法が適用可能である。
すなわち

1. $\left\|Az-y\right\|\leqslant\epsilon$ を満たし，$\displaystyle\left\|Pz\right\|^2$ を最小化する $z$ を探す
2. $\displaystyle \left\|Az-y\right\|+\lambda\left\|Pz\right\|^2,$ を最小化する $z$ を探す
ここで $\lambda$ は正則化パラメータ。

最初の方法は，十分にデータを近似し，かつ，「基準」$\left\|Pz\right\|$ を最小化するという意味で「正則」な $z$ を探す方法である。
二番目の方法は，$\lambda$ が正則化の程度と解のデータへの近似とをコントロールする。
標準正則化理論は，最良の $\lambda$ を決定する手法を提供する。
標準正則化の手法は，上式に制約を導入することで変分原理の問題としている。
最小化するコストは物理的制約条件を満たす良い解を反映している。
すなわち，データへの近似もよく，かつ，正則化項 $\left\|Pz\right\|^2$ も小さいことを意味する。
$P$ は問題の物理的制約を表しており，2 次の変分原理であり，解空間内での唯一解が存在する。
標準正則化手法は，不良設定問題に対して注意深い分析が必要であることを注記しておく。
ノルム $\left\|\cdot\right\|$，正則化関数 $\left\|Pz\right\|$, および，汎関数空間の選択は数学的性質と，物理的説得性を有する必要がある。
これらにより，正しい正則化の詳細条件が定まる。

変分原理は物理学，経済学，工学，で幅広く用いられている。例えば物理学における基本法則は変分原理を用いて，
エネルギーやラグランジェ関数を用いて簡潔に表現されている。

## 標準正則化理論 (Poggio1985)


1. [計算論的視覚と正則化理論 Poggio, Torre, Koch, 1985](https://komazawa-deep-learning.github.io/2021cogpsy/1985Poggio_Computational_Vision_and_Regularization_Theory.pdf){:target="_blank"}
1. [皮質における物体認識の階層モデル Riesenhuber and Poggio (1999) Nature](https://komazawa-deep-learning.github.io/2021cogpsy/1999Riesenhuber_Poggio_Hierarchical_models_of_object_recognition_in_cortex.pdf){:target="_blank"}


## 力学的エネルギー = 運動エネルギー + 位置エネルギー(ポテンシャル)

$$
E = K + U\\
E = \frac{1}{2}mv^2 + mgh
$$

- 統計物理学: 巨視的な物体，すなわち莫大な数の個別的な粒子，原子や分子，からなる物体のふるまいやっ性質を支配している特別な型の法則性を研究する学問分野

- [熱力学第一法則 エネルギー保存則](https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC%E4%BF%9D%E5%AD%98%E3%81%AE%E6%B3%95%E5%89%87)
- [熱力学第二法則 エントロピーは増大する](https://ja.wikipedia.org/wiki/%E7%86%B1%E5%8A%9B%E5%AD%A6%E7%AC%AC%E4%BA%8C%E6%B3%95%E5%89%87)

### エントロピー
熱力学的エントロピーと情報論的エントロピーが存在するが式は同じである。
### (熱)力学的エントロピー

<!-- ### [自由エネルギー原理](friston_FEP)<br> -->

ヘルムホルツの自由エネルギー:
$$
F = U - TS
$$

$F$ はヘルムホルツの自由エネルギー，$T$ は温度，$S$ はエントロピーである。<https://kotobank.jp/word/%E8%87%AA%E7%94%B1%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC-76745>

- 熱力学の第一法則 エネルギー保存則
- 熱力学の第二法則

ギブスの自由エネルギー

$$
G = F + pV
$$

ある位置 $i$ にある粒子があるとする。各位置にそれぞれ $n_i$ の粒子が存在するとする。
はおのおの区別できないものとすれば，全ての状態は何通りあるかを表す式は次式となる:

$$
W=\frac{N!}{\prod_i n_i!}
$$

エントロピーとはこの状態の数 $W$ の負の対数である.
$$
H=\frac{1}{N}\log W=\frac{1}{N}\log N!-\frac{1}{N}\sum_i\log n_i!
$$

以下のスターリングの近似公式 ($\log N!\approx N\log N - N$) を用いると以下の式を得る

$$
H=-\lim_{N\rightarrow\infty}\sum_i\left(\frac{n_i!}{N}\right)
\log\left(\frac{n_i!}{N}\right)=-\sum_i p_i\log p_i
$$

$$
S = k \ln W
$$

ここで，$k$ は[ボルツマン定数](https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AB%E3%83%84%E3%83%9E%E3%83%B3%E5%AE%9A%E6%95%B0)であり，$W$ は系の微視的な状態を表す。
一方で統計力学におけるエントロピーの定義は以下の通り:

$$
S=k\left<\ln\frac{1}{p(\omega)}\right>=-k\sum_{\omega}p(\omega)\ln p(\omega)
$$

上式中 $\left<\;\right>$ は[アンサンブル平均](https://ja.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E9%9B%86%E5%9B%A3)と呼ばれ，巨視的に同条件下にある力学系が系を構成する分子間に相関がなければ，系は微視的にはすべての状態をとりうることから，巨視的状態において統計的に系はすべての状態をとりうることが仮定される。系の時間的平均と空間間的平均が同じであると仮定できるときその系は**エルゴード性**を有するという。
エルゴード性により時間平均と空間平均とを区別しないで(しばしば意図的に混乱させて)用いることが行われる。
