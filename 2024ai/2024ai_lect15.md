---
title: 第15回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>


$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/>
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 12/Jul/2024<br/>
Appache 2.0 license<br/>
</div>

# 連絡事項

* 病気，インターン，公式行事への出席，卒論実験などでやむを得ない事情で欠席する場合には，事前に連絡してください。連絡があれば考慮します。
  * 欠席に対する救済措置を希望する場合は，事前に連絡し，後日欠席が証明できる資料を提示して，相談してください。
* 交通機関の遅延などによる遅刻，欠席の場合には，当日で状況を連絡してください。
* [課題提出用 GDrive](https://drive.google.com/drive/folders/1raA3F2KUOiOKWO1XxeKgnrVysJKLhXoS?usp=drive_link)

<div class="figcenter">
<img src="/2024assets/2024Novel_Research_Ideas_fig1_2_.svg" style="width:77%">
<div class="figcaption" style="width:66%">

研究のアイデア出し。LLM によるアイデアと専門家のアイデアを自然言語処理分野の 7 つの話題について比較。
条件１は人間の専門家，条件２は LLM によるアイデア，条件３は LLM のアイデア出しを人間の専門家がランク付け。
結果を専門家が評価５項目に渡って評価した結果。赤アスタリスクは統計的に有意であることを示す。
下図縦軸は評定値 10 点満点。
[Chenglei Si+2024](https://arXiv.org/2409.04109)
</div></div>

<br/><br/><br/>

<div class="figcenter">
<img src="/2024assets/2022Wei_fig11.svg" style="width:77%">
<div class="figcaption">

課題毎のモデルの性能。
図の縦軸は精度。
横軸は，モデルの持つパラメータ数。M は 100 万，B は１億，B は 1000 億。
パラメータが多いと大規模言語モデルと呼ばれる。モデルの持つパラメータが多くなると，突然，性能が向上する点が存在する。
大規模言語モデル (LLM) の知能創発。[Wei+2022](https://arXiv.org/2206.07682/) Fig.11
</div></div>

<br/><br/><br/>

<div class="figcenter">
<img src="/2024assets/2024Superintelligence_fig1.jpg" style="width:77%">
<div class="figcaption" style="width:66%">

知能爆発のシナリオ。[SITUATIONAL AWARENESS](https://situational-awareness.ai/from-agi-to-superintelligence/) <br/>
自動化された AI 研究は，アルゴリズムの進歩を加速させ，1 年で 5OOM 以上の計算能力を向上させることができる。
知能爆発的時，AI システムは人間よりもはるかに賢いものになっているかも知れない。
<!-- Automated AI research could accelerate algorithmic progress, leading to 5+ OOMs of effective compute gains ina year.
The AI systems we’d have by the end of an intelligence explosion would be vastly smarter than humans. -->
</div></div>

# 復習

<img src="/assets/1986pdp_chap8_Fig1.svg" style="width:39%">

1. [ディープラーニング概説, 2015, LeCun, Bengio, Hinton, Nature](https://komazawa-deep-learning.github.io/2021/2015LeCun_Bengio_Hinton_NatureDeepReview.pdf){:target="_blank"}
1. [ゴール駆動型深層学習モデルを用いた感覚皮質の理解 Yamins(2016) Nature](https://project-ccap.github.io/2016YaminsDiCarlo_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex.pdf){:target="_blank"}
1. [ディープラーニングレビュー Storrs ら, 2019, Neural Network Models and Deep Learning, 2019](https://komazawa-deep-learning.github.io/2021/2019Storrs_Golan_Kriegeskorte_Neural_network_models_and_deep_learning.pdf){:target="_blank"}
1. [深層学習と脳の情報処理レビュー Kriegestorte, 2015, Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing](2015Kriegeskorte_Deep_Neural_Networks-A_New_Framework_for_Modeling_Biological_Vision_and_Brain_Information_Processing.pdf){:target="_blank"}
1. [生物の視覚と脳の情報処理をモデル化する新しい枠組み Kriegeskorte, Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing, 2015](https://project-ccap.github.io/2015Kriegeskorte_Deep_Neural_Networks-A_New_Framework_for_Modeling_Biological_Vision_and_Brain_Information_Processing.pdf){:target="_blank"}
1. [計算論的認知神経科学 Kriegeskorte and Douglas, 2018, Cognitive computational neuroscience](https://project-ccap.github.io/2018Kriegeskorte_Douglas_Cognitive_Computational_Neuroscience.pdf){:target="_blank"}
1. [視覚系の畳み込みニューラルネットワークモデル，過去現在未来 Lindsay, 2020, Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future](https://project-ccap.github.io/2020Lindsay_Convolutional_Neural_Networks_as_a_Model_of_the_Visual_System_Past_Present_and_Future.pdf){:target="_blank"}

### 神経細胞 (neuron)

脳は莫大な数($10^{10}$ 個以上ともいわれる)の神経単位(ニューロン neuron)から成り立っている。
このニューロンが脳の情報処理における基本単位である。
複数のニューロンが結合してニューラルネットワークが形成されている。

個々のニューロンは、単純な処理しか行なわないが、脳はこのニューロンが相互に結合された並列処理システムであると捕えることができる。

<!-- The human brain consists of a large number (more than a billion) of neural cells that process informations.
Each cell works like a simple processor and only the massive interaction between all cells and their parallel processing makes the brain's abilities possible.

Below you see a sketch of such a neural cell, called a neuron: -->

神経回路網を構成しているのはこのニューロン (神経細胞) であり，ニューロンは，細胞体，樹状突起，軸索，とよばれる部分からなる。
樹状突起はアンテナ(入力)，軸索は送電線 (出力) と考えれば分かりやすい。

ニューロンの内部と外部とでは $Na^+$ , $K^+$ イオンなどの働きにより電位差がある。
通常，内部電位は外部よりも低い。
外部を 0 としたときの内部の電位を膜電位という。
入力信号が無いときの膜電位を静止膜電位という (約 $-70 mV$ ぐらいである)。

情報は樹状突起から電気信号の形でニューロンに伝達され，すべての樹状突起からの電気信号が加え合わされる。
樹状突起からやってくる外部電気信号の影響で膜電位が一定の値 (しきい値 約 $-55mV$) を越えると約 1 msec の間膜電位が急激に高くなる。
このことをニューロンが興奮した(あるいは発火した)という。
ニューロンの興奮は、軸索をとおって別のニューロンに伝達される。

この電気信号が一定の値を超えると(しきい値)そのニューロンが発火する。

ニューロンは膜電位が $-55mV$ より高くなれば興奮し，そうでなければ興奮しない。
この意味で $-55mV$ 付近を閾値という。
また，一旦興奮したニューロンはしばらくは興奮することができない。
これを不応期という。

<!--% 電位変化に現われる興奮(パルス)，不応期などの概念を図\ref{GENESIS-Neuron} によって確認されたい。-->
<img src="/2024assets/hh.svg">

ニューロンの興奮(1 msec だけなのでパルス pulse と呼ぶことがある)は軸索をとおって他のニューロンに伝達される。
軸索を通る興奮の伝達速度は 100 m/s くらいである。
たとえば，文字を見て音声を発声するまでの応答時間は，たかだか，1 秒程度で、ニューロンの応答時間を 10 数ミリ秒とすると多めに見積もっても 100 程度のニューロンしか通過していないことになる。
このことは「100 step のプログラムの制約」と呼ばれる。

### 神経細胞の結合様式

ニューロンからニューロンへ情報が伝達される部分をシナプス (synapse) と呼ぶ。
シナプスに興奮が到達するたびに送り手側 (シナプス前ニューロン) のニューロンからある種の化学物質が放出される。
この化学物質は受け手側 (シナプス後ニューロン) の膜電位をわずかに変化させる。

化学物質の種類によって，膜電位を高めるように作用する場合 (興奮性のシナプス結合) と逆に低めるように作用する場合 (抑制性のシナプス結合) とがある。
この他のシナプス結合としては，別の興奮性のシナプス結合の伝達効率を抑制するように働くシナプス結合 (シナプス前抑制) が存在することが知られている。
多くの研究者の努力にもかかわらず，今のところシナプス結合の種類はこの 3 種類しか発見されていない。
我々は上記 3 種類のシナプスだけを考えればよさそうである。

送り手のシナプスの興奮が興奮的に働くか抑制的に働くかは，送り手の側の細胞の種類によって異なることが知られている (Dale の法則)。

一つのニューロンには多いもので数万個のシナプス結合が存在する。
多数の軸索にシナプス結合を通して興奮(あるいは抑制)が伝えられると細胞体を伝わる途中で重なり合う。
すべての膜電位の変化の総和によってニューロンの膜電位の変化が決定される。
すべてのシナプス結合の和のことを空間加算という。
あるシナプスによって膜電位が変化し，その変化が減衰する前に次の興奮が伝達されれば，まだ残っている直前の電位変化に加え合わされて膜電位の変化が起きる。
このことを時間加算という。

シナプスに興奮が到達すると 0.3 msec 程度の時間遅れの後シナプス結合部の膜電位がわずかに変化する。
１つのシナプスが生成する膜電位の変化は 0.1 mV から 30 mV ぐらいのものまで様々なシナプス結合が存在する。

樹状突起を介したニューロン間の結合の強さは，しばしば変化することが知られている。
これを学習という。

### ヘッブの学習則 Hebbian rule

細胞 A の軸索が細胞 B を興奮させるのに十分なほど近くにあるか，繰り返し，あるいは一貫して細胞 B の発火に関与している場合，細胞 B を発火させる細胞の一つとしての A の効率が高まるように，細胞の一方または両方に何らかの成長や代謝の変化が起こる。<!-- When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased. -->


# AI の歴史

1. 第一次 AI ブーム 1950-
2. 第二次 AI ブーム 1980-
3. 第三次 AI ブーム 2010-


* [前期第三回資料](https://komazawa-deep-learning.github.io/2024ai/2024ai_lect03/){:target="_blank"}

#### 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='/assets//mcculloch.jpg' style="width:38%">
<img src='/assets//pitts.jpg' style='width:50%'><br>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br> -->
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_{i}=\phi\left(\sum_jw_{ij}x_j\right),\tag{eq:formal_neuron}
$$

ここで $y_{i}$ は $i$ 番目のニューロンの出力，$x_{j}$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$ と $j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='/assets/Formal_r.svg' style="width:84%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br> -->
形式ニューロン
</center>

---

#### ローゼンブラット Rosenblatt のパーセプトロン

<center>
<img src='/assets/rosenblatt.jpg' style="width:49%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//rosenblatt.jpg' style="width:49%"><br> -->
フランク・ローゼンブラット
</center>

<!--
$$ \mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x} $$
-->

<center>
<img src='/assets/perceptron.png' style="width:74%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//perceptron.png' style="width:74%"></br> -->
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center>

<center>
<img src='/assets/Neuron_Hand-tuned.png' style="width:69%"><br/>
<!-- <img src='https://komazawa-deep-learning.github.io/assets//Neuron_Hand-tuned.png' style="width:69%"></br>
 -->
ニューロンの模式図 wikipedia より
</center>

<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->
