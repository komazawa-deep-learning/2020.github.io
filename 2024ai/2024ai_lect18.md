---
title: 第18回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+4" color="navy"><strong>人工知能 II (自然言語処理，あるいは系列情報処理編)</strong></font><br/><br/>
<!-- <font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/> -->
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 11/Oct/2024<br/>
Appache 2.0 license<br/>
</div>

# キーワード

* トークナイザ (トークン化器), 単語埋め込みモデル，あるいはワード2ベック (word embedding modeling, word2vec), エンコーダ・デコーダモデルあるいは，符号化器・復号化器モデル (encodeer-decoder models)，コサイン類似度 (cosine similarity), レーベンシュタイン距離 (Levenshtein distances)

### デモ (java script)

* [Attention is all you need](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb)

* [RNN 文字ベース言語モデル. 漱石「こころ」](https://komazawa-deep-learning.github.io/character_demo.html)

### 実習ファイル

* [足し算モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb)
* [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb)
* [注意つき翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)
* [バニラ風味 注意なし翻訳モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb)

# 1. [Hopfield and Hinton, Nobel Prize in Physics 2024 受賞記念](https://www.nobelprize.org/prizes/physics/2024/popular-information/){:target="_blank"}

今年の受賞者は，物理学のツールを活用して，今日の強力な機械学習の基礎を築くのに役立つ手法を構築した。
ホップフィールドは，情報を保存し再構築できる構造を考案した。
ヒントンは，データから独自に特性を発見できる手法を発明し，現在使用されている大規模な人工ニューラルネットワークにとって重要なものとなった。
<!-- This year’s laureates used tools from physics to construct methods that helped lay the foundation for today’s powerful machine learning.
John Hopfield created a structure that can store and reconstruct information.
Geoffrey Hinton invented a method that can independently discover properties in data and which has become important for the large artificial neural networks now in use. -->

### 彼らは物理学を用いて情報のパターンを見つけ出した<!-- They used physics to find patterns in information-->

多くの人が，コンピュータが言語を翻訳したり，画像を解釈したり，さらにはそれなりの会話を行うことができることを経験している。
しかし，おそらくあまり知られていないのは，この種のテクノロジーが膨大なデータの分類や分析など，研究において長い間重要な役割を果たしてきたということだ。
機械学習は過去15～20年で爆発的に発展し，人工ニューラルネットワークと呼ばれる構造を利用している。
今日，人工知能について語る場合，この技術を指すことが多い。
<!-- Many people have experienced how computers can translate between languages, interpret images and even conduct reasonable conversations.
What is perhaps less well known is that this type of technology has long been important for research, including the sorting and analysis of vast amounts of data.
The development of machine learning has exploded over the past fifteen to twenty years and utilises a structure called an artificial neural network.
Nowadays, when we talk about artificial intelligence, this is often the type of technology we mean. -->

コンピュータは思考することができないが，記憶や学習といった機能は模倣することができる。
今年のノーベル物理学賞受賞者たちは，これを可能にした。
物理学の基本的な概念と手法を用いて，ネットワークの構造を利用して情報を処理する技術を開発した。
<!-- Although computers cannot think, machines can now mimic functions such as memory and learning.
This year’s laureates in physics have helped make this possible. Using fundamental concepts and methods from physics, they have developed technologies that use structures in networks to process information. -->

機械学習は，レシピのような従来のソフトウェアとは異なる。
ソフトウェアはデータを受け取り，明確な説明に従って処理を行い，結果を生成する。これは，誰かが材料を集め，レシピに従ってそれらを処理し，ケーキを作るのとよく似ている。
これに対し，機械学習ではコンピュータが例から学習し，段階的な指示では管理できないほど曖昧で複雑な問題にも取り組むことができる。その一例が，画像を解釈してその中に含まれる物体を識別することである。
<!--Machine learning differs from traditional software, which works like a type of recipe.
The software receives data, which is processed according to a clear description and produces the results, much like when someone collects ingredients and processes them by following a recipe, producing a cake.
Instead of this, in machine learning the computer learns by example, enabling it to tackle problems that are too vague and complicated to be managed by step by step instructions. One example is interpreting a picture to identify the objects in it. -->

#### 脳を模倣する<!-- #### Mimics the brain-->

人工ニューラルネットワークは，ネットワーク構造全体を使用して情報を処理する。
当初の着想は，脳の仕組みを理解したいという欲求から生まれた。
1940 年代には，研究者たちは脳のニューロンとシナプスのネットワークの基礎となる数学について推論を始めていた。
また，神経科学者ドナルド・ヘッブが，ニューロン間の接続が連携して機能するときに強化されるという学習の発生に関する仮説を立てたことで，心理学からも新たな知見がもたらされた。
<!-- An artificial neural network processes information using the entire network structure.
The inspiration initially came from the desire to understand how the brain works.
In the 1940s, researchers had started to reason around the mathematics that underlies the brain’s network of neurons and synapses.
Another piece of the puzzle came from psychology, thanks to neuroscientist Donald Hebb’s hypothesis about how learning occurs because connections between neurons are reinforced when they work together. -->

その後，これらの考え方は，人工ニューラルネットワークをコンピュータシミュレーションとして構築することで，脳のネットワークの機能を再現しようとする試みに引き継がれた。
この場合，脳のニューロンは異なる値を持つノードで模倣され，シナプスはノード間の接続で表され，その接続は強くなったり弱くなったりする。
ドナルド・ヘッブの仮説は，訓練と呼ばれる処理を通じて人工ネットワークを更新する際の基本的な学習則の一つとして，現在でも使用されている。
<!--Later, these ideas were followed by attempts to recreate how the brain’s network functions by building artificial neural networks as computer simulations.
In these, the brain’s neurons are mimicked by nodes that are given different values, and the synapses are represented by connections between the nodes that can be made stronger or weaker.
Donald Hebb’s hypothesis is still used as one of the basic rules for updating artificial networks through a process called training. -->

<div class="figcenter">
<img src="https://www.nobelprize.org/uploads/2024/10/popular-physicsprize2024-figure2-2-1024x530.jpg" style="width:77%">
</div>

1960 年代の終わりには，いくつかの期待外れの理論的結果により，多くの研究者がこれらのニューラルネットワークが実用化されることはないだろうと疑うようになった。
しかし，人工ニューラルネットワークへの関心は1980年代に再び高まり，その年に受賞した研究者たちの研究を含むいくつかの重要なアイデアが注目を集めた。
<!-- At the end of the 1960s, some discouraging theoretical results caused many researchers to suspect that these neural networks would never be of any real use.
However, interest in artificial neural networks was reawakened in the 1980s, when several important ideas made an impact, including work by this year’s laureates. -->

#### 連想記憶<!-- #### Associative memory-->

映画館や講堂でよく見かける傾斜した床を表す言葉など，めったに使わないかなり変わった単語を思い出そうとしているとしよう。
記憶をたどってみる。
Ramp..., rad...idal? いや rake だ!
<!--ランプのようなもの…ラジアル…？いや，それじゃない。
レーキだ！
Imagine that you are trying to remember a fairly unusual word that you rarely use, such as one for that sloping floor often found in cinemas and lecture halls.
You search your memory.
It’s something like ramp… perhaps rad…ial? No, not that.
Rake, that’s it! -->

適切な単語を見つけるために類似した単語を探索するこのプロセスは，物理学者ジョン・ホップフィールドが1982年に発見した連想記憶を彷彿とさせる。
ホップフィールド・ネットワークはパターンを保存でき，それを再現する方法も備えている。
ネットワークに不完全なパターンやわずかに歪んだパターンが与えられると，その方法によって最も類似した保存パターンを見つけることができる。
<!-- This process of searching through similar words to find the right one is reminiscent of the associative memory that the physicist John Hopfield discovered in 1982.
The Hopfield network can store patterns and has a method for recreating them.
When the network is given an incomplete or slightly distorted pattern, the method can find the stored pattern that is most similar. -->

ホップフィールドは以前，物理学のバックグラウンドを活かして分子生物学の理論的な問題を研究していた。
神経科学に関する会議に招かれた際，彼は脳の構造に関する研究に出会った。
彼はそこで学んだことに魅了され，単純な神経ネットワークの力学について考え始めた。
ニューロンが同時に作用すると，ネットワークの個々の構成要素だけを見ている人には見えない，新しい強力な特性が生み出される可能性がある。
<!-- Hopfield had previously used his background in physics to explore theoretical problems in molecular biology.
When he was invited to a meeting about neuroscience he encountered research into the structure of the brain.
He was fascinated by what he learned and started to think about the dynamics of simple neural networks.
When neurons act together, they can give rise to new and powerful characteristics that are not apparent to someone who only looks at the network’s separate components. -->

1980年，ホップフィールドはプリンストン大学を辞し，大陸を横断した。
南カリフォルニアのパサデナにあるカリフォルニア工科大学（Caltech）の化学・生物学の教授職のオファーを受けたのだ。
同大学では，ニューラルネットワークに関する自身のアイデアを開発するための無料実験に利用できるコンピューターリソースを利用することができた。
<!-- In 1980, Hopfield left his position at Princeton University, where his research interests had taken him outside the areas in which his colleagues in physics worked, and moved across the continent.
He had accepted the offer of a professorship in chemistry and biology at Caltech (California Institute of Technology) in Pasadena, southern California.
There, he had access to computer resources that he could use for free experimentation and to develop his ideas about neural networks.-->

しかし，彼は物理学の基礎を捨てたわけではなく，そこから，多数の小さな部品が連携して機能するシステムが，新しい興味深い現象を生み出す仕組みについての理解を得た。
特に，原子スピンによって特殊な特性を持つ磁性材料について学んだことは有益であった。
隣接する原子のスピンは互いに影響し合うため，同じ方向のスピンを持つドメインを形成することができる。
彼は，スピンが互いに影響し合うことで物質がどのように発展するかを説明する物理学を利用して，ノードと接続からなるモデルネットワークを作成することができた。
<!--However, he did not abandon his foundation in physics, where he found inspiration for his under­standing of how systems with many small components that work together can give rise to new and interesting phenomena.
He particularly benefitted from having learned about magnetic materials that have special characteristics thanks to their atomic spin – a property that makes each atom a tiny magnet.
The spins of neighbouring atoms affect each other; this can allow domains to form with spin in the same direction.
He was able to make a model network with nodes and connections by using the physics that describes how materials develop when spins influence each other. -->

#### ネットワークは画像の景観を保存する<!-- #### The network saves images in a landscape-->

ホップフィールドが構築したネットワークは，強度の異なる接続によって結合されたノードで構成されている。
各ノードは個別の値を保存することができ，ホップフィールドの最初の研究では，白黒写真のピクセルのように，0または1のいずれかを保存することができた。
<!--The network that Hopfield built has nodes that are all joined together via connections of different strengths.
Each node can store an individual value – in Hopfield’s first work this could either be 0 or 1, like the pixels in a black and white picture. -->

ホップフィールドは，物理学におけるスピン系のエネルギーに相当する特性を用いてネットワークの全体的な状態を説明した。エネルギーは，ノードのすべての値と，それらの間の接続のすべての強度を使用する数式を用いて計算される。
ホップフィールドネットワークは，ノードに供給される画像によってプログラムされ，ノードには黒（0）または白（1）の値が与えられる。
次に，保存された画像のエネルギーが低くなるように，エネルギー式を用いてネットワークの接続が調整される。
別のパターンがネットワークに送られると，ノードを一つずつ順に調べ，そのノードの値が変更された場合にネットワークのエネルギーが低くなるかどうかを確認するルールがある。
黒いピクセルが白に置き換えられるとエネルギーが低くなることが判明した場合，色が変更される。
この手順は，これ以上改善が見込めなくなるまで続けられる。この段階に達すると，ネットワークは学習に使用された元の画像を再現していることが多い。
<!-- Hopfield described the overall state of the network with a property that is equivalent to the energy in the spin system found in physics; the energy is calculated using a formula that uses all the values of the nodes and all the strengths of the connections between them.
The Hopfield network is programmed by an image being fed to the nodes, which are given the value of black (0) or white (1).
The network’s connections are then adjusted using the energy formula, so that the saved image gets low energy.
When another pattern is fed into the network, there is a rule for going through the nodes one by one and checking whether the network has lower energy if the value of that node is changed.
If it turns out that energy is reduced if a black pixel is white instead, it changes colour.
This procedure continues until it is impossible to find any further improvements. When this point is reached, the network has often reproduced the original image on which it was trained.-->

これは，1つのパターンだけを保存する場合はそれほど目立たないかもしれない。
おそらく，画像そのものを保存して，それをテスト中の別の画像と比較すればいいのに，と思うかもしれないが，ホップフィールドの方法は特殊で，同時に複数の画像を保存でき，ネットワークは通常，それらの画像を区別することができる。
<!-- This may not appear so remarkable if you only save one pattern.
Perhaps you are wondering why you don’t just save the image itself and compare it to another image being tested, but Hopfield’s method is special because several pictures can be saved at the same time and the network can usually differentiate between them. -->

ホップフィールドは，ネットワークに保存された状態を検索することを，摩擦により動きが遅くなる山と谷の風景の中をボールが転がっていくことに例えた。
ボールが特定の場所に落とされた場合，ボールは最も近い谷に転がり落ちてそこで止まる。
ネットワークに保存されたパターンのいずれかに近いパターンが与えられた場合，同様に，エネルギー地形の谷底に到達するまで前進し続けるため，最も近いパターンを記憶の中から見つけることができる。
<!-- Hopfield likened searching the network for a saved state to rolling a ball through a landscape of peaks and valleys, with friction that slows its movement.
If the ball is dropped in a particular location, it will roll into the nearest valley and stop there.
If the network is given a pattern that is close to one of the saved patterns it will, in the same way, keep moving forward until it ends up at the bottom of a valley in the energy landscape, thus finding the closest pattern in its memory. -->

ホップフィールド・ネットワークは，ノイズを含むデータや一部が消去されたデータの再作成にも使用できる。
<!--The Hopfield network can be used to recreate data that contains noise or which has been partially erased. -->

<div class="figcenter">
<img src="https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure3-1024x657.jpg" style="width:55%">
</div>

ホップフィールドやその他の研究者たちは，ホップフィールド・ネットワークの機能の詳細を開発し続けている。
その中には，0 や 1 以外のあらゆる値を保存できるノードも含まれる。
ノードを画像の画素と考えると，黒や白だけでなく，さまざまな色を持つことができる。
改良された方法により，より多くの画像を保存し，それらが非常に似通っている場合でも区別することが可能になった。
多くのデータ点から構築されている限り，あらゆる情報を識別または再構築することが可能である。
<!-- Hopfield and others have continued to develop the details of how the Hopfield network functions, including nodes that can store any value, not just zero or one.
If you think about nodes as pixels in a picture, they can have different colours, not just black or white.
Improved methods have made it possible to save more pictures and to differentiate between them even when they are quite similar.
It is just as possible to identify or reconstruct any information at all, provided it is built from many data points. -->

#### 19 世紀の物理学を用いた分類<!-- #### Classification using nineteenth-century physics-->

画像を想起することはできるが，それが何を表しているかを解釈するには，もう少し必要だ。
<!-- Remembering an image is one thing, but interpreting what it depicts requires a little more. -->

幼い子供でも，さまざまな動物を指さして，それが犬なのか，猫なのか，リスなのかを自信を持って言うことができる。
時には間違えることもあるが，すぐにほとんどの場合正しく言えるようになる。子供は，図や種や哺乳類といった概念の説明を見なくても，これを学ぶことができる。
それぞれの種類の動物の例をいくつか見れば，子供たちはすぐに異なるカテゴリーを理解する。
人は，猫を認識したり，言葉を理解したり，部屋に入って何かが変わったことに気づいたりするが，それは周囲の環境を経験することによってである。
<!-- Even very young children can point at different animals and confidently say whether it is a dog, a cat, or a squirrel.
They might get it wrong occasionally, but fairly soon they are correct almost all the time. A child can learn this even without seeing any diagrams or explanations of concepts such as species or mammal.
After encountering a few examples of each type of animal, the different categories fall into place in the child’s head.
People learn to recognise a cat, or understand a word, or enter a room and notice that something has changed, by experiencing the environment around them. -->

ホップフィールドが連想記憶に関する論文を発表した当時，ジェフリー・ヒントンは米国ピッツバーグのカーネギーメロン大学で働いていた。 彼はそれ以前に，英国とスコットランドで実験心理学と人工知能を研究しており，機械が人間と同様にパターン処理を学習し，情報を分類し解釈するための独自のカテゴリーを見つけ出すことができるかどうかを考えていた。
ヒントンは同僚のテレンス・セジュノフスキーとともに，ホップフィールドのネットワークを出発点として，統計物理学のアイデアを活用しながら，新しいものを構築するためにそれを拡張した。
<!--When Hopfield published his article on associative memory, Geoffrey Hinton was working at Carnegie Mellon University in Pittsburgh, USA. He had previously studied experimental psychology and artificial intelligence in England and Scotland and was wondering whether machines could learn to process patterns in a similar way to humans, finding their own categories for sorting and interpreting information.
Along with his colleague, Terrence Sejnowski, Hinton started from the Hopfield network and expanded it to build something new, using ideas from statistical physics. -->

統計力学は，気体中の分子のように，多くの類似した要素で構成される系を説明する。
気体中の個々の分子をすべて追跡することは困難，あるいは不可能であるが，それらをまとめて考え，圧力や温度といった気体の全体的な性質を決定することは可能である。
気体分子は，それぞれの速度で気体の体積全体に広がる可能性があり，それでも同じ全体的な性質が得られる。
<!-- Statistical physics describes systems that are composed of many similar elements, such as molecules in a gas.
It is difficult, or impossible, to track all the separate molecules in the gas, but it is possible to consider them collectively to determine the gas’ overarching properties like pressure or temperature.
There are many potential ways for gas molecules to spread through its volume at individual speeds and still result in the same collective properties.-->

個々の構成要素が共同して存在できる状態は，統計物理学を用いて分析することができ，その発生確率を計算することができる。
状態によっては，他の状態よりも発生確率が高いものもある。これは利用可能なエネルギーの量に依存するが，この量は 19 世紀の物理学者ルートヴィヒ・ボルツマンの式で表される。
ヒントンのネットワークは，この式を利用しており，その方法は1985年に「ボルツマン・マシン」という印象的な名称で発表された。
<!--The states in which the individual components can jointly exist can be analysed using statistical physics, and the probability of them occurring calculated.
Some states are more probable than others; this depends on the amount of available energy, which is described in an equation by the nineteenth-century physicist Ludwig Boltzmann.
Hinton’s network utilised that equation, and the method was published in 1985 under the striking name of the Boltzmann machine. -->

### 同じタイプの新しい例を認識する<!--#### Recognising new examples of the same type-->

ボルツマン・マシンは通常，2 種類の異なるノードとともに使用される。
情報は，可視ノードと呼ばれる1つのグループに供給される。他のノードは隠れ層を形成する。
隠れノードの値と接続も，ネットワーク全体のエネルギーに寄与する。
<!-- The Boltzmann machine is commonly used with two different types of nodes.
Information is fed to one group, which are called visible nodes. The other nodes form a hidden layer.
The hidden nodes’ values and connections also contribute to the energy of the network as a whole. -->

このマシンは，ノードの値を1つずつ更新するルールを適用することで実行される。
最終的に，ノードのパターンが変化する状態になるが，ネットワーク全体の特性は変わらない。
各パターンには，ボルツマン方程式に従ってネットワークのエネルギーによって決定される特定の確率が存在する。
マシンが停止すると，新しいパターンが作成される。これがボルツマンマシンを生成モデルの初期の例としている。
<!--The machine is run by applying a rule for updating the values of the nodes one at a time.
Eventually the machine will enter a state in which the nodes’ pattern can change, but the properties of the network as a whole remain the same.
Each possible pattern will then have a specific probability that is determined by the network’s energy according to Boltzmann’s equation.
When the machine stops it has created a new pattern, which makes the Boltzmann machine an early example of a generative model.-->

<div class="figcenter">
<img src="https://www.nobelprize.org/uploads/2024/09/popular-physicsprize2024-figure4-1024x594.jpg" style="width:55%">
</div>

ボルツマンマシンは学習することができる。
ただし，指示からではなく，例を示されることで学習する。
ネットワークの接続部分の値を更新することで訓練を行い，訓練時に可視ノードに与えられた例となるパターンが，マシンが実行された際に発生する可能性が最も高くなるようにする。
この訓練中に同じパターンが複数回繰り返された場合，そのパターンの発生確率はさらに高くなる。
また，訓練は，マシンが訓練された例に類似した新しいパターンを出力する確率にも影響を与える。
<!-- The Boltzmann machine can learn – not from instructions, but from being given examples.
It is trained by updating the values in the network’s connections so that the example patterns, which were fed to the visible nodes when it was trained, have the highest possible probability of occurring when the machine is run. If the same pattern is repeated several times during this training, the probability for this pattern is even higher.
Training also affects the probability of outputting new patterns that resemble the examples on which the machine was trained.-->

訓練されたボルツマンマシンは，以前に見たことのない情報でも，見覚えのある特徴を認識することができる。
友人の兄弟に会ったと想像してみよう。
それらが親戚であることはすぐに分かる。
同様に，ボルツマンマシンは，訓練事例に含まれるカテゴリーに属するものであれば，まったく新しい例を認識し，類似しない資料と区別することができる。
<!-- A trained Boltzmann machine can recognise familiar traits in information it has not previously seen.
Imagine meeting a friend’s sibling, and you can immediately see that they must be related.
In a similar way, the Boltzmann machine can recognise an entirely new example if it belongs to a category found in the training material, and differentiate it from material that is dissimilar. -->

ボルツマンマシンは，その原型ではかなり非効率的で，解を見つけるのに長い時間がかかった。
しかし，ヒントンが研究を続けたように，さまざまな方法で開発が進められると，状況はより興味深いものになっていく。
後続のバージョンでは，いくつかの素子間の接続が削除され，間引きが行われた。
これにより，マシンがより効率的になる可能性があることが判明した。
<!-- In its original form, the Boltzmann machine is fairly inefficient and takes a long time to find solutions.
Things become more interesting when it is developed in various ways, which Hinton has continued to explore.
Later versions have been thinned out, as the connections between some of the units have been removed. It turns out that this may make the machine more efficient. -->

1990 年代には，多くの研究者が人工ニューラルネットワークへの関心を失ったが，Hinton は研究を続けた。
また，彼は2006年に同僚の Simon Osindero, Yee Whye Teh, Ruslan Salakhutdinov とともに，層状に重ねられた一連のボルツマンマシンでネットワークを事前学習する方法を開発し，新たな成果の爆発的な増加に貢献した。
この事前学習により，ネットワーク内の接続に最適な出発点が与えられ，画像内の要素を認識するための学習が最適化された。
<!-- During the 1990s, many researchers lost interest in artificial neural networks, but Hinton was one of those who continued to work in the field.
He also helped start the new explosion of exciting results; in 2006 he and his colleagues Simon Osindero, Yee Whye Teh and Ruslan Salakhutdinov developed a method for pretraining a network with a series of Boltzmann machines in layers, one on top of the other.
This pretraining gave the connections in the network a better starting point, which optimised its training to recognise elements in pictures. -->

ボルツマン・マシンは，より大きなネットワークの一部として使用されることが多い。例えば，視聴者の好みに応じた映画やテレビ番組の推奨に使用できる。
<!-- The Boltzmann machine is often used as part of a larger network. For example, it can be used to recommend films or television series based on the viewer’s preferences. -->

#### 機械学習 - 現在と未来<!-- #### Machine learning – today and tomorrow-->

ジョン・ホップフィールドとジェフリー・ヒントンは，1980 年代以降の研究により，2010 年頃に始まった機械学習革命の基礎を築いた。
<!-- Thanks to their work from the 1980s and onward, John Hopfield and Geoffrey Hinton have helped lay the foundation for the machine learning revolution that started around 2010. -->

現在我々が目撃している発展は，ネットワークの訓練に使用できる膨大な量のデータへのアクセスと，コンピューティング能力の飛躍的な向上により可能となった。
今日の人工ニューラルネットワークは，多くの場合，巨大で多くの層から構成されている。これを深層ニューラルネットワークと呼び，その学習方法を深層学習と呼ぶ。
<!-- The development we are now witnessing has been made possible through access to the vast amounts of data that can be used to train networks, and through the enormous increase in computing power.
Today’s artificial neural networks are often enormous and constructed from many layers. These are called deep neural networks and the way they are trained is called deep learning. -->

1982 年の Hopfield の連想記憶に関する論文をざっと見ると，この発展についてある程度の見通しが得られる。
その論文では，30 個のノードを持つネットワークが使用されていた。
すべてのノードが互いに接続されている場合，435 の接続がある。
ノードには値があり，接続には異なる強度があり，合計すると，追跡すべきパラメータは 500 未満である。
また，彼は 100 ノードのネットワークも試したが，当時使用していたコンピュータでは複雑すぎた。
これを今日の巨大な言語モデルと比較すると，これは 1 兆以上のパラメータ（1 億億の1 億倍）を含むネットワークとして構築されている。
<!-- A quick glance at Hopfield’s article on associative memory, from 1982, provides some perspective on this development.
In it, he used a network with 30 nodes.
If all the nodes are connected to each other, there are 435 connections.
The nodes have their values, the connections have different strengths and, in total, there are fewer than 500 parameters to keep track of.
He also tried a network with 100 nodes, but this was too complicated, given the computer he was using at the time.
We can compare this to the large language models of today, which are built as networks that can contain more than one trillion parameters (one million millions). -->

現在，多くの研究者が機械学習の応用分野の開発に取り組んでいる。
どの分野が最も有望かはまだわからないが，この技術の開発と利用をめぐる倫理的な問題についても幅広い議論が行われている。
<!--Many researchers are now developing machine learning’s areas of application.
Which will be the most viable remains to be seen, while there is also wide-ranging discussion on the ethical issues that surround the development and use of this technology. -->

物理学は機械学習の発展に貢献するツールを提供してきたため，研究分野としての物理学が人工ニューラルネットワークからどのような恩恵を受けているのかを見るのは興味深い。
機械学習は，これまでのノーベル物理学賞受賞者たちの研究分野と関連の深い分野で，以前から利用されてきた。
例えば，ヒッグス粒子の発見に必要な膨大なデータをふるいにかけて処理するために機械学習が利用された。
その他の応用例としては，ブラックホールの衝突による重力波の測定におけるノイズの低減や，太陽系外惑星の探索などがある。
<!-- Because physics has contributed tools for the development of machine learning, it is interesting to see how physics, as a research field, is also benefitting from artificial neural networks.
Machine learning has long been used in areas we may be familiar with from previous Nobel Prizes in Physics.
These include the use of machine learning to sift through and process the vast amounts of data necessary to discover the Higgs particle.
Other applications include reducing noise in measurements of the gravitational waves from colliding black holes, or the search for exoplanets.-->

近年では，分子や物質の特性を計算したり予測したりする際にも，この技術が利用され始めている。例えば，タンパク質分子の構造を計算してその機能を決定したり，より効率的な太陽電池に使用するのに最適な特性を持つ物質の新しいバージョンを特定したりするなどである。
<!--In recent years, this technology has also begun to be used when calculating and predicting the properties of molecules and materials – such as calculating protein molecules’ structure, which determines their function, or working out which new versions of a material may have the best properties for use in more efficient solar cells. -->

#### さらに詳しく<!-- #### Further reading-->

今年の受賞に関する追加情報（英語による科学的背景を含む）は，スウェーデン王立科学アカデミーのウェブサイト（www.kva.se）および www.nobelprize.org で入手できる。
www.nobelprize.org では，記者会見のビデオやノーベル賞受賞講演などを視聴できる。
ノーベル賞および経済学賞に関連する展示会や活動に関する情報は，www.nobelprizemuseum.se で入手できる。
<!--Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Lectures and more.
Information on exhibitions and activities related to the Nobel Prizes and the Prize in Economic Sciences is available at www.nobelprizemuseum.se. -->

* [Hopfield1982, Neural networks and physical systems with emergent collective computationalabilitie](/2024cogpsy/1982Hopfield_Neural_networks_and_physical_systems_with_emergent_collective_computational_abilities.pdf){:target="_blank"}
* [Hopfiled&Tank1985, "Neural" Computation of Decisions in Optimization Problems](/2024cogpsy/1985Hopfield_Tank_Neural_computation_of_decisions_in_optimization_problems.pdf){:target="_blank"}
* [Rumelhart, Hinton, McClelland 1986, A General Framework for Parallel Distributed Processing, PDP book, chapt. 2](/2024cogpsy/1986Rumelhart_PDPbook_chapter2.pdf){:target="_blank"}
* [Rumelhart, Hinton, & Williams, 1986, Learning Internal Representations by Error Propagation, PDP book, chapt. 8](/2024cogpsy/1986Rumelhart_Hinton_Williams_PDP_Chapt8.pdf){:target="_blank"}
* [ジェフェリー・ヒントンのマクセル賞受賞記念講演(2016)](/Hinton_Maxwell_award/){:target="_blank"}
* [浅川, 2013, アトラクタニューラルネットワークモデルの数理解析とその神経心理学への応用](/2024cogpsy/2013Asakawa_JPsychoReview_Published.pdf){:target="_blank"}
* [浅川 2000, ニューラルネットワークの基礎](/2024cogpsy/main-web2010.pdf){:target="_blank"}

# 意味論

ここでは意味論の研究史を心理学関連領域に絞ってまとめることを試みます。
<!-- 神経心理学症状との関連については
付録 <a target="_blank" href="https://github.com/ShinAsakawa/wbai_aphasia/blob/master/2019Primer_AphasiaDyslexia.pdf">失語，失読に関する神経心理学モデルの基礎</a> をご覧ください。
-->

意味についての言及は言語学者 Firth さらに遡れば Witgenstein まで辿ることが可能です。
ですがここでは直接関連する研究として以下をとりげます

- 第 1 世代 意味微分法 Osgood
- 第 2 世代 潜在意味解析 Ladauer
- 第 3 世代 潜在ディレクリ配置，トピックモデル
- 第 4 世代 分散埋め込みモデル word2vec とその後継モデル
- 最近の展開


## 1952 年 意味微分法 Semantics Differential (SD)
チャールズ・オズグッドによって提案された意味微分法は，被験者に対象を評価させる際に形容詞対を用います。
形容詞対は 5 件法あるいはその他の変種によって評価されます。
得られた結果を 評価対象 X 形容詞対の行列にします。
すなわち評価対象者の平均を求めて得た行列を **固有値分解**，正確には因子分析 FA を行います。
最大固有値から順に満足の行くまで求めます。
固有値行列への射影行列を因子負荷量と呼びます。得られた結果を下図に示しました。

<center>
<img src="/assets/1957Osgood_Tab1.svg" style="width:48%"><br/>
From  Osgood (1952) Tab. 1
</center>
<!-- <a target="_blank" href="/assets/1957Osgood_Tab1.svg">Osgood (1952) Tab. 1</a>-->

上図では，50 対の形容詞対によって対象を評価した値が描かれています。

<a target="_blank" href="https://ja.wikipedia.org/wiki/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90">因子分析(FA)</a> 形容詞対による多段階評定

<center>

<img src="/assets/1957Osgood_Fig2.svg" style="width:46%"><br/>
From  Osgood (1952) Fig.2
<!--- <a target="_blank" href="/assets/1957Osgood_Fig2.svg">Osgood (1952) Fig. 2</a>-->
</center>

意味微分法においては，研究者の用意した形容詞対の関係に依存して対象となる概念やモノ，コトが決まります。
従って研究者の想定していない概念空間については言及できないという点が問題点として指摘できます。

このことは評価対象がよくわかっている問題であれば精度良く測定できるという長所の裏返しです。

一般的な意味，対象者が持っている意味空間全体を考えるためには，50 個の形容詞対では捉えきれないことも意味します。従って以下のような分野に適用する場合には問題が発生する可能性があると言えます

- 神経心理学的な症状である **意味痴呆** semantic dimentia を扱う場合
- 入試問題などの一般知識を評価したい場合
- 一般言語モデルを作成する場合


## 3.2. 1997 年 潜在意味分析 Latent Semantic Analysis (LSA, LSI)
- **潜在意味分析**: <a target="_blank" href="https://ja.wikipedia.org/wiki/%E7%89%B9%E7%95%B0%E5%80%A4%E5%88%86%E8%A7%A3">特異値分解(SVD)</a> は，当時増大しつつあったコンピュータ計算資源を背景に一般意味論に踏み込む先鞭をつけたと考えることができます。

すなわち先代の意味微分法が持つ問題点である，評価方法が 50 対の形容詞であること，
50 をいくら増やしても，結局は研究者の恣意性が排除できないこと，評価者が人間であるため大量の評価対象を評価させることは，
心理実験参加者の拘束時間を長くするため現実的には不可能であることを解消するために，辞書そのものをコンピュータで解析するという手法を採用しました。

1. 辞書の項目とその項目の記述内容とを考えます
2. 特定の辞書項項目にはどの単語が使われているいるのかという共起行列 内容 $\times$ 単語 を
考え，この行列について **特異値分解** を行います。

Osgood の意味微分法で用いられた行列のサイズと比較すると，単語数が数万，項目数は数万から数十万に増加しています。
数の増加は網羅する範囲の拡大を意味します。
下図は持ちられたデータセット例を示したものです。

<center>

<img src="/assets/1997Landauer_Dumais_FigA2.svg" style="width:48%"><br/>
From Landauer and Duman (1997) Fig. A2
</center>

LSA (LSI) の問題点としては以下図を見てください

<center>

<img src="/assets/1997Landauer_Fig3.svg" style="width:46%"><br/>
From Landauer and Dumas (1997) Fig.3
</center>

上図は，得られた結果を元に類義語テストを問いた場合に特異値分解で得られる次元数を横軸に，正解率を縦軸にプロットした図です。
次元を上げると成績の向上が認められます。
ですが，ある程度 300 以上の次元を抽出しても返って成績が低下することが示されています。

次元数を増やすことで本来の類義語検査に必要な知識以外の情報が含まれてしまうため推察されます。


<!--- <a target="_blank" href="/assets/1997Landauer_Fig3.svg">Landauer (1997) Fig. 3</a>-->

## 3.3. 2003 年 潜在ディレクリ配置 Latent Direchlet Allocation (LDA)

潜在ディレクリ配置 Latent Direchlet Allocation: LDA[^LDA] は LSA (LSI) を確率的に拡張したモデルであると考えることができます。すなわち LDA では単語と項目との関係に確率的な生成モデルを仮定します。

[^LDA]: 伝統的な統計学においては Fischer の線形判別分析を LDA と表記します。ですがデータサイエンス，すなわち統計学の一分野では近年の潜在ディレクリ配置の成功により LDA と未定義で表記された場合には潜在ディクレクリ配置を指すことが多くなっています。

その理由としては，対象となる項目，しばしば **トピック** と言い表すと，項目の説明に用いられる単語との間に，決定論的な関係を仮定しないと考えることによります。確率的な関係を仮定することにより柔軟な関係をモデル化が可能であるからです。

例えば，ある概念，話題(トピック) "神経" を説明する場合を考えます。
"神経" を説明するには多様な表現や説明が可能です。
"神経" を説明する文章を数多く集めてると，単語 "脳" は高頻度で出現すると予想できます。
同様にして "細胞" や "脳" も高頻度で観察できるでしょう。ところが単語 "犬" は低頻度でしょう。
単語 "アメフラシ" や "イカ" は場合によりけりでしょう。どちらも神経生理学の発展に貢献した実験動物ですから単語 "アメフラシ" や "イカ" が出現する文章もあれば，単語 "脳梗塞" や単語 "失語" と同時に出現する確率もありえます。
このように考えると確率的に考えた方が良い場合があることが分かります。

### ディレクリ分布

もう一点，ノンパラメトリックモデルについて説明します。
parametric model はパラーメータを用いたモデルほどの意味です。
心理統計学の古典的な教科書では，ノンパラメトリック検定とは母集団分布のパラメータに依存 **しない** 統計的検定という意味で用いられます。
一方 LDA の場合には推定すべき分布のパラメータ(の数)を **事前に定めない** という意味で **ノンパラメトリック** なモデルであると言います。
すなわちある話題 (トピック) とそれを説明する単語の出現確率について，取り扱う現象の複雑さに応じてモデルを記述するパラメータ数を適応的に増やして行くことを考えます。

数学的既述は省略しますが，[ベータ分布]("https://en.wikipedia.org/wiki/Beta_distribution"){:target="_blank"} を用いると区間 $[a,b]$ の間をとる分布でパラメータにより分布が柔軟に記述できます。
ベータ分布の多次元拡張を [ディククリ分布](https://en.wikipedia.org/wiki/Dirichlet_distribution){:target="_blank"} と言います。

確率空間に対して一定の成約を付した表現をシンプレックスと言ったりします。<!--例えばコインの裏表は
2 値ですからベータ分布を用いても表す事ができます。
たとえばじゃんけんで対戦相手が，グー，チョキ，パー のいずれかを出す確率は，2 つが分かれば 3 つ目の手は自ずと分かってきます。
このような関係は 3 つの手の確率分布でディククリ分布として扱うことが可能です。
下図はウィキペディアから持ってきました。この図はそのようなじゃんけんの手の出現確率をディレクリ分布として表現した例だと思ってください。

<!--## ディククリ分布 (多次元ベルヌーイ分布)-->
<!-- 多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定-->

<div class="figcenter">

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png" style="width:46%"><br/>
<!--<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" style="width:49%"></br>-->
<div class="figcaption">
多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定<br/>
図は <https://en.wikipedia.org/wiki/Dirichlet_distribution> より
</div></div>

トピック毎の単語の出現確率も上図と同じ枠組みで記述することが可能です。
かつ，上図ではとりうる値が 3 つの場合ですが，話題が複雑になれば適応的に選択肢の数，すなわちディレクリ分布の次元数が増加することになります。

### プレート表記

あらかじめ定められた数のパラメータを用いて分布を記述するのではなく，
解くべき問題の複雑さに応じて適応的にパラメータ数を定めることに対応して，
LDA あるいはトピックモデルの図示方法として **プレート表記** plate notation があります。
下図にプレート表記の例を示しました。

<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_02.svg" style="width:47%"><br/>
<div class="figcaption">
プレート表記: ノンパラメトリックモデルの表現に用いられる
</div></div>

- 丸は確率変数
- 矢印は確率的依存関係を表現
- 観測変数は影付き(文献によっては二重丸)
- プレートは繰り返しを表す

Y からパラメータ X が生成される場合，矢印を使ってその依存関係を表現します。
ノンパラメトリックモデルの場合，矢印の数を予め定めません。そのため矢印を多数描くのが煩雑なので，一つの箱代用して表現します。
これがプレート表記になります。

観測可能な変数をグレー，または二重丸で表し，観測不能な，類推すべきパラメータを白丸で表記します。
実際には観測不可能な潜在パラメータを観測データから類推することになります。

大まかなルールとして，潜在変数をギリシャアルファベット表記，観測される変数はローマアルファベット表記の場合が多いですが，一般則ですので例外もあります。

下図に潜在ディレクリ配置  LDA のプレート表記を示しました。
<!--### 潜在ディレクリ配置のプレート表記-->

<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_03.svg" style="width:47%"><br/>
<!--<img src="/assets/2009Blei_Topic_Models_04.svg" style="width:94%"></br>-->
</div>

### トピックと単語の関係

トピックモデルの要点をまとめた下図はこれまでの説明をすべて含んでいます。


<div class="figcenter">
<img src="/assets/2009Blei_Topic_Models_01.svg" style="width:47%"><br/>
<div class="figcaption">
 出典: ブライのスライド(2009)より，文章は話題(トピック)の混合<br/>
 各文章はその話題から文章が生成されたと考える
</div></div>

興味深い応用例として Mochihashi ら(2009) の示した教師なし学習による日本語分かち書き例を示します。
下図は源氏物語をトピックモデルにより分かち書きさせた例です。どこに空白を挿入すると文字間の隣接関係を表現できるかをトピックモデルで解くことを考えた場合，空白の挿入位置が確率的に定まると仮定して居ます。

<div class="figcenter">
<img src="/assets/2009Mochihashi_Fig10.svg" style="width:49%"><br/>
</div>

Mochihashi らは，ルイス・キャロルの小説 "不思議の国のアリス" 原文から空白を取り除き，文字間の隣接関係から文字の区切り，すなわち空白を推定することを試みました。結果を下図に示しました。

<div class="figcenter">
<img src="/assets/2009Mochihashi_Fig12.svg" style="width:49%"><br/>
</div>

### 原著論文

- [Blei&Jordan(2003) Latent Dirichlet Allocation 原著論文](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf){:target="_blank"}
- [中華レストラン過程 CRP 発展モデル:](http://arxiv.org/abs/0710.0845){:target="_blank"}
- [インド食堂過程 IBP](http://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf){:target="_blank"}
<!-- - パチンコ過程 -->

### R による実装

- https://cran.r-project.org/web/packages/lda/index.html
- https://cran.r-project.org/web/packages/topicmodels/index.html


## 3.4. 2013 年 word2vec, 単語埋め込み, ベクトル埋め込みモデル

<div class="figcenter">
<img src="/assets/Mikolov_portrait.jpg" style="width:19%">
<img src="/assets/2015Mikolov_NIPSportrait.png" style="width:24%"><br/>
ミコロフ
</div>

- ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示しました。
ワードツーベックと発音します。
Word2vec は実装に 2 種類あリます。それぞれ **CBOW** と **skip-gram** と命名されています。
"シーボウ" または "シーバウ" と日本人は言ったりすることが多いようです。

有名な "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝されました。

下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目してください。
ベクトルは方向と大きさを持っている矢印で表現されます。矢印の原点を移動することを考えます。
たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで持ってくると，その矢印は "QUEEN" を重なることが予想できます。
これがアナロジー問題の解放の直感的説明になります。

<center>
<img src="/assets/2013Mikolov_KingQueenFig.svg" style="width:49%">
</center>

上図右は同じ word2vec でできた空間に対して，統語関係 syntax を解かせた場合を示しています。
"KING" から "KINGS" へ向かう矢印を "QUEEN" まで持ってくると "QUEENS" に重なることが見て取れます。

このことから上図右の赤矢印で示されたベクトルは **複数形** への変換という統語情報，文法情報を表現しているとみなすことが可能です。

伝統的な言語学の知識では，統語構造と意味構造は別個に取り組む課題であると考えられてきました。
ところが word2vec が示す意味空間はそのような区別を考える必要があるのか否かについて問題を提起しているように思われます。

逆に一つのモジュールで処理することができるのであれば，分割して扱う意味があるのかどうかを考える切っ掛けになると考えます。

もう一つ面白い結果を下図に示します。下図は word2vec によって世界の国とその首都との関係を主成分分析 PCA で 2 次元に描画した図です。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%"><br/>
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かっておおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると大まかに我々の知識を表現できることを示唆していると考えます。

word2vec の実装には 2 種類あります。
どちらを使っても同じような結果を得ることができます。

- CBOW: Continous Bog of Words 連続単語袋
- skip-gram: スキップグラム

両者は反対の関係になります。下図を参照してください。

<center>
<img src="/assets/2013Mikolov_Fig1.svg" style="width:49%"><br/>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あるいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

すると上記例文は

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなります。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決めます。
- Mikolov が類推に用いたデータ例を下図に示しました。国名と対応する首都名，国名とその通貨名，などは意味的関係です。一方罫線下方は文法関係です。
形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などです。

<div class="figcenter">
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br/>
From Milolov (2013) Tab. 1
</div>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。

<!--<img src="/assets/2013Mikolov_skip-gram_cbow.svg" style="width:74%">
<img src="/assets/skip-gram.svg" style="width:74%">
<img src="/assets/skip-gram_cbow.svg" style="width:74%">
</center>-->

日本語のウィキペディアを用いた word2vec と NTT 日本語の語彙特性との関連に関心のある方は
[日本語 Wikipedia の word2vec 表現と語彙特性との関係, 近藤・浅川(2017)](/2017jpa_word2vec_NTTdict.pdf){:target="_blank"} をご覧ください

## さらなる蘊蓄 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティーとして与えることで精度が向上します。


## 発展 文章埋め込みモデルへ

単語の word2vec による表現は 3 層パーセプトロンの中間層の活性値として表現されます。

単語より大きなまとまりの意味表現，たとえば，文，段落，などの表現をどのように得るのかが問題になります。
ここで詳細には触れませんが，文表現ベクトルは各単語表現の総和であると考えるのがもっとも簡単な表現になります。
すなわち次文:


```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

の文表現を得るためには，各単語の word2vec 表現を足し合わせることが行われます。
ただし，単純に足し合わせたのでは BOW 単語袋表現と同じことですので，単語の順序情報が失われていることになります。
この辺りをどう改善すれば良いのかが議論されてきました。

### 文献

* <a target="_blank" href="https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf">word2vec オリジナル論文</a> 2013年 Mikolov
* <a target="_blank" href="https://fasttext.cc/">fastText</a> 高速文埋め込みモデル
* その発展 <a target="_blank" href="../2018jsai.pdf">浅川, 岡, 楠見 (2018)</a>
<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

<!--


<center>
<img src="https://www.tensorflow.org/images/linear-relationships.png" style="width:84%"><br>
<p algin="left" style="width:74%">
Source: <https://www.tensorflow.org/tutorials/representation/word2vec>
</p>
</center>

- <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論の蘊蓄</a>
-->

#### 埋め込みベクトルの応用例

<div class="figcenter">
<img src="/assets/2008Mitchell_fig1ja.svg" style="width:49%"><br/>
<div class="figcaption" style="text-align: left;width: 66%; background-color: cornsilk;">
Mitchell2008 図 1. 任意の名詞刺激に対するfMRI活性化を予測するモデルの形式。
fMRI の活性化は、2段階 プロセスで予測される。
第 1 段階では，入力刺激語の意味を，典型的な単語使用を示す大規模なテキストコーパスから値を抽出した中間的な意味的特徴の観点から符号化する。
第 2 段階では，これらの中間的な意味的特徴のそれぞれに関連する fMRIシグネチャ の線形結合として，fMRI 画像を予測する。
<!--Form of the model for predicting fMRI activation for arbitrary noun stimuli.
fMRI activation is predicted in a two-step process.
The first step encodes the meaning of the input stimulus word in terms of intermediate semantic features whose values are extracted from a large corpus of text exhibiting typical word use.
The second step predicts the fMRI image as a linear combination of the fMRI signatures associated with each of these intermediate semantic features. -->
</div></div>

# 1. 単語埋め込みモデル word2vec

* ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示した (Mikolov+2013)。
* Word2vec は実装に 2 種類ある。それぞれ **CBOW** と **skip-gram** と命名されている。
* "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝された。
* 下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目。
ベクトルは方向と大きさを持っている矢印で表現される。
このとき，矢印の原点を移動することを考える。
たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで持ってくると，その矢印は "QUEEN" を重なることが予想できる。
これがアナロジー問題の解放の直感的説明である。
* word2vec の実装には 2 種類あります。どちらを使っても同じような結果を得ることができます。
    1. CBOW: Continous Bog of Words 連続単語袋
    2. skip-gram: スキップグラム

両者は反対の関係になrる。下図を参照。

<center>
<img src="/assets/2013Mikolov_Fig1.svg" style="width:49%"><br>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あ
るいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。

```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなる。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決める。
- Mikolov が類推に用いたデータ例を下図に示した。
国名と対応する首都名，国名とその通貨名，などは意味的関係である。
一方罫線下方は文法関係である。形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などである。


### 文献

* [word2vec オリジナル論文, Mikolov2013](https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf)
* [fasttext高速版](https://fasttext.cc/)
* [その発展 浅川, 岡, 楠見 (2018)](../2018jsai.pdf)
* [日本語 Wikipedia の word2vec 表現と語彙特性の関係 近藤，浅川 (2017)](../2017jpa_word2vec_NTTdict.pdf)

<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

## 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティー
として与えることで精度が向上します。

<center>
<img src="/assets/2013Mikolov_Tab1.svg" style="width:49%"><br>
From Milolov (2013) Tab. 1
</center>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，
異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画して
みれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば
良いのかについて示唆に富むと考えます。

<center>
<img src="/assets/2013Mikolov_FigCountries.svg" style="width:49%">
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かって
おおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると
大まかに我々の知識を表現できることを示唆していると考えます。
