---
title: 第05回 2024年度開講 駒澤大学 人工知能 I
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
<!-- <img src="/assets/header_logo.png" sytle="width:09%"> -->
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 17/May/2024<br/>
Appache 2.0 license<br/>
</div>

## 実習ファイル

* [顔検出デモ 簡略版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0517face_detection_demo.ipynb){:target="_blank"}

## 人工知能の哲学的問題

1. [記号接地問題 wikipedia](https://en.wikipedia.org/wiki/Symbol_grounding_problem) (シンボルグラウンディング問題 symbol grounding problem)
ステファン・ハナッドによって提唱された問題。
接地が意味の必要条件かどうかという問いに答えるには，記号接地問題の定式化が必要である： 記号接地問題とは「形式的記号体系の意味解釈を，頭の中にあると仮定されている意味に依拠するのではなく，システムに内在する，他の無意味な記号以外のものに帰着させるかという問題。<!--[2]。-->

2. [中国語の部屋 chinese room](https://en.wikipedia.org/wiki/Chinese_room) 哲学者 ジョン・サール (John Searle) によって提唱された問題
3. [身体性 embodiment](https://en.wikipedia.org/wiki/Embodiment) 心理学的および生物学的システムを，心と体をひとつの実体としてとらえる総合的な方法でモデル化すること，知的行動の一般原則の共通セットを形成すること，そして制御された環境におけるロボット・エージェントの実験的使用である。

<!--## [記号接地問題 wikipedai](https://en.wikipedia.org/wiki/Symbol_grounding_problem) The symbol grounding problem-->

<!-- 最近，心の純粋な記号モデルの範囲と限界について，また認知モデルにおけるコネクショニズムの適切な役割について，多くの議論がなされている。
本稿では，記号接地問題，すなわち，形式的な記号システムの意味解釈を，頭の中の意味に寄生 parasitic させるのではなく，システムに内在させるにはどうすればよいのか，について論じる。
無意味な記号トークンの意味は，その（恣意的な）形状のみに基づいて操作されるが，どのようにして他の無意味な記号以外に根拠を与えることができるのだろうか？
この問題は，中国語/中国語辞書だけで中国語を学ぼうとするのに似ている。
解決策の候補をスケッチしてみよう： 記号表現は，2 種類の非記号表現，すなわち -->

<!-- 1) アイコン的表現: 遠方の物体や事象の近位感覚投影の類似である，
2) カテゴリー的表現: 物体や対象のカテゴリ不変の特徴を感覚投影から選び出された，学習された，生得的な特徴検出器。
基礎的な記号は，(非記号的) カテゴリの表現に基づく。
基礎的記号は，これらの（非記号的な）カテゴリー表現に基づいて割り当てられた，物体や出来事のカテゴリーの名前である。
3) 高次記号表現は，これらの基礎的記号に基づき (設置する)，カテゴリーのメンバーシップ関係（例：X は Z でり，ゆえに Y である）を記述する記号列で構成される。
コネクショニズムは，カテゴリー表象の根底にある不変的な特徴を学習するメカニズムとして自然な候補の 1 つであり，それによって名前を，それが表す遠位対象の近位投影に結びつける。
このようにコネクショニズムは，純粋に記号的なモデリングに匹敵するというよりは，非記号的／記号的なハイブリッドな心のモデルにおける補完的な構成要素として見ることができる。
しかし，このようなハイブリッドモデルには自律的な記号モジュールは存在しない。
記号機能は，カテゴリー名が感覚表象にボトムアップで接地された結果として，本質的に専用の記号システムとして現れるだろう。
記号操作は，記号トークンの恣意的な形だけでなく，記号トークンが根拠とするアイコンやカテゴリー不変量の非恣意的な形によっても支配されるだろう。 -->
<!-- There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling.
This paper describes the symbol grounding problem: How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads?
How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols?
The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone.
A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds:
(1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and
(2) categorical representations, which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections.
Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations.
Higher-order
 (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., An X is a Y that is Z).
Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for.
In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling.
Such a hybrid model would not have an autonomous symbolic module, however; the symbolic functions would emerge as an intrinsically dedicated symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations.
Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded. -->

<!-- Stevan Harnad は 1990年の論文によると、記号接地問題の他のいくつかの定義を暗黙のうちに表現している ([Harnad1990](https://web-archive.southampton.ac.uk/cogprints.org/3106/))。 -->
<!-- According to his 1990 paper, Stevan Harnad implicitly expresses a few other definitions of the symbol grounding problem:[2]-->

<!-- 1. 記号接地問題とは，「...形式的な記号システムの意味解釈を...どのようにするかという問題である。...頭の中の意味に寄生するのではなく，システムに内在する…」 「...他の無意味な記号以外では…」
2. 記号接地問題とは，「...その（恣意的な）形状のみに基づいて操作される無意味な記号トークンの意味…」が，「...他の無意味な記号以外の何かに...」どのように根拠づけられるかという問題である。
3. 「...記号の根拠付けの問題は，サール（1980）の有名な「中国の部屋の議論」において、内在的意味（あるいは「意図性」）の問題と呼ばれている。」
4. 記号接地問題とは、「...記号と記号のメリーゴーランドからどうやって降りることができるか」という問題である。 -->

<!--1. The symbol grounding problem is the problem of how to make the "...semantic interpretation of a formal symbol system..." "... intrinsic to the system, rather than just parasitic on the meanings in our heads..." "...in anything but other meaningless symbols..."
2. The symbol grounding problem is the problem of how "...the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes..." can be grounded "...in anything but other meaningless symbols."
3. "...the symbol grounding problem is referred to as the problem of intrinsic meaning (or 'intentionality') in Searle's (1980) celebrated 'Chinese Room Argument'"
4. The symbol grounding problem is the problem of how you can "...ever get off the symbol/symbol merry-go-round..."-->

<!--To answer the question of whether or not groundedness is a necessary condition for meaning, a formulation of the symbol grounding problem is required: The symbol grounding problem is the problem of how to make the "...semantic interpretation of a formal symbol system..." "... intrinsic to the system, rather than just parasitic on the meanings in our heads..." "...in anything but other meaningless symbols".[2] -->

## 復習

内積は，<span style="color:teal">$\overrightarrow{x}$</span> と表記している高等学校の教科書が多い。
駄菓子菓子，ここでは，太文字を使って次のように表記する <span style="color:blue">$\mathbf{x}$</span>

相関係数を，二次元上の n 個の点からなる散布図から計算される量であると考えるのが，統計学の伝統であった。
具体的には，x から y への回帰係数と y から x への回帰係数の幾何平均に相当する。

一方，先週取り上げたベクトルを用いた解釈では，n 次元上の二本のベクトル x と y のなす角の余弦 (コサイン) を相関係数と呼ぶ

<!-- 内積の定義は，以下の通りである:
$$\tag{2:内積の定義}
\begin{align}
\left(\mathbf{x}\cdot\mathbf{y}\right) & =\left|\mathbf{x}\right|\left|\mathbf{y}\right|\cos\theta=x_1y_1+x_2y
_2+\ldots+x_{n}y_{n}\\
                                       & =\sum_{i=1}^{n}x_{i}y_{i}\\
\end{align}$$

(2) 式を変形すると
$$
\cos\theta=\frac{\mathbf{x}^{\top}\mathbf{y}}{\left|\mathbf{x}\right| \left|\mathbf{y}\right|}
$$
を得る。
すなわち (ピアソンの積率) 相関係数とは，2 つの変量 $x$ と $y$ とを多次元ベクトルと考え，それぞれの平均を引いたベクトル間のなす角の余弦 ($\cos) である。
余弦 $\cos$ の値は $[-1,1]$ であることから，相関係数の範囲は -1 から +1 までの値を取ることが分かる。

加えて，相関係数が 1 であとは，2 本のベクトルが一致することを意味し，相関係数が 0 であるとは，2 本のベクトルが直交することを表す，
相関係数が -1 とは，2 本のベクトルが反対方向であることを表している。 -->

<!-- <div style="background-color:lightgray;width:66%;text-align:left;"> -->

#### ピアソンの積率相関係数と平均偏差ベクトル

$n$ 個のデータからなる $n$ 次元ベクトル $\mb{x}=\Brc{x_1, x_2,\ldots, x_n}^{\top}$, $\mb{y}=\Brc{y_1, y_2, \ldots, y_n}^{\top}$ の個々の要素から平均値を引いたベクトルを平均偏差ベクトルという。

$$
\pmatrix{x_1-\Bar{x}\cr x_2-\Bar{x}\cr\vdots\cr x_n-\Bar{x}},\qquad
   \pmatrix{y_1-\Bar{y}\cr y_2-\Bar{y}\cr\vdots\cr y_n-\Bar{y}},
$$

すべての要素が $1$ であるベクトル ${\bf 1}=(\;\overbrace{1,1,\ldots,1}^{n個}\;)^{\top}$ によって張られる部分空間 $L\Brc{\mb{1}}$ への射影行列をつくると以下のようになる:

$$
\mb{P}=\mathbf{1}\Brc{\mb{1}^{\top}\mb{1}}^{-1}{\mb{1}^{\top}}=\pmatrix{
                        1/n    & \cdots & 1/n    \cr
                        \vdots & \ddots & \vdots \cr
                        1/n    & \cdots & 1/n    \cr}
$$

この射影行列に右から $\mb{y}$ を乗ずると, $\mb{Py}=\Brc{\Bar{y},\Bar{y},\ldots,\Bar{y}}^{\top}$ となる。

したがって, 平均偏差ベクトルは次式で与えられる: $\mb{y}-\Bar{y}\mb{1}=\mb{y}-\mb{Py}=\Brc{\mb{I}-\mb{P}}\mb{y}$

すなわち, 平均偏差ベクトルとは $L\Brc{\mathbf{1}}$ の補空間への射影ベクトルである。

この平均偏差ベクトルの長さの 2 乗 $\Norm{x}^2$ をデータ数で割ったものは，以下のとおり:

$$
\frac{1}{n}\Norm{x}^2=\frac{1}{n}\Brc{\mb{x},\mb{x}}=\frac{1}{n}\sum_1^n\Brc{x-\Bar{x}}^2=s_x^2,
$$

平均偏差ベクトルをもちいると次式 $x$ と $y$ との (ピアソンの積率) 相関係数 $r_{xy}$ の関係式を得る:

$$\begin{aligned}
r_{xy} &= \frac{S_{xy}}{S_xS_y} \left(=\frac{\text{$x$ と $y$ との共分散}}{\text{$x$ の標準偏差}\times \text{$y$ の標準偏差}}
      = \frac{\sum_i\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sqrt{\sum_i\left(x_i-\bar{x}\right)^{2}}\,\sqrt{\sum_i\left(y_i-\bar{y}\right)^{2}}}\right)\\
&=\frac{\IP{\mb{x}}{\mb{y}}}{\Norm{\mb{x}}\Norm{\mb{y}}}
\left(=
    \frac{\text{ベクトル $\mb{x}$ とベクトル $\mb{y}$ との内積}}
         {\text{ベクトル $\mb{x}$ の長さ (ノルム) $\times$ ベクトル $\mb{y}$ の長さ(ノルム)}}
\right)\\
&=\cos\theta
\end{aligned}$$


* **相関係数が 1 ($r_{xy}=+1$):** 2 本のベクトルが一致することを意味し，
* **相関係数が 0 ($r_{xy}=0$):** 2 本のベクトルが直交することを表す，
* **相関係数が -1 ($r_{xy}=-1$):** 2 本のベクトルが反対方向であることを表している


# 回帰 Regression

$$
y = ax + b
$$

誤差は，次式となる:
$$
\epsilon = y - \left(ax + b\right)
$$

ここで，誤差の分散 (誤差ベクトルの自分自身との内積)
$$
\mathbf{\epsilon}^{\top}\mathbf{\epsilon}=\left\{\mathbf{y}-\left(a\mathbf{x}-\mathbf{b}\right)\right\}^{\top}\left\{\mathbf{y}-\left(a\mathbf{x}-b\right)\right\}
$$

<div class="figcenter">
<img src="/2024assets/2024_0514Pythagoras.svg" style="width:33%;">
</div>

すなわち，単回帰の $y = ax + b$ における $a$ の求め方とは，ベクトル $\mathbf{y}$ からベクトル $\mathbf{x}$ へ垂線を下ろした場合，その距離が最も短くなる。
そのような場合の $a$ は，ピタゴラスの定理，あるいは三平方の定理が成り立つので，$\left|a\mathbf{x}\right|^{2}+\left|\epsilon\right|^{2}=\left|\mathbf{y}\right|^2$ となる。

### 基本仮定


$$
\mb{y}=\mb{X\beta}+\mb{\epsilon}\qquad(\mb{X}=\{\mb{x}_1,\mb{x}_2,\ldots,\mb{x}_p\}).
$$

$\mb{y}$ を従属変数,
$\mb{X}=\{\mb{x}_1,\mb{x}_2,\ldots,\mb{x}_p\}$ を独立変数,
$\mb{\beta}$ を回帰係数,
$\mb{\epsilon}$ を誤差項または残差 という.

* 仮定 1: $E\Brc{\mb{\epsilon}}={\bf{0}}$.
* 仮定 2: $V\Brc{\mb{\epsilon}}=\sigma^2\mb{I}$.
* 仮定 3: $\Rank{\Brc{\mb{X}}}=p$.
* 仮定 4: 誤差項 $\epsilon_i$ はそれぞれ独立に $N\Brc{0,\sigma^2}$ に従う.

## 解法

### ベクトル幾何学的解法

<div class="figcenter">
<img src="/2024assets/2024_0517Projection_concept.svg" style="width:44%;">
</div>

誤差項 $\mb{\epsilon}$ は回帰モデルによって説明されない部分である。
この $\mb{\epsilon}$ のノルムを最小にするよに $\mb{\beta}$ を定める。
すなわち $\NSQ{\epsilon}=\Norm{\mb{y}-\mb{X\beta}}^2\rightarrow\min$.
これは $L\Brc{X}$ 上へ $\mb{y}$ を射影することに相当する。
この射影は $\Prj{X}\mb{y}$ によって与えられる。

これによって
$\Hat{\mb{y}}=\mb{X}\Hat{\mb{\beta}}=\Prj{X}\mb{y}$.

すなわち
$$
\Hat{\mb{\beta}}=\RegP{X}{y}.
$$

$\Hat{\mb{\beta}}$ を $\mb{\beta}$ の **最小 $2$ 乗推定量** という。

$\mb{X}$ によって張られる線形部分空間 $L\Brc{\mb{X}}$ への射影行列 $\Prj{X}$ を $\mb{P}$ と表現すれば回帰式は

$$
\mb{y}=\mb{Py}+\mb{\epsilon},
$$

となる。

さらに $\mb{\epsilon}=\mb{y}-\mb{Py}=\Brc{\mb{I}-\mb{P}}\mb{y}$,
すなわち, 回帰式を射影行列による独立変数ベクトルの分解

$$
\mb{y}=\mb{Py}+\Brc{\mb{I}-\mb{P}}\mb{y},
$$
と考えることができる。


### 別解 1. 最小 $2$ 乗解

誤差の $2$ 乗和 $\Norm{\mb{\epsilon}}^2$ を最小にする。
$$\begin{aligned}
\Norm{\mb{\epsilon}}^2 &= \Brc{\mb{y}-\mb{X\beta}}'\Brc{\mb{y}-\mb{X\beta}}\\
                       &= \mb{y'y}-2\mb{\beta'X'y}+\mb{\beta}'\Brc{\mb{X'X}}\mb{\beta},
\end{aligned}$$

だから
$$
\frac{\partial \Norm{\mb{\epsilon}}^2}{\partial \mb{\beta}} = -2\mb{X}^{\top}\mb{y}+2\mb{X}^{\top}\mb{X\beta} = 0
$$

$$
\mb{X}^{\top}\mb{X\beta} = \mb{X}^{\top}\mb{y}
$$

これを解いて, $\Hat{\mb{\beta}}=\RegP{X}{y}$.

### 別解 2.

誤差ベクトルと予測ベクトル $\Hat{\mb{y}}=\mb{X\beta}$ とは直交するからその内積は $0$ である。

$$\begin{eqnarray*}
\IP{\Brc{\mb{X\beta}}}{\Brc{\mb{y}-\mb{X\beta}}}&=&0\\
\mb{\beta'X'y}-\mb{\beta'X'X\beta}&=&0.
\end{eqnarray*}$$

これを解いて, $\Hat{\mb{\beta}}=\RegP{X}{y}$.


## 固有顔 Eigenfaces と [フィッシャー顔 Fisherfaces](http://www.scholarpedia.org/article/Fisherfaces)

PCA (Princple Component Analysis: 主成分分析) とは，データ数，一つのデータあたりの個数を m とすると，データは
n 行 m 列の行列となる。
オリベッティ顔データセットであれば $\mathbf{X}$ は 400 行 $\times$ 4096 列となる。

## フィッシャー顔<!-- ## 2.3 Fisherfaces-->

コンピュータ・ビジョン，パターン認識，機械学習における重要な問題は，目前の課題に適切なデータ表現を定義することである。
<!-- A key problem in computer vision, pattern recognition and machine learning is to define an appropriate data representation for the task at hand.-->

入力データを表現する 1 つの方法は，データの分散の大部分を表す部分空間を見つけることである。
これは主成分分析 (PCA) を用いることで得られる。
顔画像に PCA を適用すると，固有顔 eigenfaces の集合が得られる。
これらの固有顔は，学習データの共分散行列の最大の固有値に関連する固有ベクトルである。
こうして見つかった固有ベクトルは，最小二乗 (LS) 解に対応する。
これは，データの分散を確実に維持しながら，サンプルベクトル内の元の特徴 (次元) 間の不要な既存の相関を排除するため，データを表現するための強力な方法である。
<!--One way to represent the input data is by finding a subspace which represents most of the data variance. This can be obtained with the use of Principal Components Analysis (PCA).
When applied to face images, PCA yields a set of eigenfaces.
These eigenfaces are the eigenvectors associated to the largest eigenvalues of the covariance matrix of the training data.
The eigenvectors thus found correspond to the least-squares (LS) solution.
This is indeed a powerful way to represent the data because it ensures the data variance is maintained while eliminating unnecessary existing correlations among the original features (dimensions) in the sample vectors. -->

目的が表現ではなく分類である場合，LS 解は最も望ましい結果をもたらさないかもしれない。
このような場合，同じクラスのサンプルベクトルを特徴表現の一点に写像し，異なるクラスのサンプルベクトルを可能な限り遠くに写像する部分空間を見つけたい。
この目標を達成するために導き出された手法は，判別分析 (DA) として知られている。
<!-- When the goal is classification rather than representation, the LS solution may not yield the most desirable results.
In such cases, one wishes to find a subspace that maps the sample vectors of the same class in a single spot of the feature representation and those of different classes as far apart from each other as possible.
The techniques derived to achieve this goal are known as discriminant analysis (DA).-->

最も有名な DA が線形判別分析 (LDA) であり，これは 1936 年に R.A.Fisher によって提案されたアイデアから派生している。
LDA が顔画像の集合の部分空間表現を見つけるために使用されるとき，その空間を定義する結果の基底ベクトルは，フィッシャー顔 Fisherfaces として知られている。
<!--The most known DA is Linear Discriminant Analysis (LDA), which can be derived from an idea suggested by R.A. Fisher in 1936.
When LDA is used to find the subspace representation of a set of face images, the resulting basis vectors defining that space are known as Fisherfaces. -->


線形判別分析は，統計学者である R.A.フィッシャー卿によって発明された手法である。(Fisher1936)。
1936 年の論文では，HAD の例題にもなっている 3 種類のアヤメの分類に適用した。
<!-- 分類学上の問題で複数の測定値を使用すること [8]。
しかし，主成分分析 (PCA) がこれほどうまくいったのに，なぜ別の次元削減法が必要なのだろうか？ -->

PCA は，データの全分散を最大化する特徴の線形結合を見つける。
これは明らかにデータを表現する強力な方法だが，クラスを考慮しないため，成分を捨てるときに多くの識別情報が失われる可能性がある。
<!-- 分散が外部ソースによって生成される状況を想像してみよ。 -->
PCA によって同定された成分は，必ずしも識別情報を全く含んでいないため，投影されたサンプルは一緒に塗りつぶされ，分類は不可能になる。
クラス間を最もよく分離する特徴の組み合わせを見つけるために，線形判別分析は **クラス間のばらつき，群間分散** と **クラス内のばらつき，郡内分散** の比率を最大化する。
考え方は単純で，同じクラスは互いに密に集まり，異なるクラスは互いにできるだけ離れるべきである。
このことを応用したものが，フィッシャー顔 (Fihserfaces) (Belhumeur+1997)，[3] である。
<!--The Linear Discriminant Analysis was invented by the great statistician Sir R. A. Fisher, who successfully used it for classifying flowers in his 1936 paper.
The use of multiple measurements in taxonomic problems [8].
But why do we need another dimensionality reduction method, if the Principal Component Analysis (PCA) did such a good job?
The PCA finds a linear combination of features that maximizes the total variance in data.
While this is clearly a powerful way to represent data, it doesn’t consider any classes and so a lot of discriminative information may be lost when throwing components away.
Imagine a situation where the variance is generated by an external source, let it be the light.
The components identified by a PCA do not necessarily contain any discriminative information at all, so the projected samples are smeared together and a classification becomes impossible.
In order to find the combination of features that separates best between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to within-classes scatter.
The idea is simple: same classes should cluster tightly together, while different classes are as far away as possible from each other.
This was also recognized by Belhumeur, Hespanha and Kriegman and so they applied a Discriminant Analysis to face recognition in [3]. -->


<div class="figcenter">
<img src="/2024assets/2012Wagner_Face_Recognition_fig1.svg" style="width:49%;">
</div>

### アルゴリズムの説明<!-- ### 2.3.1 Algorithmic Description-->

$\mathbf{X}$ を，$c$ 個のクラスから抽出されたサンプルを持つランダム・ベクトルとする。
<!--Let X be a random vector with samples drawn from c classes: -->
$$\tag{8}
\mathbf{X}= \left\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{c}\right\}.
$$

$$\tag{9}
\mathbf{x}_i =\left\{x_1,x_2,\ldots,x_n\right\}.
$$

分散行列 S_B と S_W は次のように計算される：<!-- The scatter matrices S_B and S_W are calculated as: -->
$$
\begin{align}
S_B &= \sum_{i=1}^{c} N_{i}\left(\mu_i-\mu\right)\left(\mu_i-\mu\right)^{\top}\tag{10}\\
S_W &= \sum_{i=1}^{c}\sum_{x_j\in X_i}\left(x_j-\mu_j\right)\left(x_j-\mu_j\right)^{\top},\tag{11}
\end{align}
$$
ここで，$\mu$ は全平均である:<!--where mu is the total mean:-->
$$
\mu = \frac{1}{N}\sum_{i=1}^{N}x_i.
$$

かつ，$\mu_i$ は群平均 $i\in\left\[1,\ldots,c\right\]$:

$$
\mu_i = \frac{1}{\left|X_i\right|}\sum_{x_j\in X_i}x_j,
$$

フィッシャーの古典的アルゴリズムは，クラス分離可能性基準を最大化する射影 W を探す：
<!-- Fisher’s classic algorithm now looks for a projection W, that maximizes the class separability criterion: -->

$$
W_{\text{opt}}=\arg\max_{W}\frac{\left|W^{\top}S_{B}W\right|}{\left|W^{\top}S_{W}W\right|}
$$

[3] に従えば，この最適化問題の解は一般固有値問題を解くことによって与えられる：
<!-- Following [3], a solution for this optimization problem is given by solving the General Eigenvalue Problem: -->

$$\begin{aligned}
\mathbf{S}_{B}\mathbf{v}_i       &= \lambda_{i} \mathbf{S}_{W}\mathbf{v}_{i}\\
\mathbf{S}_{W}^{-1}\mathbf{S}_{B}\mathbf{v}_{i}&= \lambda_{i} \mathbf{v}_{i}
\end{aligned}$$

解決すべき問題が 1 つ残っている： $S_W$ のランクは最大でも $(N-c)$ であり，$N$ 個のサンプルと c 個のクラスが存在する。
パターン認識問題では，サンプル数 $N$ は入力データの次元 (画素数) より小さいことがほとんどなので，分散行列 $S_W$ は特異行列になる ([2]を参照)。
[3] では，データに対して主成分分析を実行し，サンプルを (N-c) 次元空間に投影することでこれを解決した。
その後，$S_W$ が特異でなくなったので，線形判別分析が縮小データに対して実行される。
最適化問題は次のように書き換えられる：
<!-- There’s one problem left to solve: The rank of S_W is at most (N−c), with N samples and c classes.
In pattern recognition problems the number of samples N is almost always smaller than the dimension of the input data (the number of pixels), so the scatter matrix S_W becomes singular (see [2]).
In [3] this was solved by performing a Principal Component Analysis on the data and projecting the samples into the (N−c)-dimensional space.
A Linear Discriminant Analysis was then performed on the reduced data, because S_W isn’t singular anymore.
The optimization problem can be rewritten as:-->

$$\begin{aligned}
W_{opt} &= \arg\max_{W}\left|W^{\top}S_{T}W\right|\\
W_{fld} &= \arg\max_{W}\frac{\left|W^{\top}W_{pca}^{\top}S_{B}W_{pca}W\right|}{\left|W^{\top}W_{pca}^{\top}S_{W}W_{pca}W\right|}
\end{aligned}$$

サンプルを $(c-1)$ 次元空間に投影する変換行列 $W$ は次式で与えられる：
<!-- The transformation matrix $W$, that projects a sample into the (c−1)-dimensional space is then given by: -->

$$\tag{18}
W=W_{fld}^{\top}W_{pca}^{\top}
$$

最後の注意：S_W と S_B は対称行列だが，2 つの対称行列の積は必ずしも対称ではない。
したがって，一般的な行列の固有値ソルバを利用する必要がある。
OpenCV の cv::eigen は，現在のバージョンでは対称行列に対してのみ動作する。
非対称行列に対しては固有値と特異値は等価ではないので，特異値分解 (SVD) も利用でき無い。
<!-- One final note: Although S_W and S_B are symmetric matrices, the product of two symmetric matrices is not necessarily symmetric.
So you have to use an eigenvalue solver for general matrices.
OpenCV’s cv::eigen only works for symmetric matrices in its current version; since eigenvalues and singular values aren’t equivalent for non-symmetric matrices you can’t use a Singular Value Decomposition (SVD) either. -->

# 回帰

## 基本仮定

$$
\mathbf{y}=\mathbf{X\beta}+\mathbf{\epsilon}\qquad(\mathbf{X}=\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_p\}).
$$

$\mathbf{y}$ を従属変数,
$\mathbf{X}=\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_p\}$ を独立変数,
$\mathbf{\beta}$ を回帰係数，$\mathbf{\epsilon}$ を誤差項または残差 という。

* 仮定 1: $\mathbb{E}\left(\mathbf{\epsilon}\right)=\mathbf{0}$.
* 仮定 2: $\mathbb{V}\left(\mathbf{\epsilon}\right)=\sigma^2\mathbf{I}$.
* 仮定 3: $\text{rank}\;\left(\mathbf{X}\right)=p$.
* 仮定 4: 誤差項 $\epsilon_i$ はそれぞれ独立に $N\left(0,\sigma^2\right)$ に従う。

## 解法

### ベクトル幾何学的解法

誤差項 $\mathbf{\epsilon}$ は回帰モデルによって説明されない部分である。
この $\mathbf{\epsilon}$ のノルムを最小にするように $\mathbf{\beta}$ を定める。
すなわち $\left|\mathbf{\epsilon}\right|^2=\left|\mathbf{y}-\mathbf{X\beta}\right|^2\rightarrow\min$。

これは $L\left(X\right)$ 上へ $\mathbf{y}$ を射影することに相当する。
この射影は $\mathbf{X}\left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{y}$ によって与えられる。

これによって

$$
\hat{\mathbf{y}}=\mathbf{X}\hat{\mathbf{\beta}}=\mathbf{X}\left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{y}
$$

すなわち
$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$
$\hat{\mathbf{\beta}}$ を $\mathbf{\beta}$ の **最小 $2$ 乗推定量** という。

$\mathbf{X}$ によって張られる線形部分空間 $L\left(\mathbf{X}\right)$ への射影行列 $\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'$ を $\mathbf{P}$ と表現すれば回帰式は
$$
\mathbf{y}=\mathbf{Py}+\mathbf{\epsilon},
$$
となる。

さらに
$\mathbf{\epsilon}=\mathbf{y}-\mathbf{Py}=\left(\mathbf{I}-\mathbf{P}\right)\mathbf{y}$，すなわち，回帰式を射影行列による独立変数ベクトルの分解
$$
\mathbf{y}=\mathbf{Py}+\left(\mathbf{I}-\mathbf{P}\right)\mathbf{y},
$$
と考えることができる。

### 別解 1. 最小 $2$ 乗解

誤差の $2$ 乗和 $\left|\mathbf{\epsilon}\right|^2$ を最小にする。

$$\begin{aligned}
\left|\mathbf{\epsilon}\right|^{2} &= \left(\mathbf{y}-\mathbf{X\beta}\right)'\left(\mathbf{y}-\mathbf{X\beta}\right)\\
&=\mathbf{y'y}-2\mathbf{\beta'X'y$} + \mathbf{\beta}'\left(\mathbf{X'X}\right)\mathbf{\beta},
\end{aligned}$$

だから

$$\begin{aligned}
\frac{\partial \left|\mathbf{\epsilon}\right|^2}{\partial \mathbf{\beta}} =
-2\mathbf{X'y}+2\mathbf{X'X\beta} &=& 0\\
\mathbf{X'X\beta} &=& \mathbf{X'y}
\end{aligned}$$

これを解いて,

$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$

## 別解 2.

誤差ベクトルと予測ベクトル $\hat{\mathbf{y}}=\mathbf{X\beta}$ とは 直交するからその内積は $0$ である。

$$\begin{aligned}
\left(\left(\mathbf{X\beta}\right)\cdot\left(\mathbf{y}-\mathbf{X\beta}\right)\right) &=& 0\\
\mathbf{\beta'X'y}-\mathbf{\beta'X'X\beta} &=& 0.
\end{aligned}$$

これを解いて，
$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$

<div class="memo">

**平均偏差ベクトル**:

$n$ 個のデータからなる $n$ 次元ベクトル
$\mathbf{x}=\left(x_1, x_2,\ldots, x_n\right)'$,
$\mathbf{y}=\left(y_1, y_2, \ldots, y_n\right)'$
の個々の要素から平均値を引いたベクトルを平均偏差ベクトルという。
$$
\pmatrix{x_1-\overline{x}\cr x_2-\overline{x}\cr\vdots\cr x_n-\overline{x}},\qquad
\pmatrix{y_1-\overline{y}\cr y_2-\overline{y}\cr\vdots\cr y_n-\overline{y}},
$$
この平均偏差ベクトルの意味を考える. すべての要素が $1$ であるベクトル
$\mathbf{1}=(\;\overbrace{1,1,\ldots,1}^{n個}\;)'$
によって張られる部分空間 $L\left({\bf 1}\right)$
への射影行列をつくると
$\mathbf{P}=\mathbf{1}\left(\mathbf{1}'\mathbf{1}\right)^{-1}\mathbf{1}'=\pmatrix{
             1/n    & \cdots & 1/n    \cr
             \vdots & \ddots & \vdots \cr
             1/n    & \cdots & 1/n    \cr}$
となる。

この射影行列に右から $\mathbf{y}$ を乗ずると,
$\mathbf{Py}=\left(\overline{y},\overline{y},\ldots,\overline{y}\right)'$ となる。

したがって，平均偏差ベクトルは，
$$
\mathbf{y}-\overline{y}\mathbf{1} = \mathbf{y}-\mathbf{Py} = \left(\mathbf{I}-\mathbf{P}\right)\mathbf{y}
$$
すなわち, 平均偏差ベクトルとは $L\left(\mathbf{1}\right)$ の補空間への射影ベクトルである。
この平均偏差ベクトルの長さの 2 乗
$\left|x\right|^2$ をデータ数で割ったものは

$$
\frac{1}{n}\left|x\right|^2=\frac{1}{n}\left(\mathbf{x},\mathbf{x}\right)=\frac{1}{n}\sum_1^n\left(x-\overline{x}\right)^2=s_x^2,
$$

平均偏差ベクトルをもちいると $x$ と $y$ との相関係数は

$$r_{xy}=\frac{S_{xy}}{S_xS_y}
=\frac{\left(\mathbf{x}\cdot\mathbf{y}\right)}{\left|\mathbf{x}\right|\left|\mathbf{y}\right|}=\cos\theta_{xy}
$$
</div>


### 単回帰

説明変数, 被説明変数とも $1$ 個の場合を**単回帰**という. 回帰モデルは $\mathbf{y}=\mathbf{x}\beta+\epsilon$ である。

### ベクトル幾何学的解法

平均偏差ベクトルを用いれば

$$
\hat{\beta} = \left(\mathbf{x}^{\top}\mathbf{x}\right)^{-1}\mathbf{x}^{\top}\mathbf{y} =
\frac{\sum\left(x-\overline{x}\right)\left(y-\overline{y}\right)}{\sum\left(x-\overline{x}\right)^2}
=\frac{S_{xy}}{S_x^2}.
$$

あるいは，平均偏差ベクトルを用いなくても，すべての要素が $1$ である $n$ 次元ベクトル $(\mathbf{1})$ をもちいて
$\mathbf{X}=\left(\mathbf{1},\mathbf{x}\right)^{\top}$ とする。

さらに, $\mathbf{\beta}=\left(\alpha,\beta\right)$ を用いて改めて回帰方程式を:

$$
\mathbf{y}=\mathbf{X\beta}+\epsilon
$$

とおく, このとき

$$\begin{aligned}
\hat{\beta} &= \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}\\
\end{aligned}$$


$$\begin{aligned}
&= \begin{pmatrix}\mathbf{1}^{\top1} & \mathbf{1}^{\top}\mathbf{x}\cr \mathbf{1}'\mathbf{x} & \mathbf{x'x}^{-1}\end{pmatrix}
    \begin{pmatrix}\mathbf{1}'\mathbf{y}\cr \mathbf{x'y}\end{pmatrix}\\
\end{aligned}$$

$$\begin{aligned}
&= \begin{pmatrix}n&\sum x\cr \sum x&\sum x^2\end{pmatrix}^{-1}
\begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
&= \frac{1}{n\sum x^2-\left(\sum x\right)^2}\pmatrix{\sum x^2 & -\sum x\cr -\sum x&n\end{pmatrix}
\begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
\end{aligned}$$

$$\begin{aligned}
&=\frac{1}{n^2S_x^2}
\begin{pmatrix}n\left(S_x^2+{\overline{x}}^2\right)&-n\overline{x}\cr
         -n\overline{X}               & n      \end{pmatrix}
         \begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
&=\frac{1}{S_x^2}\pmatrix{S_x^2+\overline{x}^2&-\overline{x}\cr-\overline{x}&1}
\begin{pmatrix}\overline{y} \cr S_{xy}+\overline{x}\overline{y}\end{pmatrix}\\
\end{aligned}$$


$$\begin{aligned}
&=\frac{1}{S_x^2}
\begin{pmatrix}
\overline{y}\left(S_x^2+\overline{x}^2\right)-\overline{x}\left(S_{xy}+\overline{x}\overline{y}\right)\cr
S_{xy}+\overline{x}\overline{y}-\overline{x}\overline{y}\end{pmatrix}.
\end{aligned}$$

ゆえに

$$
\begin{pmatrix}\alpha\cr\beta\end{pmatrix}=
\begin{pmatrix}\displaystyle\overline{y}-\frac{S_{xy}}{S_x^2}\overline{x}\cr \displaystyle\frac{S_{xy}}{S_x^2}\end{pmatrix}.
$$

つまり

$$\begin{aligned}
y_i & = \alpha + \beta x_i + \epsilon_i\\
    & = \overline{y} - \beta \overline{x} + \beta x_i + \epsilon_i,
\end{aligned}$$

あるいは,

$$
y_i-\overline{y}=\beta\left(x_i-\overline{x}\right)+\epsilon_i.
$$

データから平均値を引いた値の回帰と定数 $\alpha$ を含めた回帰は同じものである。
この場合，$\alpha=\overline{y}-\beta\overline{x}$ である。

### 最小 $2$ 乗解

$i$ 番目のデータを

$$\begin{aligned}
y_i        &=&\beta\; x_i + \alpha + \epsilon_i\qquad(i=1,2,\ldots,n)\\
y_i-\overline{y}&=&\beta\left(x_i-\overline{x}\right)+\epsilon_i\qquad(i=1,2,\ldots,n),
\end{aligned}$$

とする。
実測値 $y_i$ と予測値 $\beta x_i+\alpha$
との差の $2$ 乗 $\left(y_i-\beta x_i-\alpha\right)^2=\epsilon_i^2$
を全データについて加算した $2$ 乗和

$$
\sum_i^n\left(y_i-\beta x_i-\alpha\right)^2=\sum_i^n\epsilon_i^2=Q,
$$

を, 回帰パラメータ $\alpha$, $\beta$ についてそれぞれ偏微分して $0$ とおく,

$$\begin{aligned}
\displaystyle{\pmatrix{
\displaystyle\frac{\partial Q}{\partial\alpha}\cr
\displaystyle\frac{\partial Q}{\partial \beta}\cr}} & = &
 \pmatrix{
 -2\sum\left(y_i-\beta x_i-\alpha\right)\cr
 -2\sum x_i\left(y_i-\beta x_i-\alpha\right)\cr} &= \pmatrix{0\cr0\cr}\\
&& \pmatrix{
 \sum y_i    & -\beta\sum x_i   & -n\alpha        \cr
 \sum x_iy_i & -\beta\sum x_i^2 & -\alpha\sum x_i \cr} &= \pmatrix{0\cr0\cr}.
\end{aligned}$$

これを $\alpha$, $\beta$ について解くと,

$$
\alpha = \overline{y} - \beta \overline{x}.
$$

さらに，

$$\begin{aligned}
\sum x_iy_i-\beta\sum x_i^2 -\left(\overline{y}-\beta\overline{x}\right)n\overline{x} &= 0\\
\beta\left(\sum x_i^2 - n\left(\overline{x}\right)^2\right) &= \sum x_iy_i - n\overline{x}\overline{y}\\
\beta ns_x^2 &= ns_{xy} \\
\beta &= \frac{s_{xy}}{s_x^2}.
\end{aligned}$$
