---
title: 第05回 2024年度開講 駒澤大学 人工知能 I
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
<!-- <img src="/assets/header_logo.png" sytle="width:09%"> -->
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 17/May/2024<br/>
Appache 2.0 license<br/>
</div>

## 人工知能の哲学的問題

1. [記号接地問題 wikipedia](https://en.wikipedia.org/wiki/Symbol_grounding_problem) (シンボルグラウンディング問題 symbol grounding problem)
ステファン・ハナッドによって提唱された問題。
接地が意味の必要条件かどうかという問いに答えるには，記号接地問題の定式化が必要である： 記号接地問題とは「形式的記号体系の意味解釈を，頭の中にあると仮定されている意味に依拠するのではなく，システムに内在する，他の無意味な記号以外のものに帰着させるかという問題。<!--[2]。-->

2. [中国語の部屋 chinese room](https://en.wikipedia.org/wiki/Chinese_room) 哲学者 ジョン・サール (John Searle) によって提唱された問題
3. [身体性 embodiment](https://en.wikipedia.org/wiki/Embodiment) 心理学的および生物学的システムを，心と体をひとつの実体としてとらえる総合的な方法でモデル化すること，知的行動の一般原則の共通セットを形成すること，そして制御された環境におけるロボット・エージェントの実験的使用である。

<!--## [記号接地問題 wikipedai](https://en.wikipedia.org/wiki/Symbol_grounding_problem) The symbol grounding problem-->

<!-- 最近，心の純粋な記号モデルの範囲と限界について，また認知モデルにおけるコネクショニズムの適切な役割について，多くの議論がなされている。
本稿では，記号接地問題，すなわち，形式的な記号システムの意味解釈を，頭の中の意味に寄生 parasitic させるのではなく，システムに内在させるにはどうすればよいのか，について論じる。
無意味な記号トークンの意味は，その（恣意的な）形状のみに基づいて操作されるが，どのようにして他の無意味な記号以外に根拠を与えることができるのだろうか？
この問題は，中国語/中国語辞書だけで中国語を学ぼうとするのに似ている。
解決策の候補をスケッチしてみよう： 記号表現は，2 種類の非記号表現，すなわち -->

<!-- 1) アイコン的表現: 遠方の物体や事象の近位感覚投影の類似である，
2) カテゴリー的表現: 物体や対象のカテゴリ不変の特徴を感覚投影から選び出された，学習された，生得的な特徴検出器。
基礎的な記号は，(非記号的) カテゴリの表現に基づく。
基礎的記号は，これらの（非記号的な）カテゴリー表現に基づいて割り当てられた，物体や出来事のカテゴリーの名前である。
3) 高次記号表現は，これらの基礎的記号に基づき (設置する)，カテゴリーのメンバーシップ関係（例：X は Z でり，ゆえに Y である）を記述する記号列で構成される。
コネクショニズムは，カテゴリー表象の根底にある不変的な特徴を学習するメカニズムとして自然な候補の 1 つであり，それによって名前を，それが表す遠位対象の近位投影に結びつける。
このようにコネクショニズムは，純粋に記号的なモデリングに匹敵するというよりは，非記号的／記号的なハイブリッドな心のモデルにおける補完的な構成要素として見ることができる。
しかし，このようなハイブリッドモデルには自律的な記号モジュールは存在しない。
記号機能は，カテゴリー名が感覚表象にボトムアップで接地された結果として，本質的に専用の記号システムとして現れるだろう。
記号操作は，記号トークンの恣意的な形だけでなく，記号トークンが根拠とするアイコンやカテゴリー不変量の非恣意的な形によっても支配されるだろう。 -->
<!-- There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling.
This paper describes the symbol grounding problem: How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads?
How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols?
The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone.
A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds:
(1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and
(2) categorical representations, which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections.
Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations.
Higher-order
 (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., An X is a Y that is Z).
Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for.
In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling.
Such a hybrid model would not have an autonomous symbolic module, however; the symbolic functions would emerge as an intrinsically dedicated symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations.
Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded. -->

<!-- Stevan Harnad は 1990年の論文によると、記号接地問題の他のいくつかの定義を暗黙のうちに表現している ([Harnad1990](https://web-archive.southampton.ac.uk/cogprints.org/3106/))。 -->
<!-- According to his 1990 paper, Stevan Harnad implicitly expresses a few other definitions of the symbol grounding problem:[2]-->

<!-- 1. 記号接地問題とは，「...形式的な記号システムの意味解釈を...どのようにするかという問題である。...頭の中の意味に寄生するのではなく，システムに内在する…」 「...他の無意味な記号以外では…」
2. 記号接地問題とは，「...その（恣意的な）形状のみに基づいて操作される無意味な記号トークンの意味…」が，「...他の無意味な記号以外の何かに...」どのように根拠づけられるかという問題である。
3. 「...記号の根拠付けの問題は，サール（1980）の有名な「中国の部屋の議論」において、内在的意味（あるいは「意図性」）の問題と呼ばれている。」
4. 記号接地問題とは、「...記号と記号のメリーゴーランドからどうやって降りることができるか」という問題である。 -->

<!--1. The symbol grounding problem is the problem of how to make the "...semantic interpretation of a formal symbol system..." "... intrinsic to the system, rather than just parasitic on the meanings in our heads..." "...in anything but other meaningless symbols..."
2. The symbol grounding problem is the problem of how "...the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes..." can be grounded "...in anything but other meaningless symbols."
3. "...the symbol grounding problem is referred to as the problem of intrinsic meaning (or 'intentionality') in Searle's (1980) celebrated 'Chinese Room Argument'"
4. The symbol grounding problem is the problem of how you can "...ever get off the symbol/symbol merry-go-round..."-->

<!--To answer the question of whether or not groundedness is a necessary condition for meaning, a formulation of the symbol grounding problem is required: The symbol grounding problem is the problem of how to make the "...semantic interpretation of a formal symbol system..." "... intrinsic to the system, rather than just parasitic on the meanings in our heads..." "...in anything but other meaningless symbols".[2] -->

## 復習

内積は，<span style="color:teal">$\overrightarrow{x}$</span> と表記している高等学校の教科書が多い。
駄菓子菓子，ここでは，太文字を使って次のように表記する <span style="color:blue">$\mathbf{x}$</span>

<!-- 内積の定義は，以下の通りである:
$$\tag{2:内積の定義}
\begin{align}
\left(\mathbf{x}\cdot\mathbf{y}\right) & =\left|\mathbf{x}\right|\left|\mathbf{y}\right|\cos\theta=x_1y_1+x_2y
_2+\ldots+x_{n}y_{n}\\
                                       & =\sum_{i=1}^{n}x_{i}y_{i}\\
\end{align}$$

(2) 式を変形すると
$$
\cos\theta=\frac{\mathbf{x}^{\top}\mathbf{y}}{\left|\mathbf{x}\right| \left|\mathbf{y}\right|}
$$
を得る。
すなわち (ピアソンの積率) 相関係数とは，2 つの変量 $x$ と $y$ とを多次元ベクトルと考え，それぞれの平均を引いたベクトル間のなす角の余弦 ($\cos) である。
余弦 $\cos$ の値は $[-1,1]$ であることから，相関係数の範囲は -1 から +1 までの値を取ることが分かる。

加えて，相関係数が 1 であとは，2 本のベクトルが一致することを意味し，相関係数が 0 であるとは，2 本のベクトルが直交することを表す，
相関係数が -1 とは，2 本のベクトルが反対方向であることを表している。 -->

<!-- <div style="background-color:lightgray;width:66%;text-align:left;"> -->

#### ピアソンの積率相関係数と平均偏差ベクトル

$n$ 個のデータからなる $n$ 次元ベクトル $\mb{x}=\Brc{x_1, x_2,\ldots, x_n}^{\top}$, $\mb{y}=\Brc{y_1, y_2, \ldots, y_n}^{\top}$ の個々の要素から平均値を引いたベクトルを平均偏差ベクトルという。

$$
\pmatrix{x_1-\Bar{x}\cr x_2-\Bar{x}\cr\vdots\cr x_n-\Bar{x}},\qquad
   \pmatrix{y_1-\Bar{y}\cr y_2-\Bar{y}\cr\vdots\cr y_n-\Bar{y}},
$$

すべての要素が $1$ であるベクトル ${\bf 1}=(\;\overbrace{1,1,\ldots,1}^{n個}\;)^{\top}$ によって張られる部分空間 $L\Brc{\mb{1}}$ への射影行列をつくると以下のようになる:

$$
\mb{P}=\mathbf{1}\Brc{\mb{1}^{\top}\mb{1}}^{-1}{\mb{1}^{\top}}=\pmatrix{
                        1/n    & \cdots & 1/n    \cr
                        \vdots & \ddots & \vdots \cr
                        1/n    & \cdots & 1/n    \cr}
$$

この射影行列に右から $\mb{y}$ を乗ずると, $\mb{Py}=\Brc{\Bar{y},\Bar{y},\ldots,\Bar{y}}^{\top}$ となる。

したがって, 平均偏差ベクトルは次式で与えられる: $\mb{y}-\Bar{y}\mb{1}=\mb{y}-\mb{Py}=\Brc{\mb{I}-\mb{P}}\mb{y}$

すなわち, 平均偏差ベクトルとは $L\Brc{\mathbf{1}}$ の補空間への射影ベクトルである。

この平均偏差ベクトルの長さの 2 乗 $\Norm{x}^2$ をデータ数で割ったものは，以下のとおり:

$$
\frac{1}{n}\Norm{x}^2=\frac{1}{n}\Brc{\mb{x},\mb{x}}=\frac{1}{n}\sum_1^n\Brc{x-\Bar{x}}^2=s_x^2,
$$

平均偏差ベクトルをもちいると次式 $x$ と $y$ との (ピアソンの積率) 相関係数 $r_{xy}$ の関係式を得る:

$$\begin{aligned}
r_{xy} &= \frac{S_{xy}}{S_xS_y} \left(=\frac{\text{$x$ と $y$ との共分散}}{\text{$x$ の標準偏差}\times \text{$y$ の標準偏差}}
      = \frac{\sum_i\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sqrt{\sum_i\left(x_i-\bar{x}\right)^{2}}\,\sqrt{\sum_i\left(y_i-\bar{y}\right)^{2}}}\right)\\
&=\frac{\IP{\mb{x}}{\mb{y}}}{\Norm{\mb{x}}\Norm{\mb{y}}}
\left(=
    \frac{\text{ベクトル $\mb{x}$ とベクトル $\mb{y}$ との内積}}
         {\text{ベクトル $\mb{x}$ の長さ (ノルム) $\times$ ベクトル $\mb{y}$ の長さ(ノルム)}}
\right)\\
&=\cos\theta
\end{aligned}$$


* **相関係数が 1 ($r_{xy}=+1$):** 2 本のベクトルが一致することを意味し，
* **相関係数が 0 ($r_{xy}=0$):** 2 本のベクトルが直交することを表す，
* **相関係数が -1 ($r_{xy}=-1$):** 2 本のベクトルが反対方向であることを表している


# 回帰 Regression

$$
y = ax + b
$$

誤差は，次式となる:
$$
\epsilon = y - \left(ax + b\right)
$$

ここで，誤差の分散 (誤差ベクトルの自分自身との内積)
$$
\mathbf{\epsilon}^{\top}\mathbf{\epsilon}=\left\{\mathbf{y}-\left(a\mathbf{x}-\mathbf{b}\right)\right\}^{\top}\left\{\mathbf{y}-\left(a\mathbf{x}-b\right)\right\}
$$

<div class="figcenter">
<img src="/2024assets/2024_0514Pythagoras.svg" style="width:33%;">
</div>

すなわち，単回帰の $y = ax + b$ における $a$ の求め方とは，ベクトル $\mathbf{y}$ からベクトル $\mathbf{x}$ へ垂線を下ろした場合，その距離が最も短くなる。
そのような場合の $a$ は，ピタゴラスの定理，あるいは三平方の定理が成り立つので，$\left|a\mathbf{x}\right|^{2}+\left|\epsilon\right|^{2}=\left|\mathbf{y}\right|^2$ となる。

### 基本仮定


$$
\mb{y}=\mb{X\beta}+\mb{\epsilon}\qquad(\mb{X}=\{\mb{x}_1,\mb{x}_2,\ldots,\mb{x}_p\}).
$$

$\mb{y}$ を従属変数,
$\mb{X}=\{\mb{x}_1,\mb{x}_2,\ldots,\mb{x}_p\}$ を独立変数,
$\mb{\beta}$ を回帰係数,
$\mb{\epsilon}$ を誤差項または残差 という.

* 仮定 1: $E\Brc{\mb{\epsilon}}={\bf{0}}$.
* 仮定 2: $V\Brc{\mb{\epsilon}}=\sigma^2\mb{I}$.
* 仮定 3: $\Rank{\Brc{\mb{X}}}=p$.
* 仮定 4: 誤差項 $\epsilon_i$ はそれぞれ独立に $N\Brc{0,\sigma^2}$ に従う.

## 解法

### ベクトル幾何学的解法

<div class="figcenter">
<img src="/2024assets/2024_0517Projection_concept.svg" style="width:44%;">
</div>

誤差項 $\mb{\epsilon}$ は回帰モデルによって説明されない部分である。
この $\mb{\epsilon}$ のノルムを最小にするよに $\mb{\beta}$ を定める。
すなわち $\NSQ{\epsilon}=\Norm{\mb{y}-\mb{X\beta}}^2\rightarrow\min$.
これは $L\Brc{X}$ 上へ $\mb{y}$ を射影することに相当する。
この射影は $\Prj{X}\mb{y}$ によって与えられる。

これによって
$\Hat{\mb{y}}=\mb{X}\Hat{\mb{\beta}}=\Prj{X}\mb{y}$.

すなわち
$$
\Hat{\mb{\beta}}=\RegP{X}{y}.
$$

$\Hat{\mb{\beta}}$ を $\mb{\beta}$ の **最小 $2$ 乗推定量** という。

$\mb{X}$ によって張られる線形部分空間 $L\Brc{\mb{X}}$ への射影行列 $\Prj{X}$ を $\mb{P}$ と表現すれば回帰式は

$$
\mb{y}=\mb{Py}+\mb{\epsilon},
$$

となる。

さらに $\mb{\epsilon}=\mb{y}-\mb{Py}=\Brc{\mb{I}-\mb{P}}\mb{y}$,
すなわち, 回帰式を射影行列による独立変数ベクトルの分解

$$
\mb{y}=\mb{Py}+\Brc{\mb{I}-\mb{P}}\mb{y},
$$
と考えることができる。


### 別解 1. 最小 $2$ 乗解

誤差の $2$ 乗和 $\Norm{\mb{\epsilon}}^2$ を最小にする。
$$\begin{aligned}
\Norm{\mb{\epsilon}}^2 &= \Brc{\mb{y}-\mb{X\beta}}'\Brc{\mb{y}-\mb{X\beta}}\\
                       &= \mb{y'y}-2\mb{\beta'X'y}+\mb{\beta}'\Brc{\mb{X'X}}\mb{\beta},
\end{aligned}$$

だから
$$
\frac{\partial \Norm{\mb{\epsilon}}^2}{\partial \mb{\beta}} = -2\mb{X}^{\top}\mb{y}+2\mb{X}^{\top}\mb{X\beta} = 0
$$

$$
\mb{X}^{\top}\mb{X\beta} = \mb{X}^{\top}\mb{y}
$$

これを解いて, $\Hat{\mb{\beta}}=\RegP{X}{y}$.

### 別解 2.

誤差ベクトルと予測ベクトル $\Hat{\mb{y}}=\mb{X\beta}$ とは直交するからその内積は $0$ である。

$$\begin{eqnarray*}
\IP{\Brc{\mb{X\beta}}}{\Brc{\mb{y}-\mb{X\beta}}}&=&0\\
\mb{\beta'X'y}-\mb{\beta'X'X\beta}&=&0.
\end{eqnarray*}$$

これを解いて, $\Hat{\mb{\beta}}=\RegP{X}{y}$.


## 固有顔 Eigenfaces と [フィッシャー顔 Fisherfaces](http://www.scholarpedia.org/article/Fisherfaces)

PCA (Princple Component Analysis: 主成分分析) とは，データ数，一つのデータあたりの個数を m とすると，データは
n 行 m 列の行列となる。
オリベッティ顔データセットであれば $\mathbf{X}$ は 400 行 $\times$ 4096 列となる。

## フィッシャー顔<!-- ## 2.3 Fisherfaces-->

コンピュータ・ビジョン，パターン認識，機械学習における重要な問題は，目前の課題に適切なデータ表現を定義することである。
<!-- A key problem in computer vision, pattern recognition and machine learning is to define an appropriate data representation for the task at hand.-->

入力データを表現する 1 つの方法は，データの分散の大部分を表す部分空間を見つけることである。
これは主成分分析 (PCA) を用いることで得られる。
顔画像に PCA を適用すると，固有顔 eigenfaces の集合が得られる。
これらの固有顔は，学習データの共分散行列の最大の固有値に関連する固有ベクトルである。
こうして見つかった固有ベクトルは，最小二乗 (LS) 解に対応する。
これは，データの分散を確実に維持しながら，サンプルベクトル内の元の特徴 (次元) 間の不要な既存の相関を排除するため，データを表現するための強力な方法である。
<!--One way to represent the input data is by finding a subspace which represents most of the data variance. This can be obtained with the use of Principal Components Analysis (PCA).
When applied to face images, PCA yields a set of eigenfaces.
These eigenfaces are the eigenvectors associated to the largest eigenvalues of the covariance matrix of the training data.
The eigenvectors thus found correspond to the least-squares (LS) solution.
This is indeed a powerful way to represent the data because it ensures the data variance is maintained while eliminating unnecessary existing correlations among the original features (dimensions) in the sample vectors. -->

目的が表現ではなく分類である場合，LS 解は最も望ましい結果をもたらさないかもしれない。
このような場合，同じクラスのサンプルベクトルを特徴表現の一点に写像し，異なるクラスのサンプルベクトルを可能な限り遠くに写像する部分空間を見つけたい。
この目標を達成するために導き出された手法は，判別分析 (DA) として知られている。
<!-- When the goal is classification rather than representation, the LS solution may not yield the most desirable results.
In such cases, one wishes to find a subspace that maps the sample vectors of the same class in a single spot of the feature representation and those of different classes as far apart from each other as possible.
The techniques derived to achieve this goal are known as discriminant analysis (DA).-->

最も有名な DA が線形判別分析 (LDA) であり，これは 1936 年に R.A.Fisher によって提案されたアイデアから派生している。
LDA が顔画像の集合の部分空間表現を見つけるために使用されるとき，その空間を定義する結果の基底ベクトルは，フィッシャー顔 Fisherfaces として知られている。
<!--The most known DA is Linear Discriminant Analysis (LDA), which can be derived from an idea suggested by R.A. Fisher in 1936.
When LDA is used to find the subspace representation of a set of face images, the resulting basis vectors defining that space are known as Fisherfaces. -->


線形判別分析は，統計学者である R.A.フィッシャー卿によって発明された手法である。(Fisher1936)。
1936 年の論文では，HAD の例題にもなっている 3 種類のアヤメの分類に適用した。
<!-- 分類学上の問題で複数の測定値を使用すること [8]。
しかし，主成分分析 (PCA) がこれほどうまくいったのに，なぜ別の次元削減法が必要なのだろうか？ -->

PCA は，データの全分散を最大化する特徴の線形結合を見つける。
これは明らかにデータを表現する強力な方法だが，クラスを考慮しないため，成分を捨てるときに多くの識別情報が失われる可能性がある。
<!-- 分散が外部ソースによって生成される状況を想像してみよ。 -->
PCA によって同定された成分は，必ずしも識別情報を全く含んでいないため，投影されたサンプルは一緒に塗りつぶされ，分類は不可能になる。
クラス間を最もよく分離する特徴の組み合わせを見つけるために，線形判別分析は **クラス間のばらつき，群間分散** と **クラス内のばらつき，郡内分散** の比率を最大化する。
考え方は単純で，同じクラスは互いに密に集まり，異なるクラスは互いにできるだけ離れるべきである。
このことを応用したものが，フィッシャー顔 (Fihserfaces) (Belhumeur+1997)，[3] である。
<!--The Linear Discriminant Analysis was invented by the great statistician Sir R. A. Fisher, who successfully used it for classifying flowers in his 1936 paper.
The use of multiple measurements in taxonomic problems [8].
But why do we need another dimensionality reduction method, if the Principal Component Analysis (PCA) did such a good job?
The PCA finds a linear combination of features that maximizes the total variance in data.
While this is clearly a powerful way to represent data, it doesn’t consider any classes and so a lot of discriminative information may be lost when throwing components away.
Imagine a situation where the variance is generated by an external source, let it be the light.
The components identified by a PCA do not necessarily contain any discriminative information at all, so the projected samples are smeared together and a classification becomes impossible.
In order to find the combination of features that separates best between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to within-classes scatter.
The idea is simple: same classes should cluster tightly together, while different classes are as far away as possible from each other.
This was also recognized by Belhumeur, Hespanha and Kriegman and so they applied a Discriminant Analysis to face recognition in [3]. -->


<div class="figcenter">
<img src="/2024assets/2012Wagner_Face_Recognition_fig1.svg" style="width:49%;">
</div>

### アルゴリズムの説明<!-- ### 2.3.1 Algorithmic Description-->

$\mathbf{X}$ を，$c$ 個のクラスから抽出されたサンプルを持つランダム・ベクトルとする。
<!--Let X be a random vector with samples drawn from c classes: -->
$$\tag{8}
\mathbf{X}= \left\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{c}\right\}.
$$

$$\tag{9}
\mathbf{x}_i =\left\{x_1,x_2,\ldots,x_n\right\}.
$$

分散行列 S_B と S_W は次のように計算される：<!-- The scatter matrices S_B and S_W are calculated as: -->
$$
\begin{align}
S_B &= \sum_{i=1}^{c} N_{i}\left(\mu_i-\mu\right)\left(\mu_i-\mu\right)^{\top}\tag{10}\\
S_W &= \sum_{i=1}^{c}\sum_{x_j\in X_i}\left(x_j-\mu_j\right)\left(x_j-\mu_j\right)^{\top},\tag{11}
\end{align}
$$
ここで，$\mu$ は全平均である:<!--where mu is the total mean:-->
$$
\mu = \frac{1}{N}\sum_{i=1}^{N}x_i.
$$

かつ，$\mu_i$ は群平均 $i\in\left\[1,\ldots,c\right\]$:

$$
\mu_i = \frac{1}{\left|X_i\right|}\sum_{x_j\in X_i}x_j,
$$

フィッシャーの古典的アルゴリズムは，クラス分離可能性基準を最大化する射影 W を探す：
<!-- Fisher’s classic algorithm now looks for a projection W, that maximizes the class separability criterion: -->

$$
W_{\text{opt}}=\arg\max_{W}\frac{\left|W^{\top}S_{B}W\right|}{\left|W^{\top}S_{W}W\right|}
$$

[3] に従えば，この最適化問題の解は一般固有値問題を解くことによって与えられる：
<!-- Following [3], a solution for this optimization problem is given by solving the General Eigenvalue Problem: -->

$$\begin{aligned}
\mathbf{S}_{B}\mathbf{v}_i       &= \lambda_{i} \mathbf{S}_{W}\mathbf{v}_{i}\\
\mathbf{S}_{W}^{-1}\mathbf{S}_{B}\mathbf{v}_{i}&= \lambda_{i} \mathbf{v}_{i}
\end{aligned}$$

解決すべき問題が 1 つ残っている： $S_W$ のランクは最大でも $(N-c)$ であり，$N$ 個のサンプルと c 個のクラスが存在する。
パターン認識問題では，サンプル数 $N$ は入力データの次元 (画素数) より小さいことがほとんどなので，分散行列 $S_W$ は特異行列になる ([2]を参照)。
[3] では，データに対して主成分分析を実行し，サンプルを (N-c) 次元空間に投影することでこれを解決した。
その後，$S_W$ が特異でなくなったので，線形判別分析が縮小データに対して実行される。
最適化問題は次のように書き換えられる：
<!-- There’s one problem left to solve: The rank of S_W is at most (N−c), with N samples and c classes.
In pattern recognition problems the number of samples N is almost always smaller than the dimension of the input data (the number of pixels), so the scatter matrix S_W becomes singular (see [2]).
In [3] this was solved by performing a Principal Component Analysis on the data and projecting the samples into the (N−c)-dimensional space.
A Linear Discriminant Analysis was then performed on the reduced data, because S_W isn’t singular anymore.
The optimization problem can be rewritten as:-->

$$\begin{aligned}
W_{opt} &= \arg\max_{W}\left|W^{\top}S_{T}W\right|\\
W_{fld} &= \arg\max_{W}\frac{\left|W^{\top}W_{pca}^{\top}S_{B}W_{pca}W\right|}{\left|W^{\top}W_{pca}^{\top}S_{W}W_{pca}W\right|}
\end{aligned}$$

サンプルを $(c-1)$ 次元空間に投影する変換行列 $W$ は次式で与えられる：
<!-- The transformation matrix $W$, that projects a sample into the (c−1)-dimensional space is then given by: -->

$$\tag{18}
W=W_{fld}^{\top}W_{pca}^{\top}
$$

最後の注意：S_W と S_B は対称行列だが，2 つの対称行列の積は必ずしも対称ではない。
したがって，一般的な行列の固有値ソルバを利用する必要がある。
OpenCV の cv::eigen は，現在のバージョンでは対称行列に対してのみ動作する。
非対称行列に対しては固有値と特異値は等価ではないので，特異値分解 (SVD) も利用でき無い。
<!-- One final note: Although S_W and S_B are symmetric matrices, the product of two symmetric matrices is not necessarily symmetric.
So you have to use an eigenvalue solver for general matrices.
OpenCV’s cv::eigen only works for symmetric matrices in its current version; since eigenvalues and singular values aren’t equivalent for non-symmetric matrices you can’t use a Singular Value Decomposition (SVD) either. -->

# 回帰

## 基本仮定

$$
\mathbf{y}=\mathbf{X\beta}+\mathbf{\epsilon}\qquad(\mathbf{X}=\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_p\}).
$$

$\mathbf{y}$ を従属変数,
$\mathbf{X}=\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_p\}$ を独立変数,
$\mathbf{\beta}$ を回帰係数，$\mathbf{\epsilon}$ を誤差項または残差 という。

* 仮定 1: $\mathbb{E}\left(\mathbf{\epsilon}\right)=\mathbf{0}$.
* 仮定 2: $\mathbb{V}\left(\mathbf{\epsilon}\right)=\sigma^2\mathbf{I}$.
* 仮定 3: $\text{rank}\;\left(\mathbf{X}\right)=p$.
* 仮定 4: 誤差項 $\epsilon_i$ はそれぞれ独立に $N\left(0,\sigma^2\right)$ に従う。

## 解法

### ベクトル幾何学的解法

誤差項 $\mathbf{\epsilon}$ は回帰モデルによって説明されない部分である。
この $\mathbf{\epsilon}$ のノルムを最小にするように $\mathbf{\beta}$ を定める。
すなわち $\left|\mathbf{\epsilon}\right|^2=\left|\mathbf{y}-\mathbf{X\beta}\right|^2\rightarrow\min$。

これは $L\left(X\right)$ 上へ $\mathbf{y}$ を射影することに相当する。
この射影は $\mathbf{X}\left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{y}$ によって与えられる。

これによって

$$
\hat{\mathbf{y}}=\mathbf{X}\hat{\mathbf{\beta}}=\mathbf{X}\left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top}\mathbf{y}
$$

すなわち
$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$
$\hat{\mathbf{\beta}}$ を $\mathbf{\beta}$ の **最小 $2$ 乗推定量** という。

$\mathbf{X}$ によって張られる線形部分空間 $L\left(\mathbf{X}\right)$ への射影行列 $\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'$ を $\mathbf{P}$ と表現すれば回帰式は
$$
\mathbf{y}=\mathbf{Py}+\mathbf{\epsilon},
$$
となる。

さらに
$\mathbf{\epsilon}=\mathbf{y}-\mathbf{Py}=\left(\mathbf{I}-\mathbf{P}\right)\mathbf{y}$，すなわち，回帰式を射影行列による独立変数ベクトルの分解
$$
\mathbf{y}=\mathbf{Py}+\left(\mathbf{I}-\mathbf{P}\right)\mathbf{y},
$$
と考えることができる。

### 別解 1. 最小 $2$ 乗解

誤差の $2$ 乗和 $\left|\mathbf{\epsilon}\right|^2$ を最小にする。

$$\begin{aligned}
\left|\mathbf{\epsilon}\right|^{2} &= \left(\mathbf{y}-\mathbf{X\beta}\right)'\left(\mathbf{y}-\mathbf{X\beta}\right)\\
&=\mathbf{y'y}-2\mathbf{\beta'X'y$} + \mathbf{\beta}'\left(\mathbf{X'X}\right)\mathbf{\beta},
\end{aligned}$$

だから

$$\begin{aligned}
\frac{\partial \left|\mathbf{\epsilon}\right|^2}{\partial \mathbf{\beta}} =
-2\mathbf{X'y}+2\mathbf{X'X\beta} &=& 0\\
\mathbf{X'X\beta} &=& \mathbf{X'y}
\end{aligned}$$

これを解いて,

$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$

## 別解 2.

誤差ベクトルと予測ベクトル $\hat{\mathbf{y}}=\mathbf{X\beta}$ とは 直交するからその内積は $0$ である。

$$\begin{aligned}
\left(\left(\mathbf{X\beta}\right)\cdot\left(\mathbf{y}-\mathbf{X\beta}\right)\right) &=& 0\\
\mathbf{\beta'X'y}-\mathbf{\beta'X'X\beta} &=& 0.
\end{aligned}$$

これを解いて，
$$
\hat{\mathbf{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{y}.
$$

<div class="memo">

**平均偏差ベクトル**:

$n$ 個のデータからなる $n$ 次元ベクトル
$\mathbf{x}=\left(x_1, x_2,\ldots, x_n\right)'$,
$\mathbf{y}=\left(y_1, y_2, \ldots, y_n\right)'$
の個々の要素から平均値を引いたベクトルを平均偏差ベクトルという。
$$
\pmatrix{x_1-\overline{x}\cr x_2-\overline{x}\cr\vdots\cr x_n-\overline{x}},\qquad
\pmatrix{y_1-\overline{y}\cr y_2-\overline{y}\cr\vdots\cr y_n-\overline{y}},
$$
この平均偏差ベクトルの意味を考える. すべての要素が $1$ であるベクトル
$\mathbf{1}=(\;\overbrace{1,1,\ldots,1}^{n個}\;)'$
によって張られる部分空間 $L\left({\bf 1}\right)$
への射影行列をつくると
$\mathbf{P}=\mathbf{1}\left(\mathbf{1}'\mathbf{1}\right)^{-1}\mathbf{1}'=\pmatrix{
             1/n    & \cdots & 1/n    \cr
             \vdots & \ddots & \vdots \cr
             1/n    & \cdots & 1/n    \cr}$
となる。

この射影行列に右から $\mathbf{y}$ を乗ずると,
$\mathbf{Py}=\left(\overline{y},\overline{y},\ldots,\overline{y}\right)'$ となる。

したがって，平均偏差ベクトルは，
$$
\mathbf{y}-\overline{y}\mathbf{1} = \mathbf{y}-\mathbf{Py} = \left(\mathbf{I}-\mathbf{P}\right)\mathbf{y}
$$
すなわち, 平均偏差ベクトルとは $L\left(\mathbf{1}\right)$ の補空間への射影ベクトルである。
この平均偏差ベクトルの長さの 2 乗
$\left|x\right|^2$ をデータ数で割ったものは

$$
\frac{1}{n}\left|x\right|^2=\frac{1}{n}\left(\mathbf{x},\mathbf{x}\right)=\frac{1}{n}\sum_1^n\left(x-\overline{x}\right)^2=s_x^2,
$$

平均偏差ベクトルをもちいると $x$ と $y$ との相関係数は

$$r_{xy}=\frac{S_{xy}}{S_xS_y}
=\frac{\left(\mathbf{x}\cdot\mathbf{y}\right)}{\left|\mathbf{x}\right|\left|\mathbf{y}\right|}=\cos\theta_{xy}
$$
</div>


### 単回帰

説明変数, 被説明変数とも $1$ 個の場合を**単回帰**という. 回帰モデルは $\mathbf{y}=\mathbf{x}\beta+\epsilon$ である。

### ベクトル幾何学的解法

平均偏差ベクトルを用いれば

$$
\hat{\beta} = \left(\mathbf{x}^{\top}\mathbf{x}\right)^{-1}\mathbf{x}^{\top}\mathbf{y} =
\frac{\sum\left(x-\overline{x}\right)\left(y-\overline{y}\right)}{\sum\left(x-\overline{x}\right)^2}
=\frac{S_{xy}}{S_x^2}.
$$

あるいは，平均偏差ベクトルを用いなくても，すべての要素が $1$ である $n$ 次元ベクトル $(\mathbf{1})$ をもちいて
$\mathbf{X}=\left(\mathbf{1},\mathbf{x}\right)^{\top}$ とする。

さらに, $\mathbf{\beta}=\left(\alpha,\beta\right)$ を用いて改めて回帰方程式を:

$$
\mathbf{y}=\mathbf{X\beta}+\epsilon
$$

とおく, このとき

$$\begin{aligned}
\hat{\beta} &= \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}\\
\end{aligned}$$


$$\begin{aligned}
&= \begin{pmatrix}\mathbf{1}^{\top1} & \mathbf{1}^{\top}\mathbf{x}\cr \mathbf{1}'\mathbf{x} & \mathbf{x'x}^{-1}\end{pmatrix}
    \begin{pmatrix}\mathbf{1}'\mathbf{y}\cr \mathbf{x'y}\end{pmatrix}\\
\end{aligned}$$

$$\begin{aligned}
&= \begin{pmatrix}n&\sum x\cr \sum x&\sum x^2\end{pmatrix}^{-1}
\begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
&= \frac{1}{n\sum x^2-\left(\sum x\right)^2}\pmatrix{\sum x^2 & -\sum x\cr -\sum x&n\end{pmatrix}
\begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
\end{aligned}$$

$$\begin{aligned}
&=\frac{1}{n^2S_x^2}
\begin{pmatrix}n\left(S_x^2+{\overline{x}}^2\right)&-n\overline{x}\cr
         -n\overline{X}               & n      \end{pmatrix}
         \begin{pmatrix}\sum y\cr \sum xy\end{pmatrix}\\
&=\frac{1}{S_x^2}\pmatrix{S_x^2+\overline{x}^2&-\overline{x}\cr-\overline{x}&1}
\begin{pmatrix}\overline{y} \cr S_{xy}+\overline{x}\overline{y}\end{pmatrix}\\
\end{aligned}$$


$$\begin{aligned}
&=\frac{1}{S_x^2}
\begin{pmatrix}
\overline{y}\left(S_x^2+\overline{x}^2\right)-\overline{x}\left(S_{xy}+\overline{x}\overline{y}\right)\cr
S_{xy}+\overline{x}\overline{y}-\overline{x}\overline{y}\end{pmatrix}.
\end{aligned}$$

ゆえに

$$
\begin{pmatrix}\alpha\cr\beta\end{pmatrix}=
\begin{pmatrix}\displaystyle\overline{y}-\frac{S_{xy}}{S_x^2}\overline{x}\cr \displaystyle\frac{S_{xy}}{S_x^2}\end{pmatrix}.
$$

つまり

$$\begin{aligned}
y_i & = \alpha + \beta x_i + \epsilon_i\\
    & = \overline{y} - \beta \overline{x} + \beta x_i + \epsilon_i,
\end{aligned}$$

あるいは,

$$
y_i-\overline{y}=\beta\left(x_i-\overline{x}\right)+\epsilon_i.
$$

データから平均値を引いた値の回帰と定数 $\alpha$ を含めた回帰は同じものである。
この場合，$\alpha=\overline{y}-\beta\overline{x}$ である。

### 最小 $2$ 乗解

$i$ 番目のデータを

$$\begin{aligned}
y_i        &=&\beta\; x_i + \alpha + \epsilon_i\qquad(i=1,2,\ldots,n)\\
y_i-\overline{y}&=&\beta\left(x_i-\overline{x}\right)+\epsilon_i\qquad(i=1,2,\ldots,n),
\end{aligned}$$

とする。
実測値 $y_i$ と予測値 $\beta x_i+\alpha$
との差の $2$ 乗 $\left(y_i-\beta x_i-\alpha\right)^2=\epsilon_i^2$
を全データについて加算した $2$ 乗和

$$
\sum_i^n\left(y_i-\beta x_i-\alpha\right)^2=\sum_i^n\epsilon_i^2=Q,
$$

を, 回帰パラメータ $\alpha$, $\beta$ についてそれぞれ偏微分して $0$ とおく,

$$\begin{aligned}
\displaystyle{\pmatrix{
\displaystyle\frac{\partial Q}{\partial\alpha}\cr
\displaystyle\frac{\partial Q}{\partial \beta}\cr}} & = &
 \pmatrix{
 -2\sum\left(y_i-\beta x_i-\alpha\right)\cr
 -2\sum x_i\left(y_i-\beta x_i-\alpha\right)\cr} &= \pmatrix{0\cr0\cr}\\
&& \pmatrix{
 \sum y_i    & -\beta\sum x_i   & -n\alpha        \cr
 \sum x_iy_i & -\beta\sum x_i^2 & -\alpha\sum x_i \cr} &= \pmatrix{0\cr0\cr}.
\end{aligned}$$

これを $\alpha$, $\beta$ について解くと,

$$
\alpha = \overline{y} - \beta \overline{x}.
$$

さらに，

$$\begin{aligned}
\sum x_iy_i-\beta\sum x_i^2 -\left(\overline{y}-\beta\overline{x}\right)n\overline{x} &= 0\\
\beta\left(\sum x_i^2 - n\left(\overline{x}\right)^2\right) &= \sum x_iy_i - n\overline{x}\overline{y}\\
\beta ns_x^2 &= ns_{xy} \\
\beta &= \frac{s_{xy}}{s_x^2}.
\end{aligned}$$


## [The Beautiful Applications of Calculus in Real Life](https://ali.medium.com/the-beautiful-applications-of-calculus-in-real-life-952a8371319e)

思い起こせば Richard Feynman が信じていた真実から記事を始めた。
Feynman にとって「微積分は神が語る言語である。」
この主張に対して，Feynman は神を信じていなかったと反論する人もいるかもしれない。
しかし，Stenven Strogatz は Yale 大学での講義 「微積分の美」の中で，なぜ Feynman がこのような主張をしたのかを見事に説明している。
<!-- As you recall, I started the article with a truth that Richard Feynman believed in.
To Feynman, “calculus is the language in which God speaks.”
Some of you may refute this claim by saying that Feynman didn’t believe in God.
However, Steven Strogatz explains brilliantly why Feynman made this claim in his “The Beauty of Calculus” lecture at Yale University. -->

Harman Wouk は受賞歴のある作家である。
彼は一般に歴史小説を書いている。
彼は第二次世界大戦についての非常に詳細な小説を書きたいと考えている。
マンハッタン計画の結果である原子爆弾が，戦争の結末を決めることになる。
そのため，Harman Wouk はマンハッタン計画に携わった当時まだ若かった科学者，Richard Feynman に話を聞きたいという。
現代で最も有名な科学者の一人が Richard Feynman だからだ。
楽しい会話の後，Feynman は Wouk に微積分を知っているかと尋ねる。
Wouk が否定的な返事をすると「習ったほうがいい。神が話す言語なのだから」と Feynman は言う。
<!-- Harman Wouk is an award-winning writer.
He generally writes historical novels.
He wants to write a very detailed novel about World War II.
The result of the Manhattan Project, the atomic bomb, would determine the war’s end.
That is why Herman Wouk wants to speak with the then-young scientist who worked on the Manhattan Project, Richard Feynman.
That is because one of the most well-known scientists of the modern era is Richard Feynman.
After a pleasant conversation, Feynman asks Wouk if he knows calculus.
When Wouk gives a negative response, “You had better learn it; it’s the language God talks,” says Feynman. -->

ハーマン・ウークは信者であり、ファインマンの言うとおりにした。
彼は微積分を学ぶために個人教師を雇い、高校に入学して基礎から学ぼうとした。
その後、彼は 「The Language God Talks」という宗教と科学についての本を書いた。
その著書の中にこんなくだりがある：
<!-- Herman Wouk was a believer and did as Feynman said. He hired a private teacher to learn calculus, and he wanted to enroll in a high school and learn from the basics.
Afterward, he wrote a book about religion and science called “The Language God Talks.”
In his book, this paragraph is written: -->

私は大学の書店で新入生のテキストを手に取り，ざっと目を通した。
大学時代を人文科学で過ごした私のような数学の無知を助けてくれるようなテキストに出会えることを期待していた。
<!-- I picked up and skimmed freshman texts in college bookstores, hoping to come across one that might help a mathematical ignoramus like me, who had spent his college year in the humanities — Herman Wouk, The Language God Talks, p.6 -->

Feynman も，単に会話のためにこのようなことを言ったわけではない。
彼以前の多くの著名な科学者たちも，この真理に気づいていた。
例えば，Newton は空に浮かぶ月がなぜ地球に落ちてこないのか不思議に思っていた。
彼のラテン語も英語もそれを説明するには不十分だった。
微積分を学んだ後，彼は奇跡的な状況を微分方程式を使って人類に明確に説明した。
Newton の後，数学者と物理学者は微積分を使って段階的に現代世界を構築していった。
<!-- Feynman didn’t just say this for the sake of conversation, either.
Many recognized scientists before him were also aware of this truth.
For example, Newton wondered why the Moon in the sky didn’t fall to Earth. He had some things in mind but could never explain them; neither his Latin nor his English was enough to explain them.
After learning calculus, he explained the miraculous situation to humanity in explicit detail using differential equations.
After Newton, mathematicians and physicists constructed the modern world step-by-step using calculus. -->




# [科学におけるモデル wikipedia より](https://en.wikipedia.org/wiki/Scientific_modelling)

科学的モデリングとは、世界の特定の部分や特徴を理解、定義、定量化、視覚化、シミュレーションしやすくするために、経験的な対象、現象、物理的プロセスを表すモデルを作り出す活動である。
現実世界の状況に関連する側面を選択・特定し、それらの特徴を持つシステムを再現するモデルを開発する必要がある。
よりよく理解するための概念モデル，運用するための操作(運用)モデル，定量化するための数学モデル，シミュレーションするための計算モデル，対象を視覚化するためのグラフィカルモデルなど，さま
ざまなタイプのモデルが，さまざまな目的のために使用される。
<!-- Scientific modelling is an activity that produces models representing empirical objects, phenomena, and physical processes, to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate.
It requires selecting and identifying relevant aspects of a situation in the real world and then developing a model to replicate a system with those features.
Different types of models may be used for different purposes, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, computational models to simulate, and graphical models to visualize the subject.-->

モデルは，多くの科学分野にとって不可欠かつ切り離せないものであり，各分野は，特定のタイプのモデルについて独自の考えを持っている[1][2]。
以下は John von Neumann の言葉である[3]。
<!--Modelling is an essential and inseparable part of many scientific disciplines, each of which has its own ideas about specific types of modelling.[1][2]
The following was said by John von Neumann.[3] -->

...科学は説明しようとはしないし，解釈しようともしない，科学は主にモデルを作る。
モデルによって，観察された現象を、言葉による解釈を加えて，観察された現象を記述する数学的構成を意味する。
このような数学的構成が正当化されるのは，それが機能すると期待されているからに他ならない。
<!-- ... the sciences do not try to explain, they hardly even try to interpret, they mainly make models.
By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena.
The justification of such a mathematical construct is solely and precisely that it is expected to work—that is, correctly to describe phenomena from a reasonably wide area. -->

1. Cartwright, Nancy. 1983. How the Laws of Physics Lie. Oxford University Press
2. Hacking, Ian. 1983. Representing and Intervening. Introductory Topics in the Philosophy of Natural Science. Cambridge University Press
3. von Neumann, J. (1995), 数理科学の方法 "Method in the physical sciences", in Bródy F., Vámos, T. (editors), The Neumann Compendium, World Scientific, p. 628; previously published in The Unity of Knowledge, edited by L. Leary (1955), pp. 157-164, and also in John von Neumann Collected Works, edited by A. Taub, Volume VI, pp. 491-498.

また，科学教育，科学哲学，システム論、知識の可視化などの分野においても，科学的モデリング[4]への注目が高まっている。
あらゆる種類の専門的な科学的モデリングに関する手法，技法，メタ理論のコレクションが増えつつある。
<!-- There is also an increasing attention to scientific modelling[4] in fields such as science education,[5] philosophy of science, systems theory, and knowledge visualization.
There is a growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling. -->


## Overview

科学モデルは，経験的な対象，現象，物理的プロセスを論理的かつ客観的な方法で表現しようとするものである。
すべてのモデルはシミュラクラであり，つまり，近似であるにもかかわらず，非常に有用な現実の単純化された反映である[6]。
モデルの構築と論争は，科学的事業の基本である。
完全で真の表現は不可能かもしれないが，科学的な議論では，例えば季節予報のための気候モデルとしてどちらがより正確かなど，与えられた課題に対してどちらがより優れたモデルであるかがしばしば議論される。
<!-- A scientific model seeks to represent empirical objects, phenomena, and physical processes in a logical and objective way.
All models are in simulacra, that is, simplified reflections of reality that, despite being approximations, can be extremely useful.[6]
Building and disputing models is fundamental to the scientific enterprise.
Complete and true representation may be impossible, but scientific debate often concerns which is the better model for a given task, e.g., which is the more accurate climate model for seasonal forecasting.[7] -->

経験科学の原理を形式化する試みは、論理学者が論理学の原理を公理化するのと同じように、解釈を用いて現実をモデル化する。
このような試みの目的は、現実に見出されるものに反する理論的帰結を生み出さないような形式的システムを構築することである。
このような形式的システムから引き出される予測やその他の記述は、これらの科学的モデルが真である限りにおいてのみ、現実世界を映し出したり写し出したりする[8][9]。
<!-- Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic.
The aim of these attempts is to construct a formal system that will not produce theoretical consequences that are contrary to what is found in reality.
Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.[8][9] -->

科学者にとってモデルは，人間の思考プロセスを増幅させることができる方法でもある[10]。
例えば，ソフトウェアでレンダリングされたモデルは，科学者が計算能力を活用して，表現されている実体，現象，プロセスについてシミュレーションし，視覚化し，操作し，直感を得ることを可能にする。
このようなコンピュータ・モデルはイン・シリコである。
その他の科学的モデルには，in vivo（実験用ラットのような生体モデル）と in vitro（組織培養のようなガラス器具内）がある[11]。
<!-- For the scientist, a model is also a way in which the human thought processes can be amplified.[10]
For instance, models that are rendered in software allow scientists to leverage computational power to simulate, visualize, manipulate and gain intuition about the entity, phenomenon, or process being represented.
Such computer models are in silico.
Other types of scientific models are in vivo (living models, such as laboratory rats) and in vitro (in glassware, such as tissue culture).[11] -->


## Basics

### 直接測定や実験の代用としてのモデル化<!-- ### Modelling as a substitute for direct measurement and experimentation-->

モデルは通常，科学者が結果を直接測定できるような実験条件を作り出すことが不可能か，非現実的な場合に使用される。
管理された条件下での結果の直接測定（「科学的方法」参照）は、モデルによる結果の推定よりも常に信頼性が高い。
<!--Models are typically used when it is either impossible or impractical to create experimental conditions in which scientists can directly measure outcomes.
Direct measurement of outcomes under controlled conditions (see Scientific method) will always be more reliable than modeled estimates of outcomes. -->

モデリングとシミュレーションにおいて，モデルとは，物理的，法的，認知的な制約によって形成された，課題駆動型の，現実認識の目的意識的な単純化と抽象化である[12]。
課題駆動型であるのは，モデルが特定の疑問や課題を念頭に置いて捉えられるからである。
単純化では，課題にとって重要でない既知の実体や観測された実体，およびそれらの関係をすべて除外する。
抽象化は，重要ではあるが，関心のある対象と同じ詳細さでは必要ない情報を集約する。
単純化も抽象化も，意図的に行われる。
しかし，それらは現実の認識に基づいて行われる。
この認識は，物理的な制約を伴うため，それ自体がすでにモデルである。
また，現在の道具や方法で合法的に観察できるものには制約があり，現在の理論で説明できるものには認知上の制約がある。
このモデルは，概念，その動作，およびそれらの関係の非公式な形式から構成され，しばしば概念モデルと呼ばれる。
モデルを実行するためには，コンピュータ・シミュレーションとして実装する必要がある。
これには，数値的近似やヒューリスティックの使用など，より多くの選択肢が必要となる[13]。
このような認識論的・計算論的制約があるにもかかわらず，シミュレーションは，理論構築，シミュレーション，、実験という科学的手法の3本目の柱として認識されている[14]。
<!-- Within modeling and simulation, a model is a task-driven, purposeful simplification and abstraction of a perception of reality, shaped by physical, legal, and cognitive constraints.[12]
It is task-driven because a model is captured with a certain question or task in mind.
Simplifications leave all the known and observed entities and their relation out that are not important for the task.
Abstraction aggregates information that is important but not needed in the same detail as the object of interest.
Both activities, simplification, and abstraction, are done purposefully.
However, they are done based on a perception of reality.
This perception is already a model in itself, as it comes with a physical constraint.
There are also constraints on what we are able to legally observe with our current tools and methods, and cognitive constraints that limit what we are able to explain with our current theories.
This model comprises the concepts, their behavior, and their relations informal form and is often referred to as a conceptual model.
In order to execute the model, it needs to be implemented as a computer simulation.
This requires more choices, such as numerical approximations or the use of heuristics.[13]
Despite all these epistemological and computational constraints, simulation has been recognized as the third pillar of scientific methods: theory building, simulation, and experimentation.[14] -->

### シミュレーション<!-- ### Simulation-->

シミュレーションは、モデルを実装する方法であり、モデルが複雑すぎて解析解が得られない場合によく用いられる。
定常状態シミュレーションは、ある瞬間のシステムに関する情報を提供する（そのような状態が存在する場合、通常は平衡状態）。
動的シミュレーションは、経時的な情報を提供する。
シミュレーションは、特定の物体や現象がどのように振る舞うかを示す。
このようなシミュレーションは、実世界のシステムや概念をモデルで表現できる場合に、テスト、分析、トレーニングに役立つ[15]。
<!--A simulation is a way to implement the model, often employed when the model is too complex for the analytical solution.
A steady-state simulation provides information about the system at a specific instant in time (usually at equilibrium, if such a state exists).
A dynamic simulation provides information over time.
A simulation shows how a particular object or phenomenon will behave.
Such a simulation can be useful for testing, analysis, or training in those cases where real-world systems or concepts can be represented by models.[15] -->

### 構造<!-- ### Structure-->

構造とは、実体のパターンや関係の認識、観察、性質、安定性をカバーする基本的で、時に無形の概念である。
子供が雪の結晶を言葉で説明することから、磁場の特性の詳細な科学的分析に至るまで、構造という概念は、科学、哲学、芸術におけるほとんどすべての探求と発見の様式の本質的な基盤である[16]。
<!-- Structure is a fundamental and sometimes intangible notion covering the recognition, observation, nature, and stability of patterns and relationships of entities.
From a child's verbal description of a snowflake, to the detailed scientific analysis of the properties of magnetic fields, the concept of structure is an essential foundation of nearly every mode of inquiry and discovery in science, philosophy, and art.[16] -->

### システム<!-- ### Systems -->

システムとは，統合された全体を形成する，相互作用する，あるいは相互依存する実体の集合であり，現実のものであれ抽象的なものであれ，である。
一般に，システムとは，要素だけでは得られない結果を共に生み出すことのできる，異なる要素の構成体または集合体である[17]。
「統合された全体」の概念は，集合の要素と他の要素との関係とは区別され，集合の要素と関係レジームの一部ではない要素との関係を形成する，関係の集合を具現化するシステムという観点から述べることもできる。
システムモデルには 2 つのタイプがある：
<!-- A system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole.
In general, a system is a construct or collection of different elements that together can produce results not obtainable by the elements alone.[17]
The concept of an 'integrated whole' can also be stated in terms of a system embodying a set of relationships which are differentiated from relationships of the set to other elements, and form relationships between an element of the set and elements not a part of the relational regime.
There are two types of system models: -->

1) 離散型では，変数が個別の時点で瞬間的に変化する、
2) 状態変数が時間に対して連続的に変化する連続的なものである[18]。

<!-- 1) discrete in which the variables change instantaneously at separate points in time and,
2) continuous where the state variables change continuously with respect to time.[18] -->

### モデルの生成<!-- ### Generating a model-->

モデル化とは、ある現象の概念的な表現としてモデルを生成する処理過程のことである。
一般に，モデルは問題の現象のいくつかの側面だけを扱う。
そして，同じ現象の 2 つのモデルは本質的に異なるかもしれない。
つまり，両者の違いは単純な構成要素の名前の変更以上のものである。
<!-- Modelling is the process of generating a model as a conceptual representation of some phenomenon.
Typically a model will deal with only some aspects of the phenomenon in question, and two models of the same phenomenon may be essentially different—that is to say, that the differences between them comprise more than just a simple renaming of components. -->

このような違いは，モデルのエンドユーザの要求が異なるためであったり，モデラー間の概念的または美的な違いや，モデリングプロセス中に行われた偶発的な決定によるものであったりする。
モデルの構造に影響を与える可能性のある考慮事項としては，縮小オントロジーに対するモデラーの好み，統計モデル対決定論モデル，離散時間対連続時間などに関する好みなどがある。
いずれにせよ，モデルのユーザーは，ある用途におけるモデルの妥当性に関連する仮定を理解する必要がある。
<!-- Such differences may be due to differing requirements of the model's end users, or to conceptual or aesthetic differences among the modelers and to contingent decisions made during the modelling process.
Considerations that may influence the structure of a model might be the modeler's preference for a reduced ontology, preferences regarding statistical models versus deterministic models, discrete versus continuous time, etc.
In any case, users of a model need to understand the assumptions made that are pertinent to its validity for a given use. -->

モデルの構築には抽象化が必要である。
仮定は，モデルの適用領域を特定するためにモデリングに用いられる。
例えば，特殊相対性理論は慣性参照枠を仮定している。
この仮定は，一般相対性理論によって文脈化され，さらに説明された。
モデルは，その仮定が有効であれば正確な予測を行い，仮定が成り立たなければ正確な予測を行わない。
このような仮定はしばしば，古い理論が新しい理論に引き継がれる際のポイントになる（一般相対性理論は非慣性参照枠でも機能する）。
<!-- Building a model requires abstraction.
Assumptions are used in modelling in order to specify the domain of application of the model.
For example, the special theory of relativity assumes an inertial frame of reference.
This assumption was contextualized and further explained by the general theory of relativity.
A model makes accurate predictions when its assumptions are valid, and might well not make accurate predictions when its assumptions do not hold.
Such assumptions are often the point with which older theories are succeeded by new ones (the general theory of relativity works in non-inertial reference frames as well). -->

### Evaluating a model

See also: Models of scientific inquiry § Choice of a theory

モデルは、何よりもまず経験的データとの整合性によって評価される。再現可能な観測結果と矛盾するモデルは、修正されるか否定されなければならない。
モデルを修正する一つの方法は、そのモデルが高い妥当性を持つとされる領域を限定することである。
その一例がニュートン物理学である。ニュートン物理学は、宇宙の非常に小さく、非常に速く、非常に巨大な現象を除いては、非常に有用である。
しかし、モデルが有効であると認められるためには、経験的データとの適合だけでは不十分である。モデルを評価する上で重要な要素には以下が含まれる[要出典]。
<!-- A model is evaluated first and foremost by its consistency to empirical data; any model inconsistent with reproducible observations must be modified or rejected.
One way to modify the model is by restricting the domain over which it is credited with having high validity.
A case in point is Newtonian physics, which is highly useful except for the very small, the very fast, and the very massive phenomena of the universe.
However, a fit to empirical data alone is not sufficient for a model to be accepted as valid. Factors important in evaluating a model include:[citation needed]-->

### 過去の観測を説明する能力<!-- ### Ability to explain past observations -->

将来の観測を予測する能力
使用コスト（特に他のモデルとの組み合わせにおいて
反証可能性（モデルの信頼度の推定を可能にする
単純さ，あるいは美的魅力
人々は効用関数を用いてモデルの評価を定量化しようとするかもしれない。
<!-- Ability to predict future observations
Cost of use, especially in combination with other models
Refutability, enabling estimation of the degree of confidence in the model
Simplicity, or even aesthetic appeal
People may attempt to quantify the evaluation of a model using a utility function. -->

### 視覚化<!-- ### Visualization -->

可視化とは、メッセージを伝えるために画像、図、アニメーションを作成する技法のことである。
視覚的なイメージによる視覚化は、人間の夜明け以来、抽象的なアイデアと具体的なアイデアの両方を伝える効果的な方法であった。
歴史上の例としては、洞窟画、エジプトの象形文字、ギリシアの幾何学、レオナルド・ダ・ヴィンチの工学的・科学的目的のための革新的な技術的描画方法などがある。
<!--Visualization is any technique for creating images, diagrams, or animations to communicate a message.
Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of man.
Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes. -->

### スペースマッピング<!-- ### Space mapping-->

スペース・マッピングとは、「準グローバル」モデリング定式化を用い て、複雑さの異なる「粗い」（理想的または低忠実度）モデルと「細かい」 （実用的または高忠実度）モデルを結びつける方法論のことである。
工学的最適化において、スペースマッピングは、非常に高速な粗いモデルと、それに関連する計算コストの高い細かいモデルとのアライメント（対応付け）を行い、細かいモデルの直接的で高価な最適化を回避する。
アライメントプロセスは、「マッピングされた」粗いモデル（サロゲートモデル）を繰り返し改良する。
<!--Space mapping refers to a methodology that employs a "quasi-global" modelling formulation to link companion "coarse" (ideal or low-fidelity) with "fine" (practical or high-fidelity) models of different complexities.
In engineering optimization, space mapping aligns (maps) a very fast coarse model with its related expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model.
The alignment process iteratively refines a "mapped" coarse model (surrogate model). -->

### [Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/models-science/)

モデルは多くの科学的文脈において中心的な重要性を持つ。
宇宙論におけるインフレーション・モデル、地球気候の一般循環モデル，DNA の二重らせんモデル，生物学における進化モデル，社会科学におけるエージェント・ベース・モデル，市場の一般均衡モデルなど，それぞれの領域におけるモデルの重要性は，その一例である（本エントリーの最後にある「その他のインターネット・リソース」の節には，これらのモデルについて論じたオンライン・リソースへのリンクが含まれている）。
科学者たちは，モデルの構築，検証，比較，修正に膨大な時間を費やし，多くのジャーナルがモデルの解釈とその意味するところの議論に費やされている。
<!-- Models are of central importance in many scientific contexts.
The centrality of models such as inflationary models in cosmology, general-circulation models of the global climate, the double-helix model of DNA, evolutionary models in biology, agent-based models in the social sciences, and general-equilibrium models of markets in their respective domains is a case in point (the Other Internet Resources section at the end of this entry contains links to online resources that discuss these models).
Scientists spend significant amounts of time building, testing, comparing, and revising models, and much journal space is dedicated to interpreting and discussing the implications of models. -->

その結果，モデルは哲学者の注目を集めるようになり，科学的モデリングの様々な側面に関する膨大な文献が存在するようになった。
哲学がモデルと関わってきた具体的な成果として，哲学文献の中で認識されているモデルの種類が急増したことが挙げられる。
探索モデル，現象学的モデル，計算モデル，発展モデル，説明モデル，貧困モデル，テストモデル，理想化モデル，理論モデル，スケールモデル，発見的モデル，戯画モデル，探索モデル，教訓モデル，空想モデル，最小モデル，玩具モデル，想像モデル，数学モデル，機械論モデル，代用モデル，象徴モデル，形式モデル，アナログモデル，道具的モデルなどが，モデルを分類するために用いられる概念の一部である。
一見するとその多さに圧倒されるが，これらの概念がモデルに関連して生じるさまざまな問題に関係していることを認識することで，コントロールすることができる。
モデルは意味論 semantics（モデルは表現するとすればどのように表現するのか），存在論 ontology （モデルとはどのようなものなのか），認識論 epistemology（モデルを使ってどのように学び、説明するのか），そしてもちろん科学哲学の他の領域においても問題を提起している。
<!-- As a result, models have attracted philosophers’ attention and there are now sizable bodies of literature about various aspects of scientific modeling.
A tangible result of philosophical engagement with models is a proliferation of model types recognized in the philosophical literature.
Probing models, phenomenological models, computational models, developmental models, explanatory models, impoverished models, testing models, idealized models, theoretical models, scale models, heuristic models, caricature models, exploratory models, didactic models, fantasy models, minimal models, toy models, imaginary models, mathematical models, mechanistic models, substitute models, iconic models, formal models, analogue models, and instrumental models are but some of the notions that are used to categorize models.
While at first glance this abundance is overwhelming, it can be brought under control by recognizing that these notions pertain to different problems that arise in connection with models.
Models raise questions in semantics (how, if at all, do models represent?), ontology (what kind of things are models?), epistemology (how do we learn and explain with models?), and, of course, in other domains within philosophy of science. -->


## 議論


## モデルって何？

- 現象の抽象化, 簡略化，単純化
    1. 概念モデル
    1. 数理モデル
    1. グラフィカルモデル: e.g. ボックスアンドアローモデル
    4. see wikipedia.en scientific_modeling

- <span style="color:Blue">モデル</span> の目的: 既知の事実や知識に基いて，現象の特定の部分や特徴を，理解，記述，規定，定量化，視覚化，模倣すること。

- 現象を理解を促すために，概念，操作，定量化，を行うために
    * <span style="color:Blue">概念モデル</span>
    * <span style="color:Blue">数理モデル</span>
    * <span style="color:Blue">グラフィカルモデル</span>
などを用いる。科学にとって <span style="color:Blue">モデル化</span> は不可欠

<div class="memo">

https://en.wikipedia.org/wiki/Scientific_modelling

科学的モデリングは科学的活動であり，その目的は，世界の特定の部分や特徴を，既存の一般的に受け入れられている知識に照らし合わせることによって，理解，定義，定量化，視覚化，シミュレーションを容易にすることである。
そのためには，現実世界の状況に関連する側面を選択・特定し，理解を深めるための概念モデル，合理化を図るための操作モデル，定量化を図るための数学モデル，対象を視覚化するための図解モデルなど，目的に応じてさまざまなタイプのモデルを使い分ける必要がある。
モデリングは，多くの科学分野にとって必要不可欠なものであり，切っても切り離せないものである。

<!-- Scientific modeling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usual ly commonly accepted knowledge.
It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to perationalise, mathematical models to quantify, and graphical models to visual ize the subject.
Modelling is an essential and inseparable part of many scientific disciplines have their own ideas about specific types of modelling. -->
</div>

<img src="https://imgs.xkcd.com/comics/tasks_2x.png" style="width:44%;">

`https://xkcd.com/1425/`

<img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Noam_Chomsky%2C_2004.jpg" style="width:33%;">
<!-- <img src="http://www.plasticoceans.org/wp-content/uploads/2016/10/noam102414.jpg" style="width:34%;"> -->

現在の言語情報処理モデルの趨勢は Chomsky の思い描いた方向には進んでいない(ように見える)。
心理学史を紐解けば，スキナーを批判して行動主義心理学を終わらせる功績が特筆される。
その結果，言語は生得的であるとのラジカルな立場をとることになったのは御存知の通りである。
