---
title: 第19回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+4" color="navy"><strong>人工知能 II (自然言語処理，あるいは系列情報処理編)</strong></font><br/><br/>
<!-- <font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/> -->
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 18/Oct/2024<br/>
Appache 2.0 license<br/>
</div>

## キーワード

コサイン類似度，文埋込みベクトル (あるいは文埋込み表現)，Transformer，符号化・復号化モデル

## 実習

* 実習ファイル [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}
    * データ: [http://www.diana.dti.ne.jp/~fujikura/List/List.html](http://www.diana.dti.ne.jp/~fujikura/List/List.html){:target="_blank"}
* [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb)

## コサイン類似度

自然言語処理，あるいは AI 分野で用いられる対象，あるいは項目間の類似度は，コサイン類似度で表現される場合がある。
**コサイン類似度とは，平均が 0 である場合の変数 $x$ と $y$ との相関係数である**

ベクトル $\mb{x}$ と $\mb{y}$ との成す角の余弦 cosine は，以下のように定義される:

$$
\cos\theta = \frac{\left(\mb{x}\cdot\mb{y}\right)}{\left|\mb{x}\right|\left|\mb{y}\right|}=\frac{\text{ベクトル $\mb{x}$ と $\mb{y}$ との内積}}{\text{ベクトル $\mb{x}$ の長さ $\times$ ベクトル $\mb{y}$ の長さ}}
= \frac{x_1y_1 + x_2y_2}{\sqrt{x_1^2+x_2^2}\sqrt{y_1^2+y_2^{2}}}.
$$

高等学校の数学では，余弦定理として登場したものである。
上式，最右辺は，高等学校の履修内容である 2 次元ベクトルとした場合の定義を表している。
上式最右辺の分母は，三平方の定理から直角三角形の斜辺の長さの積を表している。

一方，上式最右辺の分子は，ベクトル $\mb{x}$ と $\mb{y}$ とで作られる平行四辺形の面積を表す。

このように考えると，相関係数とは，平行四辺形の面積と長方形の面積との比を表しているとも解釈できる。
ただし，ベクトル方向によっては，平行四辺形の面積は負になることもある。
分母である長方形の面積は絶えず正である。
このことから相関係数の正負を決めるのは，分子の計算から決まる。

高等学校で履修した二次元のベクトルではなく，n 次元のベクトルと考えれば，上式最右辺は次式のように書き換えられる:

$$
\cos\theta=\frac{\left(\mb{x}\cdot\mb{y}\right)}{\left|\mb{x}\right|\,\left|\mb{y}\right|}
=\frac{x_1y_1+x_2y_2+\cdots+x_ny_n}{\sqrt{x_1^2+x_2^2+\cdot+x_n^2}\times\sqrt{y_1^2+y_2^2+\cdots+y_n^2}}
=\frac{\sum_{i}x_iy_i}{\sqrt{\sum_ix_i^2}\times\sqrt{\sum_iy_i^2}}
$$

統計学で x から y を予測する単回帰，simple regression では，$y=ax+b$ のような一次式，あるいは一次回帰，を考える。
回帰式の傾きを定める a を推定すれば，x と y との共分散を x の分散で除した形をしている。

$$
a = \frac{\mb{x}\cdot\mb{y}}{|\mb{x|^2}}
$$

<!--
# A2 net

<center>

<img src="/assets/2018Chen_A2-Nets_fig1ja_a.svg" style="width:39%">
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
<img src="/assets/2018Chen_A2-Nets_fig1ja_b.svg" style="width:55%"><br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>

# Relationship between self-attention and convolution

<center>
<img src="/assets/2019cordonnier_self_attention_convol.svg" style="width:66%"><br/>
<img src="/assets/2020Cordonnier_tab3.svg" style="width:66%"><br/>
From [@2020cordonnier_attention_and_convolution]
</center>

# まとめ

- MHSA は 畳み込み と同等の能力がありそうである。
- Reformer に見られるように position encodings を工夫する余地は残されているように思われる。
-->

### Transformer, [Attention is all you need](https://arxiv.org/abs/1706.03762)

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，句，節，文のベクトル表現を得る努力がなされてきた。
適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。

そこで，注意機構を積極的に取り込んだゲームチェンジャーが Transformer である。


* 注意を用いて，RNN を置き換える [Devlin+2017,Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* Transformer の注意とは，このソフトマックス関数である。
* 専門用語としては，**多頭=自己注意** Multi-Head Self-Attention (以下 MHSA と表記)と呼ぶ。
* **自己** がつく注意である理由は，トップダウン信号がないためであろう。
* 上図，クエリ，キー，バリュー ，意味としては，問い合わせ，キー（鍵），値，であるが，とりわけ，Q と K との間に明確な相違はない。
* ある問い合わせ Q に対して，キー K を与えて，その答え A となる値を得ることに相当する。
* この操作を入力情報から作り出して答えを出力する仕組みに，ワンホット表現を使う。

<!-- 下図左は上図右と同じものです。この下図右を複数個束ねると下図中央になります。 -->

- 図中央の Scaled Dot-Product Attention と書かれた右脇に小さく h と書かれている。この h とは ヘッド の意味。
- 図中央を 1 つの単位として，次に来る情報と連結させる。図右。
- リカレントニューラルネットワークでは，中間層の状態が次の時刻の処理に継続して用いられていた。
- ところが 多頭=自己注意 MHSA では一つ前の入力情報を，現在の時刻の情報に対するクエリとキーのように扱って情報を処理する。
- 図右の下から入力される情報は，input と output と書かれている。
さらに output の下には (Shifted right) と書かれています。
すなわち，時系列情報を一時刻分だけ右にずらし（シフト）させて逐次情報を処理することを意味している。
- 図右の下から入力される情報は，embedding つまり埋め込み表現 と 位置符号化 position embedding が足し合わされたもの。
埋め込み表現とは先週 word2vec で触れたベクトルで表現された，単語（あるいはそれぞれの項目）の 意味表現 に対応。

<div class="pagebreak"></div>

#### Transformer の概略図

<div class="figcenter">
<img src="/2023assets/2017Vaswani_Fig2_1ja.svg" width="15%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/2023assets/2017Vaswani_Fig2_2ja.svg" width="25%">&nbsp;&nbsp;&nbsp;
<img src="/2023assets/2017Vaswani_Fig1.svg" width="25%">
<div class="figcaption">

Transformer [2017Vaswani++](https://arxiv.org/abs/1706.03762) Fig.2 を改変。
`matmul` は行列の積，`scale` は，平均 0 分散 1 への標準化，`mask` は 0 と 1 とで，データを制限すること，`softmax` はソフトマックス関数
</div></div>
Transformer における注意 = ソフトマックス関数。<br/>

<div class="pagebreak"></div>

#### 「ちはやふる」Transformer を用いた符号化器・復号化器モデルによる百人一首

* 実習ファイル [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成する自作 Transformer モデル <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1113chihaya_Transformer.ipynb){:target="_blank"}


* データ: [http://www.diana.dti.ne.jp/~fujikura/List/List.html](http://www.diana.dti.ne.jp/~fujikura/List/List.html){:target="_blank"}
* かるた取り遊びを意識して，すべての歌で，ひらがな表記されたデータを用いた。
* 課題： 上句をエンコーダに入力し，デコーダに下句を予測させる。
* 構成単位として [Transformer](https://arxiv.org/abs/1706.03762){:target="_blank"} を用いた。
* 訓練時間短縮のため，1 層のみ。素子数は 32，注意ヘッド数 4，最大系列長 22

<div class="pagebreak"></div>

#### 結果

* エポック:1 損失:2.97051 正解率:   1.000%
* エポック:2 損失:1.22301 正解率:  54.000%
* エポック:3 損失:0.46482 正解率:  76.000%
* エポック:4 損失:0.18355 正解率:  97.000%
* エポック:5 損失:0.07385 正解率: 100.000%
* エポック:6 損失:0.03077 正解率: 100.000%

エポック 4 終了時のエラーは以下のとおり：

<div class="figcenter">
<img src="/2024assets/2023_1113chihaya_epoch4_errors.png" width="66%">
</div>
<div class="figcaption">

百人一首 上句をエンコーダに与えて，下句をデコーダに予測させた結果。3 エポック目の出力を示す。
青は正解文字，赤は，誤りを示す。旧かなである `ゐ` を間違えるのは，低頻度である可能性が考えられる。
</div>


<div class="figcenter">
<img src="/2024assets/2023_1113chihaya_charfreq.svg" width="94%">
</div>

<div class="pagebreak"></div>

### Transformer (SBERT) の文ベクトル

先に紹介した word2vec は，単語ベクトルを得る手法であるが，Transformer は文ベクトルを扱う。
そこで，文単位での類似性を検討した。
下の画像に対して，5 つの脚注がある。

<center>
<img src="/assets/coco175469.jpg" width="55%"><br/>
</center>

1. 夕暮れのハーバーに汽船と複数の鳥が浮かんでいる
2. 水面に浮かぶ4羽の水鳥と、その向こうに停泊している2隻の船
3. 船着き場に2艘の船がとまっている
4. 朝焼けの中待機場所にある旅客船とマガモ
5. 停められた船の近くで水鳥が泳いでいる<br/>
MS COCO データセットより: <http://farm5.staticflickr.com/4055/4704393899_a041476b4a_z.jpg>

上図は，MS COCO 画像データと画像に対応する脚注からなるデータセットからの一例である。
日本語文は，千葉工業大学 STAIRLABO が公開しているデータである。
人間が見れば，写真と文章とは結びいていることが分かる。
加えて，5 つの脚注も相互に似ていることが分かる。
MS COCO データセットでは，一枚の写真に 5 つの脚注が紐付けられている。

コンピュータにこれらの文章が似ていることを判断させようとすると，最近まで難しい仕事であった。
本章で紹介する，文の意味ベクトルを用いると，これらの文章が相互に似ていると判断させることが可能である。
下図は tSNE を用いて，日本語の文章の類似度を sentence BERT を用いて表現し，文章の類似度に従って地図を描いたものである。
図では，同じ写真に紐付いている文章は同じ色で表現している。

<center>
<img src="/assets/2022_0915sbert_staircoco500.svg" style="width:77%">
</center>

<!--
# 自然言語系の注意
-->

#### 言語と機能的脳画像研究を結びつけるために単語の分散表現を機械学習的手法で表現する (Mitchell+2018 他)

- [名詞の意味に関連した人間の脳活動の予測, Mitchell, 2018, Predicting Human Brain Activity Associated with the  Meanings of Nouns](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}

<div class="figcenter">
<img src="/assets/2019mitchell-54_20.png" style="width:49%">
<img src="/assets/2008Mitchell_fig1ja.svg" style="width:49%">
<div class="figcaption" style="width:94%">

Mitchell+2008 図 1. 任意の名詞刺激に対する fMRI 活性化を予測するモデルの形式<br/>
左のように 「セロリ」 から右の脳画像を予測するために，中間表現として， 兆 単位の言語コーパス (言語研究では訓練や検証に用いる言語データをコーパスと呼ぶ) から得られた **意味特徴** を用いる。

fMRI の活性化は 2 段階の処理から予測される。
第 1 段階では，入力刺激語の意味を，典型的な単語使用を示す大規模なテキストコーパスから値を抽出した中間的な意味的特徴の観点から符号化する。
第 2 段階では，これらの中間的な意味的特徴のそれぞれに関連する fMRIシグネチャ の線形結合として，fMRI 画像を予測する。
<!-- Form of the model for predicting fMRI activation for arbitrary noun stimuli.
fMRI activation is predicted in a two-step process.
The first step encodes the meaning of the input stimulus word in terms of intermediate semantic features whose values are extracted from a large corpus of text exhibiting typical word use.
The second step predicts the fMRI image as a linear combination of the fMRI signatures associated with each of these intermediate semantic features. -->
</div></div>

<br/><br/>

<div class="figure figcenter">
<img src="/assets/2008Mitchell_fig2.svg" style="width:88%">
<div class="figcaption" style="width:88%">

Mitchell (2008) 図 2. 与えられた刺激語に対する fMRI 画像の予測。<br/>

他の単語 (下図左) eat, taset, fill などの単語から セロリ を予測する回帰モデルを使って予測する。
(A) 参加者 P1 が 「セロリ」刺激語に対して、他の 58 の単語で学習した後に予測を行う。
25 個の意味的特徴のうち 3 つの特徴量のベクトルを単位長にスケーリングすることである。
(食べる, 味わう, 満たす) について学習した $c_{vi}$ 係数は，パネル上部の 3 つの画像のボクセルの色で示されている。
刺激語「セロリ」に対する各特徴量の共起値は， それぞれの画像の左側に表示されている (例えば 「食べる（セロリ）」の 共起値は 0.84)。
刺激語の活性化予測値 ((A）の下部に表示)  は 25個 の意味的 fMRI シグネチャを線形結合し， その共起値で重み付けしたものである。
この図は 予測された三次元画像の1つの水平方向のスライス [z=-12 mm in Montreal Neurological Institute (MNI) space] を示している。
(B) 「セロリ」と「飛行機」について， 他の 58 個の単語を使った訓練後に予測された fMRI 画像と観察された fMRI 画像。
予測画像と観測画像の上部（後方領域）付近にある赤と青の 2本 の長い縦筋は、左右の楔状回である。
<!-- Predicting fMRI images for given stimulus words.
(A) Forming a prediction for participant P1 for the stimulus word “celery” after training on 58 other words.
Learned $c_{vi}$ coefficients for 3 of the 25 semantic features (“eat,” “taste,” and “fill”) are depicted by the voxel colors in the three images at the top of the panel.
The co-occurrence value for each of these features for the stimulus word “celery” is shown to the left of their respective images [e.g., the value for “eat (celery)” is 0.84].
The predicted activation for the stimulus word [shown at the bottom of (A)] is a linear combination of the 25 semantic fMRI signatures, weighted by their co-occurrence values.
This figure shows just one horizontal slice [z = –12 mm in Montreal Neurological Institute (MNI) space] of the predicted three-dimensional image.
(B) Predicted and observed fMRI images for “celery” and “airplane” after training that uses 58 other words.
The two long red and blue vertical streaks near the top (posterior region) of the predicted and observed images are the left and right fusiform gyri.-->
</div></div>

<br/><br/>

<div class="figure figcenter">
<img src="/assets/2008Mitchell_fig3.svg" style="width:49%"><br/>
<div class="figcaption" style="width:88%">

Mitchell (2008) 図 3. 最も正確に予測されたボクセルの位置<br/>

参加者 P5 の訓練セット以外の単語について、予測されたボクセルの活性化と実際のボクセルの活性化の相関を表面（A）とグラスブレイン（B）で表したもの。
これらのパネルは、少なくとも 10個 の連続したボクセルを含むクラスタを示しており、それぞれのボクセルの予測-実際の相関は少なくとも 0.28 である。
これらのボクセル・クラスターは、大脳皮質全体に分布しており、左右の後頭葉と頭頂葉、左右の豆状部、中央後葉、中央前葉に位置しています。
左右の後頭葉、頭頂葉、中前頭葉、左下前頭回、内側前頭回、前帯状回に分布している。
(C) 9人の参加者全員で平均化した予測-実測相関の表面表現。
このパネルは、平均相関が 0.14 以上の連続した10 個以上のボクセルを含むクラスターを示している。
<!-- Locations of most accurately predicted voxels.
Surface (A) and glass brain (B) rendering of the correlation between predicted and actual voxel activations for words outside the training set for participant P5.
These panels show clusters containing at least 10 contiguous voxels, each of whose predicted-actual correlation is at least 0.28.
These voxel clusters are distributed throughout the cortex and located in the left and right occipital and parietal lobes; left and right fusiform,
postcentral, and middle frontal gyri; left inferior frontal gyrus; medial frontal gyrus; and anterior cingulate.
(C) Surface rendering of the predicted-actual correlation averaged over all nine participants.
This panel represents clusters containing at least 10 contiguous voxels, each with average correlation of at least 0.14. -->
</div></div>

## 符号化器・復号化器モデル (encoder-decoder model), seq2seq model

* 中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば **機械翻訳** や **対話** のモデルになる。
* 初期の翻訳モデルである "seq2seq" の概念図を示した。
* "eos" は文末 end of sentence を表す。
* 中央の "eos" の前がソース言語であり，中央の "eos" の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。

* 注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がない。
* 逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
* この点を改善することを目指すことが 2014 年以降盛んになった。
* 顕著な例が後述する **双方向 RNN**, **LSTM** を採用したり，**注意** 機構を導入することであった。

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" width="49%"><br/>
From [2014Sutskever_Sequence_to_Sequence]
rom [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="/assets/2014Sutskever_Fig2left.svg" width="44%">
<img src="/assets/2014Sutskever_Fig2right.svg" style="width:44%"><br />
From [@2014Sutskever_Sequence_to_Sequence] Fig. 2, 3
</center>



### Karapatian+(2023)

<div class="figcenter">
<img src="/2024assets/2023Karapetian_fig2.jpg" style="width:39%">
</div>

<div class="figcaption" style="width:88%">

図 2. 情景の同一性とカテゴリの復号化<!-- Figure 2. Scene identity and category decoding. --><br/>
**(A)** カテゴリ課題(青)，気晴らし課題(マゼンタ)，およびその差(黒)の EEG データにおける，情景同一性復号化の結果。
0 ミリ秒の灰色の縦破線は刺激の開始を表す。
曲線の周囲の斜線部分は SEM を示す。
有意な時点(右側，p < 0.01, FDR 調整済み) はアスタリスクで示されている。<br/>
**(B)**  課題の結果と両者の差異における，情景カテゴリーの復号化 (自然対人工) の結果<br/>
**(C)** カテゴリー化および気晴らし課題における，情景同一性の復号化のピーク<br/>
**(D)** シーンカテゴリーのデコーディングのピークにおける，多次元尺度構成法の結果<br/>
**(E)** 情景の同一性復号化と情景カテゴリ復号化の頂点復号化潜時における両課題のチャネル空間で実行されたサーチライト解析の結果と<br/>
**(F)** 情景カテゴリー復号化。<br/>
有意なチャネル (右側 p < 0.01, FDR 調整済み) は黒点で示されている。

<!-- * (A) Pairwise scene identity decoding results on EEG data from the categorization task (blue), distraction task (magenta), and their difference (black).
The vertical dashed gray line at 0 msec represents the stimulus onset.
The shaded area around the curves indicates the SEM.
Significant time points (right tailed, p<0.01, FDR-adjusted) are indicated with asterisks.
* (B) Scene category decoding (natural vs. man-made) results for both tasks and their difference.
* (C) Multidimensional scaling results for scene identity decoding from the categorization and distraction tasks at the scene identity decoding peak and
* (D) scene category decoding peak.
* (E) Results from the searchlight analysis performed in channel space in both tasks at peak decoding latency for scene identity decoding and
* (F) scene category decoding.
Significant channels (right-tailed, p < 0.01, FDR-adjusted) are depicted with black dots. -->
</div>


<div class="figcenter">
<img src="/2024assets/2023Karapetian_fig4.jpg" style="width:39%">
</div>


図 4. 人間の神経情景表現を RCNN と FCNN とでモデル化
<!-- Figure 4. Modeling human neural scene representations with an RCNN versus an FCNN. -->

* (A) 解析に用いたリカレントCNN，BLnet (Spoerer+2020) のアーキテクチャ。
ネットワークは 7 層からなり，ボトムアップ (緑の矢印) とラテラル (黒の矢印) 接続で結ばれている。
特徴量は 3 つの層 (1, 4, 7) から 8 つの異なる時間ステップで抽出され，RT は読み出し層から収集された。
* (B) すべての情景，自然情景，人工的情景について，ヒトの神経表現と 3 つの異なる層からの RCNN 特徴量 (8 つの時間ステップの中央値) に対して RSA を実行した結果。
0 ミリ秒の垂直破線は刺激開始を表す。
曲線の周りの斜線部分は SEM を表す。
有意な時点をアスタリスクで示す (右側検定 p＜0.05，FDR 補正)。
破線の縦線はピークを示す。
灰色の斜線はノイズの上限を示す。
* (C) BLnet の フィードフォワード，パラメータマッチ版である B-Dnet (Spoerer+2020) の特徴量を用いた RSA 結果。
* (D) RCNN と FCNN の結果の差の波 (両側, p < 0.05, FDR 補正)。

<!-- * (A) Architecture of BLnet (Spoerer+2020), the recurrent CNN used in the analysis.
The network consists of seven layers, linked via bottom–up (green arrows) and lateral (black arrows) connections.
Features were extracted from three layers (1, 4, and 7) at eight different time steps, and RTs were collected from the readout layer.
* (B) Results of the RSA performed on the neural representations of humans and RCNN features from three different layers (median over eight time steps), for all scenes, natural scenes, and man-made scenes.
The vertical dashed gray line at 0 msec represents the stimulus onset.
The shaded areas around the curves represent the SEM. Significant time points are denoted with asterisks (right-tailed, p < 0.05, FDR-corrected).
The dashed vertical lines indicate the peaks.
The shaded gray area represents the noise ceiling.
* (C) RSA results with features from B-Dnet (Spoerer et al., 2020), the feedforward, parameter-matched version of BLnet.
* (D) Difference waves between RCNN and FCNN results (two-tailed, p < 0.05, FDR-corrected). -->


### ERP モデル Laszlo&Armstrong(2014)<!-- ## 1.1. The ERP model-->

これまでの研究で，ERP モデルの開発を通じて，計算と認知電気生理学の間のギャップを埋める取り組みを始めた(Laszlo&Plaut2012)。
ERP モデルは，先行する PDP モデル (Harm&Seidenberg2004, Plaut+1996, Seidenberg&McClelland1989 など) を大幅に取り入れている。
図 1 は，先行モデルと同様，書記素入力の分散パターンを取り込み，隠れ層で複数の非線形変換を行った後，分散意味出力を生成するモデルの構造を示している。
<!-- In prior work, we began bridging the gap between computation and cognitive electrophysiology through development of the ERP model (Laszlo&Plaut, 2012).
The ERP model is heavily based on PDP models that preceded it (e.g., Harm&Seidenberg2004, Plaut+1996, Seidenberg&McClelland1989); Fig. 1 displays the architecture of the model, which, like its predecessors, takes a distributed pattern of orthographic input, and after multiple nonlinear transformations in hidden layers, produces a distributed semantic output. -->

<div class="figcenter">
<img src="/2024assets/2014Laszlo_Armstrong_PSP_fig1.svg" style="width:49%">
<div class="figcaption" style="width:49%">

Fig. 1. (A) Architecture of the ERP model. INH stands for ‘‘inhibitory’’. (B) The shape of the sigmoid function (inset), and of the alpha function above and below threshold. Note that for alpha units, as $t\rightarrow\infty$, $V\rightarrow\Theta$.
</div>
</div>

ERP モデルと先行モデルとの重要な違いは，行動だけでなく ERP の構成要素の効果もシミュレーションする必要があることである。
具体的には，N400 (語彙的意味的アクセスを試みる構成要素と考えられている。Kutas&Federmeier2011 参照) に関連する効果である。
これを可能にするために，ERP モデルには PDP 読みモデルには典型的なものではない神経学的現実的な特性が与えられた。
まず，ERP モデルが先行するモデルと異なるのは，興奮と抑制の分離である。
この分離により，興奮性素子よりも抑制性素子の方が多いこと (EPSP が優勢であると考えられている ERP のシミュレーションには重要)，興奮と抑制の時間経過が別々であること，抑制には高速と低速の集団があることなど，神経学的には現実的な特性がいくつか付与される。
しかし，ERP モデルには，真の皮質系にみられる数多くの特性が欠けている。
我々は，モデルにさらに神経のリアリズムを取り入れることで，より多くの N400 効果をシミュレートできるようになると考えた。
さらに，このリアリズムを提供することで，シミュレートされた効果の神経機構に関する洞察が得られる可能性がある。
これは，N400 の研究では基本的に未開拓の分野である。
<!--An important difference between the ERP model and its predecessors is that it is required to simulate not only behavior, but also ERP component effects—specifically, effects pertaining to the N400 (a component thought to represent attempted lexical-semantic access; see Kutas&Federmeier2011).
In order to enable this, the ERP model was given neurally realistic properties not typical in PDP reading models.
Primarily, the ERP model’s departure from its predecessors comes in its separation of excitation and inhibition.
This separation confers several neurally realistic properties, such as more excitatory than inhibitory units (important for simulation of ERPs, where EPSPs are thought to dominate), separate time courses of excitation and inhibition, and fast and slow populations of inhibition.
However, the ERP model lacks numerous characteristics of a true cortical system. We theorized that bringing additional neural realism to the model would enable it to simulate more N400 effects, and, further, that providing this realism could provide insight into the neural mechanisms of the simulated effects—an area essentially unexplored in the N400 literature. -->

ERP モデルは、無意味テキストに対する反応で観察された N400 効果をシミュレートした。
もちろん，無意味なテキストは文脈を伴わないという点で，自然な読みとは異なる。
したがって，ERP モデルを現実的な読みにより関連付けるには，文脈に対する感度を向上させることが重要である。
文脈の最も単純な形態であり，N400 に強い影響を与える形態は，単語の形の即時反復である (例: Nagy&Rugg1989, Rugg1985, 1990, Rugg&Nagy1987)。
この最小限の文脈では，単語の形を処理する際に，その前に何があったかに依存することが必要となる。
したがって，この現象を明確に機械論的に説明することは，孤立した項目に対する ERP 反応を理解することと，文脈のある項目に対する反応を理解することの橋渡しをする上で重要な第一歩となる。
<!-- The ERP model simulated N400 effects observed in response to unconnected text.
Of course, unconnected text is dissimilar to natural reading in that it does not involve context.
To extend the ERP model’s relevance to realistic reading, therefore, it is important to extend its sensitivity to context.
The simplest form of context, and a form that exerts a robust effect on the N400, is immediate repetition of a word form (e.g., Nagy&Rugg1989, Rugg1985, 1990, Rugg&Nagy1987).
This minimal context requires that processing a word form, in the simplest possible manner, be dependent on what has come before it.
Consequently, providing an explicit mechanistic account of this phenomenon is an important first step in making the bridge between understanding the ERP response to isolated items and understanding the response to items in context. -->

N400 反復効果は，単語形態が 2 度目に提示された際の正の反応として特徴づけられる。
この効果に関する認知理論として広く受け入れられているのは，単語の形態が最初に提示された後，その意味は直ちに非活性化されるのではなく，時間とともに減衰するというものである(Rugg1985)。
したがって，ある項目が繰り返される場合，その項目に関連する意味は依然として活性化しており，したがって，より複雑な意味処理は必要なく，その結果，繰り返しに対する N400 はより小さくなる。
この理論は広く受け入れられており，その理論が確立されて以来，基本的に異議が唱えられたことはない (Besson+1992, Laszlo&Federmeier2007, Rugg1990 など)。
N400 反復効果をシミュレーションできるように，このモデルの神経現実主義を拡張するにあたり，我々は，神経機構的な説明が，一般的に想定されている認知基盤に対して新たな洞察をもたらすかどうかを検討した。
<!-- The N400 repetition effect is characterized as a positivity in response to second presentation of a word form.
The accepted cognitive theory of this effect is that after an initial presentation of a word form, its semantics do not immediately deactivate; rather, they decay over time (Rugg1985).
Thus, when an item is repeated, its associated semantics are still active, and, therefore, less elaborate semantic processing is required, resulting in a smaller N400 to repetitions. This theory is widely accepted, and essentially has not been challenged since its formulation (e.g., Besson+1992, Laszlo&Federmeier2007, Rugg1990).
In extending the neural realism of the model to allow it to simulate N400 repetition effects, we sought to explore whether a neuro-mechanistic explanation would provide novel insight into their generally-assumed cognitive basis. -->

さらに，単語の認知に関する文献の多くは時間領域の ERP に焦点を当てているが，周波数領域の ERP から得られた洞察を強調する研究も増えている。
例えば，最近の研究では，時間領域における N400 効果は，特定の周波数帯域における変化によるものであり，N400 を生成する集団のすべてのニューロンが反復後に同じ程度にその活動を変化させる場合に観察されるような，全電力スペクトルにおける変化によるものではない可能性があることが示されている (Roehm+2007)。
周波数依存の変化は，分散型で特定の表現を符号化する局所および遠位の神経集団を同期させる脳の能力にも重要な影響を及ぼす可能性がある (Mellem+2013, Weiss&Mueller2003)。
周波数領域の効果は理論的には興味深いものであるが，計算読解の文献ではこれまでまったく研究されていない。
これらの理由から，我々はシミュレーションに周波数領域分析を組み込むことを試みた。
我々の知る限り，この分野では初めてのことである。
<!-- Additionally, although much of the word recognition literature has focused on time-domain ERPs, there is a growing body of work highlighting insights gained from ERPs in the frequency domain.
For instance, recent work has demonstrated that time-domain N400 effects may be due to changes in particular frequency bands and not to changes in the full power spectrum, as would be observed if all neurons in the population generating the N400 modulated their activity to the same degree following a repetition (Roehm+2007).
Frequency-dependent changes may also have important ramiﬁcations for the brain’s capacity to synchronize local and distal neural populations that code for a particular representation in a distributed fashion (Mellem+2013, Weiss&Mueller2003).
Frequency-domain effects are therefore of theoretical interest, but have been completely unstudied in the computational reading literature. For these reasons, we sought to incorporate frequency-domain analysis into our simulations—to our knowledge, for the ﬁrst time in this literature. -->

### アルファモデル<!-- ## 1.2. The alpha model-->

ERP モデルでは，平均的な意味活性化は平均的な N400 振幅と関連している。(脚注2)
したがって，繰り返しによって減少した N400 をシミュレートするモデルでは，反復が発生した際に平均的な意味活性化が減少することが示されなければならない。
つまり，素子には疲労する能力が備わっていなければならない。
この疲労は，意味層全体ではなく，単一素子に作用する形で選択的に発生することが重要である。
なぜなら，最近活性化していない素子は，反復ではなく新しい項目が提示された場合のように，最大限に活性化できなければならないからである。
したがって，個々の意味単位の活性化の望ましい動態は，活性化のピーク (最初の提示に対する反応) が徐々に減衰していくというものである。
これは，N400 反復効果の認知理論が提唱しているものであり，また反復による N400 振幅の減少にも必要である。
重要なのは，この動態はアルファ関数によって正式に表現できるということである。
これは、神経計算において PSP をシミュレートするために使用される。
<!--In the ERP model, mean semantic activation is linked to mean N400 amplitude.(footnote 2)
Thus, for the model to simulate reduced N400s with repetition, it must display reduced mean semantic activation when repetitions occur.
That is, units must have the capacity to become fatigued. It is important that this fatigue occur selectively, acting on single units as opposed to the entire semantic layer, because units that have not recently been active must be able to activate to maximum, as when a novel item is presented instead of a repetition.
The desired dynamic of activation for individual semantic units is thus one where a peak of activation (response to a ﬁrst presentation) is followed by gradual decay, as posited by the cognitive theory of N400 repetition effects and also as necessary to reduce N400 amplitude with repetition.
Crucially, this dynamic can be formally expressed by the alpha function; used in neural computation to simulate PSPs: -->

$$
V=\alpha t e^{t/T}\tag{1}
$$

式(1):アルファ関数。
従来用いられてきたアルファ関数 (例:Bugmann1997) では，V は膜電位 (電圧) の測定値，alpha はスケーリング定数，t は単位が活性化してからの時間ステップ数，T は V がピークに達するタイミングを決定する自由パラメータである (例:David+2006)。
alpha 関数の形状は，式(1) で定義され，シミュレーションで使用されている。
図 1 に示した。
<!-- Eq. (1): The alpha function.
In the alpha function as used classically (e.g., Bugmann1997), V is a measure of membrane potential (voltage), a a scaling constant, t the number of time steps since a unit became active, and T a free parameter that determines when V peaks (e.g., David+2006).
The shape of the alpha function, as deﬁned in Eq. (1) and as used in our simulations, is displayed in Fig. 1. -->

アルファ関数が事象関連電位のシミュレーションで使用されていることから，我々のモデルでの使用に特に適している。
それは，所望の動的が生成されるからだけでなく，皮質事象関連電位が ERP 信号のソースであるためである(Fabiani+2007)。
アルファ関数の結果である V は，意味素子の活性化がリンクされている N400 と同様に，電圧を表している。
実際，この関数の妥当性は，誘発反応の動的因果モデリングにおける類似の関数の使用によって裏付けられている(Dauizeau+2011 参照)。
この種の関数は，実際のニューロンにおける活性化の動力学に近似することが示されている (David+2006)。
したがって，N400 反復効果を実装するために必要な機能の動態に関する独立した観察結果，その効果の神経源，およびアルファ関数の計算特性が，シミュレーションの機序を示唆する方向に収束している。
したがって，ERP モデルを N400 反復効果のシミュレーションに拡張しようとする試みにおいて，我々は興奮性素子の活性化をアルファ関数の包絡線 (式(1)で指定) に制限した。
<!-- That the alpha function is used in simulation of PSPs makes it especially appropriate for use in our model, not only because it produces the desired dynamic, but also because cortical PSPs are the source of the ERP signal (Fabiani+2007).
The result, V,of the alpha function represents a voltage, as does the N400, to which semantic unit activations are linked. Indeed, the appropriateness of this function is supported by use of an analogous function in dynamic causal modeling of evoked responses (see Dauizeau+2011), where this type of function has been shown to approximate activation dynamics in actual neurons (David+2006).
Thus, independent observations about the dynamics of the function needed to implement N400 repetition effects, the neural source of those effects, and the computational properties of the alpha function converge to suggest a mechanism for simulation. Therefore, in our attempt to extend the ERP model to simulation of N400 repetition effects, we constrained excitatory unit activations to the envelope of the alpha function (as speciﬁed in Eq. (1)). -->

以下では，Laszlo&Plaut(2012) のモデルを ERP モデルと呼び続けるが，アルファ関数で制約されたモデルをアルファモデルと呼ぶ。
アルファモデルにおける興奮性素子の活性化にアルファ関数 (式(1))を適用することが，2 つのモデル間の唯一の違いである。(脚注 3)
以下に紹介するシミュレーションの目的は，アルファ関数で実装された選択的疲労要因が，N400 反復効果に対する正式に十分な機構的説明となるかどうかを判断することである。
<!-- In what follows, we will continue to refer to the Laszlo&Plaut(2012) model as the ERP model, but we will refer to the model constrained with the alpha function as the alpha model.
Application of the alpha function (Eq. (1)) to excitatory unit activation in the alpha model is the only distinction between the two models.(footnote 3)
The goal of the simulations presented below was to determine whether a selective fatigue factor, as implemented with the alpha function, constitutes a formally sufﬁcient mechanistic explanation for N400 repetition effects. -->

<div class="footnote">

2. 多数の皮質内抑制性電位 (IPSP) と興奮性電位 (EPSP) の遠心性和によって決定される電圧。
3. モデルの構造は他のすべての点で同一であるため，アルファモデルは，ERP モデルがシミュレーション可能なあらゆる現象をシミュレーションする能力を形式的に保持していることに留意されたい。
</div>

### ERP

各項目タイプについて，第 1 回および第 2 回提示時の中央頭頂電極における ERP の平均値 (図 2) を計算した。
このデータセットにおける N400 効果に対応する時間ウィンドウに合わせてデータをトリミングした: 250ー450ミリ秒 (Laszlo&Federmeier2011)。
ERP とシミュレーション分析の一貫性を最大限に高めるため，これらのデータは，統計的に定義された関心領域，すなわち N400 窓の半値全幅 (FWHM) に再びトリミングされた。
<!--Grand-averaged ERPs (Fig. 2) were computed over the middle parietal electrode for each item type on ﬁrst and second presentation.
Data were trimmed to the time-window corresponding to N400 effects in this data set: 250–450 ms (Laszlo&Federmeier2011).
To maximize the consistency of ERP and simulation analyses, these data were again trimmed to a statistically-deﬁned window of interest, the full width at half-maximum (FWHM) of the N400 window. -->

<div class="figcenter">
<img src="/2024assets/2014Laszlo_Armstrong_PSP_fig2.svg" style="width:66%">
<div class="figcaption" style="width:77%">

図 2. ERP とモデル (sERP) の時間および周波数領域のデータ。
時間領域の ERP データは，頭頂葉中央の電極部位における単語，頭文字語，擬似語，および非単語文字列の提示 1 回目と 2 回目に対する全体平均反応からなる。
同じデータが周波数領域で提示される。
時間領域の sERP データは，同じ種類の項目の提示 (1 回目と 2 回目) に対するすべての意味単位の平均反応からなる。
同じデータが周波数領域で提示される。
反復効果のシミュレーションにアルファ関数の適用が必要かどうかを評価するために実施された制御シミュレーションも，オリジナルの ERP モデルを使用して実施された。
このシミュレーションでは，すべての方法は上記で説明したものと同じであったが，アルファ関数は適用されなかった。
<!-- Fig.2. ERP and model (sERP) data in the time and frequency domains.
Time-domain ERP data consists of grand-averaged responses to ﬁrst and second presentations of words, acronyms, pseudowords, and illegal strings, over the middle parietal electrode site; the same data is presented in the frequency domain.
Time-domain sERP data consists of responses, averaged over all semantic units, to ﬁrst and second presentations of the same item types.
The same data is presented in the frequency domain.
A control simulation, performed in order to assess whether application of the alpha function is necessary for simulation of repetition effects, was also conducted using the original ERP model—in this simulation, all methods were identical to those described above, but the alpha function was not applied. -->
</div></div>



# トランスフォーマー

* 注意を用いて，RNN を置き換える [Devlin+2017,Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* 専門用語としては，**多頭=自己注意** Multi-Head Self-Attention (以下 MHSA と表記)と呼ぶ。
* 多頭とは何か，なぜ **自己** がつく注意なのかを確認してほしい。

<center>
<img src="/assets/ModalNet-19.png" style="width:15%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-20.jpg" style="width:23%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-21.png" style="width:29%">
</center>
<!--
![](assets/ModalNet-19.png){style="width:15%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-20.jpg){style="width:23%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-21.png){style="width:29%"}
</center>
-->


* 上図，クエリ，キー，バリュー に注目してください。英単語の意味どおりに解釈すれば，問い合わせ，キー（鍵），値，となる。
* つまり，ある問い合わせに対して，キーを与えて，その答えとなる値を得ること。
* この操作を入力情報から作り出して答えを出力する仕組みに，ワンホット表現を使うことがポイント

<!-- 下図左は上図右と同じものです。この下図右を複数個束ねると下図中央になります。 -->

- 図中央の Scaled Dot-Product Attention と書かれた右脇に小さく h と書かれている。この h とは ヘッド の意味。
- 図中央を 1 つの単位として，次に来る情報と連結させる。図右。
- リカレントニューラルネットワークでは，中間層の状態が次の時刻の処理に継続して用いられていた。
- ところが 多頭=自己注意 MHSA では一つ前の入力情報を，現在の時刻の情報に対するクエリとキーのように扱って情報を処理する。
- 図右の下から入力される情報は，input と output と書かれている。
さらに output の下には (Shifted right) と書かれています。
すなわち，時系列情報を一時刻分だけ右にずらし（シフト）させて逐次情報を処理することを意味している。
- 図右の下から入力される情報は，embedding つまり埋め込み表現 と 位置符号化 position embedding が足し合わされたもの。
埋め込み表現とは先週 word2vec で触れたベクトルで表現された，単語（あるいはそれぞれの項目）の 意味表現 に対応。
* さらに，下図右は，視覚用に開発れたトランスフォーマーである。

<center>
<img src="/assets/ModalNet-19.png" style="width:14%">
<!-- <img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:24%"> -->
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2019Ramachandran_fig3.jpg" style="width:44%"><br/>
<!-- <img src="https://komazawa-deep-learning.github.io/assets/2019Ramachandran_fig3.jpg" style="width:64%"><br/> -->
Left: [@2017Vaswani_transformer], Right: [@2019Ramachandran_attention_vision]
</center>

<!--
<center>

![](assets/2019Zhang_Goodfellow_SAGAN_fig2.jpg){style="width:88%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1upper.jpg){style="width:74%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1lower.jpg){style="width:74%"}<br/>
From [@2019Zhang_Goodfellow_SAGAN] Fig. 1, and 3.
画像生成において，近傍画素から情報だけでなく，関連する遠距離の特徴を利用して生成することにより一貫性のある対象やシナリオを生成可能。
各行の左の元画像上のカラー点は 5 つ の 代表的なクエリの場所を示す。
右側の 5 画像は 各クエリ位置における注意地図。最も注目されている領域が，色分けされた矢印で示されている。
</center>
-->

<!--
<center>

![](assets/2017Gupta_Non-local_fig2.svg){style="width:29%"}
![](assets/2017Gupta_Non-local_example_230_0_eps_18_9.svg){style="width:59%"}<br/>
時空の非局所ネットワークの概念図。特徴地図はテンソルとして示されている。
例えば 1024 チャンネルの場合は $T\times H\times W\times1024$ である。
$\otimes$ は行列積を，$\oplus$ は要素和を示す。
ソフトマックス演算は各行に対して実行される。
青いボックスは $1\times1\times1\times1$ の畳み込みを表す。
$512$ チャンネルのボトルネックを持つ埋め込みガウシアン版が示されている。
バニラガウス版は $\theta$ と $\phi$ とを除去することで ドット積版は $1/N$ のスケーリングでソフトマックスを置き換えることで行うことができる。
From [@2018Wang_Girshick_Non-local]
</center>
-->

<!-- <center>

![](assets/2018snail_fig2b.svg){style="width:49%"}<br/>
From [@2018Mishra_SNAIL] Fig. 2
</center>

トランスフォーマーはリカレント構造や畳み込み構造を持たず埋め込みベクトルに位置符号化器を加えることで系列情報を処理する。
しかし、逐次的な順序情報が貧弱であるとの批判がある。
とりわけ強化学習のような位置依存性に敏感な課題では問題。
トランスフォーマーモデルにおける 位置問題を解決するため，自己注意機構 と 時間的な畳み込み temporal convolution を組み合わせたモデルが
Simple Neural Attention Meta-Learner (SNAIL)[@2018Mishra_SNAIL]。
SNAIL は，メタ学習，強化学習の両方の課題に優れていることが実証された。
-->

少しだけまとめると:

- 自然言語処理，画像処理，強化学習，メタ学習の 4 分野でほほ同様の 多頭自己注意 MHSA が取り入れられている。
- クエリ，キー，バリュー の重みを学習することが MHSA の学習である。
- 従来手法である 畳み込み や LSTM を MHSA で置き換える動きがある。

# 5. BERT

- 上記のトランスフォーマーに基づいて BERT が提案された [Devlin2018](https://arxiv.org/abs/1810.04805)。
- BERT は **B**idirectional **E**ncoder **R**epresentations from **T**ransformers から命名したと原著論文には書いてあります。
- ですが，この原著論文の直前に提案されたモデルに ELMo があったため，こじつけた，ふざけた命名でしょう。
- もちろん ELMo (こちらは **E**mbeddings from **L**anguage **Mo**dels から命名されました)も BERT もセサミストリートに出てくるキャラクタです。

<!-- From singularitysalon2019/nlp.tex -->

<!--BERT の影響が大きいので，本稿でも BERT を中心に取り上げる。-->BERT の特徴を 3 つにまとめると以下の通り

1. トランスフォーマー Transformer に基づく 多頭自己注意 (MHSA) を使った多層ニューラルネットワークモデル
2. 2 つの事前訓練: **マスク化言語モデル** と **次文予測課題** を用いる
3. 事前訓練済のモデルを用いて，解くべき課題のそれぞれについて **ファインチューニング** Fine tuning を施す
4. 個別の課題は下流課題 down stream tasks と呼ばれます。上流 と 下流 との区別は，最初に行う事前訓練のことを時間的に先行するので上流，その後のファインチューニングするそれぞれの課題のことを下流課題と呼んでいます。
5. 複数の課題に対して個別にファインチューニングを行うことにより，複数の下流課題で性能向上が認められました。 [GLUE スコアボード](https://gluebenchmark.com/leaderboard), [SuperGLUE](https://super.gluebenchmark.com/leaderboard/) を参照してください。


## BERT の入力表現

- 上の図にもあったとおり BERT では入力情報が埋め込み表現だけでなく，位置符号化器の情報が加算されます。
- BERT では，埋め込み表現と位置符号化器の情報に加えて，セグメント埋め込み segment embeddings も加えた情報が入力情報となります。下図参照

<center>
<img src="/assets/2018Devlin_BERT_Fig2.svg" style="width:66%"><br/>
<!-- ![](assets/2018Devlin_BERT_Fig2.svg){style="width:84%"}<br /> -->
埋め込みトークンの総和，位置符号器，分離埋め込みの 3 者 From [@2018BERT] Fig. 2
</center>

- 上図では，下 3 行が入力情報を構成する 3 つの要素になっています。上（ピンク色）が合算した入力情報になります。
- 3 つの入力情報とはそれぞれ，下から 位置符号化器 （薄灰色），セグメント埋め込み (淡緑)，トークン埋め込み (淡黄) です。

## 位置符号器 Position encoders

- 上述のようにトランスフォーマーの入力には，単語埋め込み表現に加えて，位置符号器の信号も加算されます。

<!-- 位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\begin{align}
\text{PE}_{(\text{pos},2i)} &= \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} &= \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{align}
$$
-->

- 位置符号器による位置表現は，i 番目の位置情報をワンホット表現するのではなく，周波数領域に変換することで周期情報を表現する試みと見なすことができます。

<center>
<img src="/assets/PE_example.svg" style="width:59%"><br/>
位置符号化に用いられる符号化。位置情報を周波数情報へ変換して用いています。
<!-- ![](assets/PE_example.svg){style="width:74%"}<br/> -->
</center>

- 位置情報を周波数情報へ変換することが良いことなのか，どうなのか，は議論されている最中です。
一つの研究テーマでもあります。

- 数学的な説明は **フーリエ変換** を調べてください。任意の関数 y=f(x) では x は位置情報を表しているとみなすことができます。
従って，位置 x を与えると対応する値 y が得られることを表している式が y=f(x) です。
これに対して，任意の情報は周波数，すなわち，波の重ね合わせとして表現できます。
すべての周波数を重ね合わせると元の関数になります。
反対に，ある周波数の値は，関数 f(x) を周波数へ変換したときの特定の周波数成分として表現できます。

BERT における位置符号化器は位置情報を波の成分として表現したことになります。

このようにしてできた値を入力側と出力側で下図のように連結させたものが以下のトランスフォーマーです。

<center>
<img src="/assets/2017Vaswani_Fig1.svg" style="width:33%"><br/>
From [@2017Vaswani_transformer] Fig. 1
</center>

これまで見てきたように，トランスフォーマーでは入力信号に基づいて情報の変換が行なわれる。
この意味ではトランスフォーマーにおける 多頭 自己注意 MHSA とはボトムアップ注意の変形であるとみなしうる。
逆言すれば，RNN のように過去の履歴をすべて保持しているわけではないので，系列情報については，position encoders に頼っている側面が指摘できる。

<!-- %\input{ELMoBERTGPT_Gao2018.tex}
へーこれでインプットか？-->

## BERT の事前訓練: マスク化言語モデル

全入力系列のうち 15% をランダムに [MASK] トークンで置き換える

- 入力はオリジナル系列を [MASK] トークンで置き換えた系列
- ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測
- 80%: オリジナル入力系列を [MASK] で置換
- 10%: [MASK] の位置の単語をランダムな無関連語で置き換える
- 10%: オリジナル系列

## BERT の事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力

- 入力: the man went to the store [SEP] he bought a gallon of milk.
- ラベル:  IsNext
- 入力:  the man went to the store [SEP] penguins are flightless birds.
- ラベル:  NotNext

## BERT: ファインチューニング (微調整)

(a), (b) は文レベル課題，
(c),(d)はトークンレベル課題, E: 入力埋め込み表現, $T_i$: トークン $i$ の文脈表象。

<!--
- [CLS]: 分類出力記号,
- [SEP]: 文分離記号
-->

<center>
<img src="/assets/2018Devlin_BERT_Fig3.svg" style="width:66%"><br/>
From [@2018BERT] Fig.3
</center>

## GLUE 課題 (General Language Understanding Evaluation)
- **CoLA**: 入力文が英語として正しいか否かを判定
- **SST-2**: スタンフォード大による映画レビューの極性判断
- **MRPC**: マイクロソフトの言い換えコーパス。2文 が等しいか否かを判定
- **STS-B**: ニュースの見出し文の類似度を5段階で評定
- **QQP**: 2 つの質問文の意味が等価かを判定
- **MNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **QNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **RTE**: MNLI に似た2つの入力文の含意を判定
- **WNI**: ウィノグラッド会話チャレンジ

その他

- **SQuAD**: スタンフォード大による Q and A ウィキペディアから抽出した文
- **RACE**: 中学入試，高校入試に相当するテスト多肢選択回答

### BERT モデルのパラメータ詳細
- データ: Wikipedia (2.5B words) + BookCorpus (800M words)
- バッチサイズ: 131,072 words (1024 sequences * 128 length or 256 sequences * 512 length)
- 訓練時間: 1M steps (~40 epochs)
- 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
- BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
- BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
- 4x4 / 8x8 TPU で 4 日間

#### CoLA サンプル

1 は正しい英文，0 は非文

- 1 They drank the pub dry.
- 0 __They drank the pub__.
- 1 The professor talked us into a stupor.
- 0 __The professor talked us__.
- 1 We yelled ourselves hoarse.
- 0 __We yelled ourselves__.

#### SST-2 サンプル

0 は低評価，1 は高評価

- hide new secretions from the parental units     0
- contains no wit , only labored gags     0
- that loves its characters and communicates something rather beautiful about human nature        1
- remains utterly satisfied to remain the same throughout         0
- on the worst revenge-of-the-nerds clichés the filmmakers could dredge up        0
- that's far too tragic to merit such superficial treatment      0

<!-- - demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , pe
- rsonal film with an emotional wallop .  1
- of saucy        1
- a depressed fifteen-year-old 's suicidal poetry         0
- are more deeply thought through than in most ` right-thinking ' films   1
- goes to absurd lengths  0
- for those moviegoers who complain that ` they do n't make movies like they used to anymore      0
- the part where nothing 's happening ,   0
- saw how bad this movie was      0
- lend some dignity to a dumb story       0
 -->

#### MRPC サンプル

- 1
    - 文1: "Please, keep doing your homework," said Bavelier, the mother of three.
    - 文2: "Please, keep doing your homework," said Bavelier, the mother of 6-year-old twins and a 2-year old.
- 1
    - 文1: While Mr. Qurei is widely respected and has a long history of negotiating with the Israelis, he cannot expect such a warm welcome.
    - 文2: While Qureia is respected and has a history of negotiating with the Israelis, a warm welcome is not expected.
- 1
    - 文1: "Nobody wants to go to war with anybody about anything ... it 's always very much a last resort thing and one to be avoided," Mr Howard told Sydney radio.
    - 文2: "We don't want to go to war with anybody . . . it's always very much a last resort, and one to be avoided.
- 0
    - 文1: GMT, Tab shares were up 19 cents, or 4.4% , at A $4.56, having earlier set a record high of A $4.57.
    - 文2: Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A $4.57.
- 0
    - 文1: Martin, 58, will be freed today after serving two thirds of his five-year sentence for the manslaughter of 16-year-old Fred Barras.
    - 文2: Martin served two thirds of a five-year sentence for the manslaughter of Barras and for wounding Fearon.

<!-- - 1
    - 文1: The stock rose $2.11, or about 11 percent, to close Friday at $ 21.51 on the New York Stock Exchange.
    - 文2: PG & E Corp. shares jumped $1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday.
- 1
    - 文1: Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.
    - 文2: With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.
- 0
    - 文1: The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday.
    - 文2: The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15.
- 1
    - 文1: The DVD-CCA then appealed to the state Supreme Court.
    - 文2: The DVD CCA appealed that decision to the U.S. Supreme Court.

 -->
<!-- # BERT ファインチューニング手続き
<center>
<img src="./assets/2019Devlin_mask_method21.jpg" style="width:74%"><br/>
</center>
 -->

#### SST-B サンプル

最後の数値が評価値

- A plane is taking off.  An air plane is taking off.   5.000
- A man is playing a large flute. A man is playing a flute.     3.800
- A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800
- Three men are playing chess.    Two men are playing chess.    2.600
- A man is playing the cello.     A man seated is playing the cello.    4.250
- Some men are fighting.  Two men are fighting. 4.250
- A man is smoking.   A man is skating 0.5000

### QQP サンプル

0 は異なると判断， 1 は同じと判断すべき文

- 0
    - How is the life of a math student? Could you describe your own experiences?
    - Which level of prepration is enough for the exam jlpt5?
- 1
    - How do I control my horny emotions?
    - How do you control your horniness?
- 0
    - What causes stool color to change to yellow?
    - What can cause stool to come out as little balls?     0
- 1
    - What can one do after MBBS?
    - What do i do after my MBBS?
- 0
    - Where can I find a power outlet for my laptop at Melbourne Airport?
    - Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?
- 0
    - How not to feel guilty since I am Muslim and I'm conscious we won't have sex together?
    - I don't beleive I am bulimic, but I force throw up at least once a day after I eat something and feel guilty.  Should I tell somebody, and if so who?

#### MNLI サンプル

- 矛盾
    - Met my first girlfriend that way.
    - I didn’t meet my first girlfriend until later.
- 中立
    - 8 million in relief in the form of emergency housing.
    - The 8 million dollars for emergency housing was still not enough to solve the problem.
- 中立
    - Now, as children tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.
    - All of the children love working in their gardens.
- 含意
    - At 8:34, the Boston Center controller received a third transmission from American 11
    - The Boston Center controller got a third transmission from American 11.
- 中立
    - I am a lacto-vegetarian.
    - I enjoy eating cheese too much to abstain from dairy.
- 矛盾
    - someone else noticed it and i said well i guess that’s true and it was somewhat melodious in other words it wasn’t just you know it was really funny
    - No one noticed and it wasn’t funny at all.


### BERT 多言語対応
<center>
<img src="/assets/2019Lample_Fig1.svg" style="width:66%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center>

### BERT の発展

* BERTlogy バートロジーとして，BERT を弄り倒す研究が量産されるようになった。
* キーワードとしては，[プロンプト](https://arxiv.org/abs/2201.04337)，[センテンス BERT](https://arxiv.org/abs/1908.10084) 等がある。
* [プロンプトエンジニアリング](https://arxiv.org/abs/2107.13586) として，数多くの研究がなさている。

<center>
<img src="/assets/2019Rajasekharan_conver.png" style="width:54%"><br/>
From <https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58>
</center>

### BERT: ファインチューニング手続きによる性能比較

<center>
<img src="/assets/2019Devlin_mask_method21.jpg" style="width:33%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
-->

### BERT: モデルサイズ比較

<center>
<img src="/assets/2019Devlin_model_size20.jpg" style="width:33%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。

### BERT: モデル単方向，双方向モデル比較

<center>
<img src="/assets/2019Devlin_directionality19.jpg" style="width:33%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。


<!-- # BERT 事前訓練比較
<center>
<img src="./assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

### BERT: 事前訓練比較

<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:33%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。

<!--![](../2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

### BERT: 各モデルの特徴

- RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
- XLNet: 順列言語モデル。2 ストリーム注意
- MT-DNN: BERT ベース の転移学習に重きをおいたモデル
- GPT-2: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
- BERT: Transformerに基づく言語モデル。**マスク化言語モデル** と **次文予測** に基づく 事前訓練，各下流課題をファインチューニング。事前訓練されたモデルは一般公開済。
- DistillBERT: BERT の蒸留版
- ELMo: 双方向 RNN による文埋め込み表現
- Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<!-- # 埋め込みモデルによる構文解析
<center>
<img src="assets/2019hewitt-header.jpg" style="width:79%"><br/>
From https://github.com/john-hewitt/structural-probes
</center>

 -->
<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクトル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。

<center>
![](assets/2019Devlin_BERT01upper.svg){style="width:74%"}
[@2019Devlin_BERT]  単語のベクトル表現
</center>

単語埋め込み (word2vec[@2013Mikolov_VectorSpace];[@2013Mikolov_VectorSpace])
単語は周辺単語の共起情報 [点相互情報量 PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) に基づく[@2014LevyGoldberg:nips],[@2014Levy:3cosadd]。
すなわち周辺単語との共起情報を用いて単語の意味を定義している。

<center>
![](assets/2019Devlin_BERT01lower.svg){style="width:74%"}
</center>

形式的には，skip-gram であれ CBOW であれ同じである。

# 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<center>
![](assets/2019Devlin_BERT02upper.svg){style="widht:74%"}<br/>
単語の意味を一意に定めることができない場合

![](assets/2019Devlin_BERT02lower.svg){style="width:74%"}<br/>
</center>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，
句，節，文のベクトル表現を得る努力がなされてきた。
適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<center>
![](assets/2019Devlin_BERT03.svg){style="width:74%"}<br/>
[@2014Sutskever_Sequence_to_Sequence] より
</center>

BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。

# BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。
事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類
の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。
事前訓練語，各課題毎にファインチューニングが施される。

<center>
![](assets/mt-dnn.png){style="width:89%"}<br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>
 -->

### BERT: 埋め込みモデルによる構文解析

BERT の構文解析能力を下図示した。
各単語の共通空間に射影し，
単語間の距離を計算することにより構文解析木と同等の表現を得ることができることが報告されている[@2019HewittManning_structural]。

<center>
<img src="/assets/2019hewitt-header.jpg" style="width:39%">
&nbsp;&nbsp;
<img src="/assets/2019HewittManning_blogFig1.jpg" style="width:19%">
<img src="/assets/2019HewittManning_blogFig2.jpg" style="width:19%"><br/>
<!-- ![](assets/2019HewittManning_blogFig1.jpg){style="width:19%"}
![](assets/2019HewittManning_blogFig2.jpg){style="width:19%"}<br/>-->
BERT による構文解析木を再現する射影空間
From <https://github.com/john-hewitt/structural-probes>
</center>

- word2vec において単語間の距離は内積で定義されていました。
- このことから，文章を構成する単語で張られる線形内積空間内の距離が構文解析木を与えると見なすことは不自然ではないと予想できます。

<!--
% > **The syntax distance hypothesis**: There exists a linear transformation
% > $\mathbf{B}$ of the word representation space under which vector distance
% > encodes parse trees.  Equivalently, there exists an inner product on the
% > word representation space such that distance under the inner product
% > encodes parse trees. This (indefinite) inner product is specified by
% > $\mathbf{B}^{\top}\mathbf{B}$.

% We'll take a particular instance of this hypothesis for our probes;
% we'll use the L2 distance, and let the squared vector distances equal the tree distances, but more on this later.
-->

- そこで構文解析木を再現するような射影変換を見つけることができれば BERT を用いて構文解析が可能となるでしょう。
- 例えば上図における chef と store と was の距離を解析木を反映するような空間を見つけ出すことに相当します

<!-- % The distances we pointed out earlier between \_chef\_, \_store\_ and \_was\_, can be visualized in a vector space as follows, where $\mathbf{B}\in\mathbb{R}^{2\times3}$, mapping 3-dimensional word representations to a 2-dimensional space encoding syntax:
-->
<!--% Note in the image above that the distances between words before
% transformation by $\mathbf{B}$ aren't indicative of the tree. After the
% linear transformation, however, taking a minimum spanning tree on the
% distances recovers the tree, as shown in the following image:

% <center>
% % ![](assets/0.332019HewittManning_blogFig2.jpg}
% </center>

% Finding a parse tree-encoding distance metric Our potentially tree-encoding distances are parametrized by the linear transformation $\mathbf{B}\in\mathbb{R}^{k\times n}$,

% \begin{equation}
% \left\|h_i-h_j\right\|_B^2=\left(B\left(h_i-h_j\right)\right)^{\top}\left(B\left(h_i-h_j\right)\right)
% \end{equation}

% where $\mathbf{B}_h$ is the linear transformation of the word representation; equivalently, it is the parse tree node representation.
% This is equivalent to finding an L2 distance on the original vector space, parametrized by the positive semi-definite matrix $A=B^{\top}B$:

% \begin{equation}
% \left\|h_i-h_j\right\|_A^2=\left(h_i-h_j\right)^{\top}A\left(h_i-h_j\right)
% \end{equation}
% The set of linear transformations, $\mathbb{R}^{k\times n}$ for a given $k$ is the hypothesis class for our probing family.
% We choose $B$ to minimize the difference between true parse tree distances from a human-parsed corpus and the predicted distances from the fixed word representations transformed
% by $B$:
-->

<!-- 2 つの単語 $w_i$, $w_j$ とし単語間の距離を $d\left(w_i,w_j\right)$ とする。
適当な変換を施した後の座標を $h_i$, $h_j$ とすれば，求める変換 $B$ は次式のような変換を行なうことに相当する:
$$
\min_{B}\sum_l\frac{1}{\left|s_\ell\right|^2}\sum_{i,j}\left(d\left(w_i,w_j\right)-\left\|B\left(h_i-h_j
\right)\right\|^2\right)
$$
ここで $\ell$ は文 s の訓練文のインデックスであり，各文の長さで規格化することを意味している。
 -->

具体的には，以下のような操作をしている:

1. 文章に現れる全トークンを表すベクトルを BERT より求める。
2. すなわち BERT 全中間層ユニット活性値から構成される全ての値から構成されるベクトル群
3. 2 のベクトルが張る部分空間に全トークンを射影する。
4. 3 の部分空間内でトークン間の距離を求める。
5. 各トークンを短い順にグラフで結ぶ

<!--% where $\ell$ indexes the sentences $s_{\ell}$ in the corpus, and $\frac{1}{\left|s_\ell\right|^2}$ normalizes for the number of pairs of words in each sentence.
% Note that we do actually attempt to minimize the difference between the squared distance $\left\|h_i-h_j\right\|_B^2$ and the tree distance.
% This means that the actual vector distance $\left\|h_i-h_j\right\|_B$ will always be off from the true parse tree distances, but the tree information encoded is identical, and we found that optimizing with the squared distance performs considerably better in practice.

% Finding a parse depth-encoding norm As a second application of our method, we note that the directions of the edges in a parse tree is determined by the depth of words in the parse tree; the deeper node in the governance relationship is the governed word. The depth in the parse tree is like a norm, or length, defining a total order on the nodes in the tree. We denote this tree depth norm $\left\|w_i\right\|$.

% Likewise, vector spaces have natural norms; our hypothesis for norms is that there exists a linear transformation under which tree depth norm is encoded by the squared L2 vector norm $\left\|Bh_i\right\|_2^2$.
% Just like  for the distance hypothesis, we can find the linear transformation under which the depth norm hypothesis is best-approximated:

% \begin{equation}
% \min_B\sum_\ell\frac{1}{\left|s_\ell\right|}\sum_i\left(\left\|w_i\right\|-\left\|Bh_i\right\|^2\right)
% \end{equation}

% To be effective, the manual should follow three key principles:
% \begin{enumerate}
% -  It should be simple and write on a single page, e.g. as a bulleted list of operating procedures.
% -  It should be prioritised in a strategic order that you can start executing tomorrow.
% -  It should be reviewed, evaluated, and understood by everyone crucial to the mission.
% \end{enumerate}
-->

### BERT 実装
- BERT 実装のパラメータを以下に示した。
- 現在配布されている BERT-base あるいは性能が良い BERT-large は各層のニューロン数と全体の層数である。
- ソースコードの配布先は https://github.com/google-research/bert
- オリジナルの論文は https://arxiv.org/abs/1810.04805

* データ: Wikipedia (2.5B words) + BookCorpus (800M words)
* バッチサイズ: 131,072 words (1024 sequences $\times$ 128 length or 256 sequences $\times$ 512 length)
* 訓練ステップ: 1M steps (40 epochs)
* 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
* BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
* BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
* 訓練時間: 4x4 / 8x8 の TPU で 4 日間


### 事前訓練とマルチ課題学習

<center>
<img src="/assets/mt-dnn.png" style="width:66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

<!--
# Transformer: Attention is all you need

$$\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V$$

<center>

![](assets/2017Vaswani_Fig2_1.svg){style="width:17%"}
![](assets/2017Vaswani_Fig2_2.svg){style="width:23%"}<br />
From [@2017Vaswani_transformer] Fig. 2
</center>
-->

<!--
# Transformer(2): Attention is all you need

$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_1,\ldots,\mathop{head}_h\right)W^O
$$

where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2$$

$$\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$

$$\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$
-->

<!--
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

