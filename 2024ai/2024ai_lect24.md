---
title: 第24回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+4" color="navy"><strong>人工知能 II (自然言語処理，あるいは系列情報処理編)</strong></font><br/><br/>
<!-- <font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/> -->
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 05/Dec/2024<br/>
Appache 2.0 license<br/>
</div>


[課題提出用フォルダ](https://drive.google.com/drive/folders/1sBv6Gw19DDfeYc693M0orLP_7xvs9YS8?usp=drive_link){:target="blank"}


## 実習ファイル

* [Cart Pole のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2023notebooks/2023_0618_1_getting_started.ipynb){}:target="_blank"}

- [ランダム探索  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)
- [方策勾配法 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)
- [SARSA <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)
- [Q学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb)

<!-- https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb)-->
<!--(https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb)-->

<!-- 以下のデモは，[OpenAI](https://openai.com/) 提供の強化学習環境 [gym](https://gym.openai.com/) を用いています。

- [倒立振子 (cartpole) <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0716Gym_cartpole_rendering.ipynb){:target="_blank"}

Colaboratory 上で gym を 動作させるためには [StarAI の開発したレンダリング環境](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/) が必要です。 -->



## 参考

[DQN](https://komazawa-deep-learning.github.io/2020pytorch_tutorail_reinforcement_q_learning/)



<!--
## APE-X
@2018Horgan_APE-X

## TRPO (Trust Region Policy Optimization)
@2015Schulman_trpo

## IMPALA
<https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/>


#### TPRO と PPO
- source: <https://blog.syundo.org/post/20171204-reinforcement-learning-natural-policy-gradient-trpo-ppo/>
- さらに，上のトップ: <https://blog.syundo.org/post/20180115-reinforcement-learning/>

##### TRPO

最適化計算における更新ステップの計算に KL ダイバージェンスによる制約を加えたものが TRPO (Trust Region Policy Optimization) である。
この方法は，を KL ダイバージェンスで拘束しているため，近似的には自然勾配法と同様の手法となる。
TRPO は方策勾配法に限らず，モデルなし学習においても利用することができるが，以下では方策勾配法と組み合わせることを前提に述べる。

さて，$\theta$ でパラメタライズされた方策  $\pi_\theta(a\vert s)$ がある場合，方策勾配が

$$
\hat{g} = \mathbb{E}_\pi\left[ \nabla_\theta \log\pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

であった。これは，以下の値 $L_\theta$ の微分値である。

$$
L_\theta(\theta)=\mathbb{E}_\pi\left[\log \pi_\theta(a\vert s)A^{\pi}(s,a)\right]
$$

このとき，更新のステップを制限するために，以下のようにKLダイバージェンスで制約を課して最大化を行う。

$$
\text{maximize}_{x} L_{\theta_{\text{old}}}(\theta)
$$

$$
\text{subject to } D_{KL}(\theta_{\text{old}},\theta)\le\delta
$$

ここで， $\theta_{\text{old}}$ は $\pi_\theta$ におけるパラメタ $\theta$ の前の値である。
また，$D_{KL}$ は確率分布 $\pi_\theta$ と $\pi_{\theta_{\text{old}}}$ の間の KL ダイバージェンスであり， $D_{KL}\max(\theta_{\text{old}},\theta)$ は任意のパラメタの組み合わせに対して，KLダイバージェンスを計算したときの最大値を表す。

実用的には組み合わせが膨大になり，最大値を求めるのは難しいため，制約はヒューリスティックに以下のように平均値で代用する。
$$
\text{maximize}_x L_{\theta_{\text{old}}}(\theta)$$
$$

$$
\text{subject to} D_{KL}^{\max}(\theta_{\text{old}},\theta) \le \delta
$$

ここで， $\bar{D}_{KL}(\theta_{\text{old}},\theta)=\mathbb{E}_{s\sim p}\left[D_{KL}(\pi_\theta(\cdot\vert s),\pi_{\theta_2}(\cdot\vert s)\right]$ である。

ただし，制約において問題を解くのは簡単ではないので，以下のようにソフト制約を使う形に書き下す。

$$
\text{maximize}_x \mathbb{E}_\pi\left[L_{\theta_{\text{old}}}(\theta)-\beta\bar{D}_{KL}(\theta_{\text{old}},\theta)\right]
$$

以上が，TRPO の概要である。
-->

<!--
##### PPO

PPO (Proximal Policy Optimization) は方策の目標値をクリッピングすることで，おおまかに方策の更新を制約する方法である。
TRPO では KL ダイバージェンスを制約として利用していたが，PPO では，目的関数を以下の $L^{\text{clip}}$ として，勾配を求める。

$$
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min(r_t(\theta)\hat{A}_t,\text{clip}(r(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\right]
$$

ここで，$r_t(\theta)$ は確率の比率であり，

$$
r_t(\theta) = \frac{\pi_\theta(s_t,a_t)}{\pi_{\theta_{\text{old}}}(s_t,a_t)}
$$

である。また，$\text{clip}(r(\theta), 1−\epsilon, 1+\epsilon)$ は $r(\theta)$ が $1−\epsilon} あるいは $1+\epsilon$ を超過しないように制限する関数である。
$\text{clip}(r(\theta), 1−\epsilon , 1+\epsilon)^At$ の グラフと，$L^{\text{CLIP}}$ を以下に示す(John Schulmanらより引用)。
-->


# 強化学習

未知の環境にあるエージェントがいて，このエージェントは環境とインタラクトすることでいくつかの報酬を得ることができるとします．
エージェントは，累積報酬を最大化するように行動する必要があります。
現実的には，ゲームでハイスコアを出すロボットや，物理的なアイテムを使って物理的なタスクをこなすロボットなどが考えられますが，これらに限定されるものではありません。

<!--
Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment.
The agent ought to take actions so as to maximize cumulative rewards.
In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these.
-->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/)


