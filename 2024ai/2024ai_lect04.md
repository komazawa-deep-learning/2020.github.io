---
title: 第04回 2024年度開講 駒澤大学 人工知能 I
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
<!-- <img src="/assets/header_logo.png" sytle="width:09%"> -->
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 10/May/2024<br/>
Appache 2.0 license<br/>
</div>

# 実習ファイル

* [オリベッティ顔データベースを用いた顔認識, 固有顔 Eigenfaces と PCA vs tSNE <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0510PCA_tSNE_and_Logistic_regression_of_Olivetti_face.ipynb){:target="_blank"}

## 課題授業の提出

* 宛先: educ0233@komazawa-u.ac.jp
* Cc: 自分
* 件名: 2024_0510_学生番号
* 本文 または 添付ファイルとして，要約文


## ソーカル事件への補足

ソーカル事件への文系から反論として，多くの輩が挙げるであろう文献が，クーンの[科学革命の構造](https://www.amazon.co.jp/dp/4622016672){:target="_blank"} であろう。

## 固有値，あるいは，線形代数についての文献

* [高等学校数学科教材（行列入門）](https://www.mext.go.jp/a_menu/shotou/new-cs/senseiouen/1394142_00001.html)
* あるいは [林 周二，数学再入門 I](https://www.amazon.co.jp/dp/4121001397/)

## 相関係数について

心理統計の書籍を紐解くと，相関係数とは，共分散を標準偏差の積で割ったものである。
ここでは，別解を紹介したい。

$$\tag{0:相関係数 統計学の教科書の定義}
r_{xy} = \frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i}\left(x_{i}-\bar{x}\right)}\,\sqrt{\sum_{i}\left(y_{i}-\bar{y}\right)}}
=\frac{\text{$x$ と $y$ との共分散}}{\text{$x$ の標準偏差 $\times y$ の標準偏差}}
$$

$$\tag{1:相関係数 ベクトル表記}
r_{xy} = =\frac{\left(\mathbf{x}\cdot \mathbf{y}\right)}{\left|\mathbf{x}\right|\,\left|\mathbf{y}\right|}=\cos\theta
$$

平均偏差ベクトル: ベクトルから，その平均を引いたベクトル $\mathbf{x}=\left(x_1-\bar{x},x_2-\bar{x},\ldots,x_n-\bar{x}\right)$，ここで $\displaystyle\bar{x}=\frac{1}{n}\sum_{i}x_{i}$ とする。

* ベクトル vector とスカラ scaler
* 行列の次数
* ベクトルと行列のスカラ倍
* 行列の和と積
* 逆行列
* 内積

内積は，<span style="color:teal">$\overrightarrow{x}$</span> と表記している高等学校の教科書が多い。
駄菓子菓子，ここでは，太文字を使って次のように表記する <span style="color:blue">$\mathbf{x}$</span>

内積の定義は，以下の通りである:
$$\tag{2:内積の定義}
\begin{align}
\left(\mathbf{x}\cdot\mathbf{y}\right) & =\left|\mathbf{x}\right|\left|\mathbf{y}\right|\cos\theta=x_1y_1+x_2y_2+\ldots+x_{n}y_{n}\\
                                       & =\sum_{i=1}^{n}x_{i}y_{i}\\
\end{align}$$

(2) 式を変形すると
$$
\cos\theta=\frac{\mathbf{x}^{\top}\mathbf{y}}{\left|\mathbf{x}\right| \left|\mathbf{y}\right|}
$$
を得る。

## 主成分分析

PCA (Princple Component Analysis: 主成分分析) とは，データ数，一つのデータあたりの個数を m とすると，データは n 行 m 列の行列となる。
オリベッティ顔データセットであれば $\mathbf{X}$ は 400 行 $\times$ 4096 列となる。

主成分分析とは各データに対して加重和，すなわち，それぞれの重みを $w_i$ とすれば，$y_{i}=w_{1}x_{1}+w_{2}x_{2}+\ldots+w_{n}x_{n}$ である。

$y_{i}=\left(\mathbf{x}\cdot\mathbf{y}\right)$ ベクトル表記すれば，$\mathbf{y}=\mathbf{Xw}$ となる。

主成分分析とは $\mathbf{y}$ の分散を最大化するための $\mathbf{w}=\left(w_{1},w_{2},\ldots,w_{m}\right)$ を見つけることである。

このとき，$w_{i}, (i\in\left[1,m\right])$ に制限をつけないと，$\mathbf{y}$ の分散は際限なく大きくなる。
そこで，$\left(\mathbf{x}\cdot\mathbf{x}\right)=1$ という制限をつける。

そうすると，ラグランジェの未定乗数 (Lagrange's multiplier) を用いて，

$$
\mathcal{L}=\max\left[\left(y^{\top}y\right)\right]=\max\left[\mathbf{w}^{\top}\mathbf{X}^{\top}\mathbf{Xw}-\lambda\left(\mathbf{w}^{\top}\mathbf{w}-1\right)\right]
$$


$\mathcal{L}$ を $\mathbf{w}$ について微分して 0 とおけば，次式を得る:
$$\begin{align}
\frac{\partial Q}{\partial w} = &\mathbf{X}^{\top}\mathbf{Xw} - \lambda\mathbf{w} &=& 0\notag\\
                                &\mathbf{Cw}                                      &=&\lambda\mathbf{w}\\
\end{align}$$

ここで $\mathbf{C}$ は分散共分散行列である。
上式は，固有方程式 (eigen equation) と呼ばれる。
上式左辺は，ベクトル $\mathbf{w}$ を変換行列 $\mathbf{C}$ によって変換されるベクトル $\mathbf{y}$ が，ベクトル $\mathbf{w}$ の $\lambda$ 倍になるということを表している。




# 固有顔 Eigenfaces

