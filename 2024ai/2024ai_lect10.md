---
title: 第10回 2024年度開講 駒澤大学 人工知能 I
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+1" color="navy"><strong>人工知能 I</strong></font><br/><br/>
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 21/Jun/2024<br/>
Appache 2.0 license<br/>
</div>

## 課題提出時の注意

educ0233@komazawa-u.ac.jp に対して共有設定をしてから提出してください。

## 告知

* [WCCI 2024 AIガバナンス公開フォーラム](https://groups.oist.jp/ja/ncu/event/wcci-forum){:target="_blank"}


## 実習

* [ニューラルネットワークで遊んでみよう](/tensorflow-playground/){:target="_blank"}
<!-- * [ニューラルネットワークで遊んでみよう](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"} -->

* [機械学習総復習 いくつかの分類器を用いた顔分類 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0620LFW_classifcaition_demo.ipynb){:target="_blank"} SVC, precision, recall, F1 などの概念を説明

* [多項回帰とパーセプトロンによる過剰適合のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0621polynomilal_fittings_demo.ipynb){:target="_blank"}

* [ソフトマックス関数解題 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1107softmax.ipynb){:target="_blank"}
また，ソフトマックス関数は，エネルギー関数とみなすことも可能である。

* [ニューラルネットワークモデルの定義 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1028komazawa_neural_networks_primer.ipynb){:target="_blank"}
* [3 層パーセプトロンと確率的勾配降下法のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
* [ccap 資料初心者のためのニューラルネットワーク <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2022notebooks/2022_0418ccap_neural_networks_for_primer.ipynb){:target="_blank"}
* [画像認識 PyTorch の基礎編 AlexNet <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb){:target="_blank"}



## キーワード

ドロップアウト，スキップ結合，バッチ正則化，L1，L2 正則化，

## 先週の復習

### 排他的論理和 exclusive OR

<!--
|　 X 　| 　 Y 　| and 論理積| or 論理和 | XOR |
|:---:|:---:|:------:|:----------:|:---:|
|　 0　 |　 0 　 | 　 0 　 | 　 0 　 |  0  |
|　 0　 |　 1 　 | 　 0 　 | 　 1 　 |  1  |
|　 1　 |　 0 　 | 　 0 　 | 　 1 　 |  1  |
|　 1　 |　 1 　 | 　 1 　 | 　 1 　 |  0  |


|　 X 　 |　 Y 　 | and (論理積)| or (論理和) | XOR |
|:---:|:---:|:------:|:----------:|:---:|
|　 0　 | 　 0 　 |      |          |   |
|　 0　 | 　 1 　 |      |          |   |
|　 1　 | 　 0 　 |      |          |   |
|　 1　 | 　 1 　 |      |          |   |
-->

<div class="wrap">
<div class="left_col">
<img src="/2024assets/xor.svg" style="width:89%;align:center;"><br/>

<br/><br/>
</div>
<div class="right_col">
<img src="/2024assets/xor-graph.svg" style="width:69%;align:center;"><br/>

<br/><br/>
</div>
</div>


# 形式ニューロンによる論理演算

### 多入力一出力という単純化

$$
y=sign\left(\sum_{i=1}^{N}w_{i}x_{i}+b\right)
$$


ステップ関数
<img src="/assets/step_function.svg" style="width:49%">

<div class="figcenter">
  <img src="/assets/perceptron.png" width="69%">
  <div class="figcaption">パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より</div>
</div>

#### 論理回路の設計

基本的な論理回路と簡単な記憶回路を神経回路網で構成する方法を考えてみます。
シリコンウェハ上に構成される論理回路をニューロン素子でも実現できることを示し以下に引用したウィーナーの言葉を裏付ける根拠を示すことにします。

#### AND (論理積)回路

2 入力 1 出力の回路において、2 つの入力が共に真であるときのみ真を出力し、そうでなければ偽となる論理演算である論理積 (AND) を考えます。
論理積は引数を 2 つとる演算であり、出力を $y$ とすれば $y = f(x_1, x_2)$ のように書くことができます。
$x_1$, $x_2$ ともに 1 または 0 の値をとるものとすれば、$y$ が 1 であるためには $x_1 = 1$ かつ $x_2 = 1$ でなければなりません

<img src="/assets/formal_proto.svg" style="width:69%">


#### OR (論理和)回路

<img src="/assets/formal_proto.svg" style="width:69%">

#### NOT (否定)回路

<img src="/assets/formal_one.svg" style="width:69%">

## 排他的論理和 (XOR) 回路

<img srac="/assets/xor.svg" style="width:49%">

<img src="/assets/xor-graph.svg" style="width:29%">


### PDP book (1986) chapter 8 Figure 2

<img src="/2024assets/1986pdp_chap8_Fig2.svg" style="width:39%">

#### 内部表象

<img src="/2024assets/1986pdp_chap8_Fig1.svg" style="width:39%">


おそらく人類史上初，哲学的な意味ではなく<font color="Red">内部表象が計算可能</font>になった

#### 排他的論理和の別解

<img src="/2024assets/xor-w-direct.svg" style="width:49%">

#### 簡単な記憶回路 フリップフラップ回路

AND 素子と NOT 素子とを繋いで簡単な記憶回路を作ることができる

<img src="/2024assets/flip-flop2.svg" style="width:29%">

図で各素子は $1$ か $0$ かを値として取りうる **形式ニューロン** だとする。
今、入力 $x$ と入力 $y$ とが共に $1$ であれば $A=1$, $B=0$ あるいは $A=0$, $B=1$ のときだけこの回路は安定である。

ここで $x=0$, $y=1$ とすると $A=0$, $B=1$ の状態になり、 $x=1$, $y=0$ とすると $A=1$, $B=0$ の状態になる。
しかも、この状態は $x=y=1$ に入力を戻しても保存される。
これは $1$ ビットの記憶回路でありフリップフラップ回路(flip-flop circuit) と呼ばれる。

このことは AND と NOT を実現できる神経回路素子があれば記憶回路を作ることができることを示している。
しかも工学的に実現されている回路と完全に等価である。
フリップフロップ回路を何個かまとめてレジスタ (register) と呼ぶ。
市販されている PC の CPU の性能を指して 64 ビットマシンと呼ぶのは、このレジスタの大きさ(記憶装置への基本的な入出力単位の基本でもある)による。

一般にコンピュータの速度はこのフリップフラップ回路が安定するまでの時間に依存します。
なぜなら、コンピュータの基本動作は原理的に、上述のフリップフラップ回路が安定するのを待って、次の命令をレジスタに読み込むことの繰り返しだからである。


### 残差ネット (ResNet, He et. al, 2015)

<center>
<img src='/assets/ResNet_Fig2.svg' style='width:19%'>
<img src='/assets/2015ResNet30.svg' style='width:66%'><br>
He (2015) より
</center>


### Fast R-CNN と Faster R-CNN (2014)

- R-CNN によって，位置 where 情報と 物体 what 情報 とを多層畳み込みニューラルネットワークで表現する試みが，発展。実時間で物体の切り出しと認識とが行えるようになった。[Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf){:target="_blank"}, [YOLO](https://arxiv.org/pdf/1506.02640.pdf){:target="_blank"}, [SSD](https://arxiv.org/pdf/1512.02325.pdf){:target="_blank"},

<center>
<img src="/assets/2015Fast_R-CNN_Fig1.svg" width="49%">
<img src="/assets/2015Faster_RCNN_RPN.svg" width="44%"><br/>
左: Fast R-CNN の模式図。
右: Faster RCNN の領域提案ネットワーク
</center>

