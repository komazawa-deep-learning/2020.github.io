---
title: 第07回 2024年度開講 駒澤大学 人工知能 I
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

<div align="center">
<font size="+1" color="navy"><strong>人工知能 I</strong></font><br/><br/>
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 31/May/2024<br/>
Appache 2.0 license<br/>
</div>

## 実習ファイル

* [第4週，第5週と同じ オリベッティ顔データベースを用いた顔認識, 固有顔 Eigenfaces と PCA vs tSNE <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0510PCA_tSNE_and_Logistic_regression_of_Olivetti_face.ipynb){:target="_blank"}


## ロジスティック回帰 (Logistic regression)

ロジスティック回帰に用いられる回帰式は，変数 $X$ の生じる確率を次式で定義する:

$$
P(x) = \frac{1}{1+e^{-x}}
$$

もう少し一般化すると，項目反応理論 (IRT: Item Response Theory) において，用いられる回帰式 $\displaystyle P(x)=\frac{1}{1+e^{-a \left(x-b\right)+c}}$ となる。

ニューラルネットワーク研究においては，左辺 (LHS) の分母 (denominator) 第二項 $e$ の肩に乗る数 $x$ が，IRT では $a(x-b)+c$ と変形されている。
IRT における変換式は，一次変換，すなわち，入力された値 $x$ から $b$ を引き，その値を $a$ 倍して，c を加えることを行っている。

この変換は，$b$ を平均，$a$ を傾き，$c$ を切片と考える事ができる。
このため，変数 $x$ の値をどう解釈するかに関与するとみなすことができる。

シグモイド関数のグラフは以下のコードで確認できる:

```python
import matplotlib.pyplot as plt
import numpy as np
try:
    import japanize_matplotlib
except ImportError:
    !pip install japanize_matplotlib
    import japanize_matplotlib

x = np.linspace(-8,8)
y = 1/(1+np.exp(-x))

plt.plot(x,y,label='シグモイド関数')
plt.legend()
plt.show()
```

ニューラルネットワークでは，ハイパータンジェント関数 $\tanh$ が使われることもある。

```
x = np.linspace(-8,8)

y = 1/(1+np.exp(-x))
plt.plot(x,y,label='シグモイド関数')

y = np.tanh(x)
plt.plot(x,y, label='ハイバータンジェント関数 ')

plt.grid()
plt.legend()
plt.show()
```

2014 年以降，整流線形化関数 (ReLU: Rectified Linear unit ) が使われることもある。

```python
x = np.linspace(-8,8)

y = 1/(1+np.exp(-x))
plt.plot(x,y,label='シグモイド関数')

y = np.tanh(x)
plt.plot(x,y, label='ハイバータンジェント関数 ')

y = [_x if _x > 0 else 0 for _x in x]
plt.plot(x,y, label='ReLU 関数')

plt.grid()
plt.legend()
plt.show()
```

以下は，加えて ReLU6 関数も加えた

```python
y = 1/(1+np.exp(-x))
plt.plot(x,y,label='シグモイド関数')

y = np.tanh(x)
plt.plot(x,y, label='ハイバータンジェント関数 ')

y = [_x if _x > 0 else 0 for _x in x]
plt.plot(x,y, label='ReLU 関数')


y = [0 if _x < 0 else _x for _x in x]
y = [_x if _x < 6 else 6 for _x in y]
plt.plot(x,y, label='ReLU6 関数')

plt.grid()
plt.legend()
plt.show()
```


## softmax 関数

多クラス分類の場合には，ソフトマックス (softmax) を用いる

$$
P(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$


## ソフトマックス関数とシグモイド関数との関係

$x$ と $y$ と二つの選択肢を，$x$ か $x$ でないか，であるとすれば以下となる:

$$
\frac{e^{x}}{e^{x}+e^{y}} = \frac{1}{1+e^{-(x-y)}}
$$

