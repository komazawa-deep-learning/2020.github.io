---
title: 第23回 2024年度開講 駒澤大学 人工知能 II
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div style="text-align:right">
<img src="/2024assets/qrcode_2024_0920.png" style="width:19%">
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
\newcommand{\of}[1]{\left(#1\right)}
$$

<div align="center">
<font size="+4" color="navy"><strong>人工知能 II (自然言語処理，あるいは系列情報処理編)</strong></font><br/><br/>
<!-- <font size="+1" color="navy"><strong>人工知能 II</strong></font><br/><br/> -->
</div>

<div align='right'>
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br>
Date: 29/Nov/2024<br/>
Appache 2.0 license<br/>
</div>

* [1129第23回提出用フォルダ](https://drive.google.com/drive/folders/1a-EVSIStUQ5-nc0hDQFYxL81FifGG3Go?usp=drive_link){:target="_blank"}


# 実習

* [Annotated Transformers <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007Annotated_Transformer.ipynb){:target="_blank"}
* [BERT head visualization <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1007BERT_head_view.ipynb){:target="_blank"}
* [TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb){:target="_blank"}

* アルファ碁, アルファ碁ゼロ, DQN, [Atari ゲーム (OpenAI Gym)](https://gym.openai.com/){:target="_blank"}
* [エージェント57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark){:target="_blank"}

* [ランダム探索 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_2_maze_random.ipynb){:target="_blank"}
* [方策勾配法 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_3_policygradient.ipynb){:target="_blank"}
- [SARSA <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_5_Sarsa.ipynb){:target="_blank"}
- [Q 学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_rl_ogawa_2_6_Qlearning.ipynb){:target="_blank"}


<!-- - [日本語 BERT 2 つの文の距離を求める <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb){:target="_blank"} -->
<!-- * [Google Colab で OpenAI の Gym 環境を動かすための下準備](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1106Remote_rendering_OpenAI_Gym_envs_on_Colab.ipynb){:target="_blank"} -->

<!-- * [PyTorch チュートリアルによる DQN (2021_1105 現在未完成)](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105reinforcement_q_learning.ipynb){:target="_blank"} -->


<!-- * [Google Colab で OpenAI の Gym 環境を動かすための下準備](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1106Remote_rendering_OpenAI_Gym_envs_on_Colab.ipynb) -->

<!-- * [PyTorch チュートリアルによる DQN (2021_1105 現在未完成)](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105reinforcement_q_learning.ipynb) -->

<!-- (file:///Users/asakawa/study/2020personal/2020-1030deepmind_agent57.md)-->
<!-- 1. [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
2. [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html) -->

<!-- 1 と 2 は，ローカルディスクに git clone あり。
強化学習の基礎概念である。
jekyell が動作しないので確認できないのだが。
# 昨年の 第8回の markdown file (lect08.md) より
-->

* [宇宙人の夢: アートシーンの出現](https://shinasakawa.github.io/2022/2021Snell_clip-art_ja){:target="_blank"}
* [CLIP のデモ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2020_1014CLIP_ja_basics.ipynb){:target="_blank"}
* アルファ碁, アルファ碁ゼロ, DQN, [Atari ゲーム (OpenAI Gym)](https://gym.openai.com/){:target="_blank"}
* [エージェント 57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark){:target="_blank"}

<!-- (file:///Users/asakawa/study/2020personal/2020-1030deepmind_agent57.md)-->
<!-- 1. [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
2. [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html){:target="_blank"} -->

<!-- 1 と 2 は，ローカルディスクに git clone あり。
強化学習の基礎概念である。
jekyell が動作しないので確認できないのだが。
# 昨年の 第8回の markdown file (lect08.md) より
-->

<!-- * [Deep Reinforcement Learning: Playing CartPole through Asynchronous Advantage Actor Critic (A3C) with tf.keras and eager execution](https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296){:target="_blank"}

* [ruippeixotog/cartpole_v0.py](https://gist.github.com/ruippeixotog/cde7cae770e72916e209b915521bb18f){:target="_blank"} -->

<!--* Mastering the game of Go with deep neural networks and tree search
* Mastering the game of Go without human knowledge-->

<!-- アルファ碁, アルファ碁ゼロ, DQN, [Atari ゲーム (OpenAI Gym)](https://gym.openai.com/)，

* Mastering the game of Go with deep neural networks and tree search -->

* [REINFORCE.js](https://komazawa-deep-learning.github.io/reinforcejs/)

# DeepMind による強化学習

* [DQN: Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236){:target="_blank"}
* [AlphaGo: Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961){:target="_blank"}
* [AlphaGo Zero: Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270){:target="_blank"}


<div class="figcenter">
<img src="/assets/2016AlphaGo_Fig1a.svg" style="width:44%;">
<img src="/assets/2016AlphaGo_Fig1b.svg" style="width:49%;"><br/>
AlphaGo の模式図，原著論文より
</div>

<!-- <div class="figcaption"> -->

**図 1. ニューラルネットワークの学習パイプラインとアーキテクチャ**<br/>
**a**  高速ロールアウト方針 $p_\pi$ と教師あり学習 (SL: supervised learning) 方針ネットワーク $p_\sigma$ は，棋盤上の位置データセットにおける人間の専門家の動きを予測するために学習される。
強化学習 (RL: Reinforcement Learning) 方針ネットワーク $p_\rho$ は教師あり学習方針ネットワークに初期化され，方針勾配学習によって方針ネットワークの以前のバージョンに対する結果 (すなわち，より多くのゲームに勝つこと) を最大化するように改善される。
新しいデータ集合は，強化学習方針ネットワークと自己対戦ゲームを行うことによって生成される。
最後に，価値ネットワーク $v_\theta$ が回帰によって訓練され，自己対戦データセットから位置における期待結果（つまり，現在のプレイヤーが勝つかどうか）を予測する。<br/>
**b**  AlphaGo で使用されているニューラルネットワークアーキテクチャの模式図。
方針ネットワークは，碁盤上の位置の表現 $s$ を入力とし，それをパラメータ $\sigma$ （教師あり学習方針ネットワーク）または $\rho$ （強化学習方針ネットワーク）を持つ多層畳み込み層に通し，碁盤上の確率地図によって表される，合理的な手 $a$ に関する確率分布 $p_{\sigma}(a|s)$ または $p_{\rho}(a|s)$ を出力する。
価値ネットワークも同様に，パラメータ $\theta$ を持つ多数の畳み込み層を使用するが，位置 $s'$ における期待結果を予測するスカラ値 $v_{\theta}(s')$ を出力する。
<!-- Figure 1. Neural network training pipeline and architecture.
a, A fast rollout policy pπ and supervised learning (SL) policy network pσ are trained to predict human expert moves in a data set of positions.
A reinforcement learning (RL) policy network pρ is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network.
A new data set is generated by playing games of self-play with the RL policy network.
Finally, a value network vθ is trained by regression to predict the expected outcome (that is, whether the current player wins) in positions from the self-play data set.
b, Schematic representation of the neural network architecture used in AlphaGo. The policy network takes a representation of the board position s as its input, passes it through many convolutional layers with parameters σ (SL policy network) or ρ (RL policy network), and outputs a probability distribution σpa(|s) or ρpa(|s) over legal moves a, represented by a probability map over the board.
The value network similarly uses many convolutional layers with parameters θ, but outputs a scalar value vθ(s′) that predicts the expected outcome in position s′. -->
<!-- </div> -->

### アルファ碁ゼロにおける強化学習<!-- ### Reinforcement learning in AlphaGo Zero-->

我々の新手法は，パラメータ $\theta$ を持つ深層ニューラルネットワーク $f_{\theta}$ を使用する。
このニューラルネットワークは，位置とその履歴の生の碁盤表現 s を入力とし，手の確率と価値， $(p,v)=f_{\theta}(s)$ の両方を出力する。
手の確率のベクトル $p$ は，各手 $a$ (パスを含む) を選択する確率を表し，$p_a=Pr(a|s)$ である。
価値 $v$ はスカラ評価であり，現在のプレイヤーが位置 $s$ から勝つ確率を推定する。
このニューラルネットワークは，方針ネットワークと価値ネットワーク(12) の両方の役割を 1 つのアーキテクチャにまとめたものである。
ニューラルネットワークは，バッチ正規化(18) と整流非線形 (19) を持つ畳み込み層 (16,17) の多数の残差ブロック(4)  から構成される (「手続き」節参照)。
<!--Our new method uses a deep neural network fθ with parameters θ.
This neural network takes as an input the raw board representation s of the position and its history, and outputs both move probabilities and a value, (p,v)=fθ(s).
The vector of move probabilities p represents the probability of selecting each move a (including pass), pa=Pr(a|s).
The value v is a scalar evaluation, estimating the probability of the current player winning from position s.
This neural network combines the roles of both policy network and value network12 into a single architecture.
The neural network consists of many residual blocks4 of convolutional layers(16,17) with batch normalization18 and rectifier nonlinearities(19) (see Methods) -->

AlphaGoZero のニューラルネットワークは，新しい強化学習アルゴリズムによって，自己対局のゲームから学習される。
各位置 s において，ニューラルネットワーク $f_{\theta}$ によって導かれる MCTS 探索が実行される。
MCTS 探索は，各手を打つ確率 $\pi$ を出力する。
これらの探索確率は通常，ニューラルネットワーク $f_{\theta}(s)$ の生の手確率 $p$ よりもはるかに強い手を選択する。
したがって，MCTS は強力な方針改善演算子と見なすことができる (20,21)。
探索を伴う自己対局の各手を選択するために改善された MCTS に基づく方針を使用し，その後，価値のサンプルとしてゲームの勝者 z を使用するのは，強力な方針評価演算子と見なすことができる。
ニューラルネットワークのパラメータは，手の確率と値 $(p,v)=f_{\theta}(s)$ が，改善された探索確率と自己対局の勝者 $(\pi,z)$ とより密接に一致するように更新される。
これらの新しいパラメータは自己対局の次の反復で使用され，探索をより強力にする。
図 1 は自己対局の訓練パイプラインを示している。
<!-- The neural network in AlphaGo Zero is trained from games of selfplay by a novel reinforcement learning algorithm.
In each position s, an MCTS search is executed, guided by the neural network fθ.
The MCTS search outputs probabilities π of playing each move.
These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network fθ(s); MCTS may therefore be viewed as a powerful policy improvement operator(20,21).
Self-play with search—using the improved MCTS-based policy to select each move, then using the game winner z as a sample of the value—may be viewed as a powerful policy evaluation operator.
The main idea of our reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure22,23: the neural network’s parameters are updated to make the move probabilities and value (p,v)=f_θ(s) more closely match the improved search probabilities and self-play winner (π,z); these new parameters are used in the next iteration of self-play to make the search even stronger.
Figure 1 illustrates the self-play training pipeline. -->

<div class="figcenter">
<img src="/2024assets/2017Silver_AlphaGoZero_fig1_.svg" style="width:44%;">
</div>

**図 1 AlphaGoZero における自己対戦強化学習**<br/>
**a**: プログラムは自分自身と対局 $s_1,\ldots,s_T$ を行う。
各位置 $s_t$ において，最新のニューラルネットワーク $f_\theta$ を用いて MCTS（モンテカルロ木探索）$\alpha_\theta$ が実行される（図 2 参照）。
移動は MCTS によって計算された探索確率に従って，$a_t\sim\pi_t$ で選択される。
終端位置 $s_T$ はゲームの勝者 $z$ を計算するためにゲームのルールに従って得点される。<br/>
**b**, AlphaGoZero におけるニューラルネットワークの学習。
ニューラルネットワークは生の盤面位置 $s_t$ を入力とし，それをパラメータ $\theta$ を持つ多層畳み込み層に通し，手に関する確率分布を表すベクトル $p_t$ と，現在のプレイヤーが位置 $s_t$ で勝利する確率を表すスカラー値 $v_t$ の両方を出力する。
ニューラルネットワークのパラメータ $\theta$ は，探索確率 $\pi_t$ に対する方針ベクトル $p_t$ の類似性を最大化し，予測勝者 $v_t$ とゲーム勝者 $z$ との誤差を最小化するように更新される（式(1)参照）。
新しいパラメータは，$a$ のように，次の自己プレイの繰り返しで使用される。
<!-- Figure 1 Self-play reinforcement learning in AlphaGo Zero.
a, The program plays a game s1, ..., sT against itself.
In each position st, an MCTS (Monte Carlo Tree Search) αθ is executed (see Fig. 2) using the latest neural network fθ.
Moves are selected according to the search probabilities computed by the MCTS, at ∼ πt.
The terminal position sT is scored according to the rules of the game to compute the game winner z.
b, Neural network training in AlphaGo Zero. The neural network takes the raw board position st as its input, passes it through many convolutional layers with parameters θ, and outputs both a vector pt, representing a probability distribution over moves, and a scalar value vt, representing the probability of the current player winning in position st.
The neural network parameters θ are updated to maximize the similarity of the policy vector pt to the search probabilities πt, and to minimize the error between the predicted winner vt and the game winner z (see equation (1)).
The new parameters are used in the next iteration of self-play as in a. -->

MCTS はニューラルネットワーク $f_\theta$ を用いてシミュレーションを行う (図 2)。
探索木の各辺 (s,a) には事前確率 $P(s,a)$，訪問回数 $N(s,a)$，行動価値 $Q(s,a)$ が格納されている。
各シミュレーションはルート状態から開始し，葉ノード s’ に遭遇するまで，$U(s,a)\sim P(s,a)/(1 + N(s,a)$) (文献12, 24) である上限信頼限界 $Q(s,a)+U(s,a)$ を最大にする手を反復的に選択する。
この葉の位置は，事前確率と評価 $(P(s',\cdot),V(s'))=f_\theta(s’)$ の両方を生成するために，ネットワークによって一度だけ展開され評価される。
シミュレーションで通過した各辺 (s,a) は，その訪問回数 N(s,a) をインクリメントするように更新され，その行為価値をこれらのシミュレーションの平均評価に更新する，$Q(s,a)=1/N(s,a)\sum_{s'|s,a\mapsto s'}V(s’)$ ここで s,a→s’ はシミュレーションが最終的に位置 $s$ から移動 $a$ をとって $s’$ に到達したことを示す。
<!-- The MCTS uses the neural network fθ to guide its simulations (see Fig. 2).
Each edge (s,a) in the search tree stores a prior probability P(s,a), a visit count N(s,a), and an action value Q(s,a).
Each simulation starts from the root state and iteratively selects moves that maximize an upper confidence bound Q(s,a)+U(s,a), where U(s,a)∝P(s,a)/(1 + N(s,a)) (refs 12, 24), until a leaf node s' is encountered.
This leaf position is expanded and evaluated only once by the network to generate both prior probabilities and evaluation, (P(s',·),V(s'))=f_\theta(s').
Each edge (s, a) traversed in the simulation is updated to increment its visit count N(s,a), and to update its action value to the mean evaluation over these simulations, Q(s,a)=1/N(s,a)\sum_{}V(s') where s,a→s' indicates that a simulation eventually reached s' after taking move a from position s. -->

<div class="figcenter">
<img src="/2024assets/2017Silver_AlphaGoZero_fig2_.svg" style="width:54%;">
</div>

**図 2. AlphaGo Zero における MCTS**<!-- Figure 2. MCTS in AlphaGo Zero.--><br/>
**a**, 各シミュレーションは，最大行動価値 Q と，保存された事前確率 P とそのエッジの訪問回数 N に依存する上限信頼区間 U を持つエッジを選択することによって木を探索する（これは一度探索されると増分される）。<br/>
**b**, 枝ノードが展開され，関連する位置 s がニューラルネットワーク $P(s,-),V(s))=f_{\theta}(s)$ によって評価される。<br/>
**c**, 行動価値 Q は，その行動の下の部分木内のすべての評価 V の平均を追跡するように更新される。<br/>
**d**, 探索が完了すると，$N^{1/\tau}$ に比例した探索確率 $\pi$ が返される。
ここで，N はルート状態からの各移動の訪問回数，$\tau$ は温度を制御するパラメータ。
<!-- a, Each simulation traverses the tree by selecting the edge with maximum action value Q, plus an upper confidence bound U that depends on a stored prior probability P and visit count N for that edge (which is incremented once traversed).
b, The leaf node is expanded and the associated position s is evaluated by the neural network (P(s,·),V(s))=f_θ(s); the vector of P values are stored in the outgoing edges from s.
c, Action value Q is updated to track the mean of all evaluations V in the subtree below that action.
d, Once the search is complete, search probabilities π are returned, proportional to N^{1/τ}, where N is the visit count of each move from the root state and τ is a parameter controlling temperature.-->

MCTS はニューラルネットワークパラメータ $\theta$ とルート位置 s が与えられると，打つべき手を推奨する探索確率のベクトル $\pi=a_\theta(s)$ を計算し，各手に対する指数化された訪問回数$\pi_{a}\propto N(s,a)^{1/\tau}$ に比例する自己プレイアルゴリズムと見なすことができる。
ここで $\tau$ は温度パラメータである。
<!-- MCTS may be viewed as a self-play algorithm that, given neural network parameters θ and a root position s, computes a vector of search probabilities recommending moves to play, π=α_θ(s), proportional to
the exponentiated visit count for each move, π_a∝N(s,a)^{1/τ}, where τ is a temperature parameter. -->

ニューラルネットワークは，MCTSを用いて各手を打つ自己再生強化学習アルゴリズムによって学習される。
まず，ニューラルネットワークはランダムな重み $\theta_0$ に初期化される。
続く各反復 $i\ge 1$ において，自己対局ゲームが生成される (図 1a)。
各タイム・ステップ $t$ で，ニューラルネットワーク $f_{\theta_{i-1}}$ の前の反復を用いて MCTS 探索 $\pi_t =a_{\theta_{t- i}}(s_t)$ が実行され，探索確率 $\pi_{t}$ をサンプリングすることによって手が打たれる。
ゲームは，両プレイヤーがパスしたとき，探索値が諦めの閾値を下回ったとき，またはゲームが最大長を超えたときにステップ $T$ で終了する。
その後，ゲームは $r_T\in\left[-1,+1\right]$ の最終報酬を与えるように採点される (詳細は方法参照)。
各タイムステップ $t$ のデータは $(s_t,\pi_t, z_t)$ として保存され $z_t=\pm r_T$ はステップ $t$ における現在のプレイヤーから見たゲームの勝者である。
並行して (図1b)，新しいネットワーク・パラメータ $\theta_i$ が，自己対局の最後の反復の全タイム・ステップ間で一様にサンプリングされたデータ $(s,\pi,z)$ から学習される。
ニューラルネットワーク $(p,v)=f_{\theta_{i}}(s)$ は，予測値 v と自己対局の勝者 z との間の誤差を最小化し，ニューラルネットワークの移動確率 $p$ と探索確率 $\pi$ との類似性を最大化するように調整される。
具体的には，パラメータ $\theta$ は，平均二乗誤差と交差エントロピー損失をそれぞれ合計する損失関数lの勾配降下によって調整される：
<!-- The neural network is trained by a self-play reinforcement learning algorithm that uses MCTS to play each move.
First, the neural network is initialized to random weights θ0. At each subsequent iteration i≥1, games of self-play are generated (Fig. 1a).
At each time-step t, an MCTS search π =αθ−t i 1(st) is executed using the previous iteration of neural network θ − f i 1 and a move is played by sampling the search probabilities πt.
A game terminates at step T when both players pass, when the search value drops below a resignation threshold or when the game exceeds a maximum length; the game is then scored to give a final reward of rT ∈ {− 1,+ 1} (see Methods for details).
The data for each time-step t is stored as (st, πt, zt), where zt = ± rT is the game winner from the perspective of the current player at step t.
In parallel (Fig. 1b), new network parameters θi are trained from data (s, π, z) sampled uniformly among all time-steps of the last iteration(s) of self-play.
The neural network = (p, v) fθ (s) i is adjusted to minimize the error between the predicted value v and the self-play winner z, and to maximize the similarity of the neural network move probabilities p to the search probabilities π.
Specifically, the parameters θ are adjusted by gradient descent on a loss function l that sums over the mean-squared error and cross-entropy losses, respectively: -->
$$\tag{1}
(\mathbf{p},v)=f_{\theta}(s) \text{ and } l=(z-v)^{2} - \pi^{\top}n \log\mathbf{p} + c\left\|\theta\right\|^{2}
$$
ここで c は L2 重みの正則化レベルを制御するパラメータである（過学習を防ぐため）。
<!-- where c is a parameter controlling the level of L2 weight regularization (to prevent overfitting). -->

<center>
<div style="text-align: left;width: 88%;background-color: powderblue;">
ムーブ３７:
ムーブ３７とはアルファ碁とイセドル氏との対局において，アルファ碁の打った決定的な３７手目（第四局）のことを指す。
アルファ碁の置いた３７手目の石はとくに評判になったため記憶に新しい。
ムーブ３７の意味は以下のように解釈できる。人類が積み上げてきた経験則である定石が，盤石とは限らないことを示した点にある。
囲碁の世界王者であっても経験に基づいた知識である。すべての可能な石の配置で構成される囲碁空間は膨大が量の人類が築き上げてきた経験の結集であっても，可能な囲碁配置空間の一部に過ぎず結果として偏っていた可能性がある。
このことを示してくれたのがムーブ３７といえる。
確かに千年以上の歳月をかけて人類が探索してきた碁石の可能な配置空間は広大である。
だが可能な囲碁の配置空間は，現在まで我われが探索してきた空間よりも広大であったのであり，アルファ碁なくしては人類が到達し得ない地平が存在したのである。
それまでは，知の地平の存在に気がついていた碩学が存在したとしても，具体例を示すことができず，従って説得力 を持った議論が展開できなかったのである。
</div>
</center>

* Mastering the game of Go without human knowledge

<center>
<div style="text-align: left;width: 88%;background-color: powderblue;">
人工知能の長年の目標は， 困難な領域でも超人的な能力をタブラ・ラサ方式で学習するアルゴリズムである。
最近では AlphaGo が囲碁の世界チャンピオンを破った初めてのプログラムとなった。
AlphaGo の木探索は， 深層ニューラルネットワークを用いて局面の評価と手の選択を行う。
このニューラルネットワークは，人間の熟練した手からの教師付き学習と， 自分自身の競技からの強化学習によって訓練されている。
ここでは， 人間のデータやガイダンス， ゲームルール以外の領域の知識を必要としない， 強化学習のみに基づいたアルゴリズムを導入する。
AlphaGo は自分自身の教師となり AlphaGo 自身の手の選択や AlphaGo のゲームの勝敗を予測するようにニューラルネットワークが学習される。
このニューラルネットワークは， 木探索の強度を向上させ， その結果， より質の高い手の選択が可能となり， 次の反復ではより強力な自己対戦が可能となる。
タブララサからスタートした私たちの新しいプログラム AlphaGo Zero は，既発表のチャンピオンに敗れた AlphaGo に対して 100-0 で勝利するという超人的な成績を達成した。
</div>
</center>

* [David Silver homepage](https://www.davidsilver.uk/)





# 強化学習，条件付けの古典

<img src="https://www.nobelprize.org/images/pavlov-12840-content-portrait-mobile-tiny.jpg" width="24%">
<a href="https://www.nobelprize.org/prizes/medicine/1904/pavlov/biographical/">Ian Pavlov</a>&nbsp;&nbsp;
<img src="https://www.bfskinner.org/wp-content/gallery/1970s-1990/BFS-IN-THE-OFFICE.jpg" width="24%">
<a href="https://www.bfskinner.org/archives/photos/">Burrhus Frederic Skinner</a>&nbsp;&nbsp;
<!-- <img src="http://incompleteideas.net/sutton-head5.jpg" width="24%"> -->
<img src="https://cloudfront.ualberta.ca/-/media/science/people/rsutton/sutton.jpg" width="24%">
<a href="http://incompleteideas.net/">Richard S. Sutton,</a>&nbsp;&nbsp;
<img src="https://people.cs.umass.edu/~barto/barto2006-lowres.jpg" width="24%">
<a href="https://people.cs.umass.edu/~barto/">Andrew G. Barto</a>
</div>


- [パブロフ](https://en.wikipedia.org/wiki/Ivan_Pavlov) (Ivan Petrovich Pavlov; 1849/Sep/14-1936/Feb/27)古典的条件づけ 1904 年ノーベル医学生理学賞
- [スキナー](https://en.wikipedia.org/wiki/B._F._Skinner) (Burrhus Frederic Skinner; 1904/Mar/20-1990/Aug/18) 道具的条件付け， オペラント条件づけ，[スキナー箱, Skinner(1938) Fig.1, page 39 より](./assets/1938Skinner_Fig1_skinnerBOX.jpg)
- [Sutton](http://incompleteideas.net/) and [Barto](http://www-anw.cs.umass.edu/~barto/) の強化学習 [初版 1998年](http://incompleteideas.net/book/first/the-book.html), [第2版 2018年](http://incompleteideas.net/book/the-book-2nd.html), [初版は翻訳あり](https://www.amazon.co.jp/dp/4627826613/)，第2版は pdf ファイルで[ダウンロード可能](http://incompleteideas.net/book/bookdraft2017nov5.pdf)


# 強化学習とは何か？

<center>
<img src="/assets/2018Sutton_Fig3j.svg" style="width:74%"><br/>
<p align="center" style="width:74%">
Sutton & Barto (2018) Fig. 3.2 を改変
</p>
</center>

強化学習という言葉は古い言葉ですが機械学習の文脈では，環境とその環境におかれた動作主（エージェントと言ったり，ロボットシステムだったりします）が，環境と相互作用しながらより良い行動を形成するためのモデルです。
動作主は，環境から受け取った現在の状態を分析して，次にとるべき行動を選択します。このとき将来に渡って報酬が最大となるような行動を学習する手法の一つです。

2015 年には，Google 傘下のデープマインドというスタートアップチームが開発した囲碁プログラム AlphaGo がプロ棋士のイ・セドル氏に勝利し話題になりました。
AlphaGo は強化学習を基本技術の一つとして用いています。

1. [強化学習(1): 基礎](https://komazawa-deep-learning.github.io/rl01_elements.pdf)
2. [強化学習(2): エージェントと環境](https://komazawa-deep-learning.github.io/rl02_agentAndEnv.pdf)
3. [強化学習(3): 目標と報酬](https://komazawa-deep-learning.github.io/rl03_goalAndReward.pdf)
4. [強化学習(4): マルコフ決定過程](https://komazawa-deep-learning.github.io/rl04_mdp.pdf)
5. [強化学習(5): 価値反復，方策反復](https://komazawa-deep-learning.github.io/rl05_vi.pdf)
6. [強化学習(6): ](https://komazawa-deep-learning.github.io/rl06_advanced.pdf)
6. [強化学習(7): ](https://komazawa-deep-learning.github.io/rl07_robotics.pdf)

<!-- - エージェントと環境，マルコフ決定過程 MDP，POMDP，効用関数，ベルマン方程式，探索と利用のジレンマ，SARSA:
- 価値，方策，Q 学習，モデルベース対モデルフリー，アクター=クリティック:
- 深層 Q 学習:
- ゲーム AI へ (AlphaGo，AlphaGoZero，OpenAI five):
- セルフプレイ:
- 最近の発展 A3C，Rainbow，RDT，World model: -->

<!--
### 方策，報酬，価値観数，(環境)モデル

- $s,s'$: 状態 state
- $a$: 行動，行為 action
- $r$: 報酬 reward
- $t$: 時間 (離散時間 $t=1,2,\ldots,T$)
- $p(s',r\vert s, a)$: 状態 $s$ で行為 $a$ を行ったとき，報酬 $r$ を受け取って 状態 $s'$ に遷移する確率
- $p(s'\vert s, a)$: 状態 $s$ で行為 $a$ を行った場合，状態 $s'$ へ遷移する確率
- $r(s,a)$: 状態 $s$ で行為 $a$ を行った場合の即時報酬 immediate reward の期待値
- $r(s,a,s')$: 行為 $a$ を行った場合状態が $s$ から $s'$ へ変化したときの即時報酬の期待値
- $v_\pi(s)$: 方策 $\pi$ での状態 $s$ の価値 (期待報酬)
- $v_*(s)$: 最適方策化での状態 $s$ の価値
- $q_\pi(s,a)$: 方策 $\pi$ のもとで状態 $s$ で行為 $a$ をおこなた場合の価値
- $q_*(a)$: 行動 $a$ を行った場合の期待報酬(真の報酬) <!-- true value (expected reward) of action $a$-->

<!--
- $Q_t(a)$: 時刻 $t$ での $q_*(a)$ の期待値 qestimate at time $t$ of $q_*(a)$
- $N_t(a)$: 時刻 $t$ で行為 $a$ を行った回数 number of times action $a$ has been selected up prior to time $t$
- $H_t(a$) 時刻 $t$ で行為 $a$ を行う傾向(選好 preference) learned preference for selecting action a at time $t$
- $\pi_t(a)$: 時刻 $t$ で行為 $a$ を選択する確率 probability of selecting action a at time $t$
- $R_t$: $\pi_t$ が与えられた場合，時刻 $t$ における の期待報酬 estimate at time $t$ of the expected reward given $\pi_t$
--->


## 複雑な状況をどう理解して解決するのか？

- 強化学習というニューラルネットワークモデルがあるわけではない
- 動的で複雑な環境に対処 $\rightarrow$ **強化学習** + DL $\rightarrow$ 一般人工知能への礎

- DQN ATARIのビデオゲーム, [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)
- AlphaGo 囲碁, [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961)
- AlphaGoZero 囲碁, [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)

# Deep Q Network

<center>

<img src="/assets/2015DQNFig1.svg" width="66%"><br/>
DQNの模式図, 原著論文より
</center>

<!--
- [ギャラガ1](/assets/MOV_0013s.mp4)
- [ギャラガ2](/assets/MOV_0071s.mp4)
-->

- **Q 学習** Q learning に DNN を採用
- CNN が LeNet, [@1998LeCun] そうであったように，強化学習 RL も昔からの技術 [@Sutton_and_Barto1998]
- ではなぜ，今になって囲碁や自動運転に応用できるようになったのか？
  - $\Rightarrow$ コンピュータの能力, データ規模，アルゴリズムの改良, エコシステム(ArXiv, Linux, Git, ROS, AMT, TensorFlow)

<!--
# 強化学習

- 強化学習 $\Rightarrow$ 意思決定
  - **エージェント** agent が **行動**(行為) action をする
  - 行動によって **状態** が変化する
  - **環境** から与えられる **報酬** によって**目標**が決定
- 深層学習: $\Rightarrow$ 表現，表象
  - 教師信号として目標が与えられる
  - 目標を達成するために外部状況の **表現** を獲得

<center>
<font size="+2" color="Green">強化学習 + 深層学習 = 人工知能</font>
</center>

  - 強化学習 $\Rightarrow$ 目標の設定
  - 深層学習 $\Rightarrow$ 内部表象の獲得機構を提供

# 用語の整理

- 教師信号なし **報酬信号** reward signal
- 遅延フィードバック

- **価値** Value
- **行為** Action
- **状態** State
- TD 学習
  - **Sarsa**
  - **Q 学習**
  - **アクタークリティック**
- 報酬 $R_t$: **スカラ値**
  - 時刻 $t$ でエージェントのとった行動を評価する指標
  - エージェントは**累積報酬** cumulative reward の最大化する
  - 報酬仮説: **目標は累積期待報酬の最大化として記述可能**
-->

## DQN 結果

<center>

<img src="/assets/2015Mnih_DQNFig.png" style="width:39%"><br/>
</center>


## YouTube 上でのデモ動画

* ブロック崩し: [https://www.youtube.com/watch?v=V1eYniJ0Rnk](https://www.youtube.com/watch?v=V1eYniJ0Rnk)
* スペースインベーダー: [https://www.youtube.com/watch?v=W2CAghUiofY](https://www.youtube.com/watch?v=W2CAghUiofY)

<!--- packman: [https://www.youtube.com/watch?v=r3pb-ZDEKVg](https://www.youtube.com/watch?v=r3pb-ZDEKVg)
- OpenMind selfplay: [https://www.youtube.com/watch?v=OBcjhp4KSgQ](https://www.youtube.com/watch?v=OBcjhp4KSgQ)

-->
<!--
* DQN の動画 スペースインベーダー

<center>

<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video1.mp4" type="video/mp4" >
</center>

* DQN の動画 ブロック崩し

<center>

<div>
<video style="width:33%" controls src="/assets/2015Mnih_DQN-Nature_Video2.mp4" type="video/mp4" >
</div>
</center>
-->

<!-- ## なぜ DQN には難しいのか？

<center>
<div>
<video style="width:74%" controls src="/assets/Montezuma.mp4" type="video/mp4" /></br>
**Montezuma**
</div>
</center>

<center>
<video style="width:74%" controls src="/assets/PrivateEye.mp4" type="video/mp4" /></br>
**Private Eye**
</center>

<center>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/Montezuma.mp4">
</video>
<video width="39%" autoplay loop markdown="0" controls muted>
  <source src="./assets/privateEye.mp4">
</video>
 -->

## 人間にはできて強化学習には難しいこと

- Montenzuma's Revenge の動画 [https://www.youtube.com/watch?v=Klxxg9JM5tY](https://www.youtube.com/watch?v=Klxxg9JM5tY)
- Private Eys の動画 [https://www.youtube.com/watch?v=OfyS-Wj1M78](https://www.youtube.com/watch?v=OfyS-Wj1M78)

<!---
## エージェントと環境

- At each step $t$ the agent:
  - Executes action $A_t$
  - Receives observation $O_t$
  - Receives scalar reward $R_t$
- The environment:
  - Receives action $A_t$
  - Emits observation $O_{t+1}$
  - Emits scalar reward $R_{t+1}$
- $t$ increments at env. step
-->

<!--
- **エージェント**: 学習と意思決定を行う主体
  1. **行動** action **$A_t$** を行い
  1. 環境の **観察** observation **$O_t$** を行う
  1. 環境からスカラ値の **報酬** reward **$R_t$** を受け取る
- **環境**: エージェント外部の全て
  1. エージェントから **行為** $A_t$ を受け取り
  1. エージェントに **観察** $O_{t+1}$ を与え
  1. エージェントへ **報酬** $R_{t+1}$ を与える

## エージェントの要素

- **方策** Policy
- **価値関数** Value function
- **モデル** エージェントが持つ環境の表象

## 方策 policy

- **方策** : エージェントの行為
- 決定論的方策: $a=\pi(S)$
- 確率論的方策: $\pi(a|s)=p(A_{t=a}|S_{t=s})$

## 価値関数
- 将来の報酬予測
- 状態評価(良/悪)
- 行為の選択
$$
v_\pi(S)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+\ldots|S_{t=s}\right\}
$$

## 強化学習のモデル
- 価値ベース
  - 方策:なし
  - 価値関数:あり
- 方策ベース
  - 方策:あり
  - 価値関数:なし
- アクター=クリティック Actor Critic
  - 方策: あり
  - 価値関数: あり

- モデルフリー
  - 方策，価値関数: あり
  - モデル: なし
- モデルベース
  - 方策，価値関数: あり
  - モデル: あり

## 探索と利用のジレンマ Exploration and exploitaion dilemma
- 過去の経験から，一番良いと思う行動ばかりをしていると，さらに良い選択肢を見つけ出すことができない **探索不足**
- 更に良い選択肢ばかり探していると過去の経験が活かせない **過去の経験の利用不足**

## 目標，収益，報酬

- エージェントの目標は累積報酬を最大化すること (報酬仮説)
  - **報酬仮説** Reward Hypothesis
  - 目標: 期待報酬の最大化

- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価

## 逐次的意思決定 Sequential Decision Making
- 目標 Goal: 総収益を最大化する行動を選択すること
- 行為，行動 Actions は長期的結果
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある


## 収益 Return
- **収益** return $G_t$: 割引付き収益
$$
G_t=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
$$

- 割引率 The discount $\gamma\in\left\{0,1\right\}$ : 現時点から見た将来の報酬を計算するため

- **遅延報酬** delayed reward の評価
- $0$ に近ければ __近視眼的__ 評価
- $1$ に近ければ __将来を見通した__ 評価

## 価値関数 Value Function

- **状態価値関数 $v$ ** と **行動価値関数 $q$ **

- **価値関数** $v(s)$: gives the long-term value of state $s$
- **状態価値関数** $v(s)$ of an MRP is the expected return starting from state $s$
$$
v(s)=\mathbb{E}\left\{G_t|S_{t=2}\right\}
$$

- **状態価値関数** state-value function:
$$
v_\pi(s)=\mathbb{E}_\pi\left\{G_t|S_{t=s}\right\}
$$

# ベルマン期待期待 Bellman Expectation Equation
- **状態価値関数** : 即時報酬と後続状態の割引付き報酬の和に分解できる

$$
v_\pi(s)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma v_\pi(S_{t+1}|S_t=s)\right\}
$$

- **行動価値関数** action-value function:
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{G_t|S_{t}=s,A_{t}=a\right\}
$$

- **行動価値関数** 同じく分解可能 The action-value function can similarly be decomposed,
$$
q_\pi(s,a)=\mathbb{E}_\pi\left\{R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a\right\}
$$


## 最適価値関数 Optimal Value Function
- 最適状態価値関数
$$
v_{*}(s) = \max_{\pi} v_{\pi}(s)
$$
- 最適行動価値関数
$$
q_{*}(s,a)=\max q_{\pi}(s,a)
$$

- ベルマン方程式 一般に非線形になるので難しい
-->

<!--
- No closed form solution (in general)
- Many iterative solution methods
- 幾つかの解法:
  - 価値反復
  - 方策反復
  - Q 学習
  - sarsa
-->

<!--
  - Value Iteration
  - Policy Iteration
  - Q-learning
  - Sarsa
-->

<!--
# Markov Reward Process
A Markov reward process is a Markov chain with values.

- A Markov Reward Process is a tuple $<S,P,R,\gamma>$
  - $S$ is a finite set of states
  - $P$ is a state transition probability matrix,
  - $P_{ss'}=P\of{S_{t+1}=s'\given{S_t=s}}$
  - $R$ is a reward function, $R_s=\mathbb{E}\BRc{R_{t+1}\given{S_t=s}}$
  - $\gamma$ is a discount factor

# Bellman Equation for MRPs
The value function can be decomposed into two parts:
  - immediate reward $R_{t+1}$
  - discounted value of successor state $\gamma v\of{S_{t+1}}$

$$
\begin{array}{lll}
v\of{s}&=&\mathbb{E}\BRc{G_t\given{S_t=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma\Brc{R_{t+2}+\gamma R_{t+3}+\ldots}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma G_{t+1}\given{S_{t}=s}}\\
&=&\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}\given{S_{t}=s}}}
\end{array}
$$

# Bellman Equation for MRPs

$$
v\of{s}=\mathbb{E}\BRc{R_{t+1}+\gamma v\of{S_{t+1}}\given{S_t=s}}
$$


$$
v\of{s}=R_s +\gamma\sum_{s'\in S} P_{ss'}v\of{s'}
$$

# Solving the Bellman Equation
- The Bellman equation is a linear equation
- It can be solved directly:
$$
\begin{array}{lll}
v &=& R +\gamma Pv\\
\Brc{I - \gamma P}v &=& R\\
v &=& \Brc{I-\gamma P}^{-1}R
\end{array}
$$

- Computational complexity is $O(n^3)$ for $n$ states
- Direct solution only possible for small MRPs
- There are many iterative methods for large MRPs, e.g.
  - Dynamic programming
  - Monte-Carlo evaluation
  - Temporal-Difference learning
-->


<!-- # 価値関数 Value Function -->
<!-- - 将来の報酬予測の関数 -->
<!--   - ある状態である行動を起こすとどれほどの報酬が得られるか -->
<!-- - **Q-値関数** Q-value function : 総期待報酬を得る関数<\!--gives expected total reward-\-> -->
<!--   - 方策 $\pi$ のもとで -->
<!--   - 状態 $s$ で行動 $a$ を行ったとき -->
<!-- $$ -->
<!--   Q^\pi\of{s,a}=\mathbb{E}\BRc{r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots\given{s,a}} -->
<!-- $$   -->

<!--
# 最適価値関数 Optimal Value Functions
- 最大の価値を与える関数
$$
Q^*(s,a)=\max_{\pi}Q^\pi(s,a)=Q^{\pi^*}(s,a)
$$
- 最適価値関数 $Q^*$ が得られれば最適方策 $\pi^*$ を求めることができる
$$
\pi^*(s)=\operatorname{argmax}_aQ^*(s,a)
$$

- 全ての意思決定における最適価値:
$$
\begin{array}{lll}
Q^*(s,a)&=&r_{t+1}+\gamma\max_{a_{t+1}}r_{t+2}+\gamma^2\max_{a_{t+2}}r_{t+3}+\ldots\\
		   &=&r_{t+1}+\gamma\max_{a_{t+1}}Q^*(s_{t+1},a_{t+1})
\end{array}
$$
-->

<!-- from sliver (2016) icml lecture -->
<!--
- **ベルマン方程式** Bellman equation:
$$
Q^*(s,a)=\mathbb{E}_{s'}\left\{r+\gamma\max_{a'}Q^*(s',a')|s,a\right\}.
$$
-->

<!--
# 報酬(収益) Rewards
- 時刻 $t$ における報酬 $R_t$ : **スカラ値**
- 時刻 $t$ におけるエージェント行為の評価Indicates how well agent is doing at step $t$

# Sequential Decision Making
- 目標 Goal: select actions to maximise total future reward
- 行為 Actions はmay have long term consequences
- 収益は遅延することも有る
- 直近の報酬を選ぶよりも，長期的な報酬を考えた方が良い場合がある It may be better to sacrifice immediate reward to gain more long-term reward-
- Examples:
  - A financial investment (may take months to mature)
  - Refuelling a helicopter (might prevent a crash in several hours)
  - Blocking opponent moves (might help winning chances many moves from now)
-->
