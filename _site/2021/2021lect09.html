<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>第09回 | ディープラーニングの心理学的解釈</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="第09回" />
<meta name="author" content="浅川 伸一" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep learning for psychology 駒澤大学文学部2021年度開講 心理学特講IIIA,B" />
<meta property="og:description" content="Deep learning for psychology 駒澤大学文学部2021年度開講 心理学特講IIIA,B" />
<link rel="canonical" href="http://localhost:4000/2021/2021lect09.html" />
<meta property="og:url" content="http://localhost:4000/2021/2021lect09.html" />
<meta property="og:site_name" content="ディープラーニングの心理学的解釈" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"浅川 伸一"},"url":"http://localhost:4000/2021/2021lect09.html","headline":"第09回","description":"Deep learning for psychology 駒澤大学文学部2021年度開講 心理学特講IIIA,B","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ディープラーニングの心理学的解釈" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ディープラーニングの心理学的解釈</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/2021/2021lect01.html">第01回</a><a class="page-link" href="/2021/2021lect02.html">第02回</a><a class="page-link" href="/2021/2021lect03.html">第03回</a><a class="page-link" href="/2021/2021lect04.html">第04回</a><a class="page-link" href="/2021/2021lect05.html">第05回</a><a class="page-link" href="/2021/2021lect06.html">第06回</a><a class="page-link" href="/2021/2021lect07.html">第07回</a><a class="page-link" href="/2021/2021lect08.html">第08回</a><a class="page-link" href="/2021/2021lect09.html">第09回</a><a class="page-link" href="/2021/2021lect10.html">第10回</a><a class="page-link" href="/2021/2021lect11.html">第11回</a><a class="page-link" href="/2021/2021lect12.html">第13回</a><a class="page-link" href="/2021/2021lect13.html">第14回</a><a class="page-link" href="/2021/">2021年度 駒澤大学心理学特講IIIa</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">第09回</h1><h1 id="ディープラーニングの心理学的解釈-心理学特講iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</h1>

<div align="right">
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br />
Date: 17/Apr/2020<br />
Appache 2.0 license<br />
</div>

<h1 id="ディープラーニングの心理学的解釈-心理学特講iiia-1"><a href="https://komazawa-deep-learning.github.io/2021/" target="_blank">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a></h1>

<div align="right">
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br />
Date: 26/Jun/2020<br />
Appache 2.0 license<br />
</div>

<ul>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626homework.ipynb" target="_blank">本日の課題 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
</ul>

<h1 id="実習">実習</h1>
<ul>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb" target="_blank">word2vecの先週の再録 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626BERT_head_view.ipynb" target="_blank">BERT の注意の視覚化 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb" target="_blank">日本語 BERT の実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
</ul>

<h2 id="雑談余談">雑談，余談</h2>

<p>鳥の翼や羽の構造と空を飛ぶための仕組みの解明と飛行機との関係について。
鳥類や昆虫の翅と飛行機との対比は，人間の脳に宿る知性と，人工知能，あるいはニューラルネットワークモデルとの対比がなされます。
調べる限り サイエンティフィック・アメリカン に掲載された Ford と Hayes の記事が出典のようです。
この記事によれば，鳥の羽の構造の研究だけからは，飛行機は生まれなかった。飛行機を実用化するために必要な実験は「人工翼」の風洞実験でした。
飛行機の実現がもたらしたものは，空力学の理解，鳥の羽と飛行機と飛行することについての深い理解でした。
鳥の羽の解剖学は，つぎはぎ，付け足しから成る進化の産物である羽は，かえって飛行の本質を捉えにくかったと考えられます。
「鳥の羽」と「人工の翼」との関係を，「人間の脳」あるいは脳に宿る知性と「ニューラルネットワーク」に置き換えて考えれば，
人間の知性を，微に入り際に入り調べること，そこから一旦離れて，別の材料を用いた実験を行うことで，
人間と動物と機械の全てに共通する知性について深い理解が得られるだろう，と前述の Ford と Hayes は書いています。</p>

<p>Ford, K. and Hayes, P. (1998) On Computational Wings: Rethinking the Goals of Artificial Intelligence, Scientific American, 9(4), 79-83.</p>

<h2 id="復習">復習</h2>

<h3 id="seq2sep-翻訳モデル">Seq2sep 翻訳モデル</h3>

<p>中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば <strong>機械翻訳</strong> や <strong>対話</strong> のモデルになる。
初期の翻訳モデルである “seq2seq” の概念図を示した。
“eos” は文末 end of sentence を表す。
中央の “eos” の前がソース言語であり，中央の “eos” の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。</p>

<p>注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，
それ以外の時刻ではソース言語とターゲット言語は関係がない。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
この点を改善することを目指すことが 2014 年以降盛んになった。
顕著な例が後述する <strong>双方向 RNN</strong>, <strong>LSTM</strong> を採用したり，<strong>注意</strong> 機構を導入することであった。</p>

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_S22_Fig1.svg" style="width:88%" /><br />
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 1
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2left.svg" style="width:88%" /><br />
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2right.svg" style="width:88%" /><br />
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 2
</center>

<!-- 
# 自然言語系の注意

<center>

![](assets/2015Bahdanau_attention.jpg){style="width:30%"}
![](assets/2015Luong_Fig2.svg){style="width:33%"}
![](assets/2015Luong_Fig3.svg){style="width:33%"}><br />
左: [@2014Bahdanau_NMT], 中: [@2015Luong_attention] Fig. 2, 
右: [@2015Luong_attention] Fig. 3
</center>
-->

<!-- <center style="width:74% align:center">

![](assets/1957Osgood_fig19a.jpg){style="width:30%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19b.jpg){style="width:28%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19c.jpg){style="width:23%"}<br />
<div align="left">
Osgood (1957) 図. 19 より。ある女性患者（統合失調症）の心的イメージ。左が治療前，中央が治療中，右が治療後期
</div>
</center>
 -->
<!--
# 多頭=自己注意 Multi-Head Self-Attention: MHSA

- 自然言語処理 NLP **Transformer**[@2017Vaswani_transformer]; **BERT**[@2018BERT]; **RoBERTa**[@2019RoBERTa]; **distilBERT** [@2020Sanh_distilBERT]; and more...
- 画像処理 [@2019Ramachandran_attention_vision]; **A2-Net** [@2018Chen_A2-nets_double_attention]; **U-GAT-IT** [@2019Kim_U-GAT-IT]
- 強化学習，メタ学習 **SNAIL** [@2018Mishra_SNAIL]
- 敵対生成ネットワーク **SAGAN** [@2019Zhang_Goodfellow_SAGAN]


-->

<h1 id="トランスフォーマー-が提唱した-自己注意">トランスフォーマー が提唱した 自己注意</h1>

<p>専門用語としては，<strong>多頭=自己注意</strong> Multi-Head Self-Attention (以下 MHSA と表記)と呼びます。
多頭とは何か，なぜ 自己 がつく注意なのかを確認してください。</p>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:24%" />
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/2019Ramachandran_fig3.jpg" style="width:64%" /><br />
Left: [@2017Vaswani_transformer], Right: [@2019Ramachandran_attention_vision]
</center>

<ul>
  <li>上図，クエリ，キー，バリュー に注目してください。</li>
  <li>英単語の意味どおりに解釈すれば，問い合わせ，キー（鍵），値，となります。</li>
  <li>つまり，ある問い合わせに対して，キーを与えて，その答えとなる値を得ること。</li>
  <li>この操作を入力情報から作り出して答えを出力する仕組みに，ワンホット表現を使うことがポイント</li>
</ul>

<p>下図左は上図右と同じものです。この下図右を複数個束ねると下図中央になります。</p>

<ul>
  <li>下図中央の Scaled Dot-Product Attention と書かれた右脇に小さく h と書かれています。</li>
  <li>この h とは ヘッド の意味です。</li>
  <li>下図中央を 1 つの単位として，次に来る情報と連結させます。これが下図右です。</li>
  <li>先週のリカレントニューラルネットワークでは，中間層の状態が次の時刻の処理に継続して用いられていました。</li>
  <li>ところが 多頭=自己注意 MHSA では一つ前の入力情報を，現在の時刻の情報に対するクエリとキーのように扱って情報を処理します。</li>
  <li>下図右の下から入力される情報は，input と output と書かれています。さらに output の下には (Shifted right) と書かれています。すなわち，時系列情報を一時刻分だけ右にずらし（シフト）させて逐次情報を処理することを意味しています。</li>
  <li>下図右の下から入力される情報は，embedding つまり埋め込み表現 と 位置符号化 position embedding が足し合わされたものです。埋め込み表現とは先週 word2vec で触れたベクトルで表現された，単語（あるいはそれぞれの項目）の 意味表現 に対応します。</li>
</ul>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:15%" />
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-20.png" style="width:23%" />
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-21.png" style="width:29%" />
</center>
<!--
![](assets/ModalNet-19.png){style="width:15%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-20.jpg){style="width:23%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-21.png){style="width:29%"}
</center>
-->

<!-- 
<center>

![](assets/2019Zhang_Goodfellow_SAGAN_fig2.jpg){style="width:88%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1upper.jpg){style="width:74%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1lower.jpg){style="width:74%"}<br/>
From [@2019Zhang_Goodfellow_SAGAN] Fig. 1, and 3.
画像生成において，近傍画素から情報だけでなく，関連する遠距離の特徴を利用して生成することにより一貫性のある対象やシナリオを生成可能。
各行の左の元画像上のカラー点は 5 つ の 代表的なクエリの場所を示す。
右側の 5 画像は 各クエリ位置における注意地図。最も注目されている領域が，色分けされた矢印で示されている。
</center>
-->

<!-- 
<center>

![](assets/2017Gupta_Non-local_fig2.svg){style="width:29%"}
![](assets/2017Gupta_Non-local_example_230_0_eps_18_9.svg){style="width:59%"}<br/>
時空の非局所ネットワークの概念図。特徴地図はテンソルとして示されている。
例えば 1024 チャンネルの場合は $T\times H\times W\times1024$ である。
$\otimes$ は行列積を，$\oplus$ は要素和を示す。
ソフトマックス演算は各行に対して実行される。
青いボックスは $1\times1\times1\times1$ の畳み込みを表す。
$512$ チャンネルのボトルネックを持つ埋め込みガウシアン版が示されている。
バニラガウス版は $\theta$ と $\phi$ とを除去することで ドット積版は $1/N$ のスケーリングでソフトマックスを置き換えることで行うことができる。
From [@2018Wang_Girshick_Non-local] 
</center>
-->

<!-- <center>

![](assets/2018snail_fig2b.svg){style="width:49%"}<br/>
From [@2018Mishra_SNAIL] Fig. 2
</center>

トランスフォーマーはリカレント構造や畳み込み構造を持たず埋め込みベクトルに位置符号化器を加えることで系列情報を処理する。
しかし、逐次的な順序情報が貧弱であるとの批判がある。
とりわけ強化学習のような位置依存性に敏感な課題では問題。
トランスフォーマーモデルにおける 位置問題を解決するため，自己注意機構 と 時間的な畳み込み temporal convolution を組み合わせたモデルが 
Simple Neural Attention Meta-Learner (SNAIL)[@2018Mishra_SNAIL]。
SNAIL は，メタ学習，強化学習の両方の課題に優れていることが実証された。
-->

<p>少しだけまとめると:</p>

<ul>
  <li>自然言語処理，画像処理，強化学習，メタ学習の 4 分野でほほ同様の 多頭自己注意 MHSA が取り入れられている。</li>
  <li>クエリ，キー，バリュー の重みを学習することが MHSA の学習である。</li>
  <li>従来手法である 畳み込み や LSTM を MHSA で置き換える動きがある。</li>
</ul>

<h1 id="bert-の特徴">BERT の特徴</h1>

<ul>
  <li>上記のトランスフォーマーに基づいて BERT が提案されました。</li>
  <li>BERT は <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers から命名したと原著論文には書いてあります。</li>
  <li>ですが，この原著論文の直前に提案されたモデルに ELMo があったため，こじつけた，ふざけた命名でしょう。</li>
  <li>もちろん ELMo (こちらは <strong>E</strong>mbeddings from <strong>L</strong>anguage <strong>Mo</strong>dels から命名されました)も BERT もセサミストリートに出てくるキャラクタです。</li>
</ul>

<!-- From singularitysalon2019/nlp.tex -->

<!--BERT の影響が大きいので，本稿でも BERT を中心に取り上げる。-->
<p>BERT の特徴を 3 つにまとめると以下の通り</p>

<ol>
  <li>トランスフォーマー Transformer に基づく 多頭自己注意 (MHSA) を使った多層ニューラルネットワークモデル</li>
  <li>2 つの事前訓練: <strong>マスク化言語モデル</strong> と <strong>次文予測課題</strong> を用いる</li>
  <li>事前訓練済のモデルを用いて，解くべき課題のそれぞれについて <strong>ファインチューニング</strong> Fine tuning を施す</li>
  <li>個別の課題は下流課題 down stream tasks と呼ばれます。上流 と 下流 との区別は，最初に行う事前訓練のことを時間的に先行するので上流，その後のファインチューニングするそれぞれの課題のことを下流課題と呼んでいます。</li>
  <li>複数の課題に対して個別にファインチューニングを行うことにより，複数の下流課題で性能向上が認められました。 <a href="https://gluebenchmark.com/leaderboard">GLUE スコアボード</a>, <a href="https://super.gluebenchmark.com/leaderboard/">SuperGLUE</a> を参照してください。</li>
</ol>

<h1 id="bert-の入力表現">BERT の入力表現</h1>

<ul>
  <li>上の図にもあったとおり BERT では入力情報が埋め込み表現だけでなく，位置符号化器の情報が加算されます。</li>
  <li>BERT では，埋め込み表現と位置符号化器の情報に加えて，セグメント埋め込み segment embeddings も加えた情報が入力情報となります。下図参照</li>
</ul>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2018Devlin_BERT_Fig2.svg" style="width:84%" /><br />
<!-- ![](assets/2018Devlin_BERT_Fig2.svg){style="width:84%"}<br /> -->
埋め込みトークンの総和，位置符号器，分離埋め込みの 3 者 From [@2018BERT] Fig. 2
</center>

<ul>
  <li>上図では，下 3 行が入力情報を構成する 3 つの要素になっています。上（ピンク色）が合算した入力情報になります。</li>
  <li>3 つの入力情報とはそれぞれ，下から 位置符号化器 （薄灰色），セグメント埋め込み (淡緑)，トークン埋め込み (淡黄) です。</li>
</ul>

<h1 id="位置符号器-position-encoders">位置符号器 Position encoders</h1>

<ul>
  <li>上述のようにトランスフォーマーの入力には，単語埋め込み表現に加えて，位置符号器の信号も加算されます。</li>
</ul>

<!-- 位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\begin{align}
\text{PE}_{(\text{pos},2i)} &= \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} &= \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{align}
$$
-->

<ul>
  <li>位置符号器による位置表現は，i 番目の位置情報をワンホット表現するのではなく，周波数領域に変換することで周期情報を表現する試みと見なすことができます。</li>
</ul>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/PE_example.svg" style="width:74%" /><br />
位置符号化に用いられる符号化。位置情報を周波数情報へ変換して用いています。
<!-- ![](assets/PE_example.svg){style="width:74%"}<br/> -->
</center>

<ul>
  <li>
    <p>位置情報を周波数情報へ変換することが良いことなのか，どうなのか，は議論されている最中です。
一つの研究テーマでもあります。</p>
  </li>
  <li>
    <p>数学的な説明は <strong>フーリエ変換</strong> を調べてください。任意の関数 y=f(x) では x は位置情報を表しているとみなすことができます。
従って，位置 x を与えると対応する値 y が得られることを表している式が y=f(x) です。
これに対して，任意の情報は周波数，すなわち，波の重ね合わせとして表現できます。
すべての周波数を重ね合わせると元の関数になります。
反対に，ある周波数の値は，関数 f(x) を周波数へ変換したときの特定の周波数成分として表現できます。</p>
  </li>
</ul>

<p>BERT における位置符号化器は位置情報を波の成分として表現したことになります。</p>

<p>このようにしてできた値を入力側と出力側で下図のように連結させたものが以下のトランスフォーマーです。</p>

<center>

<img src="https://komazawa-deep-learning.github.io/assets/2017Vaswani_Fig1.svg" style="width:37%" /><br />
From [@2017Vaswani_transformer] Fig. 1
</center>

<p>これまで見てきたように，トランスフォーマーでは入力信号に基づいて情報の変換が行なわれる。
この意味ではトランスフォーマーにおける 多頭 自己注意 MHSA とはボトムアップ注意の変形であるとみなしうる。
逆言すれば，RNN のように過去の履歴をすべて保持しているわけではないので，系列情報については，position encoders に頼っている側面が指摘できる。
<!-- %\input{ELMoBERTGPT_Gao2018.tex}
へーこれでインプットか？
--></p>
<h1 id="bert-の事前訓練-マスク化言語モデル">BERT の事前訓練: マスク化言語モデル</h1>

<p>全入力系列のうち 15% をランダムに [MASK] トークンで置き換える</p>

<ul>
  <li>入力はオリジナル系列を [MASK] トークンで置き換えた系列</li>
  <li>ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測</li>
  <li>80%: オリジナル入力系列を [MASK] で置換</li>
  <li>10%: [MASK] の位置の単語をランダムな無関連語で置き換える</li>
  <li>10%: オリジナル系列</li>
</ul>

<h1 id="bert-の事前訓練-次文予測課題">BERT の事前訓練: 次文予測課題</h1>

<p>言語モデルの欠点を補完する目的，次の文を予測</p>

<p>[SEP] トークンで区切られた 2 文入力</p>

<ul>
  <li>入力: the man went to the store [SEP] he bought a gallon of milk.</li>
  <li>ラベル:  IsNext</li>
  <li>入力:  the man went to the store [SEP] penguins are flightless birds.</li>
  <li>ラベル:  NotNext</li>
</ul>

<h1 id="bert-ファインチューニング">BERT: ファインチューニング</h1>

<p>(a), (b) は文レベル課題，
(c),(d)はトークンレベル課題, E: 入力埋め込み表現, $T_i$: トークン $i$ の文脈表象。</p>

<!-- 
- [CLS]: 分類出力記号,
- [SEP]: 文分離記号 
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2018Devlin_BERT_Fig3.svg" style="width:88%" /><br />
From [@2018BERT] Fig.3
</center>

<h1 id="glue-general-language-understanding-evaluation">GLUE: General Language Understanding Evaluation</h1>
<ul>
  <li><strong>CoLA</strong>: 入力文が英語として正しいか否かを判定</li>
  <li><strong>SST-2</strong>: スタンフォード大による映画レビューの極性判断</li>
  <li><strong>MRPC</strong>: マイクロソフトの言い換えコーパス。2文 が等しいか否かを判定</li>
  <li><strong>STS-B</strong>: ニュースの見出し文の類似度を5段階で評定</li>
  <li><strong>QQP</strong>: 2 つの質問文の意味が等価かを判定</li>
  <li><strong>MNLI</strong>: 2 入力文が意味的に含意，矛盾，中立を判定</li>
  <li><strong>QNLI</strong>: 2 入力文が意味的に含意，矛盾，中立を判定</li>
  <li><strong>RTE</strong>: MNLI に似た2つの入力文の含意を判定</li>
  <li><strong>WNI</strong>: ウィノグラッド会話チャレンジ</li>
</ul>

<p>その他</p>

<ul>
  <li><strong>SQuAD</strong>: スタンフォード大による Q and A ウィキペディアから抽出した文</li>
  <li><strong>RACE</strong>: 中学入試，高校入試に相当するテスト多肢選択回答</li>
</ul>

<h1 id="bert-モデルの詳細">BERT モデルの詳細</h1>
<ul>
  <li>データ: Wikipedia (2.5B words) + BookCorpus (800M words)</li>
  <li>バッチサイズ: 131,072 words (1024 sequences * 128 length or 256 sequences * 512 length)</li>
  <li>訓練時間: 1M steps (~40 epochs)</li>
  <li>最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay</li>
  <li>BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意</li>
  <li>BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意</li>
  <li>4x4 / 8x8 TPU で 4 日間</li>
</ul>

<h3 id="cola-サンプル">CoLA サンプル</h3>

<p>1 は正しい英文，0 は非文</p>

<ul>
  <li>1 They drank the pub dry.</li>
  <li>0 <strong>They drank the pub</strong>.</li>
  <li>1 The professor talked us into a stupor.</li>
  <li>0 <strong>The professor talked us</strong>.</li>
  <li>1 We yelled ourselves hoarse.</li>
  <li>0 <strong>We yelled ourselves</strong>.</li>
</ul>

<h3 id="sst-2-サンプル">SST-2 サンプル</h3>

<p>0 は低評価，1 は高評価</p>

<ul>
  <li>hide new secretions from the parental units     0</li>
  <li>contains no wit , only labored gags     0</li>
  <li>that loves its characters and communicates something rather beautiful about human nature        1</li>
  <li>remains utterly satisfied to remain the same throughout         0</li>
  <li>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up        0</li>
  <li>that’s far too tragic to merit such superficial treatment      0</li>
</ul>

<!-- - demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , pe
- rsonal film with an emotional wallop .  1
- of saucy        1
- a depressed fifteen-year-old 's suicidal poetry         0
- are more deeply thought through than in most ` right-thinking ' films   1
- goes to absurd lengths  0
- for those moviegoers who complain that ` they do n't make movies like they used to anymore      0
- the part where nothing 's happening ,   0
- saw how bad this movie was      0
- lend some dignity to a dumb story       0
 -->

<h3 id="mrpc-サンプル">MRPC サンプル</h3>

<ul>
  <li>1
    <ul>
      <li>文1: “Please, keep doing your homework,” said Bavelier, the mother of three.</li>
      <li>文2: “Please, keep doing your homework,” said Bavelier, the mother of 6-year-old twins and a 2-year old.</li>
    </ul>
  </li>
  <li>1
    <ul>
      <li>文1: While Mr. Qurei is widely respected and has a long history of negotiating with the Israelis, he cannot expect such a warm welcome.</li>
      <li>文2: While Qureia is respected and has a history of negotiating with the Israelis, a warm welcome is not expected.</li>
    </ul>
  </li>
  <li>1
    <ul>
      <li>文1: “Nobody wants to go to war with anybody about anything … it ‘s always very much a last resort thing and one to be avoided,” Mr Howard told Sydney radio.</li>
      <li>文2: “We don’t want to go to war with anybody . . . it’s always very much a last resort, and one to be avoided.</li>
    </ul>
  </li>
  <li>0
    <ul>
      <li>文1: GMT, Tab shares were up 19 cents, or 4.4% , at A $4.56, having earlier set a record high of A $4.57.</li>
      <li>文2: Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A $4.57.</li>
    </ul>
  </li>
  <li>0
    <ul>
      <li>文1: Martin, 58, will be freed today after serving two thirds of his five-year sentence for the manslaughter of 16-year-old Fred Barras.</li>
      <li>文2: Martin served two thirds of a five-year sentence for the manslaughter of Barras and for wounding Fearon.</li>
    </ul>
  </li>
</ul>

<!-- - 1
	- 文1: The stock rose $2.11, or about 11 percent, to close Friday at $ 21.51 on the New York Stock Exchange.     
	- 文2: PG & E Corp. shares jumped $1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday.
- 1
	- 文1: Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.     
	- 文2: With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.
- 0
	- 文1: The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday.      
	- 文2: The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15.
- 1
	- 文1: The DVD-CCA then appealed to the state Supreme Court.  
	- 文2: The DVD CCA appealed that decision to the U.S. Supreme Court.

 -->
<!-- # BERT ファインチューニング手続き
<center>
<img src="./assets/2019Devlin_mask_method21.jpg" style="width:74%"><br/>
</center>
 -->

<h3 id="sst-b-サンプル">SST-B サンプル</h3>

<p>最後の数値が評価値</p>

<ul>
  <li>A plane is taking off.  An air plane is taking off.   5.000</li>
  <li>A man is playing a large flute. A man is playing a flute.     3.800</li>
  <li>A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800</li>
  <li>Three men are playing chess.    Two men are playing chess.    2.600</li>
  <li>A man is playing the cello.     A man seated is playing the cello.    4.250</li>
  <li>Some men are fighting.  Two men are fighting. 4.250</li>
  <li>A man is smoking.   A man is skating 0.5000</li>
</ul>

<h3 id="qqp-サンプル">QQP サンプル</h3>

<p>0 は異なると判断， 1 は同じと判断すべき文</p>

<ul>
  <li>0
    <ul>
      <li>How is the life of a math student? Could you describe your own experiences?</li>
      <li>Which level of prepration is enough for the exam jlpt5?</li>
    </ul>
  </li>
  <li>1
    <ul>
      <li>How do I control my horny emotions?</li>
      <li>How do you control your horniness?</li>
    </ul>
  </li>
  <li>0
    <ul>
      <li>What causes stool color to change to yellow?</li>
      <li>What can cause stool to come out as little balls?     0</li>
    </ul>
  </li>
  <li>1
    <ul>
      <li>What can one do after MBBS?</li>
      <li>What do i do after my MBBS?</li>
    </ul>
  </li>
  <li>0
    <ul>
      <li>Where can I find a power outlet for my laptop at Melbourne Airport?</li>
      <li>Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?</li>
    </ul>
  </li>
  <li>0
    <ul>
      <li>How not to feel guilty since I am Muslim and I’m conscious we won’t have sex together?</li>
      <li>I don’t beleive I am bulimic, but I force throw up at least once a day after I eat something and feel guilty.  Should I tell somebody, and if so who?</li>
    </ul>
  </li>
</ul>

<h3 id="mnli-サンプル">MNLI サンプル</h3>

<ul>
  <li>矛盾
    <ul>
      <li>Met my first girlfriend that way.</li>
      <li>I didn’t meet my first girlfriend until later.</li>
    </ul>
  </li>
  <li>中立
    <ul>
      <li>8 million in relief in the form of emergency housing.</li>
      <li>The 8 million dollars for emergency housing was still not enough to solve the problem.</li>
    </ul>
  </li>
  <li>中立
    <ul>
      <li>Now, as children tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.</li>
      <li>All of the children love working in their gardens.</li>
    </ul>
  </li>
  <li>含意
    <ul>
      <li>At 8:34, the Boston Center controller received a third transmission from American 11</li>
      <li>The Boston Center controller got a third transmission from American 11.</li>
    </ul>
  </li>
  <li>中立
    <ul>
      <li>I am a lacto-vegetarian.</li>
      <li>I enjoy eating cheese too much to abstain from dairy.</li>
    </ul>
  </li>
  <li>矛盾
    <ul>
      <li>someone else noticed it and i said well i guess that’s true and it was somewhat melodious in other words it wasn’t just you know it was really funny</li>
      <li>No one noticed and it wasn’t funny at all.</li>
    </ul>
  </li>
</ul>

<h1 id="事前訓練とマルチ課題学習">事前訓練とマルチ課題学習</h1>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/mt-dnn.png" style="width:66%" /><br />
From [@2019Liu_mt-dnn] Fig. 1
</center>

<!-- 
# Transformer: Attention is all you need

$$\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V$$

<center>

![](assets/2017Vaswani_Fig2_1.svg){style="width:17%"}
![](assets/2017Vaswani_Fig2_2.svg){style="width:23%"}<br />
From [@2017Vaswani_transformer] Fig. 2
</center>
-->

<!-- 
# Transformer(2): Attention is all you need

$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_1,\ldots,\mathop{head}_h\right)W^O
$$

where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$, 
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2$$

$$\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$

$$\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$
-->

<!-- 
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

<h1 id="多言語対応">多言語対応</h1>
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Lample_Fig1.svg" style="width:88%" /><br />
From [@2019Lample_Cross-lingual] Fig. 1
</center>

<h1 id="bert-の発展">BERT の発展</h1>
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Rajasekharan_conver.png" style="width:54%" /><br />
From &lt;https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58&gt;
</center>

<h1 id="bert-ファインチューニング手続きによる性能比較">BERT: ファインチューニング手続きによる性能比較</h1>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_mask_method21.jpg" style="width:66%" /><br />
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

<p>マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。</p>

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
 -->

<h1 id="bert-モデルサイズ比較">BERT: モデルサイズ比較</h1>

<center>
<img src="https://komazawa-deep-learning.githbub.io/assets/2019Devlin_model_size20.jpg" style="width:59%" /><br />
モデルのパラメータ数による性能比較
</center>

<p>パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。</p>

<!-- # BERT モデル単方向，双方向モデル比較
<center>
<img src="./assets/2019Devlin_directionality19.jpg" style="width:66%"><br/>
</center>

 -->
<h1 id="bert-モデル単方向双方向モデル比較">BERT: モデル単方向，双方向モデル比較</h1>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_directionality19.jpg" style="width:59%" /><br />
言語モデルの相違による性能比較
</center>

<p>言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。</p>

<!-- # BERT 事前訓練比較
<center>
<img src="./assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

<h1 id="bert-事前訓練比較">BERT: 事前訓練比較</h1>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:59%" /><br />
事前訓練の効果比較
</center>

<p>図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。</p>

<!--![](../2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

<h1 id="各モデルの特徴">各モデルの特徴</h1>

<ul>
  <li>RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした</li>
  <li>XLNet: 順列言語モデル。2 ストリーム注意</li>
  <li>MT-DNN: BERT ベース の転移学習に重きをおいたモデル</li>
  <li>GPT-2: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ</li>
  <li>BERT: Transformerに基づく言語モデル。<strong>マスク化言語モデル</strong> と <strong>次文予測</strong> に基づく 事前訓練，各下流課題をファインチューニング。事前訓練されたモデルは一般公開済。</li>
  <li>DistillBERT: BERT の蒸留版</li>
  <li>ELMo: 双方向 RNN による文埋め込み表現</li>
  <li>Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.</li>
</ul>

<!-- # 埋め込みモデルによる構文解析
<center>
<img src="assets/2019hewitt-header.jpg" style="width:79%"><br/>
From https://github.com/john-hewitt/structural-probes
</center>

 -->
<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクトル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。

<center>
![](assets/2019Devlin_BERT01upper.svg){style="width:74%"}
[@2019Devlin_BERT]  単語のベクトル表現
</center>

単語埋め込み (word2vec[@2013Mikolov_VectorSpace];[@2013Mikolov_VectorSpace]) 
単語は周辺単語の共起情報 [点相互情報量 PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) に基づく[@2014LevyGoldberg:nips],[@2014Levy:3cosadd]。
すなわち周辺単語との共起情報を用いて単語の意味を定義している。

<center>
![](assets/2019Devlin_BERT01lower.svg){style="width:74%"}
</center>

形式的には，skip-gram であれ CBOW であれ同じである。

# 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<center>
![](assets/2019Devlin_BERT02upper.svg){style="widht:74%"}<br/>
単語の意味を一意に定めることができない場合

![](assets/2019Devlin_BERT02lower.svg){style="width:74%"}<br/>
</center>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，
句，節，文のベクトル表現を得る努力がなされてきた。
適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<center>
![](assets/2019Devlin_BERT03.svg){style="width:74%"}<br/>
[@2014Sutskever_Sequence_to_Sequence] より
</center>

BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。

# BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。
事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類
の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。
事前訓練語，各課題毎にファインチューニングが施される。

<center>
![](assets/mt-dnn.png){style="width:89%"}<br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>
 -->

<h1 id="bert-埋め込みモデルによる構文解析">BERT: 埋め込みモデルによる構文解析</h1>

<p>BERT の構文解析能力を下図示した。
各単語の共通空間に射影し，
単語間の距離を計算することにより構文解析木と同等の表現を得ることができることが報告されている[@2019HewittManning_structural]。</p>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019hewitt-header.jpg" style="width:39%" />
&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/2019HewittManning_blogFig1.jpg" style="width:19%" />
<img src="https://komazawa-deep-learning.github.io/assets/2019HewittManning_blogFig2.jpg" style="width:19%" />
<!-- ![](assets/2019HewittManning_blogFig1.jpg){style="width:19%"}
![](assets/2019HewittManning_blogFig2.jpg){style="width:19%"}<br/>-->
BERT による構文解析木を再現する射影空間
</center>
<p>From <a href="https://github.com/john-hewitt/structural-probes">https://github.com/john-hewitt/structural-probes</a></p>

<ul>
  <li>word2vec において単語間の距離は内積で定義されていました。</li>
  <li>このことから，文章を構成する単語で張られる線形内積空間内の距離が構文解析木を与えると見なすことは不自然ではないと予想できます。</li>
</ul>

<!-- 
% > **The syntax distance hypothesis**: There exists a linear transformation
% > $\mathbf{B}$ of the word representation space under which vector distance
% > encodes parse trees.  Equivalently, there exists an inner product on the
% > word representation space such that distance under the inner product
% > encodes parse trees. This (indefinite) inner product is specified by
% > $\mathbf{B}^{\top}\mathbf{B}$.

% We'll take a particular instance of this hypothesis for our probes; 
% we'll use the L2 distance, and let the squared vector distances equal the tree distances, but more on this later.  
-->

<ul>
  <li>そこで構文解析木を再現するような射影変換を見つけることができれば BERT を用いて構文解析が可能となるでしょう。</li>
  <li>例えば上図における chef と store と was の距離を解析木を反映するような空間を見つけ出すことに相当します</li>
</ul>

<!-- % The distances we pointed out earlier between \_chef\_, \_store\_ and \_was\_, can be visualized in a vector space as follows, where $\mathbf{B}\in\mathbb{R}^{2\times3}$, mapping 3-dimensional word representations to a 2-dimensional space encoding syntax:
-->
<!--% Note in the image above that the distances between words before
% transformation by $\mathbf{B}$ aren't indicative of the tree. After the
% linear transformation, however, taking a minimum spanning tree on the
% distances recovers the tree, as shown in the following image:

% <center>
% % ![](assets/0.332019HewittManning_blogFig2.jpg}
% </center>

% Finding a parse tree-encoding distance metric Our potentially tree-encoding distances are parametrized by the linear transformation $\mathbf{B}\in\mathbb{R}^{k\times n}$, 

% \begin{equation}
% \left\|h_i-h_j\right\|_B^2=\left(B\left(h_i-h_j\right)\right)^{\top}\left(B\left(h_i-h_j\right)\right)
% \end{equation}

% where $\mathbf{B}_h$ is the linear transformation of the word representation; equivalently, it is the parse tree node representation. 
% This is equivalent to finding an L2 distance on the original vector space, parametrized by the positive semi-definite matrix $A=B^{\top}B$:

% \begin{equation}
% \left\|h_i-h_j\right\|_A^2=\left(h_i-h_j\right)^{\top}A\left(h_i-h_j\right)
% \end{equation}
% The set of linear transformations, $\mathbb{R}^{k\times n}$ for a given $k$ is the hypothesis class for our probing family.  
% We choose $B$ to minimize the difference between true parse tree distances from a human-parsed corpus and the predicted distances from the fixed word representations transformed
% by $B$: 
-->

<!-- 2 つの単語 $w_i$, $w_j$ とし単語間の距離を $d\left(w_i,w_j\right)$ とする。
適当な変換を施した後の座標を $h_i$, $h_j$ とすれば，求める変換 $B$ は次式のような変換を行なうことに相当する:
$$
\min_{B}\sum_l\frac{1}{\left|s_\ell\right|^2}\sum_{i,j}\left(d\left(w_i,w_j\right)-\left\|B\left(h_i-h_j
\right)\right\|^2\right)
$$
ここで $\ell$ は文 s の訓練文のインデックスであり，各文の長さで規格化することを意味している。
 -->

<p>具体的には，以下のような操作をしています:</p>

<ol>
  <li>文章に現れる全トークンを表すベクトルを BERT より求める。</li>
  <li>すなわち BERT 全中間層ユニット活性値から構成される全ての値から構成されるベクトル群</li>
  <li>2 のベクトルが張る部分空間に全トークンを射影する。</li>
  <li>3 の部分空間内でトークン間の距離を求める。</li>
  <li>各トークンを短い順にグラフで結ぶ</li>
</ol>

<!--% where $\ell$ indexes the sentences $s_{\ell}$ in the corpus, and $\frac{1}{\left|s_\ell\right|^2}$ normalizes for the number of pairs of words in each sentence. 
% Note that we do actually attempt to minimize the difference between the squared distance $\left\|h_i-h_j\right\|_B^2$ and the tree distance. 
% This means that the actual vector distance $\left\|h_i-h_j\right\|_B$ will always be off from the true parse tree distances, but the tree information encoded is identical, and we found that optimizing with the squared distance performs considerably better in practice.

% Finding a parse depth-encoding norm As a second application of our method, we note that the directions of the edges in a parse tree is determined by the depth of words in the parse tree; the deeper node in the governance relationship is the governed word. The depth in the parse tree is like a norm, or length, defining a total order on the nodes in the tree. We denote this tree depth norm $\left\|w_i\right\|$.

% Likewise, vector spaces have natural norms; our hypothesis for norms is that there exists a linear transformation under which tree depth norm is encoded by the squared L2 vector norm $\left\|Bh_i\right\|_2^2$. 
% Just like  for the distance hypothesis, we can find the linear transformation under which the depth norm hypothesis is best-approximated:

% \begin{equation}
% \min_B\sum_\ell\frac{1}{\left|s_\ell\right|}\sum_i\left(\left\|w_i\right\|-\left\|Bh_i\right\|^2\right)
% \end{equation}

% To be effective, the manual should follow three key principles:
% \begin{enumerate}
% -  It should be simple and write on a single page, e.g. as a bulleted list of operating procedures.
% -  It should be prioritised in a strategic order that you can start executing tomorrow.
% -  It should be reviewed, evaluated, and understood by everyone crucial to the mission.
% \end{enumerate}
-->

<h1 id="bert-実装">BERT 実装</h1>
<ul>
  <li>BERT 実装のパラメータを以下に示した。</li>
  <li>現在配布されている BERT-base あるいは性能が良い BERT-large は各層のニューロン数と全体の層数である。</li>
  <li>ソースコードの配布先は https://github.com/google-research/bert</li>
  <li>
    <p>オリジナルの論文は https://arxiv.org/abs/1810.04805</p>
  </li>
  <li>データ: Wikipedia (2.5B words) + BookCorpus (800M words)</li>
  <li>バッチサイズ: 131,072 words (1024 sequences $\times$ 128 length or 256 sequences $\times$ 512 length)</li>
  <li>訓練ステップ: 1M steps (40 epochs)</li>
  <li>最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay</li>
  <li>BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意</li>
  <li>BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意</li>
  <li>訓練時間: 4x4 / 8x8 の TPU で 4 日間</li>
</ul>

<h1 id="lstm-との異同">LSTM との異同</h1>

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2015Greff_LSTM_ja.svg" style="width:54%" />
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:26%" /><br />
左: LSTM (浅川, 2015) より，右: トランスフォーマー[@2017Vaswani_transformer]<br />
入力ゲートと入力 は Q, K と同一視，出力ゲートと V とは同一視可能？
</center>

<!-- 
# Residual attention
<center>

![](assets/2017residual_attention.svg){style="width:33%"}
![](assets/2017residual_attention_motivation.svg){style="width:65%"}<br/>
![](assets/2017residual_attention_whole_net.svg){style="width:94%"}<br/>
[@2017Wang_residual_attention] Fig. 1, 2, 3
</center>
-->

<!-- 
# A2 net

<center>
![](assets/2018Chen_A2-Nets_fig1ja_a.svg){style="width:39%"}
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
![](assets/2018Chen_A2-Nets_fig1ja_b.svg){style="width:55%"}<br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>
 -->

<h1 id="relationship-between-self-attention-and-convolution">Relationship between self-attention and convolution</h1>

<center>
<img src="http://komazawa-deep-learning.github.io/assets/2019cordonnier_self_attention_convol.svg" style="width:88%" /><br />
<img src="http://komazawa-deep-learning.github.io/assets/2020Cordonnier_tab3.svg" style="width:88%" /><br />
From [@2020cordonnier_attention_and_convolution]
</center>

<h1 id="まとめ">まとめ</h1>

<ul>
  <li>MHSA は 畳み込み と同等の能力がありそうである。</li>
  <li>Reformer に見られるように position encodings を工夫する余地は残されているように思われる。</li>
</ul>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">ディープラーニングの心理学的解釈</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">ディープラーニングの心理学的解釈</li><li><a class="u-email" href="mailto:educ0233@komazawa-u.ac.jp">educ0233@komazawa-u.ac.jp</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Deep learning for psychology  駒澤大学文学部2021年度開講 心理学特講IIIA,B </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
