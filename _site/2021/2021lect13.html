<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/assets/css/swiss.css"></head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    CommonHTML: { matchFontHeight: false },
    displayAlign: "left",
    displayIndent: "2em",
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
    }
  });
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS_CHTML"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
  <body>
<div class="header">
  <div class="wrap">
    
      <div class="header__inner header__inner--internal">
    
      <div class="header__content">
        <h1 class="header__title">
          
        </h1>
        <p class="header__tagline">
          
        </p>
      </div>
    </div>
  </div>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">第13回</h1><h1 id="ディープラーニングの心理学的解釈-心理学特講iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</h1>

<div align="right">
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br />
Date: 02/Jul/2021<br />
Appache 2.0 license<br />
</div>

<ul>
  <li>謝辞: 最後まで来てくださった皆様に感謝いたします</li>
</ul>

<h1 id="本日のメニュー">本日のメニュー</h1>

<ol>
  <li>総復習</li>
  <li>期末試験の相談
    <ul>
      <li>指定問題
        <ul>
          <li>CNN を用いた心理実験計画を一つ立案せよ。実験計画には，検証すべき仮説と，その仮説を検証するための工夫が記述されていなければならない。</li>
          <li>実験立案にあたっては，以下の レベル1 と レベル0 の応用とが区別される。レベル0 応用の方が評価点は高い。
            <ul>
              <li><strong>レベル1</strong>: CNN の画像認識精度を用いって特定の仮説を検証しようとするレベル。CNN を使って eコマースサイトを運用するような，CNN の本質を考えずに，表層的レベルの応用にとどまるもの。</li>
              <li><strong>レベル0</strong>: CNN を人間の画像認識機構のモデルとして用いる。人間の認識機構を CNN モデルと比較することで理解しようとする試み。CNN と人間の認識機構の本質に迫るもの。</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>選択問題
        <ul>
          <li>以下の文献資料のうち，一つを選んで要約する</li>
        </ul>
      </li>
      <li>知識を問う 4 択問題 数問</li>
    </ul>
  </li>
  <li>LSTM</li>
  <li>単語埋め込みモデル</li>
</ol>

<h1 id="mitchell">Mitchell</h1>

<p><br /><br /><br /></p>

<div align="center">
<hr />
<img src="../assets/2019mitchell-54_20.png" style="width:77%" /><br />
<br /><br /><br />
<hr />
<img src="../assets/2019mitchell_2.png" style="width:77%" /><br />
<br /><br /><br />
<hr />
<img src="../assets/2019mitchell_3.png" style="width:77%" /><br />
<br /><br /><br />
<hr />
<img src="../assets/2019mitchell_4.png" style="width:77%" /><br />
</div>

<p><br /><br /><br /></p>

<h1 id="資料">資料</h1>

<ul>
  <li><a href="/2021/2016RNNcamp2handout2.pdf" target="_blank">LSTM 資料</a></li>
</ul>

<h1 id="文献資料">文献資料</h1>
<ul>
  <li><a href="/2021/2020Sejnowski_Unreasonable_effectiveness_of_deep_learning_in_artificial_intelligence.pdf" target="_blank">Senjowski, Unreasonable effectiveness of deep learning in artificial intelligence, 2020</a></li>
  <li><a href="/2021/2019Storrs_Golan_Kriegeskorte_Neural_network_models_and_deep_learning.pdf" target="_blank">Storrs ら, Neural Network Models and Deep Learning, 2019</a></li>
</ul>

<h1 id="実習ファイル">実習ファイル</h1>

<ol>
  <li><a href="https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/notebooks/2021_0531ccap_word2vec.ipynb" target="_blank">単語の意味 word2vec の実演 <img src="../assets/colab_icon.svg" /></a></li>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0709torch_rnn_pynb.ipynb" target="_blank">RNN によるオノマトペ  <img src="../assets/colab_icon.svg" /></a></li>
</ol>

<!--

## 2.11. 画像と言語との融合へ向けて

以上で今回の特別企画の目標である画像と言語とのマルチモーダル統合へ向けての準備がほぼ出揃いました。
2014 年に提案されたニューラル画像脚注付けのモデルを下図に示します。


<center>
<img src="../assets/2014KarpathyImageDescriptionsFig3.svg" style="width:84%"><br>
[@2015Karpathy_FeiFei_caption]
</center>


<center>
<img src="../assets/2014Vinyals_Fig1.svg" style="width:84%"><br>
[@2014Vinyals_Bengio_Show_and_Tell]
</center>

<center>
<img src="../assets/2015Xu_ShowAttendTellFig1.svg" style="width:84%"></br>
</center>

画像に対して注意を付加した脚注付けモデルの出力例を下図に示します。

<center>
<img src="../assets/2015Xu_ShowAttendTellFig2_upper.svg" style="width:84%"><br>
[@2015Xu_Bengio_NIC_attention]
</center>

各画像対は右が入力画像であり，左はその入力画像の脚注付けである単語を出力している際にどこに注意しているのかを白色で表しています。

<center>
<img src="../assets/2015Xu_ShowAttendTellFig2_lower.svg" style="width:99%"></br>
[@2015Xu_Bengio_NIC_attention]
</center>


<center>
<img src="../assets/2014Mnih_attention.svg"></br>
</center>

Glimpse Sensor: Given the coordinates of the glimpse and an input image,
the sensor extracts a __retina-like__ representation
$\rho\left(x_t,l_{t-1}\right)$ centered at $l_{t-1}$ that contains multiple
resolution patches. 

- B) **Glimpse Network**: Given the location $\left(l_{t-1}\right)$ and input image $\left(x_t\right)$, uses the glimpse sensor to extract retina representation $\rho\left(x_t,l_{t-1}\right)$.  
The retina representation and glimpse location is then mapped into a hidden space using independent linear layers parameterized by $\theta_g^{0}$ and $\theta_g^{1}$ respectively using rectified units followed by another linear layer $\theta_2^{2}$ to combine the information from both components. 
The glimpse network $f_{g}\left(\dot;\left[\theta_g^0,\theta_g^1,\theta_g^2\right]\right)$ defines a trainable bandwidth limited sensor for the attention network producing the glimpse representation $g_t$. 
- C) **Model Architecture**: Overall, the model is an RNN. 
The core network of the model $f_h\left(\cdot;\theta_h\right)$ takes the glimpse representation $g_t$ as input and combining with the internal representation at previous time step $h_{t-1}$, produces the new internal state of the model $h_t$. 
The location network $f_l\left(\cdot;\theta_a\right)$ and the action network $f_a\left(\cdot;\theta_a\right)$ use the internal state $h_t$ of the model to produce the next location to attend to $l_t$ and the action/classification at respectively. This basic RNN iteration is repeated for a variable number of steps.[@2014Mnih_RNN_attention]
-->

<!--
#  World Models
<center>

<img src="../assets/2018Ha_WorldModel.svg" style="width:84%"></br>
[@2018Ha_WorldModels] Fig.1
</center>
<center>

<img src="../assets/2018HaWorldModelsFig1.svg"></br>
A World Model, from Scott McCloud’s Understanding Comics. (McCloud, 1993; E, 2012)
</center>

Jay Wright Forrester, the father of system dynamics,
described a mental model as:\\
    \begin{quote}
      The image of the world around us, which we carry in our
      head, is just a model. Nobody in his head imagines all
      the world, government or country. He has only selected
      concepts, and relationships between them, and uses those
      to represent the real system. \citep{1971Forrester}
    \end{quote}

<center>
-->

<center>
<img src="../assets/2015Greff_LSTM_ja.svg" style="width:74%" /><br />
<p align="left" style="width:49%">
LSTM の概念 (Shumithuber ら 2015)を改変
</p>
</center>

<center>
<img src="../assets/2010Mikolov_Fig1.svg" style="width:49%" /><br />
Mikolov(2010)
</center>

<center>

<img src="../assets/2011Mikolov_Extention_Fig1.jpg" style="width:49%" /><br />
Mikolov Extension
</center>

<center>

<img src="../assets/2001Boden_Fig5.jpg" style="width:94%" /><br />
Boden's BPTT
</center>

<!--
- モチベーション
- ニューラルネットワーク言語モデル
- 訓練アルゴリズム
  - リカレントニューラルネットワーク
  - クラス
  - エントロピー最大化言語モデル

### モチベーション

### モチベーション (2) チューリングテスト
- チューリングテストは原理的に言語モデルの問題とみなすことが可能
- 会話の履歴が与えられた時，良い言語モデルは正しい応答に高い確率を与える

- 例:
  - $P\left(\mathbf{ox{月曜日}\vert \mathbf{ox{今日は何曜日ですか？}}} = ?$\\
  - $P\left(\mathbf{ox{赤}\vert \mathbf{ox{バラは何色？}}} = ?$\\

言語モデルの問題と考えれば以下の文のような問題と等価とみなせる:\\
$P\left(\mathbf{ox{赤}\vert {\mathbf{ox{バラの色は}}}=?$

### モチベーション(3) n-グラム言語モデル

- どうすれば「良い言語モデル」を創れるか？
- 伝統的解: n-グラム言語モデル: $P\left{w\vert h}=\displaystyle\frac{C\left{h,w\right}}{C\left(h\right)}$
-->

<hr />

<h1 id="2-意味論">2. 意味論</h1>

<p>ここでは意味論の研究史を心理学関連領域に絞ってまとめることを試みます。
<!-- 神経心理学症状との関連については
付録 <a target="_blank" href="https://github.com/ShinAsakawa/wbai_aphasia/blob/master/2019Primer_AphasiaDyslexia.pdf">失語，失読に関する神経心理学モデルの基礎</a> をご覧ください。
--></p>

<p>意味についての言及は言語学者 Firth さらに遡れば Witgenstein まで辿ることが可能です。
ですがここでは直接関連する研究として以下をとりげます</p>

<ul>
  <li>第 1 世代 意味微分法 Osgood</li>
  <li>第 2 世代 潜在意味解析 Ladauer</li>
  <li>第 3 世代 潜在ディレクリ配置，トピックモデル</li>
  <li>第 4 世代 分散埋め込みモデル word2vec とその後継モデル</li>
  <li>最近の展開</li>
</ul>

<h2 id="31-1952-年-意味微分法-semantics-differential-sd">3.1. 1952 年 意味微分法 Semantics Differential (SD)</h2>
<p>チャールズ・オズグッドによって提案された意味微分法は，被験者に対象を評価させる際に形容詞対を用います。
形容詞対は 5 件法あるいはその他の変種によって評価されます。
得られた結果を 評価対象 X 形容詞対の行列にします。
すなわち評価対象者の平均を求めて得た行列を <strong>固有値分解</strong>，正確には因子分析 FA を行います。
最大固有値から順に満足の行くまで求めます。
固有値行列への射影行列を因子負荷量と呼びます。得られた結果を下図に示しました。</p>

<center>

<img src="../assets/1957Osgood_Tab1.svg" style="width:84%" /><br />
From  Osgood (1952) Tab. 1
</center>
<!-- <a target="_blank" href="../assets/1957Osgood_Tab1.svg">Osgood (1952) Tab. 1</a>-->

<p>上図では，50 対の形容詞対によって対象を評価した値が描かれています。</p>

<p><a target="_blank" href="https://ja.wikipedia.org/wiki/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90">因子分析(FA)</a> 形容詞対による多段階評定</p>

<center>

<img src="../assets/1957Osgood_Fig2.svg" style="width:64%" /><br />
From  Osgood (1952) Fig.2
<!--- <a target="_blank" href="../assets/1957Osgood_Fig2.svg">Osgood (1952) Fig. 2</a>-->
</center>

<p>意味微分法においては，研究者の用意した形容詞対の関係に依存して対象となる概念やモノ，コトが決まります。
従って研究者の想定していない概念空間については言及できないという点が問題点として指摘できます。</p>

<p>このことは評価対象がよくわかっている問題であれば精度良く測定できるという長所の裏返しです。</p>

<p>一般的な意味，対象者が持っている意味空間全体を考えるためには，50 個の形容詞対では捉えきれないことも意味します。従って以下のような分野に適用する場合には問題が発生する可能性があると言えます</p>

<ul>
  <li>神経心理学的な症状である <strong>意味痴呆</strong> semantic dimentia を扱う場合</li>
  <li>入試問題などの一般知識を評価したい場合</li>
  <li>一般言語モデルを作成する場合</li>
</ul>

<h2 id="32-1997-年-潜在意味分析-latent-semantic-analysis-lsa-lsi">3.2. 1997 年 潜在意味分析 Latent Semantic Analysis (LSA, LSI)</h2>
<ul>
  <li><strong>潜在意味分析</strong>: <a target="_blank" href="https://ja.wikipedia.org/wiki/%E7%89%B9%E7%95%B0%E5%80%A4%E5%88%86%E8%A7%A3">特異値分解(SVD)</a> は，当時増大しつつあったコンピュータ計算資源を背景に一般意味論に踏み込む先鞭をつけたと考えることができます。</li>
</ul>

<p>すなわち先代の意味微分法が持つ問題点である，評価方法が 50 対の形容詞であること，
50 をいくら増やしても，結局は研究者の恣意性が排除できないこと，評価者が人間であるため大量の評価対象を評価させることは，
心理実験参加者の拘束時間を長くするため現実的には不可能であることを解消するために，辞書そのものをコンピュータで解析するという手法を採用しました。</p>

<ol>
  <li>辞書の項目とその項目の記述内容とを考えます</li>
  <li>特定の辞書項項目にはどの単語が使われているいるのかという共起行列 内容 $\times$ 単語 を
考え，この行列について <strong>特異値分解</strong> を行います。</li>
</ol>

<p>Osgood の意味微分法で用いられた行列のサイズと比較すると，単語数が数万，項目数は数万から数十万に増加しています。
数の増加は網羅する範囲の拡大を意味します。
下図は持ちられたデータセット例を示したものです。</p>

<center>

<img src="../assets/1997Landauer_Dumais_FigA2.svg" style="width:84%" /><br />
From Landauer and Duman (1997) Fig. A2
</center>

<p>LSA (LSI) の問題点としては以下図を見てください</p>

<center>

<img src="../assets/1997Landauer_Fig3.svg" style="width:64%" /><br />
From Landauer and Dumas (1997) Fig.3 
</center>

<p>上図は，得られた結果を元に類義語テストを問いた場合に特異値分解で得られる次元数を横軸に，正解率を縦軸にプロットした図です。
次元を上げると成績の向上が認められます。
ですが，ある程度 300 以上の次元を抽出しても返って成績が低下することが示されています。</p>

<p>次元数を増やすことで本来の類義語検査に必要な知識以外の情報が含まれてしまうため推察されます。</p>

<!--- <a target="_blank" href="../assets/1997Landauer_Fig3.svg">Landauer (1997) Fig. 3</a>-->

<h2 id="33-2003-年-潜在ディレクリ配置-latent-direchlet-allocation-lda">3.3. 2003 年 潜在ディレクリ配置 Latent Direchlet Allocation (LDA)</h2>

<p>潜在ディレクリ配置 Latent Direchlet Allocation: LDA<sup id="fnref:LDA" role="doc-noteref"><a href="#fn:LDA" class="footnote" rel="footnote">1</a></sup> は LSA (LSI) を確率的に拡張したモデルであると考えることができます。すなわち LDA では単語と項目との関係に確率的な生成モデルを仮定します。</p>

<p>その理由としては，対象となる項目，しばしば <strong>トピック</strong> と言い表すと，項目の説明に用いられる単語との間に，決定論的な関係を仮定しないと考えることによります。確率的な関係を仮定することにより柔軟な関係をモデル化が可能であるからです。</p>

<p>例えば，ある概念，話題(トピック) “神経” を説明する場合を考えます。
“神経” を説明するには多様な表現や説明が可能です。
“神経” を説明する文章を数多く集めてると，単語 “脳” は高頻度で出現すると予想できます。
同様にして “細胞” や “脳” も高頻度で観察できるでしょう。ところが単語 “犬” は低頻度でしょう。
単語 “アメフラシ” や “イカ” は場合によりけりでしょう。どちらも神経生理学の発展に貢献した実験動物ですから単語 “アメフラシ” や “イカ” が出現する文章もあれば，単語 “脳梗塞” や単語 “失語” と同時に出現する確率もありえます。
このように考えると確率的に考えた方が良い場合があることが分かります。</p>

<h3 id="ディレクリ分布">ディレクリ分布</h3>

<p>もう一点，ノンパラメトリックモデルについて説明します。
parametric model はパラーメータを用いたモデルほどの意味です。
心理統計学の古典的な教科書では，ノンパラメトリック検定とは母集団分布のパラメータに依存 <strong>しない</strong> 統計的検定という意味で用いられます。一方 LDA の場合には推定すべき分布のパラメータ(の数)を <strong>事前に定めない</strong> という意味で <strong>ノンパラメトリック</strong> なモデルであると言います。
すなわちある話題(トピック)とそれを説明する単語の出現確率について，取り扱う現象の複雑さに応じてモデルを記述するパラメータ数を適応的に増やして行くことを考えます。</p>

<p>数学的既述は省略しますが，<a target="_blank" href="https://en.wikipedia.org/wiki/Beta_distribution">ベータ分布</a> を用いると区間 $[a,b]$ の間をとる分布でパラメータにより分布が柔軟に記述できます。ベータ分布の多次元拡張を <a target="_blank" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">ディククリ分布</a> と言います。</p>

<p>確率空間に対して一定の成約を付した表現をシンプレックスと言ったりします。<!--例えばコインの裏表は
2 値ですからベータ分布を用いても表す事ができます。-->
たとえばじゃんけんで対戦相手が，グー，チョキ，パー のいずれかを出す確率は，2 つが分かれば 3 つ目の手は自ずと分かってきます。このような関係は 3 つの手の確率分布でディククリ分布として扱うことが可能です。
下図はウィキペディアから持ってきました。この図はそのようなじゃんけんの手の出現確率をディレクリ分布として表現した例だと思ってください。</p>

<!--## ディククリ分布 (多次元ベルヌーイ分布)-->
<!-- 多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定-->

<center>

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png" style="width:64%" /><br />
<!--<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" style="width:49%"></br>-->
<p align="left" style="width:74%">
多次元ディレクリ分布(多次元ベータ分布)&lt;/a&gt; によるノンパラメトリック推定<br />
図は &lt;https://en.wikipedia.org/wiki/Dirichlet_distribution&gt; より
</p>
</center>

<p>トピック毎の単語の出現確率も上図と同じ枠組みで記述することが可能です。かつ，上図ではとりうる値が 3 つの場合ですが，話題が複雑になれば適応的に選択肢の数，すなわちディレクリ分布の次元数が増加することになります。</p>

<h3 id="プレート表記">プレート表記</h3>

<p>あらかじめ定められた数のパラメータを用いて分布を記述するのではなく，
解くべき問題の複雑さに応じて適応的にパラメータ数を定めることに対応して，
LDA あるいはトピックモデルの図示方法として <strong>プレート表記</strong> plate notation があります。
下図にプレート表記の例を示しました。</p>

<center>

<img src="../assets/2009Blei_Topic_Models_02.svg" style="width:74%" />&lt;/br&gt;
プレート表記: ノンパラメトリックモデルの表現に用いられる
</center>

<ul>
  <li>丸は確率変数</li>
  <li>矢印は確率的依存関係を表現</li>
  <li>観測変数は影付き(文献によっては二重丸)</li>
  <li>プレートは繰り返しを表す</li>
</ul>

<p>Y からパラメータ X が生成される場合，矢印を使ってその依存関係を表現します。ノンパラメトリックモデルの場合，矢印の数を予め定めません。そのため矢印を多数描くのが煩雑なので，一つの箱代用して表現します。
これがプレート表記になります。</p>

<p>観測可能な変数をグレー，または二重丸で表し，観測不能な，類推すべきパラメータを白丸で表記します。
実際には観測不可能な潜在パラメータを観測データから類推することになります。</p>

<p>大まかなルールとして，潜在変数をギリシャアルファベット表記，観測される変数はローマアルファベット表記の場合が多いですが，一般則ですので例外もあります。</p>

<p>下図に潜在ディレクリ配置  LDA のプレート表記を示しました。
<!--### 潜在ディレクリ配置のプレート表記--></p>

<center>

<img src="../assets/2009Blei_Topic_Models_03.svg" style="width:74%" />&lt;/br&gt;
<!--<img src="../assets/2009Blei_Topic_Models_04.svg" style="width:94%"></br>-->
</center>

<h3 id="トピックと単語の関係">トピックと単語の関係</h3>

<p>トピックモデルの要点をまとめた下図はこれまでの説明をすべて含んでいます。</p>

<center>

<img src="../assets/2009Blei_Topic_Models_01.svg" style="width:74%" />&lt;/br&gt;
<p align="left" style="width:74%">
 出典: ブライのスライド(2009)より，文章は話題(トピック)の混合&lt;/br&gt;
 各文章はその話題から文章が生成されたと考える
 </p>
</center>

<p>興味深い応用例として Mochihashi ら(2009) の示した教師なし学習による日本語分かち書き例を示します。
下図は源氏物語をトピックモデルにより分かち書きさせた例です。どこに空白を挿入すると文字間の隣接関係を表現できるかをトピックモデルで解くことを考えた場合，空白の挿入位置が確率的に定まると仮定して居ます。</p>

<center>

<img src="../assets/2009Mochihashi_Fig10.svg" style="width:84%" />&lt;/br&gt;
</center>

<p>Mochihashi らは，ルイス・キャロルの小説 “不思議の国のアリス” 原文から空白を取り除き，
文字間の隣接関係から文字の区切り，すなわち空白を推定することを試みました。結果を下図に示しました。</p>

<center>

<img src="../assets/2009Mochihashi_Fig12.svg" style="width:84%" />&lt;/br&gt;
</center>

<h3 id="原著論文">原著論文</h3>

<ul>
  <li><a target="_blank" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei, Jordan (2003) Latent Dirichlet Allocation 原著論文</a></li>
  <li>発展モデル: <a target="_blank" href="http://arxiv.org/abs/0710.0845">中華レストラン過程 CRP</a></li>
  <li><a target="_blank" href="http://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf">インド食堂過程 IBP</a> 
<!-- - パチンコ過程 --></li>
</ul>

<h3 id="r-による実装">R による実装</h3>

<ul>
  <li>https://cran.r-project.org/web/packages/lda/index.html</li>
  <li>https://cran.r-project.org/web/packages/topicmodels/index.html</li>
</ul>

<h2 id="34-2013-年-word2vec-単語埋め込み-ベクトル埋め込みモデル">3.4. 2013 年 word2vec, 単語埋め込み, ベクトル埋め込みモデル</h2>

<center>

<a href="../assets/Mikolov_portrait.jpg"> ミコロフ</a>
<a href="../assets/2015Mikolov_NIPSportrait.png"> おなじくミコロフ</a>
</center>

<ul>
  <li>ミコロフは <strong>word2vec</strong> によりニューラルネットワークによる意味実装を示しました。
ワードツーベックと発音します。
Word2vec は実装に 2 種類あリます。それぞれ <strong>CBOW</strong> と <strong>skip-gram</strong> と命名されています。
“シーボウ” または “シーバウ” と日本人は言ったりすることが多いようです。</li>
</ul>

<p>有名な “king” - “man” + “woman” = “queen” のアナロジーを解くことができると喧伝されました。</p>

<p>下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目してください。
ベクトルは方向と大きさを持っている矢印で表現されます。矢印の原点を移動する
ことを考えます。たとえば “MAN” から “WOMAN” へ向かう矢印を平行移動して “KING” まで
持ってくると，その矢印は “QUEEN” を重なることが予想できます。
これがアナロジー問題の解放の直感的説明になります。</p>

<center>

<img src="../assets/2013Mikolov_KingQueenFig.svg" style="width:94%" />
</center>

<p>上図右は同じ word2vec でできた空間に対して，統語関係 syntax を解かせた場合を示しています。
“KING” から “KINGS” へ向かう矢印を “QUEEN” まで持ってくると “QUEENS” に重なる
ことが見て取れます。</p>

<p>このことから上図右の赤矢印で示されたベクトルは <strong>複数形</strong> への変換という統語情報，
文法情報を表現しているとみなすことが可能です。</p>

<p>伝統的な言語学の知識では，統語構造と意味構造は別個に取り組む課題であると考えられてきました。
ところが word2vec が示す意味空間はそのような区別を考える必要があるのか否かについて
問題を提起しているように思われます。</p>

<p>逆に一つのモジュールで処理することができるのであれば，分割して扱う意味があるのかどうかを考える切っ掛けになると考えます。</p>

<p>もう一つ面白い結果を下図に示します。下図は word2vec によって世界の国とその首都との関係を主成分分析 PCA で 2 次元に描画した図です。</p>

<center>

<img src="../assets/2013Mikolov_FigCountries.svg" style="width:94%" />
</center>

<p>横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かって
おおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると
大まかに我々の知識を表現できることを示唆していると考えます。</p>

<p>word2vec の実装には 2 種類あります。どちらを使っても同じような結果を得ることができます。</p>

<ul>
  <li>CBOW: Continous Bog of Words 連続単語袋</li>
  <li>skip-gram: スキップグラム</li>
</ul>

<p>両者は反対の関係になります。下図を参照してください。</p>

<center>

<img src="../assets/2013Mikolov_Fig1.svg" style="width:94%" /><br />
From Mikolov (2013) Fig. 1
</center>

<p>CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を <strong>ベクトル埋め込みモデル</strong> あるいは <strong>単語埋め込みモデル</strong> と言ったりします。</p>

<ul>
  <li>CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。</li>
  <li>skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。</li>
</ul>

<p>たとえば，次の文章を考えます。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="s">"彼"</span><span class="p">,</span> <span class="s">"は"</span><span class="p">,</span> <span class="s">"意味論"</span><span class="p">,</span> <span class="s">"を"</span><span class="p">,</span> <span class="s">"論じ"</span><span class="p">,</span> <span class="s">"た"</span><span class="p">]</span>
</code></pre></div></div>

<p>表記を簡潔にするため各単語に ID をふることにします。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">"彼"</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">"は"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s">"意味論"</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s">"を"</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="s">"論じ"</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="s">"た"</span><span class="p">:</span><span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>

<p>すると上記例文は</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<p>と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>0,1,1,0,0,0] -&gt; <span class="o">[</span>1,0,0,0,0,0] <span class="c"># In:"は","意味論" Out:"彼"</span>
<span class="o">[</span>1,0,1,1,0,0] -&gt; <span class="o">[</span>0,1,0,0,0,0] <span class="c"># In:"彼","意味論","を" Out:"は"</span>
<span class="o">[</span>1,1,0,1,1,0] -&gt; <span class="o">[</span>0,0,1,0,0,0] <span class="c"># In:"彼","は","を","論じ" Out:"意味論"</span>
<span class="o">[</span>0,1,1,0,1,1] -&gt; <span class="o">[</span>0,0,0,1,0,0] <span class="c"># In:"は","意味論","論じ","た" Out:"を"</span>
<span class="o">[</span>0,0,1,1,0,1] -&gt; <span class="o">[</span>0,0,0,0,1,0] <span class="c"># In:"意味論","を","た" Out:"論じ"</span>
<span class="o">[</span>0,0,0,1,1,0] -&gt; <span class="o">[</span>0,0,0,0,0,1] <span class="c"># In:"を","論じ" 出力:"た"</span>
</code></pre></div></div>

<p>を学習することとなります。</p>

<ul>
  <li>CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決めます。</li>
  <li>Mikolov が類推に用いたデータ例を下図に示しました。国名と対応する首都名，国名とその通貨名，などは意味的関係です。一方罫線下方は文法関係です。
形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などです。</li>
</ul>

<center>

<img src="../assets/2013Mikolov_Tab1.svg" style="width:94%" /><br />
From Milolov (2013) Tab. 1
</center>

<ul>
  <li>しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。</li>
  <li>それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。</li>
  <li>ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。</li>
  <li>word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。</li>
</ul>

<!--
<img src="../assets/2013Mikolov_skip-gram_cbow.svg" style="width:74%">
<img src="../assets/skip-gram.svg" style="width:74%">
<img src="../assets/skip-gram_cbow.svg" style="width:74%">
</center>
-->

<p>日本語のウィキペディアを用いた word2vec と NTT 日本語の語彙特性との関連に関心のある方は
<a target="_blank" href="../2017jpa_word2vec_NTTdict.pdf">日本語 Wikipedia の word2vec 表現と語彙特性との関係, 近藤・浅川 (2017) </a> をご覧ください</p>

<h2 id="さらなる蘊蓄-負例サンプリング">さらなる蘊蓄 負例サンプリング</h2>

<p>Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティーとして与えることで精度が向上します。</p>

<h2 id="発展-文章埋め込みモデルへ">発展 文章埋め込みモデルへ</h2>

<p>単語の word2vec による表現は 3 層パーセプトロンの中間層の活性値として表現されます。</p>

<p>単語より大きなまとまりの意味表現，たとえば，文，段落，などの表現をどのように得るのかが問題になります。
ここで詳細には触れませんが，文表現ベクトルは各単語表現の総和であると考えるのがもっとも簡単な表現になります。
すなわち次文:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="s">"彼"</span><span class="p">,</span> <span class="s">"は"</span><span class="p">,</span> <span class="s">"意味論"</span><span class="p">,</span> <span class="s">"を"</span><span class="p">,</span> <span class="s">"論じ"</span><span class="p">,</span> <span class="s">"た"</span><span class="p">]</span>
</code></pre></div></div>

<p>の文表現を得るためには，各単語の word2vec 表現を足し合わせることが行われます。
ただし，単純に足し合わせたのでは BOW 単語袋表現と同じことですので，単語の順序情報が失われていることになります。
この辺りをどう改善すれば良いのかが議論されてきました。</p>

<h3 id="文献">文献</h3>

<ul>
  <li><a target="_blank" href="https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf">word2vec オリジナル論文</a> 2013年 Mikolov</li>
  <li><a target="_blank" href="https://fasttext.cc/">fastText</a> 高速文埋め込みモデル</li>
  <li>その発展 <a target="_blank" href="../2018jsai.pdf">浅川, 岡, 楠見 (2018)</a>
<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)--></li>
</ul>

<!--


<center>
<img src="https://www.tensorflow.org/images/linear-relationships.png" style="width:84%"><br>
<p algin="left" style="width:74%">
Source: <https://www.tensorflow.org/tutorials/representation/word2vec>
</p>
</center>

- <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論の蘊蓄</a>

-->

<!--<img src="https://upload.wikimedia.org/wikipedia/en/7/74/Elmo_from_Sesame_Street.gif" style="width:39%">-->

<!--
<img src="https://www.specialdaysofthemonth.com/wp-content/uploads/2018/01/elmo.jpg" style="width:39%">
-->
<!--](https://upload.wikimedia.org/wikipedia/en/7/74/Elmo_from_Sesame_Street.gif)-->
<!--
<img src="https://vignette.wikia.nocookie.net/muppet/images/e/e1/Bert_smile.png" style="width:29%"><br>
<p align="left" style="width:84%">
左: <https://www.specialdaysofthemonth.com/wp-content/uploads/2018/01/elmo.jpg>, 
右: <https://vignette.wikia.nocookie.net/muppet/images/e/e1/Bert_smile.png>
</center>
-->

<!--
<center>

|Task|Previous SOTA|Our Baseline|ELMo + Baseline|Increase (Absolute/Relative)|
|:---:|:---:|:---:|:---:|:---:|
|SQuAD |Liu et al. (2017) $84.4$            | $81.1$  | $85.8$         | $4.70/24.9\%$ |
|SNLI  |Chen et al. (2017) $88.6$           | $88.0$  | $88.7\pm0.17$  | $0.70/05.8\%$ |
|SRL   |He et al. (2017) $81.7$             | $81.4$  | $84.6$         | $3.20/17.2\%$ |
|Coref |Lee et al. (2017) $67.2$            | $67.2$  | $70.4$         | $3.20/09.8\%$  |
|NER   |Peters et al. (2017) $91.93\pm0.19$ | $90.15$ | $92.22\pm0.10$ | $2.06/21.0\%$  |
|SST-5 |McCann et al. (2017) $53.7$         | $51.4$  | $54.7\pm0.5$   | $3.30/06.8\%$  |

</center>

Table 1: Test set comparison of **ELMo** enhanced neural models with
state-of-the-art single model baselines across six benchmark NLP tasks. The
performance metric varies across tasks – accuracy for _SNLI_ and _SST-5_;
F1 for _SQuAD_, _SRL_ and _NER_; average **F1** for Coref. Due to the small
test sizes for __NER__ and __SST-5__, we report the mean and standard
deviation across five runs with different random seeds. The ``increase``
column lists both the absolute and relative improvements over our baseline.

- **Question answering.** The Stanford Question Answering Dataset
(**SQuAD**) (Rajpurkar et al., 2016) contains 100K+ crowd sourced
questionanswer pairs where the answer is a span in a given Wikipedia
paragraph. Our baseline model (Clark and Gardner, 2017) is an improved
version of the Bidirectional Attention Flow model in Seo et al.  (BiDAF;2017)
- **Textual entailment.** Textual entailment is the task of determining
whether a “__hypothesis__” is true, given a “__premise__”.  The
Stanford Natural Language Inference (**SNLI**) corpus (Bowman et al., 2015)
provides approximately 550K hypothesis/premise pairs.  Our baseline, the
ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the
premise and hypothesis, followed by a matrix attention layer, a local
inference layer, another biLSTM inference composition layer, and finally a
pooling operation before the output layer.
- **Semantic role labeling.** A semantic role labeling (**SRL**) system
models the predicate-argument structure of a sentence, and is often
described as answering “Who did what to whom".
- **Coreference resolution.** Coreference resolution is the task of
clustering mentions in text that refer to the same underlying real world
entities.  Our baseline model is the end-to-end span-based neural model of
Lee et al.(2017).
- **Named entity extraction.** The CoNLL 2003 NER task (Sang and
Meulder,2003) consists of newswire from the Reuters RCV1 corpus tagged with
four different entity types (PER, LOC, ORG, MISC). Following recent
state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the
baseline model uses pre-trained word embeddings, a character-based CNN
representation, two biLSTM layers and a conditional random field (CRF) loss
(Lafferty et al., 2001), similar to Collobert et al.  (2011).
- **Sentiment analysis.** The fine-grained sentiment classification task in
the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves
selecting one of five labels (from very negative to very positive) to
describe a sentence from a movie review.

<center>

![BERT Fig1](./assets/2018Devlin_BERT_Fig1.svg)
</center>

---

# BERT
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v1/)

<center>

<img src="../assets/2018Devlin_BERT_Fig1.svg" style="widht:94%"></br>
</center>

# GLUE: General Language Understanding Evaluation 

- **MNLI**: Multi-Genre Natural Language Inference is a large-scale,
crowdsourced entailment classification task (Williams et al., 2018). Given
a pair of sentences, the goal is to predict whether the second sentence is
an entailment, contradiction, or neutral with respect to the first one.
- **QQP**: Quora Question Pairs is a binary classification task where the
goal is to determine if two questions asked on Quora are semantically
equivalent (Chen et al., 2018).
- **QNLI**: Question Natural Language Inference is a version of the
Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been
converted to a binary classification task (Wang et al., 2018). The positive
examples are (question, sentence) pairs which do contain the correct
answer, and the negative examples are (question, sentence) from the same
paragraph which do not contain the answer.
- **SST-2**: The Stanford Sentiment Treebank is a binary single-sentence
classification task consisting of sentences extracted from movie reviews
with human annotations of their sentiment (Socher et al., 2013).
- **CoLA**: The Corpus of Linguistic Acceptability is a binary
single-sentence classification task, where the goal is to predict whether
an English sentence is linguistically “acceptable” or not (Warstadt et
al., 2018).
- **STS-B**: The Semantic Textual Similarity Benchmark is a collection of
sentence pairs drawn from news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1 to 5 denoting how similar
the two sentences are in terms of semantic meaning.
- **MRPC**: Microsoft Research Paraphrase Corpus consists of sentence pairs
automatically extracted from online news sources, with human annotations
for whether the sentences in the pair are semantically equivalent (Dolan
and Brockett, 2005).
- **RTE**: Recognizing Textual Entailment is a binary entailment task
similar to MNLI, but with much less training data (Bentivogli et al.,
2009).
- **WNLI**: Winograd NLI is a small natural language inference dataset
deriving from (Levesque et al., 2011). The GLUE webpage notes that there
are issues with the construction of this dataset, 7 and every trained
system that’s been submitted to GLUE has has performed worse than the 65.1
baseline accuracy of predicting the majority class.  We therefore exclude
this set out of fairness to OpenAI GPT. For our GLUE submission, we always
predicted the majority class

- **SQuAD v1.1**: The Standford Question Answering Dataset (SQuAD) is a
collection of 100k crowdsourced question/answer pairs (Rajpurkar et al.,
2016).  Given a question and a paragraph from Wikipedia

- **NER**: CoNLL 2003 Named Entity Recognition (NER) dataset.  This dataset
consists of 200k training words which have been annotated as __Person__,
__Organization__, __Location__, __Miscellaneous__, or __Other__ (non-named
entity).

- **SWAG**: The Situations With Adversarial Generations (SWAG) dataset
contains 113k sentence-pair completion examples that evaluate grounded
commonsense inference (Zellers et al., 2018).  Given a sentence from a
video captioning dataset, the task is to decide among four choices the most
plausible continuation.

<center>

<img src="../assets/2018Devlin_BERT_Tab2.svg" style="width:74%"></br>
2018Devlin_BERT_Tab. 2

<img src="../assets/2018Devlin_BERT_Tab4.svg" style="width:74%"></br>
2018Devlin_BERT_Tab. 2
</center>



- **Question answering.** The Stanford Question Answering Dataset
(**SQuAD**) (Rajpurkar et al., 2016) contains 100K+ crowd sourced
questionanswer pairs where the answer is a span in a given Wikipedia
paragraph. Our baseline model (Clark and Gardner, 2017) is an improved
version of the Bidirectional Attention Flow model in Seo et al.  (BiDAF;2017)
- **Textual entailment.** Textual entailment is the task of determining
whether a “__hypothesis__” is true, given a “__premise__”.  The
Stanford Natural Language Inference (**SNLI**) corpus (Bowman et al., 2015)
provides approximately 550K hypothesis/premise pairs.  Our baseline, the
ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the
premise and hypothesis, followed by a matrix attention layer, a local
inference layer, another biLSTM inference composition layer, and finally a
pooling operation before the output layer.
- **Semantic role labeling.** A semantic role labeling (**SRL**) system
models the predicate-argument structure of a sentence, and is often
described as answering “Who did what to whom".
- **Coreference resolution.** Coreference resolution is the task of
clustering mentions in text that refer to the same underlying real world
entities.  Our baseline model is the end-to-end span-based neural model of
Lee et al.(2017).
- **Named entity extraction.** The CoNLL 2003 NER task (Sang and
Meulder,2003) consists of newswire from the Reuters RCV1 corpus tagged with
four different entity types (PER, LOC, ORG, MISC). Following recent
state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the
baseline model uses pre-trained word embeddings, a character-based CNN
representation, two biLSTM layers and a conditional random field (CRF) loss
(Lafferty et al., 2001), similar to Collobert et al.  (2011).
- **Sentiment analysis.** The fine-grained sentiment classification task in
the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves
selecting one of five labels (from very negative to very positive) to
describe a sentence from a movie review.
-->

<h3 id="seq2sep-翻訳モデル">Seq2sep 翻訳モデル</h3>

<p>中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば <strong>機械翻訳</strong> や <strong>対話</strong> のモデルになる。
初期の翻訳モデルである “seq2seq” の概念図を示した。
“eos” は文末 end of sentence を表す。
中央の “eos” の前がソース言語であり，中央の “eos” の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。</p>

<p>注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，
それ以外の時刻ではソース言語とターゲット言語は関係がない。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
この点を改善することを目指すことが 2014 年以降盛んになった。
顕著な例が後述する <strong>双方向 RNN</strong>, <strong>LSTM</strong> を採用したり，<strong>注意</strong> 機構を導入することであった。</p>

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_S22_Fig1.svg" style="width:88%" /><br />
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 1
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2left.svg" style="width:88%" /><br />
<img src="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2right.svg" style="width:88%" /><br />
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 2
</center>

<!-- 
# 自然言語系の注意

<center>

![](assets/2015Bahdanau_attention.jpg){style="width:30%"}
![](assets/2015Luong_Fig2.svg){style="width:33%"}
![](assets/2015Luong_Fig3.svg){style="width:33%"}><br />
左: [@2014Bahdanau_NMT], 中: [@2015Luong_attention] Fig. 2, 
右: [@2015Luong_attention] Fig. 3
</center>
-->

<!-- <center style="width:74% align:center">

![](assets/1957Osgood_fig19a.jpg){style="width:30%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19b.jpg){style="width:28%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19c.jpg){style="width:23%"}<br />
<div align="left">
Osgood (1957) 図. 19 より。ある女性患者（統合失調症）の心的イメージ。左が治療前，中央が治療中，右が治療後期
</div>
</center>
-->

<!--
# 多頭=自己注意 Multi-Head Self-Attention: MHSA

- 自然言語処理 NLP **Transformer**[@2017Vaswani_transformer]; **BERT**[@2018BERT]; **RoBERTa**[@2019RoBERTa]; **distilBERT** [@2020Sanh_distilBERT]; and more...
- 画像処理 [@2019Ramachandran_attention_vision]; **A2-Net** [@2018Chen_A2-nets_double_attention]; **U-GAT-IT** [@2019Kim_U-GAT-IT]
- 強化学習，メタ学習 **SNAIL** [@2018Mishra_SNAIL]
- 敵対生成ネットワーク **SAGAN** [@2019Zhang_Goodfellow_SAGAN]

-->
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:LDA" role="doc-endnote">
      <p>伝統的な統計学においては Fischer の線形判別分析を LDA と表記します。ですがデータサイエンス，すなわち統計学の一分野では近年の潜在ディレクリ配置の成功により LDA と未定義で表記された場合には潜在ディクレクリ配置を指すことが多くなっています。 <a href="#fnref:LDA" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>



  </div>

      </div>
    </main><div class="footer">
  <div class="wrap">
<!--
Thanks to <a href="http://www.imdb.com/">IMDB</a> for all the serie informations!
-->
駒澤大学
  </div>
</div>
<!---
<script src="https://datocms-middleman-example.netlify.com/javascripts/all.js"></script>
-->

</body>

</html>
