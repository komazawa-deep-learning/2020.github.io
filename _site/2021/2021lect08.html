<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/assets/css/swiss.css"></head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    CommonHTML: { matchFontHeight: false },
    displayAlign: "left",
    displayIndent: "2em",
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
    }
  });
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS_CHTML"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
  <body>
<div class="header">
  <div class="wrap">
    
      <div class="header__inner header__inner--internal">
    
      <div class="header__content">
        <h1 class="header__title">
          
        </h1>
        <p class="header__tagline">
          
        </p>
      </div>
    </div>
  </div>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">第08回</h1><div align="right">
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br />
Date: 04/Jun/2021<br />
Appache 2.0 license<br />
</div>

<ul>
  <li>
    <p>重要な連絡: 来週 6 月 11 日は休講</p>
  </li>
  <li>
    <p><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0604Data_Augmentation.ipynb">実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></p>
  </li>
</ul>

<h1 id="cnn-の特徴">CNN の特徴</h1>

<ul>
  <li>次の 7 つを上げることができます
<!--[@2017Asakawa_deep_jps][^2017Aakawa\_deep\_jps\]-->。</li>
</ul>

<ol>
  <li>非線形活性化関数 (nonlinear activation functions)</li>
  <li>畳込み演算 (convolutional operation)</li>
  <li>プーリング処理 (pooling)</li>
  <li>データ拡張 (data augmentation)</li>
  <li>バッチ正規化 (batch normalization)</li>
  <li>ショートカット(shortcut)</li>
  <li>GPU の使用</li>
</ol>

<!-- 上記 7 つの特徴を説明するのは専門的になりすぎるので省略します。
一つだけ説明するとすれば最後のGPU とは高解像度でしかも処理速度を必要とするパソコンゲームで用いられるグラフィックボードのことです。
詳細な画像を高速に画面に表示する必要から開発されたグラフィックボードですが，大規模なニュールネットワークの計算でも用いられる数学が同じです。
そのため，ゲーム用に開発されたグラフィックボードがニューラルネットワークにも用いられるようになりました。
-->

<h2 id="ディープラーニングの特徴">ディープラーニングの特徴</h2>

<p>from Democratize AI slides</p>

<ul>
  <li>データハングリー data hungry</li>
  <li>計算資源ハングリー resource hungry</li>
  <li>理論欠如 theory lagging</li>
  <li>
    <p>不透明 opacity</p>
  </li>
  <li>ニューラルネットワークは素人の統計学である, Anderson et. al (1993)</li>
</ul>

<p>… But Neural networks are not alchemy.</p>

<h1 id="生理学">生理学</h1>

<center>
<img src="/assets/1994vanEssen_gallant_Fig2.jpg" style="width:74%" />&lt;/br&gt;
**Van Essen &amp; Gallant (1994)**

<img src="/assets/thorpe_sj_01_260.jpg" style="width:49%" />
<img src="/assets/1991Felleman_VanEssen_Fig4.svg" style="width:49%" />&lt;/br&gt;
**Left: Thorpe et al.(2001), Right: Felleman &amp; Van Essen (1992)**
</center>

<div align="center" style="width:88%">
  <img src="https://upload.wikimedia.org/wikipedia/commons/d/d2/Retinotopic_English.jpg" style="width:74%" /><br />
  <div align="left">
    source: https://en.wikipedia.org/wiki/Retinotopy
  </div>
</div>

<h2 id="ヒューベルとウィーゼル-hubel-and-wiesel-1969">ヒューベルとウィーゼル Hubel and Wiesel (1969)</h2>
<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<br /> source: https://youtu.be/4nwpU7GFYe8
</center>

<ul>
  <li>トポグラフックマッピング topographic mappings (地形図):  網膜や皮膚などの体制感覚，筋肉系のような効果器系を、中枢神経系の 1 つ以上の構造物に整然と投影した地図。
地形図は、すべての感覚系と多くの運動系で観察される。</li>
  <li>トノトピー tonotopy（ギリシャ語のtono=周波数、topos=場所）とは、異なる周波数の音が脳内で処理される場所の空間的配置のこと。
周波数が近い音は、脳内の場所的に隣接する領域で表現される。トノトピック地図とも呼ばれる <!--は 視覚系のレチノトピーに似た地形的な組織の特殊な例である。--></li>
  <li>ソマトピー somatopy: 中枢神経系上の特定の点へ身体領域の照射，投影のこと。 身体領域は 第一次体性感覚皮質 (後腹回) に投影される。 典型的には 特定の身体部位と そのそれぞれの位置を ホムンクルス homunclus (小人) 
 上に配置する 感覚ホムンクルスとして表現される。
 細かく制御されている部位 (指，舌) は体性感覚野の面積が大きい。一方 粗く制御されている部位 (体幹) は面積が小さい。内臓のような領域は体性感覚野の位置を持たない。</li>
  <li>レティノトピー retinotopy: 網膜からの視覚入力を神経細胞 にマッピングすること。哺乳類の脳に多く見られる。
この地図の大きさ、数、空間的配置は種間で異なる。
視野の隣接する点 が 同じ領域 の 隣接する 領域で 表現される とは限らないという意味で、複雑な地図となる。
例えば 第 2 次視覚野 (V2) での視覚地図は 視野の上半分に応答する網膜の部分が 視野の下半分に応答する部分から分離されて表現されている。
第３次視覚野 (V3) や 第4次視覚野 (V4) では下位の視覚野に比べて より複雑な表現がなされている</li>
</ul>

<hr />

<h2 id="ブレイクモア-と-クーパー-blackmore-and-cooper-1970">ブレイクモア と クーパー Blackmore and Cooper (1970)</h2>

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accele
rometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<!--<iframe width="845" height="676" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="ac
celerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
&lt;/br&gt; source: https://youtu.be/QzkMo45pcUo
</center>

<center>
<img src="../assets/1970BlackmoreCooper_Fig1.svg" style="width:39%" />
<img src="../assets/1970BlackmoreCooper_Fig2.svg" style="width:49%" />
</center>

<hr />

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<br />source: https://youtu.be/RSNofraG8ZE
</center>

<h1 id="心理学">心理学</h1>
<h2 id="セルフリッジ-selfridge-のパンデモニウム-pandemonium-モデル">セルフリッジ Selfridge のパンデモニウム pandemonium モデル</h2>

<center>
<img src="../assets/1958Selfridge_fig3.svg" style="width:74%" /><br />
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より

<img src="../assets/1958Selfridge_fig7.svg" style="width:84%" /><br />
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center>

<!-- <center>
<img src='../assets/1958Selfridge_fig4.svg' style='width:49%'><br>
<img src='../assets/1958Selfridge_fig5.svg' style='width:74%'><br>
<img src='../assets/1958Selfridge_fig6.svg' style='width:49%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center>
 -->

<h1 id="生理学視覚心理学との対応">生理学，視覚心理学との対応</h1>

<ul>
  <li>Julesz(1981) Textons, the elements of texture perception, and their interactions, Nature</li>
</ul>

<center>
<img src="../assets/1981Julesz-texton-Fig2.svg" style="width:84%" />&lt;/br&gt;
Julesz (1981) Fig. 2 より
</center>

<h2 id="生理学との対応-hubel-and-wiesel-のネコとサル-blackmore-のネコ-ヴァンエッセン">生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン)</h2>
<ul>
  <li>層間の結合の仕方, アーキテクチャ</li>
  <li>forward/backward 役割，機能，実現方法</li>
  <li>側抑制 lateral inhibition (これについては多層化して回避できる可能性あり)</li>
  <li>
    <p>shape from X は正しかったのか？ ただし発達心理学におけるシェイプバアスは言語発達において重要な意味を持つはず
。だからと言って乳幼児はそのように強制(脅迫？)，矯正されて育つわけではないだろう。</p>

    <ul>
      <li>Ritter (2017) Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study</li>
      <li>Landau, Smith, Jones (1992) Syntactic Context and the Shape Bias in Childrens and Adults Lexical Learning</li>
      <li>
        <p>Yamins (2016) Using goal-driven deep learning models to understand sensory cortex</p>
      </li>
      <li>Julez のアプローチは視覚研究者 Haar, SIFT, DoG などのアルゴリズム開発者と対応</li>
      <li>Poggio (1985) Computational Vision and Regularization Theory</li>
    </ul>
  </li>
</ul>

<h2 id="現代的ニューラルネットワークモデルと生理学との対応">現代的ニューラルネットワークモデルと生理学との対応</h2>

<div align="center">
<img src="/assets/2012Zeiler_Deconvolution.png" style="width:77%" />&lt;/br&gt;
<br />Zeiller 2012 より
</div>

<div align="center">
<img src="/assets/2010ZeilerKrishnanTaylorFerguss_fig.svg" style="width:88%" /><br />
Zeiler et. al. (2010) Fig. 7, and 8
</div>

<center>
<img src="/assets/2016Yamins_fig1s_ja.svg" style="width:94%" />
<!--<img src='../assets/2016Yamins_Fig1.svg' style='width:94%'>-->
<img src="/assets/2016Yamins_fig2ja.svg" style="width:94%" />
</center>

<!-- <center>
<img src='/assets/2016Yamins_fig3s_ja.svg' style='width:84%'><br/>
<div align="left" style="width:84%">
目標駆動型モデリングの構成要素。
内側の円は、与えられた数のレイヤーのモデルを含む完全なモデルクラスの部分空間を表す。
目標駆動モデルは、特に最適なモデルを発見するために、モデル・クラス内の軌道（実線）に沿ってシステムを駆動する学習アルゴリズム（黒の点線矢印）を使用して構築される。
各ゴールは、そのゴールを解くのに特に適したパラメータを含むモデルクラス（太い黒の等高線）内の引き寄せの盆地に対応していると考えることができる。
計算結果は、課題がモデルのパラメータ設定に強い制約を与えることを示す。
すなわち、与えられた課題の最適化パラメータのセットは、元の空間に比べて非常に小さいということである。
これらの目標駆動モデルは、与えられた課題領域での行動の根底にあると考えられている脳領域のニューロンの応答特性をどの程度予測できるかについて評価することができる。
例えば、単語認識のために最適化されたモデルのユニット、聴覚皮質の一次領域、ベルト領域、パラベルト領域の応答特性と比較することができる。
また、モデルを互いに比較して、異なるタイプの課題がどの程度の神経構造の共有につながるかを判断することもできる。
また、様々な構成要素のルール（教師あり、教師なし、半教師あり）を研究して、それらが生後の発達や専門知識学習の間にどのように異なるダイナミクスにつながるかを判断することもできる（緑の破線のパス）。
</div>
</center>
-->

<div align="center">
<!--<img src='../assets/2014Yamins_fig2Aja.svg' style='width:74%'>-->
<!--<img src='../assets/2014Yamins_FigS2.svg' style='width:74%'>-->
<img src="/assets/2020Schrimpf_fig1.svg" style="width:84%" /><br />
</div>

<!--<a href="/assets/2018asakawa_jdlaGtestChapt7.pdf">G 検定公式テキスト7章より</a>-->

<h1 id="3-畳込みニューラルネットcnn">3. 畳込みニューラルネット(CNN)</h1>

<p>深層学習 (ディープラーニング) の中で <strong>畳み込みニューラルネットワーク</strong> CNN と呼ばれるニューラルネットワークについて解説します。</p>

<p>最初に画像処理の概略を述べる CNN が，それまで主流であった従来の手法の性能を凌駕したことはすでに述べました。
CNN の特徴の一つに <strong>エンドツーエンド</strong> と呼ばれる考え方があります。
エンドツーエンドとは，従来手法によるパターン認識システムでは，専門家による手の込んだ詳細な作り込みを必要としていたことと異なり，面倒な作り込みをせずとも性能が向上したことを指します。</p>

<p>エンドツーエンドなニューラルネットワークにより，次のことが実現しました。</p>

<ul>
  <li>ニューラルネットワークの層ごとに，特徴抽出が行われ，抽出された特徴がより高次の層へと伝達される</li>
  <li>ニューラルネットワークの各層では，比較的単純な特徴から次第に複雑な特徴へと段階的に変化する</li>
  <li>高次層にみられる特徴は低次層の特徴より大域的，普遍的である</li>
  <li>高次層のニューロンは，低次層で抽出された特徴を共有している</li>
</ul>

<p>このことを簡単に説明してみます。</p>

<p>我々人間は，外界を認識するために必要な計算を，生物種としての発生の過程と，個人の発達を通しての経験に基づく認識システムを保持していると見ることができます。
従って我々の視覚認識には化石時代に始まる光の受容器としての眼の進化の歴史と発達を通じた個人の視覚経験が反映された結果でもあります。
人工知能の目標は，この複雑な特徴検出過程をどうやったらコンピュータが獲得できるかということでもあります。
外界を認識するために今日まで考案されてきたモデル（例えば，ニューラルネットワークサポートベクターマシンなどは）は複雑です。
ですがモデルを訓練するための学習方法はそれほど難しくありません。
この意味で画像認識課題が正しく動作するためのポイントは，認識システムが問題を解く事が可能なほど複雑であるかどうかではなく，十分に複雑が視覚環境，すなわち画像認識の場合，外部の艦橋を反映するために十分な量の像データを容易す
ことができるか否かにあります。
今日の CNN による画像認識性能の向上は，簡単な計算方法を用いて複雑な外部環境に適応できる認識システムを構築する方法が確立したからであると言うことが可能です。</p>

<p>下図 <!--[fig:2012Ng_01](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"} -->
に画像処理の例を挙げました。</p>

<!--
<center>
<img src="../assets/2012Ng_ML_and_AI_01.png" style="width:94%">
</center>
-->

<!--図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"}
-->
<!--
<center>
<img src='../assets/2013LeCun-tutorial-icml_15.svg' style="width:84%"></br>
**LeCun (2013) より**
</center>
-->

<center>
<img src="../assets/2012Ng_ML_and_AI.svg" style="width:84%" />&lt;/br&gt;
**Goodfellow et al. (2017) Fig.1 を改変**
</center>

<!--では入力画像がネコであるか否かを判断する画像認識であるとしました。-->
<p>我々は<!--ネコの--> 画像をほぼ瞬時に判断できます。
ですが画像認識の難しさは，入力画像が上図 <!--[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"}-->
に示されているように入力信号の数字の集まりでしか無いことです。
このようなデータを何度も経験することでネコを識別できるようにする必要があります。
<!-- [[fig:2012Ng_01]]{#fig:2012Ng_01 label="fig:2012Ng_01"}-->
<!--図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref" reference="fig:2012Ng_01"}-->
<!--に示したように-->コンピュータに入力される画像は数字の塊に過ぎません。</p>

<p>状況ごとにとるべき操作を命令として逐一コンピュータに与える指示する手順の集まりのことをコンピュータプログラムと呼びます。
人間がコンピュータに与えることができる操作や命令によって画像認識システムを作る場合，命令そのものが膨大になったり，そもそも説明することが難しかったりします。
例を挙げれば，お母さんを思い浮かべてくださいと言われれば誰でも，それぞれ異なるイメージであれ思い浮かべることができます。
また，提示された画像が自分の母親のものであるか，別の女性であるかの判断は人間であれば簡単です。
ところがコンピュータには難しい課題となります。
加えて母親の特徴をコンピュータに理解できる命令としてプログラムすることも難しい課題です。
つまり自分の母親の特徴を曖昧な言葉でなく明確に説明するとなるととても難しい課題となります。
というのは，女性の顔写真であればどの写真も似ていると言えるからです。
顔の造形や輪郭，髪の位置などはどの画像も類似していることでしょう。
ところがコンピュータにはこの似ている，似ていいないの区別が難しいのです。</p>

<p>加えて，同一ネコの画像であっても，被写体の向き視線の方向や光源の位置や撮影条件が異なれば画像としては異なります。
下図に示したように入力画像の中の特定の値だけを調べてみても，入力画像がネコである，そうではないかを判断することは難しい課題になります。</p>

<!--[fig:2012Ng_02](#fig:2012Ng_02){reference-type="ref"
reference="fig:2012Ng_02"}
-->
<center>
<img src="/assets/2012Ng_ML_and_AI_02.png" style="width:94%" />
</center>

<!--[fig:2012Ng_02]{#fig:2012Ng_02 label="fig:2012Ng_02"}-->

<p>現在の画像認識では，特定の画素の情報に依存せずに，入力画像が持っている特徴をとらえるように設計されます。
たとえば，ネコを認識するために必要ことは，ネコに特徴的な「ネコ目」や「ネコ足」を検出することであると考えます。
入力画像から，ネコの持つ特徴を抽出することができれば，それらの特徴を持っている入力画像はネコであると判断して良いことになります(下図
<!--[[fig:2012Ng_03]](#fig:2012Ng_03){reference-type="ref"reference="fig:2012Ng_03"}-->)。</p>

<center>
<img src="/assets/2012Ng_ML_and_AI_03.png" style="width:94%" />
</center>

<!-- [[fig:2012Ng_03]]{#fig:2012Ng_03 label="fig:2012Ng_03"}-->

<p>下図
<!--[[fig:2013LeCun_9]](#fig:2013LeCun_9){reference-type="ref"
reference="fig:2013LeCun_9"} -->
は，音声認識と画像認識の両分野において CNN が用いられる以前の従来手法 をまとめたものです。</p>

<center>
<img src="/assets/2013LeCun-tutorial-icml_9.png" style="width:94%" />
</center>
<!--[[fig:2013LeCun_9]]{#fig:2013LeCun_9 label="fig:2013LeCun_9"}-->
<p>上図<!--[[fig:2013LeCun_9]](#fig:2013LeCun_9){reference-type="ref"
reference="fig:2013LeCun_9"} -->
のような従来手法に対して，CNN
ではエンドツーエンドな特徴抽出を多層多段に重ねることによって複雑な特徴を抽出(検出)しようとしています
(下図<!-- [[fig:2013LeCun_10]](#fig:2013LeCun_10){reference-type="ref"
reference="fig:2013LeCun_10"}-->)。</p>

<center>
<img src="../assets/2013LeCun-tutorial-icml_10.png" style="width:94%" />
</center>
<!--
[fig:2013LeCun_10]{#fig:2013LeCun_10 label="fig:2013LeCun_10"}
-->

<p>コンピュータにはネコ目特徴検出器，ネコ足特徴検出器は備わっていません。そこで画像認識研究では，画像の統計的性質に基づいて特徴検出器を算出する方法を探す努力が行われてきました。
しかし，コンピュータにネコ目特徴やネコ足特徴を教えるは容易なことではありません。
このことは画像処理の分野だけに限りません，音声認識でも言語情報処理でもそれぞれの特徴器を一つ一つ定義し，チューニングするのは時間がかかり，専門的な知識も必要で困難な作業でした。</p>

<!--
<center>
<img src="../assets/2012Ng_ML_and_AI_05.png" style="width:94%">
</center>
-->
<!--[fig:2012Ng_05]{#fig:2012Ng_05 label="fig:2012Ng_05"}-->

<!--
<img src="../assets/2012Ng_ML_and_AI_04.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_06.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_07.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_08.png" style="width:94%">
-->

<h2 id="alexnet-krizensky-et-al-2012">AlexNet (Krizensky, et al., 2012)</h2>

<center>
<img src="/assets/2012AlexNet.svg" style="width:94%" /><br />
Krzensky et al (2012) より
</center>

<h2 id="goolenet-inception-szegedy-et-al-2014">GooLeNet (Inception) (Szegedy et. al, 2014)</h2>

<center>
<img src="/assets/2014Szegedy_GoogLeNet.svg" style="width:99%" /><br />
</center>

<center>
<img src="/assets/2013Uijings_Selective_Search_Fig1.svg" style="width:94%" /><br />
空間ピラミッド (2015) より
</center>

<h1 id="アレックスネット以降の流れ">アレックスネット以降の流れ</h1>

<ul>
  <li>2013年 ZF ネット</li>
  <li>2014年 GoogLeNet (インセプション)，VGG，Very Deep,  GAN, VAE</li>
  <li>2015年 残渣ネット (ResNet) この時点で，人間超え</li>
  <li>2016年 YOLO, SSD, Segmentation (Semantics/Instance)</li>
  <li>2017年 Mask R-CNN</li>
  <li>2018年 モバイルネット</li>
</ul>

<h1 id="高精度化高速化への努力">高精度化，高速化への努力</h1>

<ul>
  <li>確率的勾配降下法，オンライン学習，バッチ学習</li>
  <li>データ拡張</li>
  <li>正則化</li>
  <li>ドロップアウト</li>
  <li>非線形活性化関数</li>
  <li>最適化手法</li>
</ul>

<h1 id="キーワード">キーワード</h1>

<ul>
  <li>dropout</li>
  <li>the meaning of convolutions with width 1</li>
  <li>data augmentation</li>
  <li>batch normalization, layer normalizatoin, instance normalization</li>
  <li>skip connection</li>
  <li>R-CNN, semantic segmentation, object segmentation, panoptic segmentation</li>
  <li>Fully convolutional network</li>
</ul>

<h2 id="領域切り出し">領域切り出し</h2>

<center>
    <img src="/assets/Visual_Comparison_of_Various_Normalizaton_Methods.png" style="width:89%" /><br />
種々の正規化の比較
</center>




  </div>

      </div>
    </main><div class="footer">
  <div class="wrap">
<!--
Thanks to <a href="http://www.imdb.com/">IMDB</a> for all the serie informations!
-->
駒澤大学
  </div>
</div>
<!---
<script src="https://datocms-middleman-example.netlify.com/javascripts/all.js"></script>
-->

</body>

</html>
