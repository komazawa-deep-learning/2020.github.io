<!doctype html>
<html lang="en">
 <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>PuckWorld</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

   <!-- jquery and jqueryui -->
  <script src="./jquery-2.1.3.min.js"></script>
  <link href="./external/jquery-ui.min.css" rel="stylesheet">
  <script src="./external/jquery-ui.min.js"></script>

  <!-- bootstrap -->
  <script src="./bootstrap.min.js"></script>
  <link href="./bootstrap.min.css" rel="stylesheet">

  <!-- d3js -->
  <script type="text/javascript" src="./external/d3.min.js"></script>

  <!-- markdown -->
  <script type="text/javascript" src="./external/marked.js"></script>
  <script type="text/javascript" src="./external/highlight.pack.js"></script>
  <link rel="stylesheet" href="./external/highlight_default.css">
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- mathjax: nvm now loaded dynamically
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  -->

  <!-- rljs -->
  <script type="text/javascript" src="./lib/rl.js"></script>

  <!-- flotjs -->
  <script src="./external/jquery.flot.min.js"></script>

  <!-- GA -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-3698471-24', 'auto');
  ga('send', 'pageview');
  </script>

  <style>
  #wrap {
    width:800px;
    margin-left: auto;
    margin-right: auto;
  }
  h2 {
    text-align: center;
  }
  body {
    font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;
  }
  #draw {
    margin-left: 100px;
  }
  svg {
    border: 1px solid black;
  }
  </style>
  
  <script type="application/javascript">
    
    var W = 600, H = 600;
    var d3line = null;
    var d3agent = null;
    var d3target = null;
    var d3target2 = null;
    var d3target2_radius = null;
    var initDraw = function() {
      var d3elt = d3.select('#draw');
      d3elt.html('');

      var w = 600;
      var h = 600;
      svg = d3elt.append('svg').attr('width', w).attr('height', h)
        .append('g').attr('transform', 'scale(1)');

      // define a marker for drawing arrowheads
      svg.append("defs").append("marker")
        .attr("id", "arrowhead")
        .attr("refX", 3)
        .attr("refY", 2)
        .attr("markerWidth", 3)
        .attr("markerHeight", 4)
        .attr("orient", "auto")
        .append("path")
          .attr("d", "M 0,0 V 4 L3,2 Z");

      // draw the puck
      d3agent = svg.append('circle')
        .attr('cx', 100)
        .attr('cy', 100)
        .attr('r', env.rad * this.W)
        .attr('fill', '#FF0')
        .attr('stroke', '#000')
        .attr('id', 'puck');

      // draw the target
      d3target = svg.append('circle')
        .attr('cx', 200)
        .attr('cy', 200)
        .attr('r', 10)
        .attr('fill', '#0F0')
        .attr('stroke', '#000')
        .attr('id', 'target');

      // bad target
      d3target2 = svg.append('circle')
        .attr('cx', 300)
        .attr('cy', 300)
        .attr('r', 10)
        .attr('fill', '#F00')
        .attr('stroke', '#000')
        .attr('id', 'target2');

      d3target2_radius = svg.append('circle')
        .attr('cx', 300)
        .attr('cy', 300)
        .attr('r', 10)
        .attr('fill', 'rgba(255,0,0,0.1)')
        .attr('stroke', '#000');

      // draw line indicating forces
      d3line = svg.append('line')
        .attr('x1', 0)
        .attr('y1', 0)
        .attr('x2', 0)
        .attr('y2', 0)
        .attr('stroke', 'black')
        .attr('stroke-width', '2')
        .attr("marker-end", "url(#arrowhead)");


    }

    var updateDraw = function(a, s, r) {
      // reflect puck world state on screen
      var ppx = env.ppx; var ppy = env.ppy;
      var tx = env.tx; var ty = env.ty;
      var tx2 = env.tx2; var ty2 = env.ty2;

      d3agent.attr('cx', ppx*W).attr('cy', ppy*H);
      d3target.attr('cx', tx*W).attr('cy', ty*H);
      d3target2.attr('cx', tx2*W).attr('cy', ty2*H);
      d3target2_radius.attr('cx', tx2*W).attr('cy', ty2*H).attr('r', env.BADRAD * H);
      d3line.attr('x1', ppx*W).attr('y1', ppy*H).attr('x2', ppx*W).attr('y2', ppy*H);
      var af = 20;
      d3line.attr('visibility', a === 4 ? 'hidden' : 'visible');
      if(a === 0) {
        d3line.attr('x2', ppx*W - af);
      }
      if(a === 1) {
        d3line.attr('x2', ppx*W + af);
      }
      if(a === 2) {
        d3line.attr('y2', ppy*H - af);
      }
      if(a === 3) {
        d3line.attr('y2', ppy*H + af);
      }

      // color agent by reward
      var vv = r + 0.5;
      var ms = 255.0;
      if(vv > 0) { g = 255; r = 255 - vv*ms; b = 255 - vv*ms; }
      if(vv < 0) { g = 255 + vv*ms; r = 255; b = 255 + vv*ms; }
      var vcol = 'rgb('+Math.floor(r)+','+Math.floor(g)+','+Math.floor(b)+')';
      d3agent.attr('fill', vcol); 
    }

    var PuckWorld = function() { 
      this.reset();
    }
    PuckWorld.prototype = {
      reset: function() {
        this.ppx = Math.random(); // puck x,y
        this.ppy = Math.random();
        this.pvx = Math.random()*0.05 -0.025; // velocity
        this.pvy = Math.random()*0.05 -0.025;
        this.tx = Math.random(); // target
        this.ty = Math.random();
        this.tx2 = Math.random(); // target
        this.ty2 = Math.random(); // target
        this.rad = 0.05;
        this.t = 0;

        this.BADRAD = 0.25;
      },
      getNumStates: function() {
        return 8; // x,y,vx,vy, puck dx,dy
      },
      getMaxNumActions: function() {
        return 5; // left, right, up, down, nothing
      },
      getState: function() {
        var s = [this.ppx - 0.5, this.ppy - 0.5, this.pvx * 10, this.pvy * 10, this.tx-this.ppx, this.ty-this.ppy, this.tx2-this.ppx, this.ty2-this.ppy];
        return s;
      },
      sampleNextState: function(a) {

        // world dynamics
        this.ppx += this.pvx; // newton
        this.ppy += this.pvy;
        this.pvx *= 0.95; // damping
        this.pvy *= 0.95;

        // agent action influences puck velocity
        var accel = 0.002;
        if(a === 0) this.pvx -= accel;
        if(a === 1) this.pvx += accel;
        if(a === 2) this.pvy -= accel;
        if(a === 3) this.pvy += accel;

        // handle boundary conditions and bounce
        if(this.ppx < this.rad) {
          this.pvx *= -0.5; // bounce!
          this.ppx = this.rad;
        }
        if(this.ppx > 1 - this.rad) {
          this.pvx *= -0.5;
          this.ppx = 1 - this.rad;
        }
        if(this.ppy < this.rad) {
          this.pvy *= -0.5; // bounce!
          this.ppy = this.rad;
        }
        if(this.ppy > 1 - this.rad) {
          this.pvy *= -0.5;
          this.ppy = 1 - this.rad;
        }

        this.t += 1;
        if(this.t % 100 === 0) {
          this.tx = Math.random(); // reset the target location
          this.ty = Math.random();
        }

        // if(this.t % 73 === 0) {
        //   this.tx2 = Math.random(); // reset the target location
        //   this.ty2 = Math.random();
        // }

        // compute distances
        var dx = this.ppx - this.tx;
        var dy = this.ppy - this.ty;
        var d1 = Math.sqrt(dx*dx+dy*dy);

        var dx = this.ppx - this.tx2;
        var dy = this.ppy - this.ty2;
        var d2 = Math.sqrt(dx*dx+dy*dy);

        var dxnorm = dx/d2;
        var dynorm = dy/d2;
        var speed = 0.001;
        this.tx2 += speed * dxnorm;
        this.ty2 += speed * dynorm;

        // compute reward
        var r = -d1; // want to go close to green
        if(d2 < this.BADRAD) {
          // but if we're too close to red that's bad
          r += 2*(d2 - this.BADRAD)/this.BADRAD;
        }
        
        //if(a === 4) r += 0.05; // give bonus for gliding with no force

        // evolve state in time
        var ns = this.getState();
        var out = {'ns':ns, 'r':r};
        return out;
      }
    }
    
    function gofast() { steps_per_tick = 100; }
    function gonormal() { steps_per_tick = 10; }
    function goslow() { steps_per_tick = 1; }

    // flot stuff
    var nflot = 1000;
    function initFlot() {
      var container = $("#flotreward");
      var res = getFlotRewards();
      series = [{
        data: res,
        lines: {fill: true}
      }];
      var plot = $.plot(container, series, {
        grid: {
          borderWidth: 1,
          minBorderMargin: 20,
          labelMargin: 10,
          backgroundColor: {
            colors: ["#FFF", "#e4f4f4"]
          },
          margin: {
            top: 10,
            bottom: 10,
            left: 10,
          }
        },
        xaxis: {
          min: 0,
          max: nflot
        },
        yaxis: {
          min: -2,
          max: 1
        }
      });
      setInterval(function(){
        series[0].data = getFlotRewards();
        plot.setData(series);
        plot.draw();
      }, 100);
    }
    function getFlotRewards() {
      // zip rewards into flot data
      var res = [];
      for(var i=0,n=smooth_reward_history.length;i<n;i++) {
        res.push([i, smooth_reward_history[i]]);
      }
      return res;
    }

    var steps_per_tick = 1;
    var sid = -1;
    var action, state;
    var smooth_reward_history = [];
    var smooth_reward = null;
    var flott = 0;
    function togglelearn() { 
      if(sid === -1) {
        sid = setInterval(function() {

          for(var k=0;k<steps_per_tick;k++) {
            state = env.getState();
            action = agent.act(state);
            var obs = env.sampleNextState(action);
            agent.learn(obs.r);
            if(smooth_reward == null) { smooth_reward = obs.r; }
            smooth_reward = smooth_reward * 0.999 + obs.r * 0.001;
            flott += 1;
            if(flott === 200) {
              // record smooth reward
              if(smooth_reward_history.length >= nflot) {
                smooth_reward_history = smooth_reward_history.slice(1);
              }
              smooth_reward_history.push(smooth_reward);
              flott = 0;
            }
          }

          updateDraw(action, state, obs.r);
          if(typeof agent.expi !== 'undefined') {
            $("#expi").html(agent.expi);
          }
          if(typeof agent.tderror !== 'undefined') {
            $("#tde").html(agent.tderror.toFixed(3));
          }
          //$("#tdest").html('tderror: ' + agent.tderror_estimator.getMean().toFixed(4) + ' +/- ' + agent.tderror_estimator.getStd().toFixed(4));

        }, 20);
      } else {
        clearInterval(sid);
        sid = -1;
      }
    }
    
    function saveAgent() {
      $("#mysterybox").fadeIn();
      $("#mysterybox").val(JSON.stringify(agent.toJSON()));
    }

    function loadAgent() {
      $.getJSON( "agentzoo/puckagent.json", function( data ) {
        agent.fromJSON(data); // corss your fingers...
        // set epsilon to be much lower for more optimal behavior
        agent.epsilon = 0.05;
        $("#slider").slider('value', agent.epsilon);
        $("#eps").html(agent.epsilon.toFixed(2));
        // kill learning rate to not learn
        agent.alpha = 0;
      });
    }

    function resetAgent() {
      eval($("#agentspec").val())
      agent = new RL.DQNAgent(env, spec);
    }

    var w; // global world object
    var current_interval_id;
    var skipdraw = false;
    function start() {

      env = new PuckWorld();

      initDraw();      

      eval($("#agentspec").val())

      //agent = new RL.ActorCritic(env, spec);
      agent = new RL.DQNAgent(env, spec);
      //agent = new RL.RecurrentReinforceAgent(env, {});
      initFlot();

      // slider sets agent epsilon
      $("#slider").slider({
        min: 0,
        max: 1,
        value: agent.epsilon,
        step: 0.01,
        slide: function(event, ui) {
          agent.epsilon = ui.value;
          $("#eps").html(ui.value.toFixed(2));
        }
      });
      $("#eps").html(agent.epsilon.toFixed(2));

      togglelearn(); // start

      // render markdown
      $(".md").each(function(){
        $(this).html(marked($(this).html()));
      });
      renderJax();

    }

    var jaxrendered = false;
    function renderJax() {
      if(jaxrendered) { return; }
      (function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src  = "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
        document.getElementsByTagName("head")[0].appendChild(script);
        jaxrendered = true;
      })();
    }
    
  </script>
  <style type="text/css">
      canvas { border: 1px solid white; }
    </style>

 </head>
 <body onload="start();">

<!--   <a href="https://github.com/karpathy/reinforcejs"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a> -->
  
   <div id="wrap">
   
   <div id="mynav" style="border-bottom:1px solid #999; padding-bottom: 10px; margin-bottom:50px;">
    <div>
      <img src="loop.svg" style="width:50px;height:50px;float:left;">
      <h1 style="font-size:50px;">REINFORCE<span style="color:#058;">js</span></h1>
    </div>
    <ul class="nav nav-pills">
      <li role="presentation"><a href="index.html">About</a></li>
      <li role="presentation"><a href="gridworld_dp.html">グリッドワールド:DP</a></li>
      <li role="presentation"><a href="gridworld_td.html">グリッドワールド:TD</a></li>
      <li role="presentation"><a href="puckworld.html">パックマンワールド:DQN</a></li>
      <li role="presentation"><a href="waterworld.html">ウォーターワールド:DQN</a></li>
    </ul>
   </div>
   
   <h2>パックマンワールド: ディープ Q 学習</h2>

<textarea id="agentspec" style="width:100%;height:270px;">
// agent parameter spec to play with (this gets eval()'d on Agent reset)
var spec = {}
spec.update = 'qlearn'; // qlearn または sarsa
spec.gamma = 0.9; // 割引率, [0, 1) 0 から 1 まで
spec.epsilon = 0.2; // イプシロン貪欲探索に用いるイプシロンの初期値, [0, 1)
spec.alpha = 0.01; // 価値関数の学習率
spec.experience_add_every = 10; // 記憶を再生するために，別の経験を追加する時間ステップ数<!-- number of time steps before we add another experience to replay memory -->
spec.experience_size = 5000; // 経験を記憶再生するためのサイズ<!--size of experience replay memory-->
spec.learning_steps_per_iteration = 20;
spec.tderror_clamp = 1.0; // 頑健性のための <!--for robustness-->
spec.num_hidden_units = 100 // 中間層のニューロン数
</textarea>

<br/>
<center>
  <button class="btn btn-danger" onclick="resetAgent()" style="width:180px;height:50px;margin-bottom:5px;">エージェントの再初期化</button>
<button class="btn btn-primary" onclick="togglelearn()" style="width:130px;height:50px;margin-bottom:5px;">実行の切替</button>
<button class="btn btn-success" onclick="gofast()" style="width:80px;height:50px;margin-bottom:5px;">速</button>
<button class="btn btn-success" onclick="gonormal()" style="width:80px;height:50px;margin-bottom:5px;">普通</button>
<button class="btn btn-success" onclick="goslow()" style="width:80px;height:50px;margin-bottom:5px;">遅い</button>
</center>

<br>
探索イプシロン: <span id="eps">0.15</span> <div id="slider"></div>
<br>

<div id="draw"></div>

<br>
<div style="float:right;">
  <button class="btn btn-primary" onclick="loadAgent()" style="width:240px;height:35px;margin-bottom:5px;margin-right:20px;">訓練済エージェントの読み込み</button>
</div>
<textarea id="mysterybox" style="width:100%;display:none;">mystery text box</textarea>

<div> Experience write pointer: <div id="expi" style="display:inline-block;"></div> </div>
<div> 現在の TD 誤差: <div id="tde" style="display:inline-block;"></div> </div>
<div id="tdest"></div>

<div>
<div style="text-align:center">平均報酬 (高いほど良い)</div>
<div id="flotreward" style="width:800px; height: 400px;"></div>
</div>

<div id="exp" class="md">

### セットアップ Setup

このデモは，**パックマンワールド** の修正版です

<!-- This demo is a modification of **PuckWorld**:-->

- **状態空間** は拡張され，連続値になっています。
エージェントは自分の位置 (x,y) , 速度 (vx, vy) を観察します。緑色のターゲットと赤のターゲットの場所，全部で 8 つです。
<!-- The **state space** is now large and continuous: The agent observes its own location (x,y), velocity (vx,vy), the locations of the green target, and the red target (total of 8 numbers). -->
- エージェントは 4 つの行動を撮ることができます。 <!-- There are 4 **actions** available to the agent: To apply thrusters to the left, right, up and down. This gives the agent control over its velocity. -->
- パックマンワールドの **ダイナミクス** では エージェントの位置を変更するための速度を統合します。
緑のターゲットは，時々ランダムな場所に移動します。
赤のターゲットは常にエージェントにゆっくりと追従します。<!-- The PuckWorld **dynamics** integrate the velocity of the agent to change its position. The green target moves once in a while to a random location. The red target always slowly follows the agent. -->
- **報酬 reward** は緑のターゲットまでの距離です。緑との距離は低いほど良いと評価されます。
一方，エージェントが赤いターゲットの近く（ディスクの内側）にいる場合，エージェントと赤いターゲットまでの距離に比例して負の報酬を得ます。<!-- The **reward** awarded to the agent is based on its distance to the green target (low is good). However, if the agent is in the vicinity of the red target (inside the disk), the agent gets a negative reward proportional to its distance to the red target. -->

<!-- The optimal strategy for the agent is to always go towards the green target (this is the regular PuckWorld), but to also avoid the red target's area of effect. This makes things more interesting because the agent has to learn to avoid it. Also, sometimes it's fun to watch the red target corner the agent. In this case, the optimal thing to do is to temporarily pay the price to zoom by quickly and away, rather than getting cornered and paying much more reward price when that happens. -->

エージェントにとっての最適な戦略は，常に緑のターゲット (これは通常のパックマンワールドです)に向かって行くことです。
ですが，赤のターゲットの効果範囲を避けることが必要です。
これにより，エージェントは赤ターゲットを避けることを学ばなければならないので，ゲームをより面白くしています。
また、赤いターゲットがエージェントを追い詰めるのを見るのも楽しいです。
この場合に最適なのは，追い詰められたときに多くの報酬を支払うより，一時的に素早くズームして離れていくための代償を支払うことです。


**インターフェイス**: エージェントが経験した現在の報酬は 色  (緑＝高，赤＝低) で示されています。エージェントが取った行動 (中程度の大きさの円が動き回っている) が矢印で示されています。<!--The current reward experienced by the agent is shown as its color (green = high, red = low). The action taken by the agent (the medium-sized circle moving around) is shown by the arrow.-->

### 深層 Q 学習 (Deep Q Learning)

<!-- We're going to extend the Temporal Difference Learning (Q-Learning) to continuous state spaces. In the previos demo we've estimated the Q learning function \\(Q(s,a)\\) as a lookup table. Now, we are going to use a function approximator to model \\(Q(s,a) = f\_{\theta}(s,a)\\), where \\(\theta\\) are some parameters (e.g. weights and biases of a neurons in a network). However, as we will see everything else remains exactly the same. The first paper that showed impressive results with this approach was [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) at NIPS workshop in 2013, and more recently the Nature paper [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html), both by Mnih et al. However, more impressive than what we'll develop here, their function \\(f\_{\theta,a}\\) was an entire Convolutional Neural Network that looked at the raw pixels of Atari game console. It's hard to get all that to work in JS :( -->


TD 学習 (Q-学習) を連続状態空間に拡張してみます。
先のデモでは，Q 学習 関数 \\Q(s,a)\\) をルックアップテーブルとして推定しました。
ここで \\(Q(s,a) = f\_{\theta}(s,a)\\), をモデル化するために関数近似器を使用しようとしていますが、\\(\theta\\) はいくつかのパラメータ (例えば ネットワーク内のニューロンの重みやバイアス）です。
しかし，これから見るように，他のすべてのことはまったく同じです。
このアプローチで印象的な結果を示した最初の論文は，2013 年の NIPSワークショップ での [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) です。
さらに最近では Nature 論文 [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html) です。
両方とも Mnih らによるものでした。これをすべてJSで動作させるのは難しい。

<!-- Recall that in Q-Learning we had training data that is a single chain of \\(s\_t, a\_t, r\_t, s\_{t+1}, a\_{t+1}, r\_{t+1}, s\_{t+2}, \ldots \\) where the states \\(s\\) and the rewards \\(r\\) are samples from the environment dynamics, and the actions \\(a\\) are sampled from the agent's policy \\(a\_t \sim \pi(a \mid s\_t)\\). The agent's policy in Q-learning is to execute the action that is currently estimated to have the highest value (\\( \pi(a \mid s) = \arg\max\_a Q(s,a) \\)), or with a probability \\(\epsilon\\) to take a random action to ensure some exploration. The Q-Learning update at each time step then had the following form: -->

Q 学習では，学習データは \\(s\_t, a\_t, r\_t, s\_{t+1}, a\_{t+1}, r\_{t+1}, s\_{t+2}, \ldots \\)  の単一連鎖です。
状態 \\(s\\) と報酬 \\(r\\) は環境ダイナミクスからのサンプルです。
行動 \\(a\\) はエージェントのポリシー \\(a\_t \sim \pi(a \mid s\_t)\\) からのサンプルであることを想起してください。
Q 学習におけるエージェントの方策は，現在最も高い値を持つと推定される行動
(\\( \pi(a \mid s) = \arg\max\_a Q(s,a) \\)) を実行するか，または 確率 \\(\epsilon\\) でランダムな行動をとって何らかの探索を確実に行うことです。
そして、各時間ステップにおける Q 学習 の更新は、以下のような形式です:

$$
Q(s\_t, a\_t) \leftarrow Q(s\_t, a\_t) + \alpha \left[ r\_t + \gamma \max\_a Q(s\_{t+1}, a) - Q(s\_t, a\_t) \right]
$$

上式は，学習率 \\(\alpha\\) で確率的勾配降下法を用いて更新される，時刻 \\(t\)) での次式の損失関数が用いられます:
<!-- As mentioned, this equation is describing a regular Stochastic Gradient Descent update with learning rate \\(\alpha\\) and the loss function at time \\(t\\): -->

$$
L\_t = (r\_t + \gamma \max\_a Q(s\_{t+1}, a) - Q(s\_t, a\_t))^2
$$

ここで \\(y = r\_t + \gamma \max\_a Q(s\_{t+1}, a)\\) は，スカラ値です。この値で誤差逆伝播法により学習します。
損失関数は標準的な L2ノルム回帰の形をしていることに注意してください。
\\(\theta\\) のパラメータベクトルを 計算された \\(Q(s,a)\\) の値が、あるべき姿に少しだけ近づくように，(ベルマン方程式を満たすように） 調整されます。
これは，報酬が適切に拡散されるべきであるという制約を，環境ダイナミクスとエージェントの方策を介して，期待して，後方へ伝播するものです。
<!-- Here \\(y = r\_t + \gamma \max\_a Q(s\_{t+1}, a)\\) is thought of as a scalar-valued fixed target, while we backpropagate through the neural network that produced the prediction \\(f\_{\theta} = Q(s\_t,a\_t)\\). Note that the loss has a standard L2 norm regression form, and that we nudge the parameter vector \\(\theta\\) in a way that makes the computed \\(Q(s,a)\\) slightly closer to what it should be (i.e. to satisfy the Bellman equation). This softly encourages the constraint that the reward should be properly diffused, in expectation, backwards through the environment dynamics and the agent's policy.
 -->

In other words, Deep Q Learning is a 1-dimensional regression problem with a vanilla neural network, solved with vanilla stochastic gradient descent, except our training data is not fixed but generated by interacting with the environment.

言い換えれば，ディープ Q 学習は 素のニューラルネットワークを用いた 1次元回帰問題です。
学習データが固定ではなく，環境との相互作用によって生成されることを除いては、素の確率的勾配降下法で解くことができます。

### ベルとホイッスル (Bells and Whistles)

<!-- There are several bells and whistles that make Q Learning with function approximation tracktable in practical problems: -->
実践的な問題で関数近似をトラックテーブルにした Q 学習をするためのいくつかのベルとホイッスルがあります。

**Modeling Q(s,a)**. 前述したように Q 関数  \\(Q(s,a) = f\_{\theta}(s,a)\\) をモデル化するために関数近似器を使用しています。
取るべき自然なアプローチは，おそらく状態ベクトル \\(s\\) と行動ベクトル \\(a\\) 例えば ワンホットベクトルでエンコードされた入力を受け取り，
\\(Q(s,a)\\) を与える単一の数値を出力するニューラルネットワークを作る持つことです。
しかし，この方法では，エージェントの方策は Q 値を最大化する行動をとること，
すなわち \\( \pi(a \mid s) = \arg\max\_a Q(s,a) \\) です。
このため，実用的な問題が発生します。
この素朴な方法アプローチでは，エージェントが行動を生成しようとすると，すべての行動を反復処理し，それぞれの行動について Q 値を評価し，
最も高い Q を与えた行動をとらなければなりません。

<!-- First, as mentioned we are using a function approximator to model the Q function: \\(Q(s,a) = f\_{\theta}(s,a)\\). The natural approach to take is to perhaps have a neural network that takes as inputs the state vector \\(s\\) and an action vector \\(a\\) (e.g. encoded with a 1-of-k indicator vector), and output a single number that gives \\(Q(s,a)\\). However, this approach leads to a practical problem because the policy of the agent is to take an action that maximizes Q, that is: \\( \pi(a \mid s) = \arg\max\_a Q(s,a) \\). Therefore, with this naive approach, when the agent wants to produce an action we'd have to iterate over all actions, evaluate Q for each one and take the action that gave the highest Q.-->

<!-- A better idea is to instead predict the value \\(Q(s,a)\\) as a neural network that only takes the state \\(s\\), and produces multiple output, each interpreted as the Q value of taking that action in the given state. This way, deciding what action to take reduces to performing a single forward pass of the network and finding the argmax action. A diagram may help get the idea across: -->

それよりより良いアイデアは，状態 \\(s\\) だけを取り，複数の出力を生成するニューラルネットワークとして
値 \\(Q(s,a)\\) を予測するかわりに，複数の出力を生成することです。
これらの値は，それぞれが与えられた状態でその行動を取る際の Q 値として解釈されます。
このようにして，どのようなアクションを取るかを決定することは，ネットワークの一回のフォワードパスを実行して 最大値を与える行動を見つけることに縮約されます。
図を見ていただければ理解していただけると思います。

<div style="text-align:center; margin: 20px;">
<img src="img/qsa.jpeg" style="max-width:500px;"><br>
3 次元の状態空間（青）と 2 つの行動（赤）の単純な例。緑のノードはニューラルネットワーク f のニューロン。
左: 最大値を与える行動を見つけるために複数回の前向きパスを取る素朴ななアプローチ。
右: より効率的なアプローチ。
Q(s,a) の計算がネットワーク内のニューロン間で効果的に共有される。
与えられた状態で取るべき最良の行動を見つけるためには 1 回の前向きパスのみが必要。
<!-- Simple example with 3-dimensional state space (blue) and 2 possible actions (red). Green nodes are neurons of a neural network f. Left: A naive approach in which it takes multiple forward passes to find the argmax action. Right: A more efficient approach where the Q(s,a) computation is effectively shared among the neurons in the network. Only a single forward pass is required to find the best action to take in any given state. -->
</div>


このように定式化すると、更新アルゴリズムは次のようになります。

1. 環境中で \\(s\t, a\_t, r\_t, s\_{t+1}\\) の遷移を経験する
2. 前向きパス \\(s\_{t+1}\\) で (固定の)目標 \\(y = r\_t + \gamma \max\_a f\_{\theta}(s\_{t+1})\\) を評価する。
3. 前向き  \\(f\_{\theta}(s\_t)\\) で 次元 \\(a\_t\\) に L2 損失を適用する。 L2 損失のため 勾配は単純な形をしており，予測値は単純に \\(y\\) から減算される。
4. 勾配を逆伝播し，パラメータを更新する

<!-- Formulated in this way, the update algorithm becomes:

1. Experience a \\(s\_t, a\_t, r\_t, s\_{t+1}\\) transition in the environment.
2. Forward \\(s\_{t+1}\\) to evaluate the (fixed) target \\(y = r\_t + \gamma \max\_a f\_{\theta}(s\_{t+1})\\).
3. Forward \\(f\_{\theta}(s\_t)\\) and apply L2 regression loss on the dimension \\(a\_t\\) of the output which we want to equal to \\(y\\). Due to the L2 loss, the gradient has a simple form where the predicted value is simply subtracted from \\(y\\). The output dimensions corresponding to the other actions have zero gradient. 
4. Backpropagate the gradient and perform a parameter update.
-->

**Experience Replay Memory**. Mnih らの論文の重要な貢献は，経験のリプレイ記憶を示唆することでした。
これは 脳、特に海馬の記憶痕跡を大脳皮質と同期させる方法から着想を得たものです。
これは，更新してから経験を捨てるのではなく，経験を残しておいて，効果的に経験の訓練セットを作り上げるということです。
そうすると，その時に入ってきた経験を元に学習するのではなく，リプレイメモリからランダムに経験をサンプリングして，それぞれのサンプルを更新してきます。
繰り返しになりますが，これは，通常の機械学習のセットアップでデータセット上で 確率的勾配降下法 (SGD) を使ってニューラルネットを学習する場合と全く同じです.
違いはデータセットがエージェントの相互作用の結果であることだけです。
この機能は，観察された状態，行動，報酬の順序の相関関係を除去する効果があり，段階的なドリフトと忘却を減少させます。
アルゴリズムは以下の通り:

<!-- An important contribution of the Mnih et al. papers was to suggest an experience replay memory. This is loosely inspired by the brain, and in particular the way it syncs memory traces in the hippocampus with the cortex. What this amounts to is that instead of performing an update and then throwing away the experience tuple \\(s\_t, a\_t, r\_t, s\_{t+1}\\), we keep it around and effectively build up a training set of experiences. Then, we don't learn based on the new experience that comes in at time \\(t\\), but instead sample random expriences from the replay memory and perform an update on each sample. Again, this is exactly as if we were training a Neural Net with SGD on a dataset in a regular Machine Learning setup, except here the dataset is a result of agent interaction. This feature has the effect of removing correlations in the observed state,action,reward sequence and reduces gradual drift and forgetting. The algorihm hence becomes: -->

1. ある環境で \\(s\_t, a\_t, r\_t, s\_{t+1}\\) を経験し，訓練データセットに加える。もし閾値よりも大きければ，過去の経験を置き換える
<!-- in the environment and add it to the training dataset \\(\mathcal{D}\\). If the size of \\(\mathcal{D}\\) is greater than some threshold, start replacing old experiences. -->
2. **N** このサンプルを \\(\mathcal{D}\\) から取得し，Q 関数を更新する<!-- Sample **N** experiences from \\(\mathcal{D}\\) at random and update the Q function. -->

**Clamping TD Error**. もう一つの興味深い特徴は TD 誤差勾配を一定の最大値でクランプすることです。
つまり TD 誤差 \\(r\_t + \gamma \max\_a Q(s\_{t+1}, a) - Q(s\_t, a\_t)\\) が大きければ その最大値でクランプ（その最大値にしてしまう）する。
これにより，外れ値に対してより頑健な学習が可能になり Huber 損失を利用するという解釈もできます。

<!-- Another interesting feature is to clamp the TD Error gradient at some fixed maximum value. That is, if the TD Error \\(r\_t + \gamma \max\_a Q(s\_{t+1}, a) - Q(s\_t, a\_t)\\) is greater in magnitude then some threshold `spec.tderror_clamp`, then we cap it at that value. This makes the learning more robust to outliers and has the interpretation of using Huber loss, which is an L2 penalty in a small region around the target value and an L1 penalty further away. -->

**Periodic Target Q Value Updates**.  Mnih ら で提案された最後の修正もまた，更新とすぐに実行される動作の間の相関を減らすことを目的としています。
そのアイデアは Q ネットワーク を時々凍結させ，凍結されたコピーされたネットワーク \\(\hat{Q}\\) にして，ターゲットのみを計算するために使用することです。
このターゲットネットワークは，時々，実際の現在のQネットワークに更新されます。
つまり，ターゲットの計算にはターゲットネットワーク \\(r\_t + \gamma \max\_a \hat{Q}(s\_{t+1},a)\\)  を使用し，\\(Q(s\_t, a\_t)\\) の部分の評価には Q を使用します。
実装では \\(\hat{Q} \leftarrow Q\\) を同期させるためのコードを 1 行追加する必要があります。
このアイデアで遊んでみましたが，少なくともこの単純なおもちゃの例では，実質的な利益を与えるとは思えなかったので，簡潔にするために現在の実装では REINFORCEjs から外してあります。

<!-- The last modification suggested in Mnih et al. also aims to reduce correlations between updates and the immediately undertaken behavior. The idea is to freeze the Q network once in a while into a frozen, copied network \\(\hat{Q}\\) which is used to only compute the targets. This target network is once in a while updated to the actual current \\(Q\\) network. That is, we use the target network \\(r\_t + \gamma \max\_a \hat{Q}(s\_{t+1},a)\\) to compute the target, and \\(Q\\) to evaluate the \\(Q(s\_t, a\_t)\\) part. In terms of the implementation, this requires one additional line of code where we every now and then we sync \\(\hat{Q} \leftarrow Q\\). I played around with this idea but at least on my simple toy examples I did not find it to give a substantial benefit, so I took it out of REINFORCEjs in the current implementation for sake of brevity. -->

### DQN に用いられる REINFORCEjs API

If you'd like to use REINFORCEjs DQN in your application, define an `env` object that has the following methods:

- `env.getNumStates()` returns an integer for the dimensionality of the state feature space
- `env.getMaxNumActions()` returns an integer with the number of actions the agent can use

This seems kind of silly and the name `getNumStates` is a bit of a misnomer because it refers to the size of the state feature vector, not the raw number of unique states, but I chose this interface so that it is consistent with the tabular TD code and DP method. Similar to the tabular TD agent, the IO with the agent has an extremely simple interface:

<pre><code class="js">
// create environment
var env = {};
env.getNumStates = function() { return 8; }
env.getMaxNumActions = function() { return 4; }

// create the agent, yay!
var spec = { alpha: 0.01 } // see full options on top of this page
agent = new RL.DQNAgent(env, spec); 

setInterval(function(){ // start the learning loop
  var action = agent.act(s); // s is an array of length 8
  // execute action in environment and get the reward
  agent.learn(reward); // the agent improves its Q,policy,model, etc. reward is a float
}, 0);
</code></pre>

As for the available parameters in the DQN agent `spec`:

- `spec.gamma` is the discount rate. When it is zero, the agent will be maximally greedy and won't plan ahead at all. It will grab all the reward it can get right away. For example, children that fail the marshmallow experiment have a very low gamma. This parameter goes up to 1, but cannot be greater than or equal to 1 (this would make the discounted reward infinite).
- `spec.epsilon` controls the epsilon-greedy policy. High epsilon (up to 1) will cause the agent to take more random actions. It is a good idea to start with a high epsilon (e.g. 0.2 or even a bit higher) and decay it over time to be lower (e.g. 0.05).
- `spec.num_hidden_units`: currently the DQN agent is hardcoded to use a neural net with one hidden layer, the size of which is controlled with this parameter. For each problems you may get away with smaller networks.
- `spec.alpha` controls the learning rate. Everyone sets this by trial and error and that's pretty much the best thing we have.
- `spec.experience_add_every`: REINFORCEjs won't add a new experience to replay every single frame to try to conserve resources and get more variaty. You can turn this off by setting this parameter to 1. Default = 5
- `spec.experience_size`: size of memory. More difficult problems may need bigger memory
- `spec.learning_steps_per_iteration`: the more the better, but slower. Default = 20
- `spec.tderror_clamp`: for robustness, clamp the TD Errror gradient at this value.

### 長所と短所 (Pros and Cons)

DQN エージェントの非常に優れた特性は I/O インターフェースが非常に汎用的で非常にシンプルであることです。
エージェントは状態ベクトルを取り，離散的な行動を生成します。
任意のエージェントを使って学習できます。
しかし 実際にこのエージェントを適用する際には いくつか注意すべき点があります。

- 報酬が非常に疎な環境であれば，エージェントは学習に苦労するでしょう。
今のところ優先度スイープのサポートはありません。
TD 誤差が大きいオーバーサンプリングの経験を想像するかもしれません。
これがどのようにして最も原理的な方法でできるかは明らかではありません。
同様に、現在は適格性のトレースはありません。これは将来のバージョンで少し修正して追加される可能性があります。
- 探索はたまにランダムに行われるので，かなり素朴なものになっています。
環境が報酬を得るために、より長い正確な行動系列を必要とする場合，エージェントは，これらを偶然に見つけ出し，そこから十分に学習するのに多くの困難を伴うかもしれません。
- DQN は離散的な行動の集合数しかサポートしておらず，どのようにして (高次元の) 連続行動空間を取り入れることができるのかは明らかではありません。

高次元の連続アクション空間といえば、これこそがポリシー勾配アクター批判法の得意とするところです。Policy Gradient (PG) のデモを見てみましょう。<!--(おっと、もうすぐです。)-->

<!-- The very nice property of DQN agents is that the I/O interface is quite generic and very simple: The agent accepts state vectors, produces discrete actions, and learns from relatively arbitrary agents. However, there are several things to keep in mind when applying this agent in practice:

- If the rewards are very sparse in the environment the agent will have trouble learning. Right now there is no priority sweeping support, but one might imagine oversampling experience that have high TD errors. It's not clear how this can be done in most principled way. Similarly, there are no eligibility traces right now though this could be added with a few modifications in future versions.
- The exploration is rather naive, since a random action is taken once in a while. If the environment requires longer sequences of precise actions to get a reward, the agent might have a lot of difficulty finding these by chance, and then also learning from them sufficiently.
- DQN only supports a set number of discrete actions and it is not obvious how one can incorporate (high-dimensional) continuous action spaces.

Speaking of high-dimensional continuous action spaces, this is what Policy Gradient Actor Critic methods are quite good at! Head over to the Policy Gradient (PG) demo. (Oops, coming soon...) -->

</div>
   </div>

<br><br><br><br><br>
 </body>
</html>
