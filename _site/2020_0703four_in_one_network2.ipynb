{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020_0703four-in-one-network2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2020_0703four_in_one_network2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAEUzxJPnQqG",
        "colab_type": "text"
      },
      "source": [
        "# 2020年度駒澤大学心理学特講IIIA \n",
        "\n",
        "浅川伸一\n",
        "\n",
        "## One network, many uses\n",
        "\n",
        "This notebook follows the tutorial: [One neural network, many uses: image captioning, image search, similar images and similar words using one model](https://towardsdatascience.com/one-neural-network-many-uses-build-image-search-image-captioning-similar-words-and-similar-1e22080ce73d) \n",
        "\n",
        "Made by [@paraschopra](https://twitter.com/paraschopra)\n",
        "\n",
        "MIT License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dce5yUYZdNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U ipympl > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV4Kz6n94z84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBYQrrnK8x4g",
        "colab_type": "text"
      },
      "source": [
        "### Flickr8k Dataset\n",
        "- Flickr8k dataset を使用します。本来は使用許可を得るために [この書類](https://forms.illinois.edu/sec/1713398) をダウンロードして使用許可を得る必要があります。\n",
        "- このデータセットには 8000 枚の画像に対して 5 つの脚注ついています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FhR_H835PXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 実習のための画像データの ID を入れて，データを入手\n",
        "download = drive.CreateFile({'id': '1y2P-Z8ZlpEyNbUq2fuDAIcb4Sjp4tyUE'})\n",
        "download.GetContentFile('caption_datasets.zip')\n",
        "!unzip caption_datasets.zip\n",
        "\n",
        "download = drive.CreateFile({'id': '1FyModcTYRiHaoXHU_wloTIz13S6wpIW4'})\n",
        "download.GetContentFile('Flicker8k_Dataset.zip')\n",
        "!unzip Flicker8k_Dataset.zip > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051Slr04DSNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1-bFD13-6GWgDSxgBWQE33vPBLQfKz7U3\n",
        "download = drive.CreateFile({'id': '1rYWwq26aECq-Xmq3sOQO6oFEheeB69AH'})\n",
        "download.GetContentFile('models.zip')\n",
        "!unzip models.zip > /dev/null    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDLQzzZa6lQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "#import ipympl \n",
        "%matplotlib inline\n",
        "#%matplotlib widget\n",
        "\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "from math import floor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn                     # neural networks\n",
        "import torch.nn.functional as F           # layers, activations and more\n",
        "import torch.optim as optim  \n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4K3TC_w7xNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU の設定チェック\n",
        "is_cuda = torch.cuda.is_available()\n",
        "is_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-as7fY_Vew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(is_cuda):\n",
        "    USE_GPU = True\n",
        "else:\n",
        "    USE_GPU = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9zL6tvr_ZX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENDWORD = '<END>'\n",
        "STARTWORD = '<START>'\n",
        "PADWORD = '<PAD>'\n",
        "HEIGHT = 299\n",
        "WIDTH = 299\n",
        "INPUT_EMBEDDING = 300\n",
        "HIDDEN_SIZE = 300\n",
        "OUTPUT_EMBEDDING = 300\n",
        "\n",
        "CAPTION_FILE = 'caption_datasets/dataset_flickr8k.json'\n",
        "IMAGE_DIR = 'Flicker8k_Dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhso1SLQ_cZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [PyTorch modelzoo](https://pytorch.org/docs/stable/torchvision/models.html) から訓練済みのモデルを入手します\n",
        "import string\n",
        "inception = models.inception_v3(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir-Mkih3_vbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inception model の詳細を表示\n",
        "inception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMR_2I_H_33O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4TN5Hmi_6aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_ftrs = inception.fc.in_features\n",
        "num_ftrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCC-7qnn_8xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_parameter_requires_grad(inception, True)\n",
        "num_ftrs = inception.fc.in_features\n",
        "inception.fc = nn.Linear(num_ftrs,INPUT_EMBEDDING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-3Zdnb5ABka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception.load_state_dict(torch.load('models/inception_epochs_40.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqkOcP5mAEOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(USE_GPU):\n",
        "    inception.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0XrFJifEJf7",
        "colab_type": "text"
      },
      "source": [
        "## Class for holding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn1-m_bxD-67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flickr8KImageCaptionDataset:    \n",
        "    def __init__(self):        \n",
        "        all_data = json.load(open('caption_datasets/dataset_flickr8k.json', 'r'))\n",
        "        all_data=all_data['images']\n",
        "        \n",
        "        self.training_data = []\n",
        "        self.test_data = []\n",
        "        self.w2i = {ENDWORD: 0, STARTWORD: 1}\n",
        "        self.word_frequency = {ENDWORD: 0, STARTWORD: 0}\n",
        "        self.i2w = {0: ENDWORD, 1: STARTWORD}\n",
        "        self.tokens = 2 #END is default\n",
        "        self.batch_index = 0\n",
        "        \n",
        "        for data in all_data:\n",
        "            if(data['split']=='train'):\n",
        "                self.training_data.append(data)\n",
        "            else:\n",
        "                self.test_data.append(data)\n",
        "                \n",
        "            for sentence in data['sentences']:\n",
        "                for token in sentence['tokens']:\n",
        "                    if(token not in self.w2i.keys()):\n",
        "                        self.w2i[token] = self.tokens\n",
        "                        self.i2w[self.tokens] = token\n",
        "                        self.tokens +=1\n",
        "                        self.word_frequency[token] = 1\n",
        "                    else:\n",
        "                        self.word_frequency[token] += 1\n",
        "                        \n",
        "    def image_to_tensor(self,filename):\n",
        "        image = Image.open(filename)\n",
        "        image = TF.resize(img=image, size=(HEIGHT,WIDTH))\n",
        "       \n",
        "        image = TF.to_tensor(pic=image)\n",
        "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "        \n",
        "        return torch.unsqueeze(image,0)\n",
        "\n",
        "    \n",
        "    def return_train_batch(self): #size of 1 always        \n",
        "        #np.random.shuffle(self.training_data)\n",
        "        for index in range(len(self.training_data)):\n",
        "            \n",
        "        #index = np.random.randint(len(self.training_data))\n",
        "            sentence_index = np.random.randint(len(self.training_data[index]['sentences']))\n",
        "            output_sentence_tokens = deepcopy(self.training_data[index]['sentences'][sentence_index]['tokens'])\n",
        "            output_sentence_tokens.append(ENDWORD) #corresponds to end word\n",
        "            image = self.image_to_tensor('Flicker8k_Dataset/'+self.training_data[index]['filename'])\n",
        "            yield image, list(map(lambda x: self.w2i[x], output_sentence_tokens)), output_sentence_tokens, index\n",
        "    \n",
        "    def convert_tensor_to_word(self, output_tensor):\n",
        "        output = F.log_softmax(output_tensor.detach().squeeze(), dim=0).numpy()\n",
        "        return self.i2w[np.argmax(output)]\n",
        "    \n",
        "    def convert_sentence_to_tokens(self, sentence):        \n",
        "        tokens = sentence.split(\" \")\n",
        "        converted_tokens= list(map(lambda x: self.w2i[x], tokens))\n",
        "        converted_tokens.append(self.w2i[ENDWORD])\n",
        "        return converted_tokens\n",
        "    \n",
        "    def caption_image_greedy(self, net, image_filename, max_words=15): \n",
        "        #non beam search, no temperature implemented\n",
        "        net.eval()\n",
        "        inception.eval()\n",
        "        image_tensor = self.image_to_tensor(image_filename)\n",
        "        hidden=None\n",
        "        embedding=None\n",
        "        words = []    \n",
        "        input_token = STARTWORD\n",
        "        input_tensor = torch.tensor(self.w2i[input_token]).type(torch.LongTensor)                \n",
        "        for i in range(max_words):            \n",
        "            if(i==0):\n",
        "                out, hidden=net(input_tensor, hidden=image_tensor, process_image=True)\n",
        "            else:\n",
        "                out, hidden=net(input_tensor, hidden)\n",
        "                \n",
        "            word = self.convert_tensor_to_word(out)\n",
        "            input_token = self.w2i[word]\n",
        "            input_tensor = torch.tensor(input_token).type(torch.LongTensor)            \n",
        "            \n",
        "            if(word==ENDWORD):\n",
        "                break\n",
        "            else:\n",
        "                words.append(word)\n",
        "\n",
        "        return ' '.join(words)    \n",
        "    \n",
        "    def forward_beam(self, net, hidden, process_image, \n",
        "                     partial_sentences, sentences, topn_words=5, max_sentences=10):        \n",
        "        max_words = 50        \n",
        "        hidden_index = {}        \n",
        "        while(sentences<max_sentences):            \n",
        "            #print(\"Sentences: \",sentences)            \n",
        "            new_partial_sentences = []\n",
        "            new_partial_sentences_logp = []\n",
        "            new_partial_avg_logp= []\n",
        "            \n",
        "            if(len(partial_sentences[-1][0])>max_words):\n",
        "                break                               \n",
        "            for partial_sentence in partial_sentences:\n",
        "                input_token = partial_sentence[0][-1]\n",
        "                input_tensor = torch.tensor(self.w2i[input_token]).type(torch.FloatTensor)\n",
        "                if(partial_sentence[0][-1]==STARTWORD):\n",
        "                    out, hidden=net(input_tensor, hidden, process_image=True)\n",
        "                else:\n",
        "                    out, hidden=net(input_tensor, torch.tensor(hidden_index[input_token]))\n",
        "\n",
        "                #take first topn words and add as children to root\n",
        "                out = F.log_softmax(out.detach().squeeze(), dim=0).numpy()\n",
        "                out_indexes = np.argsort(out)[::-1][:topn_words]\n",
        "        \n",
        "                for out_index in out_indexes:                    \n",
        "                    if(self.i2w[out_index]==ENDWORD):\n",
        "                        sentences=sentences+1                        \n",
        "                    else:                    \n",
        "                        total_logp = float(out[out_index]) + partial_sentence[1]\n",
        "                        new_partial_sentences_logp.append(total_logp)\n",
        "                        new_partial_sentences.append([np.concatenate((partial_sentence[0], \n",
        "                                                                      [self.i2w[out_index]])),\n",
        "                                                      total_logp])                        \n",
        "                        len_words = len(new_partial_sentences[-1][0])                                                                        \n",
        "                        new_partial_avg_logp.append(total_logp/len_words)                        \n",
        "                        #print(self.i2w[out_index])                        \n",
        "                        hidden_index[self.i2w[out_index]] = deepcopy(hidden.detach().numpy())\n",
        "                                                   \n",
        "            #select topn partial sentences        \n",
        "            top_indexes = np.argsort(new_partial_sentences_logp)[::-1][:topn_words]                                                                                        \n",
        "            new_partial_sentences = np.array(new_partial_sentences)[top_indexes]        \n",
        "            #print(\"New partial sentences (topn):\", new_partial_sentences)        \n",
        "            partial_sentences = new_partial_sentences                    \n",
        "        return partial_sentences\n",
        "    \n",
        "    def caption_image_beam_search(self, net, image_filename, topn_words=10, max_sentences=10):        \n",
        "        net.eval()\n",
        "        inception.eval()\n",
        "        image_tensor = self.image_to_tensor(image_filename)\n",
        "        hidden=None\n",
        "        embedding=None\n",
        "        words = []           \n",
        "        sentences = 0\n",
        "        partial_sentences = [[[STARTWORD], 0.0]]\n",
        "        #root_id = hash(input_token) #for start word\n",
        "        #nodes = {}        \n",
        "        #nodes[root_id] = Node(root_id, [STARTWORD, 0], None)\n",
        "        partial_sentences = self.forward_beam(net, \n",
        "                                              image_tensor, \n",
        "                                              True, \n",
        "                                              partial_sentences, \n",
        "                                              sentences,  \n",
        "                                              topn_words, \n",
        "                                              max_sentences)\n",
        "    \n",
        "        logp = []        \n",
        "        joined_sentences = []    \n",
        "        for partial_sentence in partial_sentences:                    \n",
        "            joined_sentences.append([' '.join(partial_sentence[0][1:]),partial_sentence[1]])                                        \n",
        "        return joined_sentences\n",
        "\n",
        "    def print_beam_caption(self, net, train_filename,num_captions=0):\n",
        "        beam_sentences = f.caption_image_beam_search(net,train_filename)\n",
        "        if(num_captions==0):\n",
        "            num_captions=len(beam_sentences)\n",
        "        for sentence in beam_sentences[:num_captions]:\n",
        "            print(sentence[0]+\" [\",sentence[1], \"]\")                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ui4GIa_EQux",
        "colab_type": "text"
      },
      "source": [
        "## class for network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AflGf9OWEMtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IC_V6(nn.Module):    \n",
        "    #V2: Fed image vector directly as hidden and fed words generated as iputs back to LSTM\n",
        "    #V3: Added an embedding layer between words input and GRU/LSTM\n",
        "    \n",
        "    def __init__(self, token_dict_size):\n",
        "        super(IC_V6, self).__init__()        \n",
        "        #Input is an image of height 500, and width 500                \n",
        "        self.embedding_size = INPUT_EMBEDDING\n",
        "        self.hidden_state_size = HIDDEN_SIZE\n",
        "        self.token_dict_size = token_dict_size\n",
        "        self.output_size = OUTPUT_EMBEDDING        \n",
        "        self.batchnorm = nn.BatchNorm1d(self.embedding_size)\n",
        "        self.input_embedding = nn.Embedding(self.token_dict_size, self.embedding_size)        \n",
        "        self.embedding_dropout = nn.Dropout(p=0.22)\n",
        "        self.gru_layers = 3        \n",
        "        self.gru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_state_size, \n",
        "                          num_layers=self.gru_layers, dropout=0.22)\n",
        "        self.linear = nn.Linear(self.hidden_state_size, self.output_size)\n",
        "        self.out = nn.Linear(self.output_size, token_dict_size)\n",
        "        \n",
        "    def forward(self, input_tokens, hidden, process_image=False, use_inception=True):                        \n",
        "        if(USE_GPU):\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        \n",
        "        if(process_image):\n",
        "            if(use_inception):\n",
        "                inp=self.embedding_dropout(inception(hidden))\n",
        "            else:\n",
        "                inp=hidden\n",
        "            #inp=self.batchnorm(inp)\n",
        "            hidden=torch.zeros((self.gru_layers,1, self.hidden_state_size))\n",
        "        else:\n",
        "            inp=self.embedding_dropout(self.input_embedding(\n",
        "                input_tokens.view(1).type(torch.LongTensor).to(device)))\n",
        "            #inp=self.batchnorm(inp)                    \n",
        "        hidden = hidden.view(self.gru_layers,1,-1)\n",
        "        inp = inp.view(1,1,-1)        \n",
        "        out, hidden = self.gru(inp, hidden)\n",
        "        out = self.out(self.linear(out))                       \n",
        "        return out, hidden        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtOEv8VyES70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = Flickr8KImageCaptionDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1SrvOTYEU2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = IC_V6(f.tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGT9wU3sEWnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.load_state_dict(torch.load('models/epochs_40_loss_2_841_v6.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8laMPH3eFZaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(USE_GPU):\n",
        "    net.cuda()\n",
        "    inception.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUm5N7QPFcX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyHFLE_eFevv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#traindataset\n",
        "random_train_index =  np.random.randint(len(f.training_data))\n",
        "train_filename = 'Flicker8k_Dataset/'+f.training_data[random_train_index]['filename']\n",
        "print(\"Original caption: \",f.training_data[random_train_index]['sentences'][0]['raw'])\n",
        "print(\"\")\n",
        "print(\"Greedy caption:\", f.caption_image_greedy(net,train_filename))\n",
        "print(\"\")\n",
        "print(\"Beam caption:\")\n",
        "f.print_beam_caption(net, train_filename)\n",
        "\n",
        "IPython.display.Image(filename=train_filename) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lxvu0hSbQ9x",
        "colab_type": "text"
      },
      "source": [
        "## Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0xOr2hEapmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = torch.nn.CrossEntropyLoss(reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkcYKQkbXAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o = optim.Adam(net.parameters(), lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZIYb9ebaZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "epochs = 5\n",
        "inception.eval()\n",
        "net.train()\n",
        "loss_so_far = 0.0\n",
        "total_samples = len(f.training_data)\n",
        "\n",
        "for epoch in range(epochs):   \n",
        "    for (image_tensor, tokens, _, index) in f.return_train_batch():    \n",
        "        o.zero_grad()\n",
        "        net.zero_grad()\n",
        "\n",
        "        words = []\n",
        "        loss=0.\n",
        "        input_token = f.w2i[STARTWORD]\n",
        "        input_tensor = torch.tensor(input_token)\n",
        "        for token in tokens:\n",
        "            if(input_token==f.w2i[STARTWORD]):\n",
        "                out, hidden=net(input_tensor, image_tensor, process_image=True)\n",
        "            else:\n",
        "                out, hidden=net(input_tensor, hidden)\n",
        "\n",
        "            class_label = torch.tensor(token).view(1)\n",
        "            input_token = token\n",
        "            input_tensor = torch.tensor(input_token)\n",
        "            out = out.squeeze().view(1,-1)\n",
        "            loss += l(out,class_label)\n",
        "        loss = loss/len(tokens)\n",
        "        loss.backward()\n",
        "        o.step()\n",
        "        loss_so_far += loss.detach().item()\n",
        "        if(np.random.rand()<0.002): #5% of cases\n",
        "            print(\"Epoch: \", epoch, \n",
        "                  \", index: \", index,\n",
        "                  \" loss: \", round(loss.detach().item(),3),\n",
        "                  \" | running avg loss: \", round(loss_so_far/((epoch*total_samples)+(index+1)),3))\n",
        "            torch.save(net.state_dict(), 'models/running_save_v6.pth')\n",
        "            torch.save(net.state_dict(), 'models/running_inception_save_v6.pth')\n",
        "            net.eval()\n",
        "           \n",
        "            #test dataset\n",
        "            #random_train_index = np.random.randint(len(f.training_data))\n",
        "            random_train_index = index\n",
        "            train_filename = IMAGE_DIR+f.training_data[random_train_index]['filename']\n",
        "            print(\"Original caption: \")\n",
        "            [print(x['raw'].lower()) for x in f.training_data[random_train_index]['sentences']]\n",
        "            print(\"\")\n",
        "            print(\"Greedy caption:\", f.caption_image_greedy(net,train_filename))\n",
        "            print(\"\")\n",
        "            print(\"Beam caption:\")\n",
        "            f.print_beam_caption(net, train_filename, 3)\n",
        "            #IPython.display.Image(filename=test_filename) \n",
        "            pil_im = Image.open(train_filename, 'r')\n",
        "            plt.figure()\n",
        "            plt.imshow(np.asarray(pil_im))\n",
        "            plt.show()\n",
        "            net.train()\n",
        "    \n",
        "    print(\"\\n\\n\")\n",
        "    print(\"==== EPOCH DONE. === \")\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wyR5sn_dQbU",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions for visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCkYTYsHbdVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rand_cmap(nlabels, type='bright', first_color_black=True, last_color_black=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Creates a random colormap to be used together with matplotlib. Useful for segmentation tasks\n",
        "    :param nlabels: Number of labels (size of colormap)\n",
        "    :param type: 'bright' for strong colors, 'soft' for pastel colors\n",
        "    :param first_color_black: Option to use first color as black, True or False\n",
        "    :param last_color_black: Option to use last color as black, True or False\n",
        "    :param verbose: Prints the number of labels and shows the colormap. True or False\n",
        "    :return: colormap for matplotlib\n",
        "    \"\"\"\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    import colorsys\n",
        "    import numpy as np\n",
        "\n",
        "    if type not in ('bright', 'soft'):\n",
        "        print ('Please choose \"bright\" or \"soft\" for type')\n",
        "        return\n",
        "\n",
        "    if verbose:\n",
        "        print('Number of labels: ' + str(nlabels))\n",
        "\n",
        "    # Generate color map for bright colors, based on hsv\n",
        "    if type == 'bright':\n",
        "        randHSVcolors = [(np.random.uniform(low=0.0, high=1),\n",
        "                          np.random.uniform(low=0.2, high=1),\n",
        "                          np.random.uniform(low=0.9, high=1)) for i in range(nlabels)]\n",
        "\n",
        "        # Convert HSV list to RGB\n",
        "        randRGBcolors = []\n",
        "        for HSVcolor in randHSVcolors:\n",
        "            randRGBcolors.append(colorsys.hsv_to_rgb(HSVcolor[0], HSVcolor[1], HSVcolor[2]))\n",
        "\n",
        "        if first_color_black:\n",
        "            randRGBcolors[0] = [0, 0, 0]\n",
        "\n",
        "        if last_color_black:\n",
        "            randRGBcolors[-1] = [0, 0, 0]\n",
        "\n",
        "        random_colormap = LinearSegmentedColormap.from_list('new_map', randRGBcolors, N=nlabels)\n",
        "\n",
        "    # Generate soft pastel colors, by limiting the RGB spectrum\n",
        "    if type == 'soft':\n",
        "        low = 0.6\n",
        "        high = 0.95\n",
        "        randRGBcolors = [(np.random.uniform(low=low, high=high),\n",
        "                          np.random.uniform(low=low, high=high),\n",
        "                          np.random.uniform(low=low, high=high)) for i in xrange(nlabels)]\n",
        "\n",
        "        if first_color_black:\n",
        "            randRGBcolors[0] = [0, 0, 0]\n",
        "\n",
        "        if last_color_black:\n",
        "            randRGBcolors[-1] = [0, 0, 0]\n",
        "        random_colormap = LinearSegmentedColormap.from_list('new_map', randRGBcolors, N=nlabels)\n",
        "\n",
        "    # Display colorbar\n",
        "    if verbose:\n",
        "        from matplotlib import colors, colorbar\n",
        "        from matplotlib import pyplot as plt\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(15, 0.5))\n",
        "\n",
        "        bounds = np.linspace(0, nlabels, nlabels + 1)\n",
        "        norm = colors.BoundaryNorm(bounds, nlabels)\n",
        "\n",
        "        cb = colorbar.ColorbarBase(ax, cmap=random_colormap, norm=norm, spacing='proportional', ticks=None,\n",
        "                                   boundaries=bounds, format='%1i', orientation=u'horizontal')\n",
        "\n",
        "    return random_colormap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_aF09PbdfQP",
        "colab_type": "text"
      },
      "source": [
        "## Start visualizations (First of word embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMqRUHBvdb6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frequency_threshold = 50 #the word should have appeared at least this many times for us to visualize\n",
        "\n",
        "all_word_embeddings = []\n",
        "all_words = []\n",
        "\n",
        "for word in f.word_frequency.keys():\n",
        "    if(f.word_frequency[word]>=frequency_threshold):\n",
        "        all_word_embeddings.append(net.input_embedding(torch.tensor(f.w2i[word])).detach().numpy())\n",
        "        all_words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obm_N8dHdloH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miL4LbQLdrH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYPH4y8oeIjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_2d = tsne.fit_transform(all_word_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVKadT25eI8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_cmap = rand_cmap(10, type='bright', first_color_black=True, last_color_black=False, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxcgfF71eUz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,ax = plt.subplots(figsize=(7, 7))\n",
        "    \n",
        "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
        "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
        "annot.set_visible(False)\n",
        "\n",
        "def update_annot(ind):\n",
        "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
        "    annot.xy = pos\n",
        "    text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
        "    annot.set_text(text)\n",
        "    annot.get_bbox_patch().set_facecolor('white')\n",
        "    annot.get_bbox_patch().set_alpha(0.9)\n",
        "\n",
        "\n",
        "def hover(event):\n",
        "    vis = annot.get_visible()\n",
        "    if event.inaxes == ax:\n",
        "        cont, ind = sc.contains(event)\n",
        "        if cont:                        \n",
        "            update_annot(ind)\n",
        "            annot.set_visible(True)\n",
        "            fig.canvas.draw_idle()\n",
        "        else:            \n",
        "            if vis:\n",
        "                annot.set_visible(False)\n",
        "                fig.canvas.draw_idle()\n",
        "                \n",
        "def onpick(event):\n",
        "    ind = event.ind\n",
        "    print(ind)\n",
        "    label_pos_x = event.mouseevent.xdata\n",
        "    label_pos_y = event.mouseevent.ydata\n",
        "    annot.xy = (label_pos_x,label_pos_y)\n",
        "    annot.set_text(y[ind])\n",
        "    ax.figure.canvas.draw_idle()\n",
        "        \n",
        "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
        "#fig.canvas.mpl_connect('pick_event', onpick)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YurqF0LeYpI",
        "colab_type": "text"
      },
      "source": [
        "## find top 5 closest words due to similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAbNAu-SeZP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "def return_cosine_sorted(target_word_embedding):    \n",
        "    words = []\n",
        "    cosines = []    \n",
        "    for i in range(len(all_word_embeddings)):\n",
        "        cosines.append(1 - spatial.distance.cosine(target_word_embedding, all_word_embeddings[i]))\n",
        "        \n",
        "    sorted_indexes = np.argsort(cosines)[::-1]    \n",
        "    return np.vstack((np.array(all_words)[sorted_indexes], np.array(cosines)[sorted_indexes])).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fshDvRLsebSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words[:10]\n",
        "#print(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLlvU-NPekeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_similar_words(word, top_n=5):    \n",
        "    return return_cosine_sorted(return_embedding(word))[1:top_n+1]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qzmLslwemc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_embedding(word):\n",
        "    if(word in all_words):\n",
        "        target_embedding_index = [i for i, s in enumerate(all_words) if word in s][0]\n",
        "        return all_word_embeddings[target_embedding_index]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cqzhjZpeoL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_analogy(source_word_1, source_word_2, target_word_1, top_n=5):    \n",
        "    em_sw_1 = return_embedding(source_word_1)\n",
        "    em_sw_2 = return_embedding(source_word_2)\n",
        "    em_tw_1 = return_embedding(target_word_1)\n",
        "    \n",
        "    if((em_sw_1 is None) | (em_sw_2 is None) | (em_tw_1 is None)):\n",
        "        return 0\n",
        "    \n",
        "    target_embedding = em_tw_1 + (em_sw_2 - em_sw_1)\n",
        "    return return_cosine_sorted(target_embedding)[1:top_n+1]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPErygFoeqp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "return_similar_words('boy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NApMIG7Res_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "return_analogy('green', 'grass', 'red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6mt6qx1e1sw",
        "colab_type": "text"
      },
      "source": [
        "## embedding of images (visualize)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goWyceiGewqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cart2pol(x, y):\n",
        "    rho = np.sqrt(x**2 + y**2)\n",
        "    phi = np.arctan2(y, x)\n",
        "    return(rho, phi)\n",
        "\n",
        "def pol2cart(rho, phi):\n",
        "    x = rho * np.cos(phi)\n",
        "    y = rho * np.sin(phi)\n",
        "    return(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyLKJusge3_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "all_image_embeddings = []\n",
        "all_image_filenames = []\n",
        "\n",
        "for i in range(len(f.training_data)):\n",
        "    all_image_embeddings.append(inception(f.image_to_tensor('Flicker8k_Dataset/'\n",
        "                                                            +f.training_data[i]['filename'])).detach().numpy())\n",
        "    all_image_filenames.append(f.training_data[i]['filename'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuG2NYyHe573",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_image_embeddings_temp = all_image_embeddings[:]\n",
        "all_image_filenames_temp = all_image_filenames[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIvEQwdTe8MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.offsetbox import (TextArea, \n",
        "                                  DrawingArea, \n",
        "                                  OffsetImage,\n",
        "                                  AnnotationBbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGj9x1RSf9yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne_images = TSNE(n_components=2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ay1qChagAvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_2d = tsne.fit_transform(np.squeeze(all_image_embeddings_temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePkJgndngDNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))    \n",
        "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
        "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
        "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
        "annot.set_visible(False)\n",
        "\n",
        "def update_annot(ind):\n",
        "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
        "    annot.xy = pos\n",
        "    #text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
        "    #annot.set_text(text)\n",
        "    \n",
        "    rho = 10 #how for to draw centers of new images\n",
        "    total_radians = 2* np.pi\n",
        "    num_images = len(ind[\"ind\"])\n",
        "    if(num_images > 4): #at max 4\n",
        "        num_images=4\n",
        "    radians_offset = total_radians/num_images    \n",
        "    \n",
        "    for i in range(num_images):                \n",
        "        hovered_filename = 'Flicker8k_Dataset/'+all_image_filenames_temp[ind[\"ind\"][i]]\n",
        "        arr_img = Image.open(hovered_filename, 'r')\n",
        "        imagebox = OffsetImage(arr_img, zoom=0.3)\n",
        "        #imagebox.image.axes = ax    \n",
        "        offset = pol2cart(rho, i*radians_offset)                   \n",
        "        new_xy = (pos[0]+offset[0], pos[1]+offset[1])                \n",
        "        ab = AnnotationBbox(imagebox, new_xy)\n",
        "        ax.add_artist(ab)    \n",
        "        annot.get_bbox_patch().set_facecolor('white')\n",
        "        annot.get_bbox_patch().set_alpha(0.9)\n",
        "\n",
        "\n",
        "def hover(event):    \n",
        "    vis = annot.get_visible()\n",
        "    if event.inaxes == ax:\n",
        "        cont, ind = sc.contains(event)\n",
        "        if cont:                        \n",
        "            update_annot(ind)            \n",
        "            annot.set_visible(True)\n",
        "            fig.canvas.draw_idle()\n",
        "        else:\n",
        "            if vis:\n",
        "                annot.set_visible(False)\n",
        "                remove_all_images()\n",
        "                fig.canvas.draw_idle()\n",
        "\n",
        "def remove_all_images():\n",
        "    for obj in ax.findobj(match = type(AnnotationBbox(1, 1))):    \n",
        "        obj.remove()        \n",
        "\n",
        "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
        "#fig.canvas.mpl_connect('pick_event', onpick)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeSsAD2cgfBo",
        "colab_type": "text"
      },
      "source": [
        "## Similar images to a given image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqOs0vFsgfZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_image(filename):\n",
        "    pil_im = Image.open(filename, 'r')\n",
        "    plt.figure()\n",
        "    plt.imshow(np.asarray(pil_im))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3BW6xu4gkow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_similar_images(image_filename, top_n=5):   \n",
        "    return return_cosine_sorted_image(return_embedding_image(image_filename))[1:top_n+1]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnxRiShvgnPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_cosine_sorted_image(target_image_embedding):            \n",
        "    cosines = []    \n",
        "    for i in range(len(all_image_embeddings)):\n",
        "        cosines.append(1 - spatial.distance.cosine(target_image_embedding, all_image_embeddings[i]))        \n",
        "    sorted_indexes = np.argsort(cosines)[::-1]    \n",
        "    return np.vstack((np.array(all_image_filenames)[sorted_indexes], np.array(cosines)[sorted_indexes])).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOtw9ZUKgqJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_embedding_image(image_filename):\n",
        "    return inception(f.image_to_tensor(image_filename)).detach().numpy().squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}