---
title: "第07回"
author: 浅川 伸一
layout: home
---


# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 27/May/2021<br/>
Appache 2.0 license<br/>
</div>

- [実習 MLP Adam SGD <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
- [実習 LeNet PyTorch 版 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528LeNet_pytorch.ipynb){:target="_blank"}
- [実習 3 つの MNISt <img src='/assets/colab_icon.svg'>](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0514komazawa_3mnists.ipynb){:target="_blank"}
* [実習 いくつかの画像フィルタ 特徴点検出アルゴリズム <img src=/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_visit_feature_extractions_demo.ipynb){:target="_blank"}
* [実習 DOG などのフィルタと Harr 特徴による顔検出 a.k.a ビオラ＝ジョーンズ アルゴリズム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0528edge_and_face_detection_algorithm_not_cnn.ipynb){:target="_blank"}


### 復習

<!-- - **ディープラーニング** 深層学習, deep learning とは **信用割当問題** credit assignment problem を回避するために **畳込み** convolution 演算(処理)を用いて多層にしたニューラルネットワークのこと -->
- 畳込み演算
- カーネルサイズ
- プーリング
- ストライド
- パディング
-- ハイパーパラメータとしてのカーネル(特徴)サイズ，ストライド，パディング

$$
\left[\text{畳込み}\left(\ge1\right) \rightarrow \text{プーリング}\left(\ge0\right)\right]
\times \left(\ge1\right)\rightarrow\text{全結合層}\left(\ge1\right)
$$


### デモ

- [グーグルによるニューラルネットワークの遊び場 (プレイグランド)](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"}

<!-- - [Scavenger hunt](https://emojiscavengerhunt.withgoogle.com/){target="_blank"}
- [https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/){target="_blank"}
- [姿勢推定デモ](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html){target="_blank"}
- [Style-based GAN](https://youtu.be/kSLJriaOumA)
- [foodly による唐揚げもりつけロボット](https://rt-net.jp/service/foodly/), [YouTube](https://youtu.be/KiT_DrDjdDE) -->

# 1. ヒューベルとウィーゼルによる視覚野の生理学研究

<center>
<img src="/assets/1968Hubel_Wiesel_1.svg" style="width:47%"><br/>
Hubel と Wiesel(1959, 1962, 1968)の実験の模式図<br/>

<img src="/assets/1968Hubel_Wiesel_2.svg" style="width:47%"><br/>
Hubel と Wiesel の実験結果 (Hubel & Wiesel, 1968 の Fig.2.7をトレーシングしたもの<br/>
</center>


# LeNet5 (LeCun, 1998)

- **LeNet**. Yann LeCun (現 Facebook AI 研究所所長)による CNN 実装
 [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf){:target="_blank"} 手書き数字認識
 
<center>
<img src="/assets/1998LeNet5.png" width="66%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
LeNet5 の論文より改変
</div></center>

- 畳込層とプーリング層（発表当初はサブサンプリング）との繰り返し
    - 畳込とプーリングは<font color="green">局所結合</font>
- MNIST を用いた１０種類の手書き文字認識
- 最終２層は全結合層をつなげて最終層１０ニューロン，最終層の各ニューロンの出力がそれぞれの数字（０から９までの１０種）に対応する


<center>
<img src="/assets/1999Riesenhuber_Poggio_fig2.svg" style="width:49%"><br/>
<!-- <img src="https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/cde8974e50a598aa8c2ff88760acc450fab3fbf8/assets/1999Riesenhuber_Poggio_fig2.svg"
 style="width:89%"><br/> -->
<div style="text-align: left;width:77%;background-color:cornsilk">
モデルのスケッチ。
このモデルは、単純な細胞から作られた複雑な細胞の古典的なモデル[4]を拡張したもので、線形演算（福島の表記法では「S」ユニット，テンプレート・マッチング 図中の実線）と非線形演算（「C」プーリングユニット，最大値 MAX 演算を行う 図中破線）を持つ層の階層で構成。
細胞入力の最大値を選択、その値を用いてセルを駆動する非線形の MAX演算は複雑細胞に対して，線形入力の合計とは異なり モデルの特性の鍵となる概念。
この 2 種類の操作は 異なる位置にチューニングされた求心性結合をプールすることでパターン特異性と並進不変性を，また異なるスケールにチューニングされた求心性結合をプールすることで、スケール不変性をもたらした(図示せず)。
</div></center>


<center>
<img src="/assets/1999Riesenhuber_Poggio_fig3a.svg" style="width:49%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
MAX 機構 高度に非線形な形状調整の特性。
「最適」特徴を決定するために考案された「単純化手順」を用いて得られた下側頭葉細胞の応答（選好刺激に対する反応が等しくなるように正規化された反応)。
この実験では、もともと細胞は「水のボトル」の画像（一番左の物体）に非常に強い反応を示した。
その後、刺激を単色の輪郭に単純化したところ、細胞の発火が増加し、さらに、楕円を支える棒からなるパドルのような物体に変化した。
この物体が強い反応を引き起こすのに対し、棒や楕円だけではほとんど反応しなかった。
</div></center>

<center>
<img src="/assets/1999Riesenhuber_Poggio_fig3b.svg" style="width:66%"><br/>
<div style="text-align: left;width:77%;background-color:cornsilk">
実験とモデルの比較。
白棒はの実験用ニューロンの反応を示す。
黒と灰色の棒は 選好刺激の 幹-楕円 の基部の遷移に合わせてチューニングしたモデル細胞の反応を示している。
モデル細胞は 直上図に示したモデルを簡略化したもの。
受容野の各位置に 2 種類の S1 特徴があり、それぞれが遷移領域の左側または右側にチューンしていて、その出力が C1 ユニットに入力され MAX 関数 (黒棒) または SUM 関数 (灰色棒) を用いてプールされている。
モデル細胞は 実験ニューロンの 選好刺激が受容野内にあるときに反応が最大になるよう、C1 ユニットに接続されていた。
</div></center>


* [デモ](/conv-demo/index.html){:target="_blank"} 

<center>
<iframe src="/conv-demo/index.html" width="140%" height="640px;" style="border:none;"></iframe>

</center>

#### プーリング Pooling

ネットワークが、ある特定の場所のある特定の色合いのある特定の特徴を探してしまうのは、一番避けたいことです。
これでは良いCNNを作ることはできません。
画像は反転したり、回転したり、つぶれたりしているものがいい。
ネットワークがすべての画像の中からある物体（たとえばヒョウ）を認識できるように、同じものの写真がたくさん必要です。大きさや場所は関係ありません。
照明や斑点の数、そのヒョウが早く眠っているのか、獲物を潰しているのかなどは関係ありません。
空間的な変化が欲しい。柔軟性が必要です。
それがプーリングです。
<!-- The last thing you want is for your network to look for one specific feature in an exact shade in an exact location. 
That’s useless for a good CNN! 
You want images that are flipped, rotated, squashed, and so on. 
You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location. 
No matter what the lighting or the number of spots,or whether that leopard is fast asleep or crushing prey. 
You want **spatial variance**! You want flexibility. 
That’s what pooling is all about. -->

プーリングは，入力表現のサイズを徐々に小さくしていきます。
これにより，画像内のオブジェクトがどこにあっても検出できるようになります。
プーリングは、必要なパラメータの数を減らし、必要な計算量を減らすのに役立ちます。
また **オーバーフィッティング** の抑制にも役立ちます。
<!-- Pooling progressively reduces the size of the input representation. 
It makes it possible to detect objects in an image no matter where they’re located. 
Pooling helps to reduce the number of required parameters and the amount of computation required. 
It also helps control **overfitting**.-->

オーバーフィッティングは，テスト前に情報を理解せずに超具体的な内容を暗記してしまうのと同じようなものです。
<!-- 細かいことを暗記するときは、家でフラッシュカードを使ってやるといいでしょう。 -->
<!-- しかし、実際のテストでは、新しい情報が提示されると失敗してしまいます。 -->
<!-- Overfitting can be kind of like when you memorize super specific details before a test without understanding the information. 
When you memorize details, you can do a great job with your flashcards at home.
You’ll fail a real test, though, if you’re presented with new information.-->


訓練データに含まれるすべての犬に斑点と黒目がある場合，ネットワークは、犬に分類するためには画像に斑点と黒目がなければならないと考えるでしょう。
その学習データを使ってテストをすると、驚くほど正確に犬を分類することができます。
犬を正しく分類することができます。
しかし「犬」と「猫」しか出力されていないネットワークに，ダックスフンドとシベリアンハスキーが写っている画像を新たに入力した場合，ダックスフンドやシベリアンハスキーの画像を猫に分類してしまうことが起こりえます。
<!-- (Another example: if all of the dogs in your training data have spots and dark eyes, your network will believe that for an image to be classified as a dog, it must have spots and dark eyes. 
If you test your data with that same training data, it will do an amazing job of
classifying dogs correctly! But if your outputs are only “dog” and “cat,” and your network is presented with new images containing, say, a Rottweiler and a Husky, it will probably wind up classifying both the Rottweiler and the Husky as cats. You can see the problem!)-->

分散がないと、ネットワークは訓練データと完全に一致しない画像では役に立たなくなります。
**訓練データと検証データは必ず別々にする** 必要があります。
訓練したデータでテストを行うと、ネットワークは情報を記憶してしまいます。
新しいデータを導入すると、ひどい結果になるでしょう。

<!-- Without variance, your network will be useless with images that don’t exactly match the training data. 
**Always, always, always keep your training and testing data separate**! 
If you test with the data you trained on, your network has the information memorized! 
It will do a terrible job when it’s introduced to any new data. 
-->

<!-- ### Overfitting is not cool.

つまり、このステップでは、**特徴地図** を取得し、**プーリング層** を適用して、**プール済特徴地図** を作成します。
So for this step, you take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.-->
-->

プーリングの最も一般的な例は、**最大値プーリング**  (マックスプーリング) です。
最大値プーリングでは入力画像を重ならない領域のセットに分割します。
各エリアの出力は各エリアの最大値となります。
これにより，少ないパラメータで小さなサイズになります。
<!--The most common example of pooling is **max pooling**. 
In max pooling, the input image is partitioned into a set of areas that don’t overlap. 
The outputs of each area are the maximum value in each area. 
This makes a smaller size with fewer parameters. -->


マックスプーリングとは、画像の各スポットで最大値を掴むことです。
これにより，特徴ではない 75％ の情報を取り除くことができます。
ピクセルの最大値を取ることで，歪みを考慮することができます。
特徴が左や右に少し回転しても，プールされた特徴は同じになります。サイズやパラメータを小さくしています。
これは，モデルがその情報に対してオーバーフィットしないことを意味します。
<!-- Max pooling is all about grabbing the maximum value at each spot in the image. 
This gets rid of 75% of the information that is not the feature. 
By taking the maximum value of the pixels, you’re accounting for distortion. 
If the feature rotates a little to the left or right or whatever, the pooled feature will be the same. You’re reducing the size and parameters. 
This is great because it means that the model won’t overfit on that information.-->

<!-- **平均プーリング** または **合計プーリング** を使用することもできますが、一般的な選択肢ではありません。
実際には、最大プーリングの方が両者よりも性能が良い傾向にあります。
最大プーリングでは、最大のピクセル値を取ることになります。
平均プーリングでは、画像のその場所にあるすべてのピクセル値の平均を取ります。
実際には、より小さなフィルターを使ったり、プーリングレイヤーを完全に破棄したりする傾向があります。
これは、積極的な表現サイズの縮小に対応したものです)。 -->
 <!-- You could use **average pooling or sum pooling**, but they aren’t common choices. 
Max pooling tends to perform better than both in practice. 
In max pooling, you’re taking the largest pixel value. 
In average pooling, you take the average of all the pixel values at that spot in the image. 
(Actually, there’s a trend now towards using smaller filters or discarding pooling layers entirely. 
This is in response to an aggressive reduction in representation size.)-->

なぜ最大プーリングを選択するのか、もう少し詳しく見てみましょう。
を選択する理由と，ストライドを 2 画素にする理由をもう少し見てみたいと思いませんか？
[Dominik Scherer et. al, Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf){:target="_blank"} をご覧ください。
<!-- __Want to look a little more at why you might want to choose max pooling
and why you might prefer a stride of two pixels? Check out Dominik
Scherer et. al, [Evaluation of Pooling Operations in Convolutional
Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf).__-->

<!-- 
[ここ](http://scs.ryerson.ca/~aharley/vis/conv/flat.html){:target="_blank"} に行くと、畳み込み層の実に面白い 2D 視覚化をチェックすることができます。
画面の左端のボックスに数字を描き、出力を見てみましょう。
畳み込み層とプール層、そして推測を見ることができます。
1 つの画像の 上にカーソルを置いてみると、フィルターが適用された場所がわかります。 -->
<!-- If you go [here](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) you can check out a really interesting 2D visualization of a convolutional layer. 
Draw a number in the box on the left-hand side of the screen and then really go through the output. 
You can see the  convolved and pooled layers as well as the guesses. 
Try hovering over a single pixel so you can see where the filter was applied.-->

<!-- So now we have an input image, an applied convolutional layer, and an applied pooling layer.

Let’s visualize the output of the pooling layer!

We were here:
-->

<center>
<img src="/assets/output3.jpg" style="width:94%">
</center>
<!-- 
The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps. 
It does this by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.

See how the image has changed size?-->

<center>
<img src="/assets/output4.jpg" style="width:94%">
</center>

<!-- Cool, right?

#### Flattening

This is a pretty simple step. You flatten the pooled feature map into a sequential column of numbers (a long vector). 
This allows that information to become the input layer of an artificial neural network for further processing.


#### Fully connected layer

At this step, we add an **artificial neural network** to our convolutional neural network. 
(Not sure about artificial neural networks? [You can learn about them here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!)


The main purpose of the artificial neural network is to combine our features into more attributes. 
These will predict the classes with greater accuracy. This combines features and attributes that can predict classes better.

At this step, the error is calculated and then backpropagated. 
The weights and feature detectors are adjusted to help optimize the performance of the model. 
Then the process happens again and again and again. 
This is how our network trains on the data! 

How do the output neurons work when there’s more than one?

First, we have to understand what weights to apply to the synapses that connect to the output. 
We want to know which of the previous neurons are important for the output.

If, for example, you have two output classes, one for a cat and one for a dog, a neuron that reads “0” is absolutely uncertain that the feature belongs to a cat. A neuron that reads “1 is absolutely certain that the feature belongs to a cat. 
In the final fully connected layer, the neurons will read values between 0 and 1. 
This signifies different levels of certainty. 
A value of 0.9 would signify a certainty of 90%. 
The cat neurons that are certain when a feature is identified know that the image is a cat. 
They say the mathematical equivalent of, “These are my neurons! I should be triggered!” If this happens many times, the network learns that when certain features fire up, the image is a cat.


Through lots of iterations, the cat neuron learns that when certain features fire up, the image is a cat. 
The dog (for example) neuron learns that when certain other features fire up, the image is a dog. 
The dog neuron learns that for example again, the “big wet nose” neuron and the “floppy ear” neuron contribute with a great deal of certainty to the dog neuron.
It gives greater weight to the “big wet nose” neuron and the “floppy ear” neuron. 
The dog neuron learns to more or less ignore the “whiskers” neuron and the “cat-iris” neuron. 
The cat neuron learns to give greater weight to neurons like “whiskers” and “cat-iris.”
(Okay, there aren’t actually “big wet nose” or “whiskers” neurons. 
But the detected features do have distinctive features of the specific class.)


Once the network has been trained, you can pass in an image and the neural network will be able to determine the image class probability for that image with a great deal of certainty.

The fully connected layer is a traditional Multi-Layer Perceptron. 
It uses a classifier in the output layer. 
The classifier is usually a softmax activation function. 
Fully connected means every neuron in the previous layer connects to every neuron in the next layer. 
What’s the purpose of this layer? To use the features from the output of the previous layer to classify the input image based on the training data.

Once your network is up and running you can see, for example, that you have a 95% probability that your image is a dog and a 5% probability that your image is a cat.


Why do these numbers add up to 1.0? (0.95 + 0.05)

There isn’t anything that says that these two outputs are connected to each other. 
What is it that makes them relate to each other? 
Essentially, they wouldn’t, but they do when we introduce the **softmax function**.
This brings the values between 0 and 1 and makes them add up to 1 (100%). 
(You can read all about this on Wikipedia.) 
The softmax function takes a vector of scores and squashes it to a vector of values between 0 and 1 that add up to 1.

After you apply a softmax function, you can apply the loss function.
Cross entropy often goes hand in hand with softmax. 
We want to minimize the loss function so we can maximize the performance of our
network.

At the beginning of backpropagation, your output values would be tiny.
That’s why you might choose cross entropy loss. 
The gradient would be very low and it would be hard for the neural network to start adjusting in the right direction. 
Using cross entropy helps the network assess even a tiny error and get to the optimal state faster.
-->



<!--
## [TensorFlow HUB](https://www.tensorflow.org/hub){:target="_blank"}

- ドロップアウト，データ拡張，各種正規化: cnn.md
- 有名なモデル LeNet，Alex Net，Inception，VGG，ResNet
- R-CNN，ハイウェイネット，YOLO，SSD
- セマンティックセグメンテーション
- 転移学習，事前学習，ファインチューニング
- GAN

### インセプション Inception，残渣ネット ResNet，領域 R-CNN (Regional Convolutional Neural Networks)
- what and where routes
- 心理学的対応物(？)
  - Cadieu (2014) Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition
  - Nasr, Viswanathan, Nieder (2019) Number detectors spontaneously emerge in a deep neural network designed for visual object recognition
  - Marcus (2018) Deep Learning A Critical Appraisal
- 転移学習

### Notebooks

- [colab/text_classification_with_tf_hub_on_kaggle.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/text_classification_with_tf_hub_on_kaggle.ipynb)
Shows how to solve a problem on Kaggle with TF-Hub.
- [colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
Explores text semantic similarity with the [Universal Encoder Module](https://tfhub.dev/google/universal-sentence-encoder/2).
- [colab/tf_hub_generative_image_module.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb)
Explores a generative image module.
- [colab/action_recognition_with_tf_hub.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb)
Explores action recognition from video.
- [colab/tf_hub_delf_module.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_delf_module.ipynb)
Exemplifies use of the [DELF Module](https://tfhub.dev/google/delf/1) for landmark recognition and matching.
- [colab/object_detection.ipynb](https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb) 
Explores object detection with the use of the  [Faster R-CNN module trained on Open Images v4](https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)
-->

<!--
<center>
<img src='https://cdn-images-1.medium.com/max/1280/1*sS_WZM4GLS88XlnDLKcZ-g.png' style='width:94%'><br>
from [A guide to Face Detection in Python](https://towardsdatascience.com/a-guide-to-face-detection-in-python-3eab0f6b9fc1)
</center>
-->

<!-- - [The Complete Beginner’s Guide to Deep Learning: Convolutional Neural Networks and Image Classification](https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb){:target="_blank"}, Anne Bonner Feb. 02

- 畳込みニューラルネットワーク (Convlutional Neural Networks:CNN) とは画像認識におけるゲームチェンジャー(以後，画像認識，ビデオ分類，自動運転，ドローン，ゲームなどへの応用多数)
- [イメージネット画像コンテスト](http://image-net.org/challenges/LSVRC/){:target="_blank"} では，分類 (classification) 課題と位置 (locallization) 課題とからなる。
- コンテストは 2010 年から Li Fei-Fei さん中心となって [AMT](https://www.mturk.com/) で画像のアノテーションを行って 画像を2012 年の優勝チームが CNN を使った。通称[アレックスネット](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf){:target="_blank"}
- [スタンフォード大学の授業 CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/index.html){:target="_blank"}. 
[スライド](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture05.pdf){:target="_blank"}

# さらなる情報

- Math? [Introduction to Convolutional Neural Networks](https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf) by Jianxin Wu
- C.-C. Jay Kuo [Understanding Convolutional Neural Networks With a Mathematical Model](https://arxiv.org/abs/1609.04112).
- [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)
- [Artificial neural networks? [You can learn about them here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)


## 正規化，正則化，標準化，白色化，二重中心化

- 白色化については平井有三先生のパターン認識が参考文献で良いかな

- [Differences between normalization, standardization and regularization](https://maristie.com/blog/differences-between-normalization-standardization-and-regularization/)

---
-->



## 用語

1. 活性化関数 (シグモイド，ハイパータンジェント，整流線形化)
2. 損失関数 (平均自乗誤差: Mean Squared Errors:MSE, 交差エントロピー:Cross Entropy，負の対数尤度:Negative Log Likelihood)
3. 正則化 (L2, L1, L0, エラスティック)
4. 最適化関数 (SGD, Adam)
5. スキップ結合

### 1. 活性化関数

$y=f(x)$, $f(x)=wx+b$ のような線形結合では，多層化の意味はない。
なぜなら $f_{1}(x) = w_{1}x + b_{1}$, $f_{2}(x)=w_{2}x+b_{2}$ のように考えると，多層化ニューラルネットワークは $y=f_{2}(f_{1}(x))$ のように書くことができる。

### 2. 損失, 誤差，コスト，目的，関数

損失関数，誤差関数，目的関数などと文献によって種々の呼称が存在する。
原語では loss, error, cost, and objective functions などと表記される。
厳密に，これらを区別する場合もある。
だが，初学者のうちは，上記は，ほぼ同じものを指すと考えてよい。

これらの関数は，データとモデルのパラメータとが与えられると，一意に定まる値を与える。
一般に，この値，すなわち損失値，誤差値，目的値が最小となるようなパラメータを探すことが行なわれる。
統計学における線形回帰式のように，解析的にパラメータの値が定まる場合は少ない。
したがって，何らかの方法で，パラメータの探索が行われる。
パラメータの探索方法については，それ自体で研究分野となるほど，興味深い問題である。
この話題は，最適化関数の項で取り上げる。


- 勾配降下法
- バッチ学習，オンライ学習，ミニバッチ，確率的勾配降下法，
- ソフトマックス関数，交差エントロピー誤差


<!-- # 本日の目標
- 最小二乗法から誤差逆伝播法へ。誤差関数，損失関数，目的関数，勾配降下法 (ブラインド ハイカー アナロジー)。 信用割当問題。勾配消失問題。
- 標準正則化理論。制約付き最適化。変分原理 ([オイラー=ラグランジェ方程式](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%82%A4%E3%83%A9%E3%83%BC%EF%BC%9D%E3%83%A9%E3%82%B0%E3%83%A9%E3%83%B3%E3%82%B8%E3%83%A5%E6%96%B9%E7%A8%8B%E5%BC%8F){:target="_blank"} )-->
<!-- - 画像切り分け -->

<!--- 画像切り分け
- ニューラルネットワーク，機械学習の分野で頻繁に用いられている性能向上のための技法を紹介
- この授業の目標は深層学習の心理学的な意味付けを考えることであるので，紹介する上記の技法は無関係のように思われる
- だがそうではないことを理解することが目的
-->

## 多分本日は行わない実習

<!-- - [kminst による CNN](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019keras_kmnist_demo.ipynb){:target="_blank"} -->
- [転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529transfer_learning.ipynb){:target="_blank"}

- [MaskR-CNN によるインスタンス画像領域分割 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529Mask_R_CNN_Image_Segmentation.ipynb){:target="_blank"}
- [Deeplab のデモによる画像の意味的画像切り分け <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529Semantic_segmantation_DeepLab_Demo.ipynb){:target="_blank"}


## 畳み込み演算を利用したニューラルネットワーク

<div align="center">
<!--<img src='https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%">-->
<img src="/assets/Neocognitron.svg" style="width:74%">
<img src="/assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br>
</div>


## LeNet5 (LeCun, 1998)
<center>
<img src="/assets/1998LeCun_Fig2_CNN.svg" style='width:94%'><br>
LeCun (1998) より
</center>

## GooLeNet (Inception) (Szegedy et. al, 2014)

<center>
<img src="/assets/2014Szegedy_GoogLeNet.svg" style='width:99%'><br/>
</center>


## 1. <a name="ml">機械学習の実習</a>

<!-- - [初めての画像認識 <img src="https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/4c5e1c665109926508b3fa505914b60b7237bf62/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_ResNet50_demo.ipynb){:target="_blank"}-->

- [機械学習の超簡単デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0507_3mnists_demo.ipynb){:target="_blank"}


### 1.1. <a name="mnist">3 つのデータセット: MNIST, Fashion MNIST, KMNIST</a> 
- 機械学習分野で頻用されるデータセットとして，手書き数字認識データである MNIST があります。
- MNIST は FAIR (フェイスブック人工知能研究所) 現所長 の Yan LeCun によって作成されました。
NIST とは，アメリカ合衆国版の JIS です。すなわち，標準化機関の手書き数字認識用データセットを 修正した (modified) という意味から MNIST と呼ばれます。
- MNIST は データ数が ７万で，訓練データ数 6 万，テストデータ １ 万からなります。
データは，縦横それぞれ 28 画素からなっています。コンピュータで扱う際に，コンピュータにとってキリの良い 32 画素ではなく，
周囲を切り取ったために，28 画素になっています。
- Fasshion MNIST は， MNIST と同じ画像形式で，ファッション画像，具体的には 10 種類のアパレル画像データです。
- kmnist は日本語のくずし字データセットです。形式は MNIST, Fashion MNIST と同じです。

### 1.2. <a name="dataset">訓練データ，テストデータ，検証データ</a>
* 機械学習では，心理統計で用いられるような 仮説検定を行うこともありますが，むしろ，行わない場合も多いです。
* 理由としては，仮説検定を行うことによりも，モデルの性能を向上させることに主眼があるからという意味合いであろうと考えられます。
* ですが，考え方は母集団統計量の推定と同じような発想をします。すなわち，まだ見ぬ未知のデータに対して精度が良いモデルが優れているモデルと判断されます。
* 訓練データを使ってモデルを作成し，作成したモデルの評価をテストデータを使って評価します。
* このとき，テストデータは訓練には使いません。未知のデータに対しての精度でモデルの性能の優劣を競います。従って，モデルの精度の良いモデルが良いモデルであり，かつ，良いモデルとは，未知のデータに対してより精度が高く動作するモデルとなります。
* この点については，母集団の統計量の優劣を考える心理統計とは異なります。
* 真の母集団という，ありもしない曖昧 (かも知れない) 仮想集団について斟酌するよりも，実際のデータについて精度の優劣でモデルの性能を競うという意味では，実務的な発想と言えるでしょう。
* 機械学習におけるモデルの精度向上を目指したパラメータチューニングのことを **学習** と呼びます。

### 1.3. <a name="overfitting">過学習</a>

* モデルのパラメータを学習するときに，同じデータを用いて性能を検証することは，方法論的に間違っていると言えます。
* すでに見たことのある敵をたおせても，真の勇者とは言えません。何度でも生き返ることができる RPG とは違います。
* 見たことのあるデータ （遭遇した経験のあるモンスター）は倒せるでしょう。ですが，それでは 勇者 ではなく チキン です。
* 経験済のデータについては，完璧なスコアを示すことができるでしょう。ですが，まだ見ぬデータに対して有用な予測をすることはできません。
* このような状況を 過学習 (over-learning) あるいは オーバーフィッティング (over-fitting) といいます。
* これを避けるために、（教師あり）機械学習を行う際には，利用可能なデータの一部を テストデータセット `X_test`, `y_test` として用意しておくのが一般的です。
* 一般に k-hold out 法などと呼ばれる手法は，訓練データセットを ｋ 個に分割します。その上で，k 個に分割した 1 つのデータ群を除いた k-1 群の訓練データを用いてモデルの学習を行います。学習の都度，残しておいたデータを用いて性能を評価します。
* この方法により，最終評価に用いるテストデータを使うこと無くチューニングを行います。
* **なぜ全データを用いないで，データを分割するのか？**
  * 未知の母集団を仮定しないで，モデルの優劣を正当に評価するための方法であるとみなすことができます。

### 1.4. <a name="回帰と分類">回帰と分類</a>
* 機械学習で頻用される手法の分類に **回帰** と **分類** があります。
* 予測すべきデータが連続量の場合は，回帰
* 予測すべきデータが離散量の場合は，分類 と呼ばれます。
* 身長や体重，あるいは，明日の東京都における COVID-19 の感染者数を予測するのであれば 回帰 です。
* 一方，手書き数字認識は，予測すべきデータが 10 分類された各クラスですので 分類 と呼ばれます。
* $\mathbf{y} = \mathbf{Xw} +\mathbf{b}$ などは 線形回帰 と呼ばれます。これは中学校以来の 直線を表す 1次方程式 $y=ax+b$ と同じ形をしています。
* $y$ を予測すべき量，$x$ を与えられたデータと考えます。
* 傾き slope:$a$ と 切片 intercept:$b$ とを推定する問題が 回帰 です。
* 中学校までの数学の知識では，2 点 $(x_1, y_1)$, $(x_2, y_2)$ が与えられたとき，$a$ と $b$ とは計算して求めることが可能でした。
* では，N 個のデータ $(x_1,y_1),\cdots,(x_n,y_n)$ が与えられたとき，切片 と 傾き とはどう定めたら良いのでしょうか？


### 1.5. <a name="precision">モデルの精度を測る指標</a>
* モデルの精度とは，何でしょうか。精度とは，正しく予測できることです。分類課題の場合，
* 正しい予測と誤った予測とには，詳細な検討が必要になります。
* ここでは，精度 とは，英語で precision と accuracy と ２ つあります。日本語ではどちらも精度です。

* **精度 precision**: This computes the proportion of instances predicted as positives that were correctly evaluated (it measures how right our classifier is when it
says that an instance is positive).
* **再現率 recall**: This counts the proportion of positive instances that were correctly evaluated (measuring how right our classifier is when faced with a positive instance).
* **F1 値 F1-score**: This is the harmonic mean of precision and recall, and tries to combine both in a single number

| | 予測: + | 予測: - |
|---|----|----|
|真の値: + | True Positive (ヒット Hit)| False  Negative (ミス Miss) | 
|真の値: - | False Positive (虚報 False alarm)| True Negative (正しい棄却 Correect rejection) |


## 1.6 <a name="supervised_vs_unsupervised">教師あり学習と教師なし学習</a>
* 予測すべき数値に正解が与えられている場合，**教師あり学習 supervised learning** と呼びます。
* 一方，予測すべきデータが与えられていない場合を **教師なし学習 unsupervised learning** と呼びます。
* 手書き数字認識では，正解となるデータが与えられているので，教師あり学習となります。
* 一方で，正解データが与えられていない場合に，入力データを分類したりする場合を 教師なし学習と 呼びます。


### ロジスティック回帰

ロジスティック回帰とは 回帰の名前がついていますが，分類 問題を解くための手法です。
ある事象が生起する確率を $p$ とすれば，生起市内確率は $(1-p)$ と表せます。この確率比のことを **ロジット比** と呼びます。
ロジット比の対数が次式に従うことを仮定するのが，ロジスティック回帰です。

$$
\log\left(\frac{p}{1-p}\right) = e^{x}
$$

上式を解けば，

$$
p(x) = \frac{1}{1+e^{-x}}
$$

この式を **シグモイド関数** sigmoid function と呼びます。

<!-- #### 伏線回収

初回の授業で，COVID-19 の感染者数の変動を記述するモデルを紹介しました。
Kermack McKendrick モデルのポイントは 時刻 $t$ における感染者の増加率 $dp/dt$ は その時の感染者の比率と非感染者の比率 の積に比例する
と仮定することでした。 -->

上式を微分すると，次式を得ます:
$$
\frac{dp}{dt} = \beta p(t)\left(1-p(t)\right)
$$

上式を高等学校数学風味に書き換えると次式のようになります。

$$
y' = \beta y(1-y)
$$

ここでは $p$ を $y$ と書き換えました。
また微分を表す記号を プライム (') にしました。
この式は，高校学校 2 年生の知識で解くことができます。

あまり深入りする必要はありません。
ですが，$y$ を微分した右辺に，$x$ が入っていないことに注意です。

### 勾配降下法

重回帰では解析解が存在しました。一方，非線形問題は一般に解析解が存在しません。
その際に，目的関数を繰り返しによって求める方法があります。
**勾配降下法** gradient descent methods はその一つです。
任意の点 $x$ における関数 $f(x)$ の微分が定義されていれば，求める関数の最小値は次式:

$$
\Delta\theta = \eta\frac{\partial f}{\partial\theta}
$$

を逐次計算することで求めることができると仮定します。
ここで $\theta$  はモデルのパラメータ，$f$ は目的関数，$\eta$ は学習率，$\partial$ は **偏微分** partial differential を表します。


<!--
Authors:    J.A. Anderson, A. Pellionisz, E. Rosenfeld (eds.)
Title:      Neurocomputing 2: Directions for Research
Reference:  MIT Press, Cambridge (1990), Massachusetts

### ANNs are some kind of non-linear statistics for amateurs
-->

<!-- 
## 次の語の示すサイトを訪れ，それぞれどのようなサイトかを調べよ。
いずれも現在のエコシステムとしての役割を果たしている。

1. arXiv: <font color="white">論文置き場</font>
2. Colab: 
3. Github: <font color="white">プログラムのソースコード置き場</font>
4. Stack Oerflow: <font color="white">掲示板，ノウハウ集</font>
5. Reddit: <font color="white">掲示板，ただしビッグネーム本人が降臨することがある</font>
-->

<!-- 
# AI を学ぶ人間のための心構え
- 無知蒙昧から来るブラックボックス的な恐怖を払拭するよう務める(現時点での技術的な裏付けに基づく啓蒙活動)
- 現在の技術から予測できる近未来の展望を語ることを忌避しない(謙遜は美徳ではない)

<center>
<img src="https://blogs-images.forbes.com/markhughes/files/2016/01/Terminator-2-1200x873.jpg" style="width:32%"> 
<img src="http://zatugaku1128.com/wp-content/uploads/2016/09/%E3%83%89%E3%83%A9%E3%81%88%E3%82%82%E3%82%93.png" style="width:20%"></br>
</center>

未来はどっち？ **It will depend on you.**

# クイズ
* 次の語の組み合わせのうち不適切なものを指摘せよ

1. IBM - Watson - Joapady
2. DeepMind - AlphaGo - 囲碁
3. Google 翻訳 - ペッパー
4. Uber - 自動運転
-->

<!--
## 文献

- [労働新聞平成31年2月25日号 知識を拡張する道具 人類の歴史の延長線上に](/2019laborNews.pdf){:target="_blank"}
- [イラストで学ぶ 人工知能概論](https://www.amazon.co.jp/gp/product/4061538233/) (KS情報科学専門書) ([谷口](http://ai.tanichu.com/), 2014)
-->
<!--https://www.amazon.co.jp/gp/product/4061538233/ -->

<!--
- [Cognitive computational neuroscience](https://www.nature.com/articles/s41593-018-0210-5){target="_blank"}
-->
<!--- [Cognitive computational neuroscience](https://arxiv.org/abs/1807.11819)-->

<!--
## 小説，戯曲の中に現れた AI

- マリー・ウォルストンクラフト・シェリー　Mary Wallstoncraft Shelley，
  - フランケンシュタイン Frankenstein, or The Modern Prometheus 
  - [https://www.aozora.gr.jp/cards/001176/files/44904_35865.html](https://www.aozora.gr.jp/cards/001176/files/44904_35865.html){target="_blank"}
- カレル・チャペック　Karel Capek, 
  - ＲＵＲ ―ロッサム世界ロボット製作所 R.U.R. (Rossum's Universal Robots) 
  - [https://www.aozora.gr.jp/cards/001236/files/46345_23174.html](https://www.aozora.gr.jp/cards/001236/files/46345_23174.html){target="_blank"}
- アイザック・アシモフ Issac Asimov, 
  - われはロボット I, Robot 
  - [https://www.amazon.co.jp/dp/4150105359](https://www.amazon.co.jp/dp/4150105359){target="_blank"}
- アーサー・クラーク Arthur C. Clarke, 
  - 2001年宇宙の旅 2001: a Space Odyssey 
  - [https://www.amazon.co.jp/dp/415011000X](https://www.amazon.co.jp/dp/415011000X){target="_blank"}

## 映画 AI 
  - Matrix, Star Wars, Surrogate, ...

## TV anime
  - 鉄腕アトム，がんばれロボコン, ..., ガンダム，エヴァ，

# クイズ
* 小説，戯曲，に現れたロボット，人工知能を年代順に並べよ
1. アーサー・クラーク 2001 年宇宙の旅
2. アイザック・アシモフ われはロボット
3. カレル・チャペック ロボット
4. マリー・シェリー フランケンシュタイン
-->

## 勾配降下法 Gradient descent methods

<center>
<img src="https://miro.medium.com/max/814/1*kmmjFBP5vRkKOM1SP4URpA.png" style="width:33%"><br/>

出典: [The Complete Beginner’s Guide to Deep Learning: Artificial Neural Networks](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)
</center>


### 損失，誤差，目的，および，コスト関数

- コスト関数 cost function
- 損失関数 loss function
- 誤差関数 error function
- 目的関数 objective function

$$
p(\mathbf{y}\vert \mathbf{x};\mathbf{\theta})
$$

**最小二乗誤差**（下式）, あるいは**負の対数尤度** negative log likelifood ($-\log(x)$) など

$$
J(\mathbf{\theta})=\frac{1}{2}\mathbb{E}_{\mathbf{x,y}\sim\hat{p}_{data}}
\left\|\mathbf{y}-f(\mathbf{x};\mathbf{\theta})\right\|^2+\mbox{const.}
$$


### 交差エントロピー損失関数
ニューラルネットワークや機械学習において，予測すべき値が2値化された量，たとえば真偽値真であれば $1$ をとり，偽
であれば $0$ であったり，確率である場合には，最小化すべき目標関数(正則化項を含めて損失関数でもよい)は平均二乗
誤差 Mean Square Errors ではなく **交差エントロピー cross-entropy 損失**，あるいは交差エントロピー誤差と呼ぶ関
数が用いられる。

自乗誤差に比べて交差エントロピーを用いると学習が高速化される。
<!-- 理由は以下で説明する-->
文献的にはニューラルネットワークに交差エントロピーが導入されたのは Hinton(1989) など

交差エントロピーは次式で表される:

$$
\mathcal{L}=-t\log(y)-(1-t)\log(1-y),
$$<!-- {#eq:def-cross-entropy}-->

ここで $t$ は教師信号すなわち $1$ または $0$ をとり，$y$ はニューラルネットワークから出力された予測値。

上式は （確率とみなせる）出力 $y$ が $t$ 回起こった と解釈できる $y^t$ このときの $t$ の値はは $0$ か $1$ しか取らないので，
上式右辺は，もし $t$ が 1 であれば右辺第一項を計算し，$t$ が $0$ であれば 右辺第2項を計算することになる。

右辺第一項と右辺第二項とを別曲線として描いた下図。

<center>
<img src="/assets/cross-entropy.svg" style="width:39%"><br/>
<!--      https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/e69ca10d8b2a4e9f34943fc302e5eafc7dbd934d/assets/cross-entropy.svg-->
交差エントロピーのグラフ
</center>

ここで対数 $\log$ の底は $e$ や $2$ が用いられる。

## エントロピー
エントロピーには熱力学エントロピーと情報論的エントロピーと $2$ 種類存在するがどちらも同じ形式をしている。情報
論的には平均エントロピー $H$ を以下のように定義する

$$
H[X]=-\sum_i X_i\log(X_i)
$$ 

上式 は 平均情報量 [@Shannon1948] とも呼ばれる。連続変量の場合には総和記号 $\sum$ が積分記号 $\int$ となって 
$$
H[x]=-\int x\log(x)\;dx
$$

<center>
<img src="/assets/shannon-entropy.svg" style="width:29%"><br/>
シャノンのエントロピー
</center>

### まとめ

- コスト関数，損失関数，誤差関数，目的関数，はほぼ同じような意味で用いられる
- 代表的なコスト関数として，最小自乗誤差，交差エントロピー誤差，などがある
- 出力が確率で与えられるような問題，たとえば，分類問題などでは交差エントロピー誤差関数が用いられる



## 一般化とオーバーフィッティング，アンダーフィッティング
<!--Generalization, Overfitting and Under-fitting-->

- データへの当てはまりが良いことが良いモデルではない
- 未知のデータに対してどれほど当てはまるのかがモデルの性能を決める
<!--
* 訓練データ training data 実際に学習に用いたデータ
* テストデータ test data 未知のデータ，訓練時には使用していないデータ
-->
* オーバーフィッティング 訓練データへの過剰適合
* アンダーフィッティング 訓練データを十分に学習できない場合
<!--
* データ数(*小*) アンダーフィットする可能性**大**
-->

<center>
<img src="/assets/04_07underOverFittings.svg" style="width:59%"><br/>
</center>

- [多項回帰による過剰適合，デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb)

<!-- It's not a good idea to test a machine learning model on a dataset which we used to train it, since it won't g ive any indication of how well our model performs on unseen data. 
The ability to perform well on unseen data is called generalization, and is the desirable characteristic we want in a model.
When a model performs well on training data (the data on which the algorithm was trained) but does not perform well on test data (new or unseen data), we say that it has overfit the training data or that the model is overfitting. 
This happens because the model learns the noise present in the training data as if it was a reliable  pattern. 
Conversely, when a model does not perform well on training data (i.e. it fails to capture patterns present in the training data) as well as unseen data then it is said to be under-fitting. 
That is, the model is unable to
 capture patterns present in the training data. 
A smaller dataset can significantly increase the chance of overfitting. 
This is because it is much tougher to separate reliable patterns from noise when the dataset is small.[1]
Examples of overfitting and under-fitting-->

$y = w_0 + w_1 x$, 

$y = w_0 + w_1 x_1 + w_2 x_2$, 

$y = w_0 + w_1 x_1 +\cdots + x_nx_n$


<!--
Suppose we have the following dataset (red points in the figure), where we have only one input variable x and one output variable y. 

If we fit y = w0 + w1x to the above dataset, we get the straight line fit as shown above. 
Note that this is not a good fit since it is quite far from many data points. 
This is an example of under-fitting. 

Now, if we add another feature x2 and fit y = w0 + w1x1 + w2x2 then we'll get a curve fit as shown above. 
(Side note: This is still a linear model. 
x2 is a feature, i.e. input. 
The weights are w's and they are interacting linearly with the features x and x2. 
The curve we are fitting is a quadratic curve). 
As you can see, this is slightly better since it passes much closer to the data points above. 

If we keep adding more features we'll get a curve that is more and more complex and that passes through more and more data points. 
Above figure shows an example. 
This is an example of overfitting. 
In this case, we are performing polynomial fitting y = w0 + w1x1 + w2x2 + ... + wdxd.
Even though the fitted curve passes through almost all points, it won't perform well on unseen data. -->

### オーバーフィッティングの回避
<!-- Strategies to Avoid Overfitting

One way to avoid overfitting it to collect more data. 
However, that is not always feasible. 
Below are some other strategies to overcome the problem of overfitting - regularization and cross-validation. -->

### 正則化 Regularization

モデルの複雑さを調整する

<!--
In regularization, we combat overfitting by controlling the model's complexity, i.e. by introducing an additio
nal term in our cost function in-order to penalize large weights. This biases our model to be simpler, where s
impler is weights of smaller magnitude (or even zero). We want to make the weights smaller, because complex mo
dels and overfitting are characterized by large weights. Recall the mean-squared error cost function, 
J(w)=1nn∑i=1(y(xi)−yit)2
-->

### L2 正則化 リッジ回帰 
<!--Regularization or Ridge Regression-->

$$
\text{目的関数} = \text{誤差} + \lambda \left|w\right|^2
$$

<!--
In L2 regularization, a commonly used regularization technique, we add a penalty proportional to the squared m
agnitude of each weight. Our new cost function with L2 regularization is as follows:-
J(w)=1nn∑i=1(y(xi)−yit)2+λ||w2||
where, the first term is the same as in regular linear regression (without any regularization), and the second
 term is the regularization term. λ is a hyper-parameter that we choose and decides the regularization strengh. Larger values of λ imply more regularization, i.e. smaller values for the model parameters. ||w2|| is w12  w22 + ... wd2. 
-->
- L2 正則化はパラメータの絶対値が大きくなると罰則項 pernalty term として作用

<!--
L2 regularization penalizes the larger weights more (since the penalty is proportional to the weight squared).
 For example, reducing w = 10 to w = 9 has a larger effect on the penalty term (102-92) than reducing w = 3 to
 w = 2 (32-22).  
-->
### L1 正則化 Lasso 回帰 <!--Regularization or Lasso Regression-->

$$
\text{目的関数} = \text{誤差} + \lambda\left|w\right|
$$

<!--
In L1 regularization, we the penalty term is λ ||w||. That is, our cost function is:
J(w)=1nn∑i=1(y(xi)−yit)2+λ||w||
-->
<!--
An interesting property of L1 regularization is that model's parameters become sparse during optimization, i.e
. it promotes a larger number of parameters w to be zero. This is because smaller weights are equally penalize
d as larger weights, whereas in L2 regularizations, larger weights are being penalized much more. This sparse 
property is often quite useful. For example, it might help us identify which features are more important for m
aking predictions, or it might help us reduce the size of a model (the zero values don't need to be stored). 
Ordinary least square (which we saw earlier in linear regression) with L2 regularization is known as Ridge Reg
ression and with L1 regularization it is known as Lasso Regression.
Cross Validation and Validation Datasets
-->

### 正則化項

- 簡潔さ原理 simplicity principle L1
- 滑らかさ原理 smoothness principle L2
- 疎性原理 sparseness principle L0

<center>
<img src="/assets/Regularization.svg" style="width:44%"><br/>
</center>

#### 正則化項の影響

<center>
<img src="/assets/2001Hastie_p84.png" style="width:33%">
<img src="/assets/2001Hastie_p89.png" style="width:33%"><br/>
<img src="/assets/2001Hastie_p91.png" style="width:49%"><br/>
</center>
Hastie (2001) より

### まとめ

- アンダーフィッテイングとオーバーフィッティング
- データ数に比べて，推定すべきパラメータが多過ぎ = オーバーフィッティング
- データ数に比べて，推定すべきパラメータが少な過ぎ = アンダーフィッティング
- 正則化 L1, L2, L0, エラスティック
- 正則化項の大きさ $\lambda$ はハイパーパラメータと呼ぶ


## 交差妥当性 cross validation

<!-- is a method for finding the best hyper-parameters of a model. 
For example, in gradient descent, we need to choose a stopping criteria. 
The simplest stopping criteria is to check whether our accuracy is improving on the training dataset. 
However, this is prone to overfitting since the model might be capturing noise present in the training data as reliable patterns. -->

## ホールド・アウト法 Holdout method

データを訓練データと検証データに分割 
<!-- We can overcome this problem by not using the entire training data while training a model. 
Instead we will hold out some data (validation dataset) and we'll train only on remaining data. 
For example, we can split our training dataset into 70/30 and use 70% data for training and 30% data for validation. 
In the above example of gradient descent, now we train our algorithm on the training data, but check whether or not our model is getting better on the validation dataset. 
This is known as the holdout method and it is one of the simplest cross validation methods. 
We can also use the validation data for other types of experimentation. Such as if we want to run multiple experiments where we choose different features to use to train our machine learning model. -->

- k ホールド法 K-fold Cross Validation

データを k 個に分割して, k-1 データで訓練，残りの 1 で検証
<!-- In K-fold cross validation, the dataset is divided into k separate parts. We repeat training process k times. 
Each time, one part is used as validation data, and the rest is used for training a model. 
Then we average the error to evaluate a model. Note that k-fold cross validation increases the computational requirements for training our model by a factor of k.
-->

<!-- The main advantages of k-fold cross validation are that 
1. It is more robust to over-fitting than the holdout method when performing large number of experiments. 
2. It is better to use when the dataset size is small. This is because when performing k-fold cross-validation, we can use a much smaller validation split (say 10% instead of 30%) since we are testing the model on various subsamples of the data being in the 10%.
Leave-one-out cross validation is a special instance of k-fold cross validation in which k is equal to the number of data points in the dataset. 
Each time, we hold out a single data point and train a model on rest of the data. 
We use the single data point to test our model. Then we calculate the average error to evaluate a model. -->


- 初期停止 early stopping

オーバーフィッティングを避ける方法の一つ: 学習打ち切り基準

<center>
<img src="/assets/04_07earlyStopping.svg" style="width:33%"><br/>
</center>



## SGD は SDG に貢献できるのか？

報道などで昨今耳にする SDG 持続可能な成長目標 ですが，大変紛らわしいことに，ニューラルネットワーク，機械学習の分野では SGD があります。

同じ ３ 文字で同じ文字で，順番が異なるだけでややこしいですが， SGD は 確率的勾配降下法 Stochastic Gradient Descent methods のことです。
レオン・ボットーらを中心に，

前回までと同様に，この授業では，損失関数，目標関数，誤差関数，を区別せずに用います。
ニューラルネットワークに限らず最適化手法として，これら関数の最大化，もしくは最小化を行うことを学習と呼びます。



<center>
<img src='/assets/2014Imgur_Saddle_point.gif' style='width:33%'>
<img src='/assets/2014Imgur_Beales_function.gif' style='width:33%'>
<img src='/assets/2014Imgur_Long_Valley.gif' style='width:33%'>
</center>


```python
import IPython
isColab = 'google.colab' in str(IPython.get_ipython())
if isColab:
    !pip install japanize_matplotlib

import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib
%config InlineBackend.figure_format = 'retina'

# 様々な出力関数の描画
x = np.linspace(-3, 4, 100)                                        # 定義域 x の設定，本例の場合 [-3,4) を 100 刻み
relu = lambda x: np.maximum(0, x)                                  # 整流線形化関数 ReLU の定義
leaky_relu = lambda x: np.maximum(0, x) + 0.1 * np.minimum(0, x)   # リーキー ReLU の定義
elu = lambda x: (x > 0)*x + (1 - (x > 0)) * (np.exp(x) - 1)        # elu の定義
sigmoid = lambda x: (1+np.exp(-x))**(-1)                           # シグモイド関数の定義

def softmax(w, t = 1.0):                                           
    """ソフトマックス関数の定義"""
    e = np.exp(w)
    dist = e / np.sum(e)
    return dist

x_softmax = softmax(x)

plt.figure(figsize=(8,6))
plt.plot(x, relu(x), label='ReLU', lw=2)
plt.plot(x, leaky_relu(x), label='Leaky ReLU',lw=2)
plt.plot(x, elu(x), label='Elu', lw=2)
plt.plot(x, sigmoid(x), label='シグモイド関数',lw=2)
plt.legend(loc=2, fontsize=16)
plt.title('様々な活性化関数', fontsize=20)
plt.ylim([-2, 4])
plt.xlim([-3, 3])
plt.show()
```

## 整流線型ユニット ReLU (Recutified Linear Unit)

**整流線型ユニット ReLU** とは，ニューラルネットワークの活性化関数の一つです。
シグモイド関数や，ハイパータンジェント関数に比べて，極端に単純な形をしています。
駄菓子菓子，生理学との対応についても根拠を持っています。

<!-- The **ReLU** (rectified linear unit) layer is another step to our convolution layer. 
You’re applying an activation function onto your feature maps to increase non-linearity in the network. 
This is because images themselves are highly non-linear! 
It removes negative values from an activation map by setting them to zero.

Convolution is a linear operation with things like element wise matrix
multiplication and addition. 
The real-world data we want our CNN to learn will be non-linear. 
We can account for that with an operation like ReLU. 
You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy.

Want to dig deeper? Try Kaiming He, et al. [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).

If you need a little more info about [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!


Here’s how our little buddy is looking after a ReLU activation function turns all of the negative pixel values black


```python
viz_layer(activated_layer)
```

<center>
<img src="https://komazawa-deep-learning.github.io/assets/output2.jpg" style="width:84%">
</center>
-->


<center>
<img src='/assets/2013Uijings_Selective_Search_Fig1.svg' style='width:94%'><br>
空間ピラミッド (2015) より
</center>



### イメージネットコンテスト，アレックスネットの出力にみる問題点

<center>
<img src="/assets/2012AlexNetResult0.svg" style="width:33%">
<img src="/assets/2012AlexNetResult.svg" style="width:33%">
<div style="text-align: left;width:66%; background-color: cornsilk;">

アレックスネットの結果: 画像のすぐ下の英単語は正解ラベルを表す。Krizensky et. al (2012) Fig. 4 より。
ピンク色は正解ラベルの確率を表す。ブルーは不正解ラベル判断確率を表している。
チェリーが正解であるが，画像を見る限り，第一回答候補のダルマチアンを正解だと考えても問題は無いと考えられる。
</div>
</center>

### 画像切り出し

1. 物体位置
3. 物体認識 object recognition
2. 意味的切り出し semantic segmentation
4. 対象切り出し instance segmentation
5. 特徴点抽出 keypoint
6. パノプティック切り出し

<div align="center">
<img src="/assets/2017DangHa_History_Of_Object_Recognition_ja.svg" style="width:99%"><br/>
Dang and Ha (2017) より
</div>


<!-- # 転移学習

<center>
<img src="/assets/2017Li_Deeper_Broader_fig1ja.svg" style="width:84%"><br/>
</center>
-->

- [活性化関数](/activation_functions/)

<center>
<img src='/assets/2019Inception_screenshot.png' style='width:48%'><br>
<div style="text-align: left;width: 88%;background-color: cornsilk;">

映画インセプションのスクリーンショット。
 
<!-- [Netflix](https://www.netflix.com/watch/70131314?trackId=14170286&tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT){target="_blank"} <br/>
<https://www.netflix.com/watch/70131314?trackId=14170286&tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT>
<br/>
 -->
『インセプション』（原題: Inception）は、クリストファー・ノーラン監督・脚本・製作による2010年のアメリカのSFアクション映画。
第83回アカデミー賞では作品賞、脚本賞、撮影賞、視覚効果賞、美術賞、作曲賞、音響編集賞、録音賞の8部門にノミネートされ、撮影賞、視覚効果賞、音響編集賞、録音賞を受賞した。全米脚本家組合賞ではオリジナル脚本賞を受賞した。
[日本語ウィキペデイアより](https://ja.wikipedia.org/wiki/%E3%82%A4%E3%83%B3%E3%82%BB%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3){target="_blank"}
</div></center>

<center>
<img src='/assets/Inception3.svg' style="width:94%"></br>
<img src='/assets/2015GoogLeNet_Inception.svg' style="width:74%"></br>
Inception モジュール
</div>
</center>

<!-- <center>
<img src='/assets/2014Cadieu_Fig3.svg' style='width:74%'>
</center>
 -->

