<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Shin Asakawa">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>第 2 回 - 2020駒澤大学心理学特講IIIA</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet">
  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet">
  <link href="../css/specific.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u7b2c 2 \u56de";
    var mkdocs_page_input_path = "lect02.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> 2020駒澤大学心理学特講IIIA</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00check_meet/">第 0 回 事前確認</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00guidance/">第 0 回 ガイダンス</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect01/">第 1 回 05 月 08 日</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">第 2 回</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#_1">簡単な歴史</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#_2">実習</a></li>
    

    <li class="toctree-l2"><a href="#cognitive-computational-neuroscience">認知計算論的神経科学 Cognitive computational neuroscience</a></li>
    

    <li class="toctree-l2"><a href="#_3">ニューラルネットワークの歴史</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#1">第 1 次ニューロブーム</a></li>
        
            <li><a class="toctree-l3" href="#rosenblatt">ローゼンブラット Rosenblatt のパーセプトロン</a></li>
        
            <li><a class="toctree-l3" href="#2">第 2 次ニューロブーム</a></li>
        
            <li><a class="toctree-l3" href="#3">第 3 次ニューロブーム</a></li>
        
            <li><a class="toctree-l3" href="#_4">危惧</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#_7">キーワード</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#tensorflow-hub">TensorFlow HUB</a></li>
        
            <li><a class="toctree-l3" href="#inception-resnet-r-cnn-regional-convolutional-neural-networks">インセプション Inception，残渣ネット ResNet，領域 R-CNN (Regional Convolutional Neural Networks)</a></li>
        
            <li><a class="toctree-l3" href="#_8">さらなる情報</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect03/">第 3 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect04/">第 4 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect05/">第 5 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect06/">第 6 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect07/">第 7 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect08/">第 8 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect09/">第 9 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">付録</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../eco/">エコシステム</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_modules/">Python 上の標準的なライブラリの利用</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_intro/">Colabratory 導入</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_faq/">Colaboratory FAQ</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_numpy_intro_ja/">Python の基礎</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">2020駒澤大学心理学特講IIIA</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>第 2 回</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="iiia"><a href="https://komazawa-deep-learning.github.io/">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a><a class="headerlink" href="#iiia" title="Permanent link">&para;</a></h1>
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: XX/XX/2020<br/>
Appache 2.0 license<br/>
</div>

<p>
<script type="math/tex">\ldots</script> 工事中 <script type="math/tex">\ldots</script>
</p>
<h2 id="_1">簡単な歴史<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th></th>
<th>AI</th>
<th>Neural Networks</th>
<th>Psychology</th>
</tr>
</thead>
<tbody>
<tr>
<td>第一次</td>
<td>symbolic</td>
<td>Perceptron</td>
<td>Cognitive revolution</td>
</tr>
<tr>
<td></td>
<td>toy problems</td>
<td>ADALINE</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Neocognitron</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Asociatron</td>
<td></td>
</tr>
<tr>
<td>第二次</td>
<td>Expert systems</td>
<td>Back prop</td>
<td>Connectionist</td>
</tr>
<tr>
<td></td>
<td>Brooks</td>
<td>RNN</td>
<td>Neuro Imaging</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RL(Sutton &amp; Burto)</td>
<td>computational approach</td>
</tr>
<tr>
<td>第三次</td>
<td></td>
<td>Deep Learning</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>CNN</td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<h1 id="_2">実習<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="https://playground.tensorflow.org/">Google Neural Networks Playground</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/nothotdog.ipynb">nothotdog 体感デモ</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_miniumXOR_exercise00.ipynb">XOR</a></li>
<li><a href="https://drive.google.com/open?id=1aIXgVtd2CCE5tMpXD2J9fbX1j8xgk1Kf">畳込みニューラルネットワークによる日本語古典籍くずし字 kminst の認識実習</a>
&lt;!--- <a href="../2019OX_classifier.html">○☓分類器</a></li>
<li><a href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html">pose</a></li>
<li><a href="../nothotdog/">nothotdog</a>
--&gt;</li>
</ul>
<hr />
<h1 id="cognitive-computational-neuroscience">認知計算論的神経科学 Cognitive computational neuroscience<a class="headerlink" href="#cognitive-computational-neuroscience" title="Permanent link">&para;</a></h1>
<p><center>
<img src="../assets/2018Kriegeskorte_Fig2.jpg" style="width:84%"><br>
<strong>Kriegeskorte and Doglas (2018) Fig. 2より</strong>
</center></p>
<p>脳の機能を理解することはどういうことか？ 認知計算神経科学の目的は実世界の認知課題を遂行可能で，生物学的妥当性を持つ計算モデルを用いて動物や人間の神経活動と行動の多くの観測結果を説明することである。
歴史的には各分野（円）はこれら課題の回問題（白ラベル）に取り組んできた。<!--認知計算神経科学は、同時にすべての基準を満たすよう努めるています。-->
<!--
Figure 2 | What does it mean to understand how the brain works? The goal of cognitive computational neuroscience is to explain rich measurements of neuronal activity and behavior in animals and humans by means of biologically plausible computational models that perform real-world cognitive tasks. Historically, each of the disciplines (circles) has tackled a subset of these challenges (white labels). Cognitive computational neuroscience strives to meet all the criteria simultaneously.
--></p>
<h1 id="_3">ニューラルネットワークの歴史<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h1>
<p><center>
<img src='../assets/imagenet_result2017.png' style='width:74%'><br/>
画像認識の進歩
</center></p>
<p><center>
    <img src="../assets/2019Jat_Mitchell_fig1.svg" style="width:74%"><br/>
    <img src="../assets/2019Jat_Mitchell_fig4.svg" style="width:84%"><br/>
</center></p>
<h2 id="1">第 1 次ニューロブーム<a class="headerlink" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="1950">1950年代:<a class="headerlink" href="#1950" title="Permanent link">&para;</a></h3>
<ul>
<li>ウォーレン・マッカロックとワイルダー・ピッツによる <strong>形式ニューロン</strong> の提案
(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)</li>
</ul>
<p><center>
<img src='../assets/mcculloch.jpg' style="width:38%">
<img src='../assets/pitts.jpg' style='width:50%'><br>
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center></p>
<p>形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)</p>
<p>
<script type="math/tex; mode=display">
y_i=\phi\left(\sum_jw_{ij}x_j\right),\label{eq:formal_neuron}
</script>
</p>
<p>ここで <script type="math/tex">y_i</script> は <script type="math/tex">i</script> 番目のニューロンの出力，<script type="math/tex">x_j</script> は <script type="math/tex">j</script> 番目のニューロンの出力，<script type="math/tex">w_{ij}</script> はニューロン <script type="math/tex">i</script> と <script type="math/tex">j</script> との間の <strong>シナプス結合荷重</strong>。
<script type="math/tex">\phi</script> は活性化関数。</p>
<p><center>
<img src='../assets/Formal_r.svg' style="width:84%"><br>
形式ニューロン
</center></p>
<hr />
<h2 id="rosenblatt">ローゼンブラット Rosenblatt のパーセプトロン<a class="headerlink" href="#rosenblatt" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/rosenblatt.jpg' style="width:49%"><br>
フランク・ローゼンブラット
</center></p>
<!--
$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

<p><center>
<img src='../assets/perceptron.png' style="width:74%"></br>
パーセプトロンの模式図 ミンスキーとパパート「パーセプトロン」より
</center></p>
<p><center>
<img src='../assets/Neuron_Hand-tuned.png' style="width:69%"></br>
ニューロンの模式図 wikipedia より
</center></p>
<!--
##  人工ニューロン

<center>
<img src='../assets/neuron.png' style="width:49%"><br>

<img src='../assets/neuron_model.jpeg' style="width:49%"<br>
</center>
-->

<!--
## パーセプトロンの学習

$$
\mathbf{w}\leftarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
パーセプトロン perceptron は 3 層の階層型ネットワークでそれぞれ
S(sensory layer), A(associative layer), R(response layer) と呼ぶ。
$S\rightarrow A \rightarrow R$ のうち パーセプトロンの本質的な部分は
$A\rightarrow R$ の間の学習にある。

入力パターンに $P^+$ と $P^-$ とがある。
パーセプトロンは $P^+$ が入力されたとき $1$, $P^-$ のとき $0$ を出力する
機械である。
出力層($R$) の $i$ 番目のニューロンへの入力(膜電位の変化) $u_i$は
\begin{equation}
 u_i = \sum_j w_{ij}x_j - \theta_i = \left(w\right)_i\cdot\left(x\right)_i-\theta_i.\label{eq1}
\end{equation}
ここで中間層($A$)の $j$ 番目のニューロンの出力 $y_i$とこのニューロンとの
結合係数を$w_{ij}$、しきい値を$\theta_i$ とした。
このニューロンの出力$y_i$(活動電位、スパイク)は、

\begin{equation}
y_i = \lceil u_i\rceil
\qquad\left\{
\begin{array}{ll}
 1 & \mbox{if $u_i \ge 0$,}\\
 0 & \mbox{otherwize}
\end{array} \right.
\end{equation}

と表される。
-->

<!--
式(\ref{eq1})の意味を理解するために以下の図を参照

%
\footnote{
Minsky and Papert はパーセプトロンのベクトル表示について
悲観的な考え方を持っているようですが、ここでは理解のしやすさを
優先します。}%
$$
\mathbf{w}\rightarrow\mathbf{w}+\left(y-\hat{y}\right)\mathbf{x}
$$
-->

<hr />
<ul>
<li>1960 年，ミンスキーとパパートの批判</li>
<li>第一次氷河期の到来</li>
</ul>
<hr />
<h2 id="2">第 2 次ニューロブーム<a class="headerlink" href="#2" title="Permanent link">&para;</a></h2>
<ul>
<li>1986 年，PDP ブック，俗に言うバイブル，発表</li>
<li>1989 年，バプニック，サポートベクターマシン発表</li>
<li>第二次氷河期の到来</li>
</ul>
<!--
Authors:    J.A. Anderson, A. Pellionisz, E. Rosenfeld (eds.)
Title:      Neurocomputing 2: Directions for Research
Reference:  MIT Press, Cambridge (1990), Massachusetts

### ANNs are some kind of non-linear statistics for amateurs
-->

<hr />
<h2 id="3">第 3 次ニューロブーム<a class="headerlink" href="#3" title="Permanent link">&para;</a></h2>
<!--
![大規模画像認識チャレンジの結果](./assets/ilsvrc2015.svg){#fig:ilsvrc2015 style="width:49%"}
-->

<ul>
<li>2013 ICLR スタート arXiv.org に予め論文を投稿，誰でも読める，誰でも批判できる。著者はそれに答えなければならない。あっという間にトップカンファレンスとなる</li>
<li>2013 Mikolov word2vec を発表</li>
</ul>
<p><center>
<img src='../assets/Mikolov_analogy.png' style='width:94%'><br>
Mikolovの類推課題
</center></p>
<ul>
<li>2013 DeepMind DQN を発表</li>
</ul>
<!--
<center>
<div class="row post-image-bg" markdown="0">
<video width="49%" autoplay loop markdown="0"> 
<source src="../assets/2015Mnih_DQN-Nature_Video1.mp4" type="video/mp4" markdown="0">
</video>
</div>

<video width="24%" markdown="0">
<source src="../assets/2015Mnih_DQN-Nature_Video2.mp4" type="video/mp4" markdown="0">
</video>
</div>
</center>
-->

<p><center>
<iframe width="320" height="400" src="../assets/2015Mnih_DQN-Nature_Video1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="320" height="400" src="../assets/2015Mnih_DQN-Nature_Video2.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
</center></p>
<hr />
<p><center>
<img src='../assets/2015Mnih_DQNFig.png' style='width:84%'><br>
DQNの結果
</center></p>
<!--
<center>
<div class="row post-image-bg" markdown="0">
<video width="49%" markdown="0">
<source src="/assets/MOV_0013s.mp4" type="video/mp4" markdown="0">
</video>
</center>

<video width="49%" markdown="0">
<source src="/assets/MOV_0071s.mp4" type="video/mp4" markdown="0">
</video>
<video width="49%" markdown="0">
<source src="/assets/MOV_0072s.mp4" type="video/mp4" markdown="0">
</video>
-->

<!--- <a href="../assets/MOV_0013.mp4" target="_blank">ギャラガ 1</a>-->

<ul>
<li><a href="../assets/MOV_0071s.mp4" target="_blank">ギャラガのデモ</a>
<!--- <a href="../assets/MOV_0013s.mp4" target="_blank">ギャラガ 3</a>--></li>
</ul>
<!--
<iframe width="640" height="400" src="../assets/MOV_0013s.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>

<iframe width="640" height="400" src="../assets/MOV_0071s.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>

<iframe width="640" height="400" src="../assets/MOV_0072s.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
-->

<hr />
<ul>
<li>2014 Neural Image Captioning が注目を集める。</li>
</ul>
<p><center>
<img src="../assets/17VISIOn-slide-WBE2-jumbo.jpg" style="width:84%"><br>
</center></p>
<ul>
<li>Human: A group of men playing Frisbee in the park.</li>
<li>Machine: A group of young people playing a game of Frisbee.</li>
</ul>
<!--
![Vinyals et. al (2014) より](./assets/2014Vinyals_Fig5_left.jpg){#fig:NIC2 style="width:49%"}<br>

![Vinyals et. al (2014) より](./assets/2014Vinyals_Fig5_right.jpg){#fig:NIC3 style="width:49%"}
-->

<hr />
<ul>
<li>2015 画像生成技術が注目を浴びる</li>
</ul>
<p><img alt="天安門前広場の夢(撮影は自民解放軍の兵士に依頼した)" id="fig:deep_dream" src="../assets/Tenn_deepdream.jpg" style="width:49%" /></p>
<!-- 
- 2015 ディープラーニング，機械学習，ビッグデータ あるいはその心理学，発刊
-->

<ul>
<li>2015 人工知能学会が日本では「<span style="Color:Lime">深層学習</span>」と呼ぶことに決定する</li>
</ul>
<hr />
<ul>
<li>2016 GAN が注目を浴びる</li>
</ul>
<p><center>
<img src="../assets/2016Reed_GAN_Text2Image1.svg" style="width:84%"><br>
Generative Adversarial Text to Image Synthesis <arXiv:1605.05396v2>
</center></p>
<p><center>
<img src="../assets/2016Reed_GAN_Text2Image.svg" style="width:84%"><br>
Generative Adversarial Text to Image Synthesis arXiv:1605.05396v2
</center></p>
<hr />
<ul>
<li>2016 アメリカ合州国大統領候補の一人の発言を模倣する「ディープトランプ」がツィッター上で注目を集める</li>
</ul>
<p><center>
<img src="../assets/DeepTrumpf.jpg" style="width:39%">
<img src="../assets/DeepTrumpf2.png" style="width:59%"><br>
<img src="../assets/DeepTrumpfTweet.png" style="width:99%"></br>
</center></p>
<ul>
<li>2016 アルファ碁がイ・セドルを破る</li>
</ul>
<p><center>
<img src="../assets/2016AlphaGo_Fig1a.svg" style="width:84%"></br>
アルファ碁 Natureより
</center></p>
<hr />
<h2 id="_4">危惧<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://www.kantei.go.jp/jp/singi/tougou-innovation/dai4/siryo1-1.pdf">ＡＩ戦略（有識者提案）及び人間中心のＡＩ社会原則（案）について</a><ul>
<li><a href="https://www.gov-online.go.jp/cam/s5/">ソサイエティ 5.0</a> これ自体がドイツの<a href="https://en.wikipedia.org/wiki/Industry_4.0">インダストリー 4.0</a> のパクリ
  <center>
  <img src='https://upload.wikimedia.org/wikipedia/commons/c/c8/Industry_4.0.png' style='width:74%'>
  </center></li>
<li><a href="https://autonomousweapons.org/">Ban Lethal Autonomous Weapons</a>
  <center>
<iframe width="640" height="480" src="https://www.youtube.com/embed/LVwD-IZosJE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center></li>
</ul>
</li>
<li><a href="https://futureoflife.org/ai-principles-japanese/">アシロマ原理</a></li>
</ul>
<p><center>
<iframe width="640" height="360" src="https://www.youtube.com/embed/E5KAb86U780" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center></p>
<!--
<center>
    <iframe width="480" height="300" src="https://www.youtube.com/embed/LVwD-IZosJE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>
-->

<!--
<center>
<video controls loop>
  <source src="../assets/A Style-Based Generator Architecture for Generative Adversarial Networks.mp4" type="video/mp4" style="width:64%">
</video>
</center>
-->

<hr />
<h3 id="_5">ボストン・ダイナミクス社ビデオ<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<!--<iframe width="200" height="120" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<iframe width="400" height="300" src="https://www.youtube.com/embed/rVlhMGQgDkY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="400" height="300" src="https://www.youtube.com/embed/tf7IEVTDjng" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<hr />
<iframe width="640" height="400" src="https://www.youtube.com/embed/8vIT2da6N_o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p><br>
source: <a href="https://www.youtube.com/embed/8vIT2da6N_o">https://www.youtube.com/embed/8vIT2da6N_o</a></p>
<p><center>
<img src="/assets/2018Chen_CartoonGAN.svg' style="width:94%">
</center></p>
<p><center>
<iframe width="640" height="480" src="https://www.youtube.com/embed/fRj34o4hN4I" <!--https://www.youtube.com/embed/WcbGRBPkrps"--> frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;</iframe><br>
source: <a href="https://www.youtube.com/embed/fRj34o4hN4I">https://www.youtube.com/embed/fRj34o4hN4I</a>
</center></p>
<!-- <center>
<iframe width="635" height="358" src="../assets/A Style-Based Generator Architecture for Generative Adversarial Networks.mp4"  frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center> -->

<iframe width="805" height="453" src="https://www.youtube.com/embed/WcbGRBPkrps" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<hr />
<h3 id="_6">フェイク画像の生成<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p><center>
<iframe width="640" height="480" src="https://www.youtube.com/embed/o46fcRl2yxE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center></p>
<iframe width="805" height="453" src="https://www.youtube.com/embed/G06dEcZ-QTg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p><br>
source: <a href="https://youtu.be/G06dEcZ-QTg">https://youtu.be/G06dEcZ-QTg</a></p>
<h1 id="_7">キーワード<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h1>
<ul>
<li>ニューラルネットワーク neural networks</li>
<li>パーセプトロン perceptron </li>
<li>誤差逆伝播法 back propagation </li>
<li>勾配降下法 gradient descent methods</li>
<li>ソフトマックス関数 softmax function</li>
<li>交差エントロピー cross entropy</li>
<li>確率的勾配降下法 stochastic gradient descent</li>
</ul>
<p><center>
<img src='../assets/1994vanEssen_gallant_Fig2.jpg' style='width:74%'></br>
<strong>Van Essen &amp; Gallant (1994)</strong></p>
<p><img src='../assets/thorpe_sj_01_260.jpg' style='width:49%'>
<img src='../assets/1991Felleman_VanEssen_Fig4.svg' style='width:49%'></br>
<strong>Left: Thorpe et al.(2001), Right: Felleman &amp; Van Essen (1992)</strong>
</center></p>
<p><center>
<img src='../assets/2017Goodfellow_Fig1_4ja.svg' style="width:84%"></br>
<strong>Goodfellow et al. (2017) Fig.1 を改変</strong>
</center></p>
<p><center>
<img src='../assets/2013LeCun-tutorial-icml_9.svg' style="width:84%"></br>
<strong>LeCun (2013) より</strong>
</center></p>
<p><center>
<img src='../assets/2013LeCun-tutorial-icml_10.svg' style="width:84%"></br>
<strong>LeCun (2013) より</strong>
</center></p>
<p><center>
<img src='../assets/2013LeCun-tutorial-icml_15.svg' style="width:84%"></br>
<strong>LeCun (2013) より</strong>
</center></p>
<p><center>
<img src='../assets/2012Ng_ML_and_AI.svg' style="width:84%"></br>
<strong>Goodfellow et al. (2017) Fig.1 を改変</strong>
</center></p>
<p><center>
<img src='../assets/2015Xu_Show_Attend_and_Tell.svg' style="width:84%"></br>
</center></p>
<hr />
<h2 id="tensorflow-hub"><a href="https://www.tensorflow.org/hub">TensorFlow HUB</a><a class="headerlink" href="#tensorflow-hub" title="Permanent link">&para;</a></h2>
<h2 id="inception-resnet-r-cnn-regional-convolutional-neural-networks">インセプション Inception，残渣ネット ResNet，領域 R-CNN (Regional Convolutional Neural Networks)<a class="headerlink" href="#inception-resnet-r-cnn-regional-convolutional-neural-networks" title="Permanent link">&para;</a></h2>
<ul>
<li>what and where routes</li>
<li>心理学的対応物(？)</li>
<li>/2015documents/2014Cadieu_Deep_Neural_Networks_Rival_the_Representation_of_Primate_IT_Cortex_for_Core_Visual_Object_Recognition.pdf</li>
<li>/2019documents/2019NasrViswanathanNieder_Number_detectors_spontaneously_emerge_in_a_deep_neural_network_designed_for_visual_object_recognition.pdf</li>
<li>/2018documents/2018Marcus_Deep_Learning_A_Critical_Appraisal.pdf</li>
<li>転移学習</li>
</ul>
<h3 id="notebooks">Notebooks<a class="headerlink" href="#notebooks" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/text_classification_with_tf_hub_on_kaggle.ipynb">colab/text_classification_with_tf_hub_on_kaggle.ipynb</a>
Shows how to solve a problem on Kaggle with TF-Hub.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb">colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb</a>
Explores text semantic similarity with the <a href="https://tfhub.dev/google/universal-sentence-encoder/2">Universal Encoder Module</a>.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb">colab/tf_hub_generative_image_module.ipynb</a>
Explores a generative image module.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb">colab/action_recognition_with_tf_hub.ipynb</a>
Explores action recognition from video.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_delf_module.ipynb">colab/tf_hub_delf_module.ipynb</a>
Exemplifies use of the <a href="https://tfhub.dev/google/delf/1">DELF Module</a> for landmark recognition and matching.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">colab/object_detection.ipynb</a> 
Explores object detection with the use of the  <a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">Faster R-CNN module trained on Open Images v4</a>.</li>
</ul>
<hr />
<!--
<center>
<img src='https://cdn-images-1.medium.com/max/1280/1*sS_WZM4GLS88XlnDLKcZ-g.png' style='width:94%'><br>
from [A guide to Face Detection in Python](https://towardsdatascience.com/a-guide-to-face-detection-in-python-3eab0f6b9fc1)
</center>

-->

<ul>
<li>
<p><a href="https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb">The Complete Beginner’s Guide to Deep Learning: Convolutional Neural Networks and Image Classification</a>, Anne Bonner Feb. 02</p>
</li>
<li>
<p>畳込みニューラルネットワーク (Convlutional Neural Networks:CNN) とは画像認識におけるゲームチェンジャー(以後，画像認識，ビデオ分類，自動運転，ドローン，ゲームなどへの応用多数)</p>
</li>
<li><a href="http://image-net.org/challenges/LSVRC/">イメージネット画像コンテスト</a>では，分類 (classification) 課題と位置 (locallization) 課題とからなる。</li>
<li>コンテストは 2010 年から Li Fei-Fei さん中心となって <a href="https://www.mturk.com/">AMT</a> で画像のアノテーションを行って 画像を2012 年の優勝チームが CNN を使った。通称<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">アレックスネット</a></li>
<li><a href="http://cs231n.stanford.edu/index.html">スタンフォード大学の授業 CS231n: Convolutional Neural Networks for Visual Recognition</a>. スライド](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture05.pdf)</li>
</ul>
<h2 id="_8">さらなる情報<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<ul>
<li>Math? <a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu</li>
<li>C.-C. Jay Kuo <a href="https://arxiv.org/abs/1609.04112">Understanding Convolutional Neural Networks With a Mathematical Model</a>.</li>
<li><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">the absolute basics of activation functions, you can find that here</a>!</li>
<li>Artificial neural networks? <a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">You can learn about them here</a>!)</li>
</ul>
<!--
#### ReLU layer
The **ReLU** (rectified linear unit) layer is another step to our convolution layer. 
You’re applying an activation function onto your feature maps to increase non-linearity in the network. 
This is because images themselves are highly non-linear! 
It removes negative values from an activation map by setting them to zero.

Convolution is a linear operation with things like element wise matrix
multiplication and addition. 
The real-world data we want our CNN to learn will be non-linear. 
We can account for that with an operation like ReLU. 
You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy.
-->

<!--
Want to dig deeper? Try Kaiming He, et al. [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).

If you need a little more info about [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!
-->

<!--
Here’s how our little buddy is looking after a ReLU activation function
turns all of the negative pixel values black



wzxhzdk:0


<center>
<img src="../assets/output2.jpg" style="width:84%">
</center>


#### Pooling
The last thing you want is for your network to look for one specific feature in an exact shade in an exact location. 
That’s useless for a good CNN! 
You want images that are flipped, rotated, squashed, and so on. 
You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location. 
No matter what the lighting or the number of spots,or whether that leopard is fast asleep or crushing prey. 
You want **spatial variance**! You want flexibility. 
That’s what pooling is all about.

Pooling progressively reduces the size of the input representation. 
It makes it possible to detect objects in an image no matter where they’re located. 
Pooling helps to reduce the number of required parameters and the amount of computation required. 
It also helps control **overfitting**.

Overfitting can be kind of like when you memorize super specific details before a test without understanding the information. 
When you memorize details, you can do a great job with your flashcards at home.
You’ll fail a real test, though, if you’re presented with new information.

(Another example: if all of the dogs in your training data have spots and dark eyes, your network will believe that for an image to be classified as a dog, it must have spots and dark eyes. 
If you test your data with that same training data, it will do an amazing job of
classifying dogs correctly! But if your outputs are only “dog” and “cat,” and your network is presented with new images containing, say, a Rottweiler and a Husky, it will probably wind up classifying both the Rottweiler and the Husky as cats. You can see the problem!)


Without variance, your network will be useless with images that don’t exactly match the training data. 
**Always, always, always keep your training and testing data separate**! 
If you test with the data you trained on, your network has the information memorized! 
It will do a terrible job when it’s introduced to any new data.

#### Overfitting is not cool.
So for this step, you take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.

The most common example of pooling is **max pooling**. 
In max pooling, the input image is partitioned into a set of areas that don’t overlap. 
The outputs of each area are the maximum value in each area. 
This makes a smaller size with fewer parameters.

Max pooling is all about grabbing the maximum value at each spot in the image. 
This gets rid of 75% of the information that is not the feature. 
By taking the maximum value of the pixels, you’re accounting for distortion. 
If the feature rotates a little to the left or right or whatever, the pooled feature will be the same. You’re reducing the size and parameters. 
This is great because it means that the model won’t overfit on that information.

You could use **average pooling or sum pooling**, but they aren’t common choices. 
Max pooling tends to perform better than both in practice. 
In max pooling, you’re taking the largest pixel value. 
In average pooling, you take the average of all the pixel values at that spot in the image. 
(Actually, there’s a trend now towards using smaller filters or discarding pooling layers entirely. 
This is in response to an aggressive reduction in representation size.)

__Want to look a little more at why you might want to choose max pooling
and why you might prefer a stride of two pixels? Check out Dominik
Scherer et. al, [Evaluation of Pooling Operations in Convolutional
Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf).__

If you go [here](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) you can check out a really interesting 2D visualization of a convolutional layer. 
Draw a number in the box on the left-hand side of the screen and then really go through the output. 
You can see the  convolved and pooled layers as well as the guesses. 
Try hovering over a single pixel so you can see where the filter was applied.


So now we have an input image, an applied convolutional layer, and an applied pooling layer.

Let’s visualize the output of the pooling layer!

We were here:

<center>
<img src="../assets/output3.jpg" style="width:94%">
</center>

The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps. 
It does this by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.

See how the image has changed size?

<center>
<img src="../assets/output4.jpg" style="width:94%">
</center>

Cool, right?

#### Flattening

This is a pretty simple step. You flatten the pooled feature map into a sequential column of numbers (a long vector). 
This allows that information to become the input layer of an artificial neural network for further processing.


#### Fully connected layer
At this step, we add an **artificial neural network** to our convolutional neural network. 
(Not sure about artificial neural networks? [You can learn about them here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!)
-->

<!--
The main purpose of the artificial neural network is to combine our features into more attributes. 
These will predict the classes with greater accuracy. This combines features and attributes that can predict classes better.

At this step, the error is calculated and then backpropagated. 
The weights and feature detectors are adjusted to help optimize the performance of the model. 
Then the process happens again and again and again. 
This is how our network trains on the data! 

How do the output neurons work when there’s more than one?

First, we have to understand what weights to apply to the synapses that connect to the output. 
We want to know which of the previous neurons are important for the output.

If, for example, you have two output classes, one for a cat and one for a dog, a neuron that reads “0” is absolutely uncertain that the feature belongs to a cat. A neuron that reads “1 is absolutely certain that the feature belongs to a cat. 
In the final fully connected layer, the neurons will read values between 0 and 1. 
This signifies different levels of certainty. 
A value of 0.9 would signify a certainty of 90%. 
The cat neurons that are certain when a feature is identified know that the image is a cat. 
They say the mathematical equivalent of, “These are my neurons! I should be triggered!” If this happens many times, the network learns that when certain features fire up, the image is a cat.


Through lots of iterations, the cat neuron learns that when certain features fire up, the image is a cat. 
The dog (for example) neuron learns that when certain other features fire up, the image is a dog. 
The dog neuron learns that for example again, the “big wet nose” neuron and the “floppy ear” neuron contribute with a great deal of certainty to the dog neuron.
It gives greater weight to the “big wet nose” neuron and the “floppy ear” neuron. 
The dog neuron learns to more or less ignore the “whiskers” neuron and the “cat-iris” neuron. 
The cat neuron learns to give greater weight to neurons like “whiskers” and “cat-iris.”
(Okay, there aren’t actually “big wet nose” or “whiskers” neurons. 
But the detected features do have distinctive features of the specific class.)


Once the network has been trained, you can pass in an image and the neural network will be able to determine the image class probability for that image with a great deal of certainty.

The fully connected layer is a traditional Multi-Layer Perceptron. 
It uses a classifier in the output layer. 
The classifier is usually a softmax activation function. 
Fully connected means every neuron in the previous layer connects to every neuron in the next layer. 
What’s the purpose of this layer? To use the features from the output of the previous layer to classify the input image based on the training data.

Once your network is up and running you can see, for example, that you have a 95% probability that your image is a dog and a 5% probability that your image is a cat.


Why do these numbers add up to 1.0? (0.95 + 0.05)

There isn’t anything that says that these two outputs are connected to each other. 
What is it that makes them relate to each other? 
Essentially, they wouldn’t, but they do when we introduce the **softmax function**.
This brings the values between 0 and 1 and makes them add up to 1 (100%). 
(You can read all about this on Wikipedia.) 
The softmax function takes a vector of scores and squashes it to a vector of values between 0 and 1 that add up to 1.

After you apply a softmax function, you can apply the loss function.
Cross entropy often goes hand in hand with softmax. 
We want to minimize the loss function so we can maximize the performance of our
network.

At the beginning of backpropagation, your output values would be tiny.
That’s why you might choose cross entropy loss. 
The gradient would be very low and it would be hard for the neural network to start adjusting in the right direction. 
Using cross entropy helps the network assess even a tiny error and get to the optimal state faster.


#### Want more? Check out
-->

<ul>
<li><a href="https://www.youtube.com/watch?v=mlaLLQofmR8">video by Geoffrey Hinton</a> on the softmax function</li>
<li><a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/">A Friendly Introduction to Cross Entropy Loss</a> by Rob DiPietro</li>
<li><a href="https://peterroelants.github.io/posts/cross-entropy-softmax/">How to Implement a Neural Network Intermezzo 2</a> by Peter Roelants</li>
<li>
<p><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">画像分類の基礎</a>)</p>
</li>
<li>
<p><a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu </p>
</li>
<li>Yann LeCun’s original article, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-Based Learning Applied to Document Recognition</a></li>
<li><a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The Nine Deep Learning Papers You Need to Know About</a> (Understanding CNNs part 3) by Adit Deshpande</li>
</ul>
<!--
from keras import backend as K

# with a Sequential model
get_3rd_layer_output = K.function([model.layers[0].input],
                                  [model.layers[3].output])
layer_output = get_3rd_layer_output([x])[0]

wzxhzdk:1


## t-SNE

- t-SNE は「ティーズニー」と発音します。
    - ちなみに $f(x)$ はどのように発音するか知っていますか？あるいは $f\left(x\vert y\right)$ は？
- 心理学者以外では支配的(かも)
- t: **$t$-分布
- S: **S**tochastic 確率的
- N: **N**eigbor 隣接(隣人)
- E: **E**mbedding 埋め込み
- PCA, FA, 古典的 MDS (Torgerson) が固有値に基づくのに対して，t-SNE は多次元空間と低次元空有間への写像について確率的な仮定を考え，両者の分布が近づくように学習を行う。
    - ここで，2 つの分布の距離を考える。距離の定義には様々が提案がなされているが **カルバック=ライブラー** ダイバージェンス(あるいは KL 距離)が用いられる。[本日の付録](https://www.cis.twcu.ac.jp/~asakawa/2019komazawa/lect06supp/)参照
- 以下は van der Maaten and Hinton (2008) のオリジナル論文に掲載された結果である

<center>
<img src='../assets/2008vanderMaaten_tSNE_Fig2.svg' style="width:74%"><br>
van der Maaten and Hinton (2008) Fig.2 

<img src='../assets/2008vanderMaaten_tSNE_Fig3.svg' style="width:74%">
van der Maaten and Hinton (2008) Fig.3 
</center>

<center>
<img src='../assets/t-and-norm-dists.svg' style='width:49%'></br>
t 分布($\nu=1$)と標準正規分布の確率密度分布 pdf
</center>


- [PCA と t-SNE の比較実験](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_pca_tsne_fashion_mnist.ipynb)
- スチューデントの $t$ 分布
[$t$ 分布](https://en.wikipedia.org/wiki/Student%27s_t-distribution) の確率密度関数 pdf は以下のとおり: <font size='+2' color='green'>数学愛好者<strike><font color='red' style='bold'>数学恐怖症 For all math-phobia</font></strike>の皆様へ</font>

\begin{equation}
p(x,\nu)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}.
\end{equation}

- おそろしい形をしていますが，ポイントは $\Gamma$ 関数(がんまかんすう)であり，内部で使われている
$\nu$ (「にゅう」と読むギリシャアルファベット)は自由でデータ数 $-1$ です。
- $\Gamma$ は[ガンマ関数](https://simple.wikipedia.org/wiki/Gamma_function)であり，
階乗の連続量への拡張とみなすことができます。

- 最も簡単な場合 $\nu=1$ を考えれば，上式は以下のようになります。

\begin{equation}
p(x,\nu=1)=\frac{\Gamma\left(1\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{1}{2}\right)}\left(1+x^2\right)^{-1}.
\end{equation}

- さらに $\Gamma(1)=1$, $\displaystyle\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ を考慮すれば，以下の式を得ます。

\begin{equation}
p(x,\nu=1)=\frac{1}{\pi}\frac{1}{1+x^2},
\end{equation}

- $\pi$ は円周率で定数ですから，グラフの形を考えるときには無視して構いません。従って $t$ 分布の本質は $\displaystyle\frac{1}{1+x^2}$ であることになります。

- <a href="https://colab.research.google.com/notebook?hl=ja#create=true&language=python3" target="_blank">codolab</a> で確認してみましょう。


wzxhzdk:2


- ガンマ関数の概形を描いてみましょう


wzxhzdk:3


- つづいて正規分布と $t$-分布とを比較してみましょう


wzxhzdk:4


- [t-SNE](https://lvdmaaten.github.io/drtoolbox/)
- [tSNEJS demo](https://cs.stanford.edu/people/karpathy/tsnejs/), [blog](http://karpathy.github.io/2014/07/02/visualizing-top-tweeps-with-t-sne-in-Javascript/)
- [Embedding Projector](https://harveyslash.github.io/TSNE-Embedding-Visualisation)
- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)



# 自分の Windows で環境構築するには

- [パッケージマネージャは Chocolatey](https://chocolatey.org/), Mac なら [homebrew](https://brew.sh/)
- [Python 環境は anaconda](https://www.anaconda.com/), もしくは[miniconda](https://docs.conda.io/en/latest/miniconda.html)
- [Python](https://www.python.org/)のバージョンは 2.7 系と 3 系とありますが，3 系で良いでしょう
- Python をブラウザ上で動作させるためには [jupyter notebook](https://jupyter.org/) anaconda もしくは chocolatory, homebrew からインストールできます。ananconda などを用いることで複雑なライブラリ間の依存関係を吸収することができます。
- [Jupyter notebook のクラウド環境は Google Colaboartory](https://colab.research.google.com/notebooks/welcome.ipynb?hl=ja#scrollTo=luKDxbnWyNGy)
- Python 上で動作するディープラーニング(深層学習)のフレームワークには，[Tensorflow](https://www.tensorflow.org/), [keras](https://keras.io/), [PyTorch](https://pytorch.org/) などがあります。
- その他にも数多くのフレームワークが存在しますが，GitHub での星の数のグラフを見てください。
TensorFlow が圧倒的であることがわかります。
<center>
<img src='../assets/2019-03-03github_stars0.png' style='width:94%'></br>
</center>
TensorFlow を除いてプロットしてみるとそれ以外のフレームワークの動向を見てみると
Keras と PyTorch が注目すべきであることがわかります。
<center>
<img src='../assets/2019-03-03github_stars.png' style='width:94%'></br>
</center>

# 資料
- 学習済みのモデルを再利用するためには <a href="https://www.tensorflow.org/hub/" target="_blank">TensorFlow Hub</a>
- 参考資料: <a href="https://insights.stackoverflow.com/trends?tags=python%2Cjavascript%2Cjava%2Cc%23%2Cphp%2Cc%2B%2B" target="_blank">Stackoverflow の言語トレンド</a>
- <a href=https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_python_for_primers.ipynb" target="_blank">Python の初歩</a>
- <a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazaawa_kmnist_pca_tsne.ipynb" target="_blank">PCA と tSNE との比較 kmninst を用いて</a>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../lect03/" class="btn btn-neutral float-right" title="第 3 回">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../lect01/" class="btn btn-neutral" title="第 1 回 05 月 08 日"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright (c) 2020</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../lect01/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../lect03/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
