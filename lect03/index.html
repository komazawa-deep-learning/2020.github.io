<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Shin Asakawa">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>第 3 回 - 2020駒澤大学心理学特講IIIA</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet">
  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet">
  <link href="../css/specific.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u7b2c 3 \u56de";
    var mkdocs_page_input_path = "lect03.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> 2020駒澤大学心理学特講IIIA</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00check_meet/">第 0 回 事前確認</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00guidance/">第 0 回 ガイダンス</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect01/">第 1 回 05 月 08 日</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect02/">第 2 回</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">第 3 回</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a></li>
    

    <li class="toctree-l2"><a href="#_1">デモ</a></li>
    

    <li class="toctree-l2"><a href="#cnn">CNN の特徴</a></li>
    

    <li class="toctree-l2"><a href="#_2">生理学</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#hubel-and-wiesel-1969">Hubel and Wiesel (1969)</a></li>
        
            <li><a class="toctree-l3" href="#blackmore-and-cooper-1970">Blackmore and Cooper (1970)</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#selfridge-pandemonium">セルフリッジ Selfridge のパンデモニウム pandemonium モデル</a></li>
    

    <li class="toctree-l2"><a href="#_3">生理学，視覚心理学との対応</a></li>
    

    <li class="toctree-l2"><a href="#psycholosophical-consideration">Psycholosophical consideration</a></li>
    

    <li class="toctree-l2"><a href="#_4">生理学，視覚心理学との対応</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#one-algorithm-hypothesis">一つのアルゴリズム仮説 One algorithm hypothesis</a></li>
        
            <li><a class="toctree-l3" href="#hubel-and-wiesel-blackmore">生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン)</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#cnn_1">畳込みニューラルネット(CNN)とは何か</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#cnn_2">CNN の詳細</a></li>
        
            <li><a class="toctree-l3" href="#activation-functions">活性化関数 activation functions</a></li>
        
            <li><a class="toctree-l3" href="#tensorflow-hub">TensorFlow HUB</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#_6">さらなる情報</a></li>
    

    <li class="toctree-l2"><a href="#dimensionality-reduction">次元削減，次元圧縮 dimensionality reduction</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#pca">PCA</a></li>
        
            <li><a class="toctree-l3" href="#t-sne">t-SNE</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect04/">第 4 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect05/">第 5 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect06/">第 6 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect07/">第 7 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect08/">第 8 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect09/">第 9 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">付録</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../eco/">エコシステム</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_modules/">Python 上の標準的なライブラリの利用</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_intro/">Colabratory 導入</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_faq/">Colaboratory FAQ</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_numpy_intro_ja/">Python の基礎</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">2020駒澤大学心理学特講IIIA</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>第 3 回</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="iiia"><a href="https://komazawa-deep-learning.github.io/">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a><a class="headerlink" href="#iiia" title="Permanent link">&para;</a></h1>
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: XX/XX/2020<br/>
Appache 2.0 license<br/>
</div>

<p>
<script type="math/tex">\ldots</script> 工事中 <script type="math/tex">\ldots</script>
</p>
<h1 id="_1">デモ<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="https://emojiscavengerhunt.withgoogle.com/">Scavenger hunt</a></li>
<li><a href="https://teachablemachine.withgoogle.com/">https://teachablemachine.withgoogle.com/</a></li>
<li><a href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html">姿勢推定デモ</a></li>
<li><a href="https://youtu.be/kSLJriaOumA">Style-based GAN</a></li>
</ul>
<h1 id="cnn">CNN の特徴<a class="headerlink" href="#cnn" title="Permanent link">&para;</a></h1>
<ul>
<li>
<p>次の 7 つを上げることができます
<!--[@2017Asakawa_deep_jps][^2017Aakawa\_deep\_jps\]-->。</p>
</li>
<li>
<p>非線形活性化関数 (nonlinear activation functions)</p>
</li>
<li>畳込み演算 (convolutional operation)</li>
<li>プーリング処理 (pooling)</li>
<li>データ拡張 (data augmentation)</li>
<li>バッチ正規化 (batch normalization)</li>
<li>ショートカット(shortcut)</li>
<li>GPU の使用</li>
</ul>
<p>上記 7 つの特徴を説明するのは専門的になりすぎるので省略します。一つだけ説明するとすれば最後の
GPU とは高解像度でしかも処理速度を必要とするパソコンゲームで用いられるグラフィックボードのことです。
詳細な画像を高速に画面に表示する必要から開発されたグラフィックボードですが，大規模なニュールネットワークの計算でも用いられる数学が同じです。
そのため，ゲーム用に開発されたグラフィックボードがニューラルネットワークにも用いられるようになりました。</p>
<ul>
<li>Sutton's blog <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter lesson</a>
, <a href="../2019sutton_bitter_lesson.pdf">その和訳</a></li>
</ul>
<!--
  - In computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.
  - This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach. 
-->

<hr />
<h1 id="_2">生理学<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<h2 id="hubel-and-wiesel-1969">Hubel and Wiesel (1969)<a class="headerlink" href="#hubel-and-wiesel-1969" title="Permanent link">&para;</a></h2>
<p><center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--<iframe width="901" height="676" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>--></p>
<hr />
<iframe width="450" height="300" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<!--<iframe width="845" height="676" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<hr />
<iframe width="450" height="300" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<!--<iframe width="784" height="627" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<p></center></p>
<hr />
<h2 id="blackmore-and-cooper-1970">Blackmore and Cooper (1970)<a class="headerlink" href="#blackmore-and-cooper-1970" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/1970BlackmoreCooper_Fig1.svg' style='width:39%'>
<img src='../assets/1970BlackmoreCooper_Fig2.svg' style='width:49%'></p>
<p></center></p>
<hr />
<!--
<center>
<video style="width:84%" src="../assets/Style-based-GAN.mp4" type="video/mp4" />
</center>
-->

<!--
図 [fig:2014Szegedy](#fig:2014Szegedy){reference-type="ref"
reference="fig:2014Szegedy"}
は画像認識の性能向上の例を示しています[@2014GoogLeNet]。

<center>
<img src="../assets/2014Szegedy_Going_deeper.jpg" style="width:74%">
<img src="../assets/2014Szegedy_Going_deeper.jpg" style="width:74%">
</center>
-->

<h1 id="selfridge-pandemonium">セルフリッジ Selfridge のパンデモニウム pandemonium モデル<a class="headerlink" href="#selfridge-pandemonium" title="Permanent link">&para;</a></h1>
<p><center>
<img src='../assets/1958Selfridge_fig3.svg' style='width:74%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より</p>
<p><img src='../assets/1958Selfridge_fig7.svg' style='width:84%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center></p>
<p><center>
<img src='../assets/1958Selfridge_fig4.svg' style='width:49%'><br>
<!--
<img src='../assets/1958Selfridge_fig5.svg' style='width:74%'><br>
-->
<img src='../assets/1958Selfridge_fig6.svg' style='width:49%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center></p>
<h1 id="_3">生理学，視覚心理学との対応<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h1>
<ul>
<li>Julesz</li>
<li>Julesz (1981) Textons, the elements of texture perception, and their interactions, Nature</li>
</ul>
<p><center>
<img src="../assets/1981Julesz-texton-Fig2.svg" style="width:84%"></br>
Julesz (1981) Fig. 2 より
</center></p>
<ul>
<li>Marr<ul>
<li>Computational approach: Vision (1908) </li>
</ul>
</li>
<li>Poggio<ul>
<li>Poggio, Torre, and Koch (1985) Computational vision and regularization theory</li>
</ul>
</li>
</ul>
<h1 id="psycholosophical-consideration">Psycholosophical consideration<a class="headerlink" href="#psycholosophical-consideration" title="Permanent link">&para;</a></h1>
<ul>
<li>Epistemology 思念的，観念的</li>
<li>Empirical Episitemology 実証的 = psychology</li>
<li>Constructive epipstemology 構成論的 = computer vision, neural networks</li>
</ul>
<h1 id="_4">生理学，視覚心理学との対応<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h1>
<ul>
<li>Julesz</li>
<li>Julesz (1981) Textons, the elements of texture perception, and their interactions, Nature</li>
</ul>
<p><center>
<img src="../assets/1981Julesz-texton-Fig2.svg" style="width:84%"></br>
Julesz (1981) Fig. 2 より
</center></p>
<ul>
<li>Marr<ul>
<li>Computational approach: Vision (1908) </li>
</ul>
</li>
<li>Poggio<ul>
<li>Poggio, Torre, and Koch (1985) Computational vision and regularization theory</li>
</ul>
</li>
</ul>
<h2 id="one-algorithm-hypothesis">一つのアルゴリズム仮説 One algorithm hypothesis<a class="headerlink" href="#one-algorithm-hypothesis" title="Permanent link">&para;</a></h2>
<ul>
<li>Metin and Frost (1989) Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus</li>
<li>Roe et al. (1992) Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex</li>
</ul>
<h2 id="hubel-and-wiesel-blackmore">生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン)<a class="headerlink" href="#hubel-and-wiesel-blackmore" title="Permanent link">&para;</a></h2>
<ul>
<li>層間の結合の仕方, アーキテクチャ</li>
<li>forward/backward 役割，機能，実現方法</li>
<li>側抑制 lateral inhibition (これについては多層化して回避できる可能性あり)</li>
<li>
<p>shape from X は正しかったのか？ ただし発達心理学におけるシェイプバアスは言語発達において重要な意味を持つはず。だからと言って乳幼児はそのように強制(脅迫？)，矯正されて育つわけではないだろう。</p>
<ul>
<li>Ritter (2017) Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study</li>
<li>Landau, Smith, Jones (1992) Syntactic Context and the Shape Bias in Childrens and Adults Lexical Learning</li>
<li>
<p>Yamins (2016) Using goal-driven deep learning models to understand sensory cortex</p>
</li>
<li>
<p>Julez のアプローチは視覚研究者 Haar, SIFT, DoG などのアルゴリズム開発者と対応</p>
</li>
<li>Poggio (1985) Computational Vision and Regularization Theory</li>
</ul>
</li>
</ul>
<p><center>
<img src='../assets/2016Yamins_Fig1.svg' style='width:94%'>
<img src='../assets/2016Yamins_Fig2.svg' style='width:94%'>
<img src='../assets/2016Yamins_Fig3.svg' style='width:94%'></p>
<p><img src='../assets/2014Yamins_Fig2.svg' style='width:74%'>
<img src='../assets/2014Yamins_FigS2.svg' style='width:74%'></p>
<p><img src="../assets/2020Schrimpf_fig1.svg" style="width:84%">
</center></p>
<p><a href="../assets/2018asakawa_jdlaGtestChapt7.pdf">G 検定公式テキスト7章より</a></p>
<p><center>
<img src='../assets/2012Zeiler_Deconvolution.png' style='width:49%'></br>
<br>Zeiller 2012 より
</center></p>
<hr />
<h1 id="cnn_1">畳込みニューラルネット(CNN)とは何か<a class="headerlink" href="#cnn_1" title="Permanent link">&para;</a></h1>
<p>本節では深層学習，特に CNN と呼ばれるニューラルネットワークについて解説します。</p>
<p>最初に画像処理の概略を述べる CNN が，それまで主流であった従来の手法の性能を凌駕したことはすでに述べました。
CNN の特徴の一つに <strong>エンドツーエンド</strong> と呼ばれる考え方があります。
エンドツーエンドとは，従来手法によるパターン認識システムでは，専門家による手の込んだ詳細な作り込みを必要としていたことと異なり，面倒な作り込みをせずとも性能が向上したことを指します。</p>
<p>エンドツーエンドなニューラルネットワークにより，次のことが実現しました。</p>
<ul>
<li>ニューラルネットワークの層ごとに，特徴抽出が行われ，抽出された特徴がより高次の層へと伝達される</li>
<li>ニューラルネットワークの各層では，比較的単純な特徴から次第に複雑な特徴へと段階的に変化する</li>
<li>高次層にみられる特徴は低次層の特徴より大域的，普遍的である</li>
<li>高次層のニューロンは，低次層で抽出された特徴を共有している</li>
</ul>
<p>このことを簡単に説明してみます。</p>
<p>我々人間は，外界を認識するために必要な計算を，生物種としての発生の過程と，個人の発達を通しての経験に基づく認識システムを保持していると見ることができます。
従って我々の視覚認識には化石時代に始まる光の受容器としての眼の進化の歴史と発達を通じた個人の視覚経験が反映された結果でもあります。
人工知能の目標は，この複雑な特徴検出過程をどうやったらコンピュータが獲得できるかということでもあります。
外界を認識するために今日まで考案されてきたモデル（例えば，ニューラルネットワークサポートベクターマシンなどは）は複雑です。
ですがモデルを訓練するための学習方法はそれほど難しくありません。
この意味で画像認識課題が正しく動作するためのポイントは，認識システムが問題を解く事が可能なほど複雑であるかどうかではなく，十分に複雑が視覚環境，すなわち画像認識の場合，外部の艦橋を反映するために十分な量の像データを容易すことができるか否かにあります。
今日の CNN による画像認識性能の向上は，簡単な計算方法を用いて複雑な外部環境に適応できる認識システムを構築する方法が確立したからであると言うことが可能です。</p>
<p>下図 <!--<a href="#fig:2012Ng_01">fig:2012Ng_01</a>{reference-type="ref"
reference="fig:2012Ng_01"} -->
に画像処理の例を挙げました。</p>
<p><center>
<img src="../assets/2012Ng_ML_and_AI_01.png" style="width:94%">
</center></p>
<!--図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"}
-->

<p>では入力画像がネコであるか否かを判断する画像認識であるとしました。
我々はネコの画像を瞬時に判断できます。ですが画像認識の難しさは，入力画像が上図 <!--<a href="#fig:2012Ng_01">[fig:2012Ng_01]</a>{reference-type="ref"
reference="fig:2012Ng_01"}-->
に示されているように入力信号の数字の集まりでしか無いことです。
このようなデータを何度も経験することでネコを識別できるようにする必要があります。
<!-- [[fig:2012Ng_01]]{#fig:2012Ng_01 label="fig:2012Ng_01"}-->
<!--図<a href="#fig:2012Ng_01">[fig:2012Ng_01]</a>{reference-type="ref" reference="fig:2012Ng_01"}-->
<!--に示したように-->コンピュータに入力される画像は数字の塊に過ぎません。</p>
<p>状況ごとにとるべき操作を命令として逐一コンピュータに与える指示する手順の集まりのことをコンピュータプログラムと呼びます。人間がコンピュータに与えることができる操作や命令によって画像認識システムを作る場合，命令そのものが膨大になったり，そもそも説明することが難しかったりします。
例を挙げれば，お母さんを思い浮かべてくださいと言われれば誰でも，それぞれ異なるイメージであれ思い浮かべることができます。また，提示された画像が自分の母親のものであるか，別の女性であるかの判断は人間であれば簡単です。ところがコンピュータには難しい課題となります。
加えて母親の特徴をコンピュータに理解できる命令としてプログラムすることも難しい課題です。つまり自分の母親の特徴を曖昧な言葉でなく明確に説明するとなるととても難しい課題となります。
というのは，女性の顔写真であればどの写真も似ていると言えるからです。顔の造形や輪郭，髪の位置などはどの画像も類似していることでしょう。ところがコンピュータにはこの似ている，似ていいないの区別が難しいのです。</p>
<p>加えて，同一ネコの画像であっても，被写体の向き視線の方向や光源の位置や撮影条件が異なれば画像としては異なります。
下図に示したように入力画像の中の特定の値だけを調べてみても，入力画像がネコであ
る，そうではないかを判断することは難しい課題になります。</p>
<!--[fig:2012Ng_02](#fig:2012Ng_02){reference-type="ref"
reference="fig:2012Ng_02"}
-->

<p><center>
<img src="../assets/2012Ng_ML_and_AI_02.png" style="width:94%">
</center></p>
<!--[fig:2012Ng_02]{#fig:2012Ng_02 label="fig:2012Ng_02"}-->

<p>現在の画像認識では，特定の画素の情報に依存せずに，入力画像が持っている特徴
をとらえるように設計されます。たとえば，ネコを認識するために必要ことは，ネ
コに特徴的な「ネコ目」や「ネコ足」を検出することであると考えます。入力画像
から，ネコの持つ特徴を抽出することができれば，それらの特徴を持っている入力
画像はネコであると判断して良いことになります(下図
<!--<a href="#fig:2012Ng_03">[fig:2012Ng_03]</a>{reference-type="ref"
reference="fig:2012Ng_03"}-->)。</p>
<p><center>
<img src="../assets/2012Ng_ML_and_AI_03.png" style="width:94%">
</center></p>
<!-- [[fig:2012Ng_03]]{#fig:2012Ng_03 label="fig:2012Ng_03"}-->

<p>下図
<!--<a href="#fig:2013LeCun_9">[fig:2013LeCun_9]</a>{reference-type="ref"
reference="fig:2013LeCun_9"} -->
は，音声認識と画像認識の両分野において CNN が用いられる以前の従来手法 をまとめたものです。</p>
<p><center>
<img src="../assets/2013LeCun-tutorial-icml_9.png" style="width:94%">
</center>
<!--[[fig:2013LeCun_9]]{#fig:2013LeCun_9 label="fig:2013LeCun_9"}-->
上図<!--<a href="#fig:2013LeCun_9">[fig:2013LeCun_9]</a>{reference-type="ref"
reference="fig:2013LeCun_9"} -->
のような従来手法に対して，CNN
ではエンドツーエンドな特徴抽出を多層多段に重ねることによって複雑な特徴を抽出(検出)しようとしています
(下図<!-- <a href="#fig:2013LeCun_10">[fig:2013LeCun_10]</a>{reference-type="ref"
reference="fig:2013LeCun_10"}-->)。</p>
<p><center>
<img src="../assets/2013LeCun-tutorial-icml_10.png" style="width:94%">
</center>
<!--
[fig:2013LeCun_10]{#fig:2013LeCun_10 label="fig:2013LeCun_10"}
--></p>
<p>コンピュータにはネコ目特徴検出器，ネコ足特徴検出器は備わっていません。そこで画像認識研究では，画像の統計的性質に基づいて特徴検出器を算出する方法を探す努力が行われてきました。
しかし，コンピュータにネコ目特徴やネコ足特徴を教えるは容易なことではありません。
このことは画像処理の分野だけに限りません，音声認識でも言語情報処理でもそれぞれの特徴器を一つ一つ定義し，チューニングするのは時間がかかり，専門的な知識も必要で困難な作業でした。</p>
<!--
<center>
<img src="../assets/2012Ng_ML_and_AI_05.png" style="width:94%">
</center>
-->

<!--[fig:2012Ng_05]{#fig:2012Ng_05 label="fig:2012Ng_05"}-->

<!--
<img src="../assets/2012Ng_ML_and_AI_04.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_06.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_07.png" style="width:94%">
<img src="../assets/2012Ng_ML_and_AI_08.png" style="width:94%">
-->

<p>まとめると，1950 年代後半以来:固定的，手工芸的特徴抽出器と学習可能な分類器を用いた認識システムを作ることが試みられてきたといえます。
これに対して CNN が主流となった現在はエンドツーエンドで学習可能な特徴抽出器を多数重ね合わせることで性能が向上しました。</p>
<p>夢のような話が続きましたが，本節の最後に逆に CNN は簡単に騙すことができる例
を挙げておきます<!--(図<a href="#fig:GAN">fig:GAN</a>{reference-type="ref"
reference="fig:GAN"})-->。</p>
<p><center>
<img src="../assets/2014Goodfellow_Fig1.png" style="width:94%">
</center></p>
<p>図では，左の画像が入力画像で，CNN は確信度 57.7パーセントでパンダである認識しました。 ところがこの画像に 0.007だけ意味のない画像(図中央)を加えるた画像(図右)を CNN は 99.3パーセントの確信度でテナガザル
(gibbon)と判断しました。この例はここでは詳しく触れることはしませんが <strong>敵対的学習</strong>と呼ぶ訓練手法を説明する際に用いられた例です <!--[@2014Goodfellow_GANHarness]-->。</p>
<p>この例からも分かることは以下のようにまとめられるでしょう。
すなわち，人間の脳を模したニューラルネットワークである CNN が大規模化像認識チャレンジにおいて人間の認識性能を越えたと報道されました。
ですが，人間の視覚認識を完全に実現したと考えるのは早計で，解くべき課題は未だ多数あるということです。
この状況は，音声認識や言語情報処理でも同様であると言えます。</p>
<ul>
<li>ドロップアウト，データ拡張，各種正規化: cnn.md</li>
<li>有名なモデル LeNet，Alex Net，Inception，VGG，ResNet</li>
<li>R-CNN，ハイウェイネット，YOLO，SSD</li>
<li>セマンティックセグメンテーション</li>
<li>転移学習，事前学習，ファインチューニング</li>
</ul>
<!--
---

###  表記

<b>深層学習</b> 文献では MLP: Multi-Layered Perceptrons と表記される場合が多い

- \(x\) をニューロンの活性値とし，入力層から数えて \(i\) 層目の \(j\) 番目のニューロンの活性値を \(x_i^{(j)}\) と表記する。
- 入力層は \(0\) 層目とみなせば \(x_1^{(0)}\)　は入力データのうち \(1\) 番目のニューロンに与える数値を表している。
- 下付き添字は番号付けされたニューロンの指す場合に使われる。
- 上付き添字はカッコを付けて階層的ニューラルネットワークの層を表すとする。このとき，べき乗と区別するためにカッコをつける。\(x_1^{2}\) は第 \(2\) 層の \(1\) 番目のニューロンの出力値を表している。
- 活性化関数は多値入力一出力 (many to one) である場合が多い。プログラミング言語に多く見られる関数の定義と同じように引数が複数個存在し，戻り値が一つである関数と酷似している。
- 活性化関数への引数は前層のニューロンとの結びつきを表現する結合強度 connection legth(結合係数 connection coefficient，重み weight とも呼ぶ)とによる荷重総和である。
- 荷重総和は総和記号 \(\sum\) を用いることで \(\sum_{i=1}^{m}w_ix_i\) と表記される。\(m\) は関与する下位層のニューロンの総数である。
- 各ニューロンの出力値を計算するためには荷重総和とバイアス bias を合わせてから，非線形写像 \(f\) によって変換されるので
\(f\left(\sum_{i}w_ix_i+b\right)\) と表記される。

---

- 写像 $f$ を **活性化関数 activation function** とする
- 第 $k$ 層の $i$ 番目のニューロンの出力値 $x_i^{(k)}$ は次のように表記できる：
$$ x_i^{(k)} = f\left(\sum_{i=1}^{m}w_{i}x_{i}^{(k-1)}+b_{i}^{(k)}\right) $$
- バイアス項は常に $1$ を出力する特別な入力値であるとみなして表記を簡単にするために $0$ 番目の入力として扱うと：
$$ x_i^{(k)} = f\left(\sum_{i=0}^{m}w_{i}x_{i}^{(k-1)}+b_{i}^{(k)}\right) $$
とする表記も行われてきた。
- C や Python のメモリ配置ではメモリ先頭番地を $0$ とする場合が多いので繰り返しは $n$ 回の繰り返しを $0$　から $n-1$ 回までのカウンタとする場合が多いので上記のようにバイアス項を $0$ 番目の要素として組み込むより，別の項として扱うことが最近は多いようである。()
- ３層パーセプトロン(実際には入力層は値をセットするだけなので演算は２層しか行われない)を表記すれば

\begin{align}
y_i=x_i^{(2)}&=f\left(\sum_{j}w_{j}^{(1)}x_{j}^{(1)}+b_{i}^{(2)}\right)\\
&=f\left(\sum_{j}w_{j}^{(1)}\left(f\left(\sum_{k}w_{k}^{(0)}x_{k}^{(0)}+b_{j}\right)\right)+b_{i}^{(2)}\right)\\
\end{align}


wzxhzdk:0


-->

<!--
---

### 出力関数
学習可能な重み係数とバイアスがを持つニューロンで \(f=\left(\sum_i w_ix_i + b\right)\) 構成される。<br>
複数入力，１出力の非線形出力関数を持つ。

出力関数としては：<br>

- $ReLU\left(x\right)=\max\left(0,x\right)$
- $ReLU6\left(x\right)=\min\left(\max\left(0,x\right),6\right)$
- $crelu\left(x\right)=\left(\left[x\right]_{+},\left[-x\right]_{-}\right)$
- \[elu\left(x\right)=\left\{
\begin{align}
x       & \hspace{3em}if x>0\\
e^{x}-1 & \hspace{3em}otherwise
\end{align}\right.\]
- $softplus\left(x\right)=\log\left(1+e^{x}\right)$
- $softsign\left(x\right)=\frac{x}{\Vert x\Vert+1}$

- ドロップアウト
- バイアス付加

- $\sigma(x)=\frac{1}{1+e^{-x}}$
- $\phi\left(x\right)=\tanh\left(x\right)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$

がある
-->

<hr />
<!-- [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)より -->

<h2 id="cnn_2">CNN の詳細<a class="headerlink" href="#cnn_2" title="Permanent link">&para;</a></h2>
<p>通常のニューラルネットワークでは，直下層のニューロンとそのすぐ上の層の全ニューロンと結合を有する。一方 CNN ではその結合が部分的である。
各ニューロンは多入力一出力の信号変換機とみなすことができ，活性化関数に非線形な関数を用いる点は通常のニューラルネットワークと同様。</p>
<p>画像処理を考える場合，典型的には一枚の入力静止画画像は 3 次元データである。次元は幅w，高さh，奥行きd であり，入力画像では奥行きが３次元，すなわち赤緑青の三原色。出力ニューロンへの入力は局所結合から小領域に限局される。</p>
<h3 id="1-cnn">1. CNNの構成<a class="headerlink" href="#1-cnn" title="Permanent link">&para;</a></h3>
<p>CNN は以下のいずれかの層から構成される：</p>
<ol>
<li><strong>畳込み層</strong></li>
<li><strong>プーリング層</strong></li>
<li><strong>完全結合層</strong>（通常のニューラルネットワークと正確に同じもの，CNN では最終 1 層または最終 1,2 層に用いる）</li>
</ol>
<p>入力信号はパラメータの値が異なる活性化関数によって非線形変換される。
畳込み層とプーリング層と複数積み重ねることで多層化を実現し，深層ニューラルネットワークとなる。</p>
<hr />
<h4 id="_5">例：<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ul>
<li><!--CNNの最も単純な場合。--> 画像データを出力信号へ変換</li>
<li>各層は別々の役割（畳込み，全結合，ReLU, プーリング）</li>
<li>入力信号は 3 次元データ，出力信号も 3 次元データ</li>
<li>学習すべきパラメータを持つ層は畳込み層，全結合層</li>
<li>学習すべきパラメータを持たない層は ReLU 層とプーリング層</li>
<li>ハイパーパラメータを持つ層は畳込み層, 全結合層, プーリング層</li>
<li>ハイパーパラメータを持たない層は ReLU層</li>
</ul>
<hr />
<p><center>
  <img src="../assets/cnn/convnet.jpeg" width="94%">
  <!--
  <div class="figcaption">
  -->
  <div>
  CNN アーキテクチャ: 入力層は生画像の画素値(左)を格納、最後層は分類確率(右)を出力。処理経路に沿った活性の各ボリュームは列として示されている。3Dボリュームを視覚化することは難しいため、各ボリュームのスライスを行ごとに配置してある。最終層のボリュームは各クラスのスコアを保持するが、ソートされた上位5スコアだけを視覚化し、それぞれのラベルを印刷してある。
  <!--
  <a href="http://cs231n.stanford.edu/">ウェブベースのデモ</a>は、ウェブサイトのヘッダーに表示されています。ここに示されているアーキテクチャは、あとで説明する小さなVGG Netです。
  -->
</div>
</center></p>
<!--
The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full <a href="http://cs231n.stanford.edu/">web-based demo</a> is shown in the header of our website. The architecture shown here is a tiny VGG Net, which we will discuss later.
-->

<!--
We now describe the individual layers and the details of their hyperparameters and their connectivities.
-->

<hr />
<!--
*Example Architecture: Overview*. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:

*アーキテクチャ例* CIFAR-10の CNN は [入力層-畳込み層-ReLLU層-プーリング層-全結合層] という構成である。
-->

<ul>
<li>入力層[32x32x3]: 信号は画像の生データ（画素値）幅w(32)，高さh(32)、色チャネル3(R, G, B)</li>
<li>畳込み層: 下位層の限局された小領域のニューロンの出力の荷重付き総和を計算(内積，ドット積）。12個のフィルタを使用すると[32x32x12]となる。</li>
<li>ReLU層の活性化関数は ReLU (Recutified Linear Unit) \(max(0,x)\)<!--入力範囲は変更されない([32x32x12])。--></li>
<li>プーリング層: 空間次元（幅,高さ）に沿ってダウンサンプリングを実行。[16x16x12]のようになる。</li>
<li>全結合層はクラスに属する確率を計算: 10 の数字のそれぞれが CIFAR-10 の 10 カテゴリーの分類確率に対応するサイズ[1x1x10]に変換。通常のニューラルネットワーク同様、全結合層のニューロンは前層の全ニューロンと結合する。</li>
</ul>
<!--
In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don't. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image. 
-->

<p>CNN は元画像（入力層）から分類確率（出力層）へ変換。学習すべきパラメータを持つ層（畳込み層，全結合層）とパラメータを持たない層（ReLU層）が存在。畳込み層と全結合層のパラメータは勾配降下法で訓練</p>
<hr />
<h3 id="2">2. 畳込層<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p><center>
<!--<img src="../assets/2012AlexNet.svg" style="width:94%">-->
<img src="../assets/Neocognitron.svg" style="width:74%">
<img src="../assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br></p>
<p><img src="../assets/1998LeCun_Fig2_CNN.svg" style="width:94%"><br>
LeNet5 (LeCun, 1998)<br></p>
<p><img src="../assets/2012AlexNet_2.svg" style="width:94%"><br>
アレックスネット(Krizensky et al. 2012)<br>
<img src="../assets/2012AlexNet_Result.svg" style="width:94%">
アレックスネットの出力例(Krizensky et al. 2012)<br>
</center></p>
<ul>
<li>畳込み層のパラメータは学習可能なフィルタの組</li>
<li>全フィルタは空間的に（幅と高さに沿って）小さくなる</li>
<li>フィルタは入力信号の深さと同一</li>
<li>第1層のフィルタサイズは例えば 5×5×3（5 画素分の幅，高さ，と深さ 3（３原色の色チャンネル）</li>
<li>各層の順方向の計算は入力信号の幅と高さに沿って各フィルタを水平または垂直方向へスライド</li>
<li>フィルタの各値と入力信号の特定の位置の信号との内積（ドット積）。</li>
<li>入力信号に沿って水平，垂直方向にフィルタをスライド</li>
<li>各空間位置でフィルタの応答を定める 2 次元の活性化地図が生成される</li>
<li>学習の結果獲得されるフィルタの形状には、方位検出器，色ブロッブ，生理学的には視覚野のニューロンの応答特性に類似</li>
<li>上位層のフィルタには複雑な視覚パタンに対応する表象が獲得される</li>
<li>各畳込み層全体では学習すべき入力信号をすべて網羅するフィルタの集合が形成される</li>
<li>各フィルタは相異なる 2 次元の活性化地図を形成</li>
<li>各フィルタの応答特性とみなすことが可能な活性化地図</li>
<li>フィルタの奥行き次元に沿って荷重総和を計算し、出力信号を生成</li>
</ul>
<hr />
<ul>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_scipy_convolve.ipynb">畳込み演算のデモ</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_kitten_filters_demo.ipynb">フィルタ演算のデモ 2019komazawa_kitten_filters_demo.ipynb</a></li>
<li><a href="/conv-demo/">畳込み演算のデモビデオ</a></li>
</ul>
<p><center>
<video controls loop>
  <source src="../assets/2019si_conv-demo.mp4" type="video/mp4" style="width:84%">
</video>
</center></p>
<ul>
<li>
<p>ビオラ，ジョーンズアルゴリズム (2001)，富士フィルムによる実装は2006年頃</p>
</li>
<li>
<p><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/How_to_Visualize_Filters_and_Feature_Maps_in_Convolutional_Neural_Networks.ipynb">実際にどのようなフィルタが構成されたのか</a></p>
</li>
</ul>
<hr />
<p><strong>局所結合</strong>: 画像のような高次元の入力を処理する場合，下位層の全ニューロンと上位層の全ニューロンとを接続することは <strong>責任割当問題回避</strong> の観点からもパラメータ数の増加は現実的ではない。<br>
代わりに各ニューロンを入力ボリュームのローカル領域のみに接続。空間的領域はニューロンの <strong>受容野</strong> と呼ばれるハイパーパラメータ（フィルタサイズとも言う）。<font color="blue">深さ次元に沿った接続性＝入力層の深さ次元</font>。
空間次元（幅と高さ）と深さ次元をどのように扱うかにより，この非対称性を再び強調することが重要です。ニューロン間の結合は空間次元（幅と高さ）にそって限局的。入力次元の深さ全体を常にカバーする。</p>
<ul>
<li>
<p>例1: 入力層のサイズが[32x32x3]（RGB CIFAR-10画像データセットなど）であれば受容野（フィルタサイズ）が 5x5 とすれば，畳込み層内の各ニューロンは入力層の [5x5x3] 小領域への結合係数を持つ。各小領域毎に 5x5x3=75 の重み係数と 1 つのバイアス項が必要である。深さ次元に沿った上層のニューロンから下位層のニューロンへの結合は下位層の深さ(色チャンネル数)と等しく 3 である。</p>
</li>
<li>
<p>例2: 入力ボリュームのサイズが[16x16x20]であるとすると 3x3 の受容野サイズで畳込層の全ニューロンの合計は 3x3x20=180 接続。接続性は空間的に局在する（3x3）が，入力深度（20）に沿っては完全結合</p>
</li>
</ul>
<p><strong>空間配置</strong>: 出力層ニューロンの数と配置については 3 つのハイパーパラメータで出力ニューロン数が定まる。</p>
<ol>
<li><strong>深さ数(フィルタ数)</strong></li>
<li><strong>ストライド幅</strong></li>
<li>
<p><strong>ゼロパディング</strong></p>
</li>
<li>
<p>出力層ニューロン数のことを出力層の <strong>深さ</strong> 数と呼ぶハイパーパラメータである。深さ数とはフィルタ数（カーネル数）とも呼ばれる。第 1 畳込み層が生画像であれば，奥行き次元を構成する各ニューロンによって種々の方位を持つ線分(エッジ検出細胞)や色ブロッブのような特徴表現を獲得可能となる。入力の同じ領域を <strong>深さ列</strong> とするニューロン集団を <strong>ファイバ</strong> ともいう。</p>
</li>
<li>
<p>フィルタを上下左右にずらす幅を <strong>ストライド幅</strong> と呼ぶ。ストライド幅が 1 ならフィルタを 1 画素ずつ移動することを意味する。ストライドが 2ならフィルタは一度に 2 画素ずつジャンプさせる。ストライド幅が大きければ入力信号のサンプリング間隔が大きく広がることを意味する。ストライド幅が大きくなれば上位層のニューロン数は減少する。</p>
</li>
<li>
<p>入力の境界上の値をゼロで埋め込むことがある。これを <strong>ゼロパディング</strong> という。ゼロパディングの量はハイパーパラメータである。ゼロパディングにより出力層ニューロンの数を制御できる。下位層の空間情報を正確に保存するには入力と出力の幅，高さは同じである必要がある。</p>
</li>
</ol>
<p>入力層のニューロン数を\(W\)，上位にある畳込み層のニューロン数を\(F\)，とすれば出力層に必要なニューロン数\(S\)は，周辺のゼロパディング を\(P\)とすれば \((W-F+2P)/S+1\) で算出できる。たとえば下図でストライド 1 とゼロパディング 0 であれば入力 7x7 でフィルタサイズが 3x3 であれば 5x5(=S=(7-3+2x0)/1+1=5) の出力である。ストライド 2 ならば 3x3=(S=(7-3+2x0)/2+1=3) となる。</p>
<!--
We can compute the spatial size of the output volume as a function of the input volume size (\\(W\\)), the receptive field size of the Conv Layer neurons (\\(F\\)), the stride with which they are applied (\\(S\\)), and the amount of zero padding used (\\(P\\)) on the border. You can convince yourself that the correct formula for calculating how many neurons "fit" is given by \\((W - F + 2P)/S + 1\\). For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more graphical example:
-->

<div>
  <img src="../assets/cnn/stride.jpeg">
  <div>
空間配置の例：入力空間の次元（x軸）が1つで受容野サイズ F=3 の場合，入力サイズ W=5, ゼロパディング P=1 であれば，<br>
<b>左図：</b>出力層ニューロン数は (5-3+2)/1+1=5 の出力層ニューロン数となる。ストライド数 S=1 の場合。<br>
<b>右図：</b>s=2，出力層ニューロン数 (5-3+2)/2+1=3 となる。ストライド S=3 ならばボリューム全体にきちんと収まらない場合もでてくる。数式で表現すれば  \\((5-3+2)=4\\) は 3 で割り切れないので、整数の値として一意に決定はできない。<br>
ニューロン結合係数は（右端に示されている）[1,0,-1]でありバイアスはゼロ。この重みはすべての黄色ニューロンで共有される。
</div>
</div>

<hr />
<p><em>ゼロパディング</em>: 上例では入力次元が 5，出力次元が 5 であった。これは受容野が 3 でゼロ埋め込みを1としたためである。ゼロ埋め込みが使用されていない場合、出力ボリュームは、どれだけの数のニューロンが元の入力に「フィット」するのであろうかという理由で、空間次元がわずか3であったであろう。ストライドが \(S=1\) のとき、ゼロ埋め込みを \(P=(F-1)/2\) に設定すると、入力ボリュームと出力ボリュームが空間的に同じサイズになる。このようにゼロパディングを使用することは一般的である。CNNについて詳しく説明している完全な理由について説明する。</p>
<p><em>ストライドの制約</em>: 空間配置ハイパーパラメータには相互の制約があることに注意。たとえば入力に\(W=10\)というサイズがあり、ゼロパディングは\(P=0\) ではなく、フィルタサイズは\(F=3\), \((W-F+2P)/S+1=(10-3+0)/2+1=4.5\)よりストライド \(S=2\) を使用することは不可能である。すなわち整数ではなくニューロンが入力にわたってきれいにかつ対称的に "適合" しないことを示す。</p>
<p><em>AlexNet</em> の論文では，第一畳込層は受容野サイズ \(F=11\)，ストライド\(S=4\)，ゼロパディングなし\(P=0\)。<br>
畳込層 \(K=96\) の深さ \((227-11)/4+1=55\)。畳込層の出力サイズは [55x55x96]。55x55x96 ニューロンは入力領域 [11x11x3] と連結。全深度列 96 個のニューロンは同じ入力領域[11×11×3]に繋がる。論文中には(224-11)/4+1 となっている。パディングについての記載はない。</p>
<p><strong>パラメータ共有</strong> パラメータ数を制御するために畳み込み層で使用される。上記の実世界の例を使用すると、最初の畳故意層には 55x55x96=290,400のニューロンがあり、それぞれ 11x11x3=363 の重みと1のバイアスがある。これにより CNN 単独の第 1 層に最大 290400x364=105,705,600 のパラメータが追加される。<!--この数は非常に高いです。--></p>
<!--
**Parameter Sharing.** Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55\*55\*96 = 290,400 neurons in the first Conv Layer, and each has 11\*11\*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.
-->

<!--
It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a **depth slice** (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96\*11\*11\*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55\*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.
-->

<p><strong>パラメータ共有</strong> により学習すべきパラメータ数が減少する。
例えば [55x55x96] のフィルタでは深さ次元は 96 個のニューロンで，各深さで同じ結合係数を使うことにすれば
ユニークな結合係数は計 96x11x11x3=34,848 となるので総パラメータ数は 34,944 となる(バイアス項 +96)。各深さで全ニューロン(55x55)は同じパラメータを使用する。逆伝播での学習では，全ニューロンの全結合係数の勾配を計算する必要がある。各勾配は各深さごとに加算され 1 つの深さあたり一つの結合係数集合を用いる。</p>
<p>ある深さの全ニューロンが同じ重み係数ベクトルを共有する場合，畳込み層の順方向パスは各深さスライス内で入力ボリュームとのニューロンの重みの <strong>畳み込み</strong> として計算できることに注意。結合荷重係数集合のことを <strong>フィルタ</strong> または <strong>カーネル</strong> と呼ぶ。入力信号との間で畳込み演算を行うこととなる。</p>
<!--
Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a **convolution** of the neuron's weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a **filter** (or a **kernel**), that is convolved with the input.
-->

<p><center>
<div>
  <img src="../assets/cnn/weights.jpeg" style="width:94%">
<div>
AlexNet の学習済フィルタ例：図の 96 個のフィルタは サイズ[11x11x3]。それぞれが 1 つの深さ内の 55×55 ニューロンで共有されている。画像の任意の位置で水平エッジ検出が必要な場合，画像の並進不変構造 translationall-invariant structure 仮定により画像中の他の場所でも有効である。 畳込み層の出力ニューロン数は 55x55 個の異なる位置すべてで水平エッジの検出を再学習する必要はない。
<!--
Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.
-->
  </div>
</div>
</center></p>
<h3 id="3">3. プーリング層<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<p>CNN では，連続する畳込み層間にプーリング層を挿入するのが一般的。プーリング層の役割は，空間次元の大きさに減少させることである。パラメータ，すなわち計算量を減らし，過学習を制御できる。プーリング層は入力の各深さ毎に独立して動作する。最大値のみをとり他の値を捨てることを <strong>マックスプーリング</strong> と呼ぶ。サイズが 2x2 のフィルタによるプーリング層では，入力の深さごとに <script type="math/tex">2</script> つのダウンサンプルを適用し、幅と高さに沿って2ずつ増やして75％の情報を破棄する。この場合 4 つの数値のうち最大値を採用することになる。</p>
<!--
It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:
-->

<p><center>
<div>
  <img src="../assets/cnn/maxpool.jpeg" width="74%">
<div>
一般的なダウンサンプリング演算は <b>マックスプーリング</b> である。図では ストライド 2 すなわち 4 つの数値の中の最大値
<!--
The most common downsampling operation is max, giving rise to <b>max pooling</b>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).
-->
</div>
</div>
</center>
<strong>平均プーリング</strong>. マックスプーリングではなく <em>L2正則化プーリング</em> を行う場合もある。平均プーリングは歴史的な意味あいがあるがマックスプーリングの方が性能が良いとの報告がある。ある画像位置には物理的に一つの値だけが存在するという視覚情報処理が仮定すべき外界の物理的制約を反映していると文学的に解釈することも可能である。</p>
<p><center>
<div>
  <img src="../assets/cnn/pool.jpeg" width="74%">
<div>
プーリング層では，入力層ニューロン数の各深さについて空間的ダウンサンプリングを行う。この例は サイズ[224x224x64]の入力層ニューロン数がフィルタサイズ 2 でプールされ，サイズ 2 の出力ニューロン数 [112x112x64] は 2 倍である。奥行き数が保持されている。
<!--
Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved.  <br />-->
</div>
</div>
</center></p>
<h3 id="4">4. 全結合層<a class="headerlink" href="#4" title="Permanent link">&para;</a></h3>
<p>全結合層のニューロンは、通常のニューラルネットワークと同じ<br>
前層の全ニューロンと結合を持つ<br></p>
<h3 id="5-cnn">5. CNN アーキテクチャ<a class="headerlink" href="#5-cnn" title="Permanent link">&para;</a></h3>
<ol>
<li>畳込層</li>
<li>プーリング層</li>
<li>全結合層</li>
</ol>
<p>層は以上 3 種類が一般的。</p>
<h3 id="6-cnn">6. CNN の層構造<a class="headerlink" href="#6-cnn" title="Permanent link">&para;</a></h3>
<p>入力層 <script type="math/tex">\rightarrow</script> [[畳込層 <script type="math/tex">\rightarrow</script> ReLU]<script type="math/tex">\times N\rightarrow</script> プーリング(?)]<script type="math/tex">\times</script> M <script type="math/tex">\rightarrow</script> [全結合層 <script type="math/tex">\rightarrow</script> ReLU] <script type="math/tex">\times</script> K <script type="math/tex">\rightarrow</script> 全結合層</p>
<p>最近のトレンドとしては大きなフィルタより小さなフィルタが好まれる傾向にある。<br>
[3x3] が好まれる理由はど真ん中がある奇関数を暗黙に仮定しているためだと思われる（浅川の妄想）。
その代わり多段にすれば [3x3] が２層で ［5x5]，３層で[7x7]の受容野を形成できるから受容野の広さを層の深さとして実装しているとも解釈できる。１層で[7x7]の受容野より３層で[7x7]の受容野を実現した方が the simpler, the better の原則に沿っているとも（文学的）解釈が可能である（またしても浅川妄想）。</p>
<p>バックプロパゲーションの計算時に広い受容野を作るより層を分けた方が GPU のメモリに乗せやすいと言う計算上の利点もある。</p>
<hr />
<h2 id="activation-functions">活性化関数 activation functions<a class="headerlink" href="#activation-functions" title="Permanent link">&para;</a></h2>
<div class="codehilite" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
%matplotlib inline

<span style="color: #228B22">## [Original](https://github.com/alrojo/tensorflow-tutorial/\</span>
<span style="color: #228B22">## blob/master/lab1_FFN/lab1_FFN.ipynb)</span>

<span style="color: #228B22"># PLOT OF DIFFERENT OUTPUT USNITS</span>
x = np.linspace(-<span style="color: #B452CD">6</span>, <span style="color: #B452CD">6</span>, <span style="color: #B452CD">100</span>)
relu = <span style="color: #8B008B; font-weight: bold">lambda</span> x: np.maximum(<span style="color: #B452CD">0</span>, x)
leaky_relu = <span style="color: #8B008B; font-weight: bold">lambda</span> x: np.maximum(<span style="color: #B452CD">0</span>, x) + <span style="color: #B452CD">0.1</span>*np.minimum(<span style="color: #B452CD">0</span>, x) 
elu = <span style="color: #8B008B; font-weight: bold">lambda</span> x: (x &gt; <span style="color: #B452CD">0</span>)*x + (<span style="color: #B452CD">1</span> - (x &gt; <span style="color: #B452CD">0</span>))*(np.exp(x) - <span style="color: #B452CD">1</span>) 
sigmoid = <span style="color: #8B008B; font-weight: bold">lambda</span> x: (<span style="color: #B452CD">1</span>+np.exp(-x))**(-<span style="color: #B452CD">1</span>)
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">softmax</span>(w, t = <span style="color: #B452CD">1.0</span>):
    e = np.exp(w)
    dist = e / np.sum(e)
    <span style="color: #8B008B; font-weight: bold">return</span> dist
x_softmax = softmax(x)

plt.figure(figsize=(<span style="color: #B452CD">6</span>,<span style="color: #B452CD">6</span>))
plt.plot(x, relu(x), label=<span style="color: #CD5555">&#39;ReLU&#39;</span>, lw=<span style="color: #B452CD">2</span>)
plt.plot(x, leaky_relu(x), label=<span style="color: #CD5555">&#39;Leaky ReLU&#39;</span>,lw=<span style="color: #B452CD">2</span>)
plt.plot(x, elu(x), label=<span style="color: #CD5555">&#39;Elu&#39;</span>, lw=<span style="color: #B452CD">2</span>)
plt.plot(x, sigmoid(x), label=<span style="color: #CD5555">&#39;Sigmoid&#39;</span>,lw=<span style="color: #B452CD">2</span>)
plt.legend(loc=<span style="color: #B452CD">2</span>, fontsize=<span style="color: #B452CD">16</span>)
plt.title(<span style="color: #CD5555">&#39;Non-linearities&#39;</span>, fontsize=<span style="color: #B452CD">20</span>)
plt.ylim([-<span style="color: #B452CD">2</span>, <span style="color: #B452CD">5</span>])
plt.xlim([-<span style="color: #B452CD">6</span>, <span style="color: #B452CD">6</span>])

<span style="color: #228B22"># softmax</span>
<span style="color: #228B22"># assert that all class probablities sum to one</span>
<span style="color: #658b00">print</span>(np.sum(x_softmax))
<span style="color: #8B008B; font-weight: bold">assert</span> <span style="color: #658b00">abs</span>(<span style="color: #B452CD">1.0</span> - x_softmax.sum()) &lt; <span style="color: #B452CD">1e-8</span>
</pre></div>


<p><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_activaton_functions.ipynb">デモファイル 2019komazawa_activation_functions.ipynb</a></p>
<p><center>
<img src="../assets/2012AlexNet_ReLU.svg" style="width:49%"><br>
アレックスネットの収束の様子
</center></p>
<!--
## 最終層（あるいは最終２層）は全結合層

ドット積を実行し、得られた結果に非線形活性化関数により出力を計算<br>
ネットワーク全体は、一方の生画像ピクセルから他方のクラススコアまで、単一の微分可能なスコア関数を依然として表現している。 最上位層の全結合層には損失関数（SVM/Softmax）がある。通常のニューラルネットワークの学習と同じ手法を用いる

---
-->

<hr />
<h2 id="tensorflow-hub"><a href="https://www.tensorflow.org/hub">TensorFlow HUB</a><a class="headerlink" href="#tensorflow-hub" title="Permanent link">&para;</a></h2>
<h3 id="inception-resnet-r-cnn-regional-convolutional-neural-networks">インセプション Inception，残渣ネット ResNet，領域 R-CNN (Regional Convolutional Neural Networks)<a class="headerlink" href="#inception-resnet-r-cnn-regional-convolutional-neural-networks" title="Permanent link">&para;</a></h3>
<ul>
<li>what and where routes</li>
<li>心理学的対応物(？)</li>
<li>/2015documents/2014Cadieu_Deep_Neural_Networks_Rival_the_Representation_of_Primate_IT_Cortex_for_Core_Visual_Object_Recognition.pdf</li>
<li>/2019documents/2019NasrViswanathanNieder_Number_detectors_spontaneously_emerge_in_a_deep_neural_network_designed_for_visual_object_recognition.pdf</li>
<li>/2018documents/2018Marcus_Deep_Learning_A_Critical_Appraisal.pdf</li>
<li>転移学習</li>
</ul>
<h3 id="notebooks">Notebooks<a class="headerlink" href="#notebooks" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/text_classification_with_tf_hub_on_kaggle.ipynb">colab/text_classification_with_tf_hub_on_kaggle.ipynb</a>
Shows how to solve a problem on Kaggle with TF-Hub.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb">colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb</a>
Explores text semantic similarity with the <a href="https://tfhub.dev/google/universal-sentence-encoder/2">Universal Encoder Module</a>.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb">colab/tf_hub_generative_image_module.ipynb</a>
Explores a generative image module.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb">colab/action_recognition_with_tf_hub.ipynb</a>
Explores action recognition from video.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_delf_module.ipynb">colab/tf_hub_delf_module.ipynb</a>
Exemplifies use of the <a href="https://tfhub.dev/google/delf/1">DELF Module</a> for landmark recognition and matching.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">colab/object_detection.ipynb</a> 
Explores object detection with the use of the  <a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">Faster R-CNN module trained on Open Images v4</a>.</li>
</ul>
<hr />
<!--
<center>
<img src='https://cdn-images-1.medium.com/max/1280/1*sS_WZM4GLS88XlnDLKcZ-g.png' style='width:94%'><br>
from [A guide to Face Detection in Python](https://towardsdatascience.com/a-guide-to-face-detection-in-python-3eab0f6b9fc1)
</center>

-->

<ul>
<li>
<p><a href="https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb">The Complete Beginner’s Guide to Deep Learning: Convolutional Neural Networks and Image Classification</a>, Anne Bonner Feb. 02</p>
</li>
<li>
<p>畳込みニューラルネットワーク (Convlutional Neural Networks:CNN) とは画像認識におけるゲームチェンジャー(以後，画像認識，ビデオ分類，自動運転，ドローン，ゲームなどへの応用多数)</p>
</li>
<li><a href="http://image-net.org/challenges/LSVRC/">イメージネット画像コンテスト</a>では，分類 (classification) 課題と位置 (locallization) 課題とからなる。</li>
<li>コンテストは 2010 年から Li Fei-Fei さん中心となって <a href="https://www.mturk.com/">AMT</a> で画像のアノテーションを行って 画像を2012 年の優勝チームが CNN を使った。通称<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">アレックスネット</a></li>
<li><a href="http://cs231n.stanford.edu/index.html">スタンフォード大学の授業 CS231n: Convolutional Neural Networks for Visual Recognition</a>. スライド](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture05.pdf)</li>
</ul>
<h1 id="_6">さらなる情報<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h1>
<ul>
<li>Math? <a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu</li>
<li>C.-C. Jay Kuo <a href="https://arxiv.org/abs/1609.04112">Understanding Convolutional Neural Networks With a Mathematical Model</a>.</li>
<li><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">the absolute basics of activation functions, you can find that here</a></li>
<li>[Artificial neural networks? <a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">You can learn about them here</a></li>
</ul>
<!--
#### ReLU layer
The **ReLU** (rectified linear unit) layer is another step to our convolution layer. 
You’re applying an activation function onto your feature maps to increase non-linearity in the network. 
This is because images themselves are highly non-linear! 
It removes negative values from an activation map by setting them to zero.

Convolution is a linear operation with things like element wise matrix
multiplication and addition. 
The real-world data we want our CNN to learn will be non-linear. 
We can account for that with an operation like ReLU. 
You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy.
-->

<!--
Want to dig deeper? Try Kaiming He, et al. [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).

If you need a little more info about [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!
-->

<!--
Here’s how our little buddy is looking after a ReLU activation function
turns all of the negative pixel values black



wzxhzdk:2


<center>
<img src="../assets/output2.jpg" style="width:84%">
</center>


#### Pooling
The last thing you want is for your network to look for one specific feature in an exact shade in an exact location. 
That’s useless for a good CNN! 
You want images that are flipped, rotated, squashed, and so on. 
You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location. 
No matter what the lighting or the number of spots,or whether that leopard is fast asleep or crushing prey. 
You want **spatial variance**! You want flexibility. 
That’s what pooling is all about.

Pooling progressively reduces the size of the input representation. 
It makes it possible to detect objects in an image no matter where they’re located. 
Pooling helps to reduce the number of required parameters and the amount of computation required. 
It also helps control **overfitting**.

Overfitting can be kind of like when you memorize super specific details before a test without understanding the information. 
When you memorize details, you can do a great job with your flashcards at home.
You’ll fail a real test, though, if you’re presented with new information.

(Another example: if all of the dogs in your training data have spots and dark eyes, your network will believe that for an image to be classified as a dog, it must have spots and dark eyes. 
If you test your data with that same training data, it will do an amazing job of
classifying dogs correctly! But if your outputs are only “dog” and “cat,” and your network is presented with new images containing, say, a Rottweiler and a Husky, it will probably wind up classifying both the Rottweiler and the Husky as cats. You can see the problem!)


Without variance, your network will be useless with images that don’t exactly match the training data. 
**Always, always, always keep your training and testing data separate**! 
If you test with the data you trained on, your network has the information memorized! 
It will do a terrible job when it’s introduced to any new data.

#### Overfitting is not cool.
So for this step, you take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.

The most common example of pooling is **max pooling**. 
In max pooling, the input image is partitioned into a set of areas that don’t overlap. 
The outputs of each area are the maximum value in each area. 
This makes a smaller size with fewer parameters.

Max pooling is all about grabbing the maximum value at each spot in the image. 
This gets rid of 75% of the information that is not the feature. 
By taking the maximum value of the pixels, you’re accounting for distortion. 
If the feature rotates a little to the left or right or whatever, the pooled feature will be the same. You’re reducing the size and parameters. 
This is great because it means that the model won’t overfit on that information.

You could use **average pooling or sum pooling**, but they aren’t common choices. 
Max pooling tends to perform better than both in practice. 
In max pooling, you’re taking the largest pixel value. 
In average pooling, you take the average of all the pixel values at that spot in the image. 
(Actually, there’s a trend now towards using smaller filters or discarding pooling layers entirely. 
This is in response to an aggressive reduction in representation size.)

__Want to look a little more at why you might want to choose max pooling
and why you might prefer a stride of two pixels? Check out Dominik
Scherer et. al, [Evaluation of Pooling Operations in Convolutional
Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf).__

If you go [here](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) you can check out a really interesting 2D visualization of a convolutional layer. 
Draw a number in the box on the left-hand side of the screen and then really go through the output. 
You can see the  convolved and pooled layers as well as the guesses. 
Try hovering over a single pixel so you can see where the filter was applied.


So now we have an input image, an applied convolutional layer, and an applied pooling layer.

Let’s visualize the output of the pooling layer!

We were here:

<center>
<img src="../assets/output3.jpg" style="width:94%">
</center>

The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps. 
It does this by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.

See how the image has changed size?

<center>
<img src="../assets/output4.jpg" style="width:94%">
</center>

Cool, right?

#### Flattening

This is a pretty simple step. You flatten the pooled feature map into a sequential column of numbers (a long vector). 
This allows that information to become the input layer of an artificial neural network for further processing.


#### Fully connected layer
At this step, we add an **artificial neural network** to our convolutional neural network. 
(Not sure about artificial neural networks? [You can learn about them here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!)
-->

<!--
The main purpose of the artificial neural network is to combine our features into more attributes. 
These will predict the classes with greater accuracy. This combines features and attributes that can predict classes better.

At this step, the error is calculated and then backpropagated. 
The weights and feature detectors are adjusted to help optimize the performance of the model. 
Then the process happens again and again and again. 
This is how our network trains on the data! 

How do the output neurons work when there’s more than one?

First, we have to understand what weights to apply to the synapses that connect to the output. 
We want to know which of the previous neurons are important for the output.

If, for example, you have two output classes, one for a cat and one for a dog, a neuron that reads “0” is absolutely uncertain that the feature belongs to a cat. A neuron that reads “1 is absolutely certain that the feature belongs to a cat. 
In the final fully connected layer, the neurons will read values between 0 and 1. 
This signifies different levels of certainty. 
A value of 0.9 would signify a certainty of 90%. 
The cat neurons that are certain when a feature is identified know that the image is a cat. 
They say the mathematical equivalent of, “These are my neurons! I should be triggered!” If this happens many times, the network learns that when certain features fire up, the image is a cat.


Through lots of iterations, the cat neuron learns that when certain features fire up, the image is a cat. 
The dog (for example) neuron learns that when certain other features fire up, the image is a dog. 
The dog neuron learns that for example again, the “big wet nose” neuron and the “floppy ear” neuron contribute with a great deal of certainty to the dog neuron.
It gives greater weight to the “big wet nose” neuron and the “floppy ear” neuron. 
The dog neuron learns to more or less ignore the “whiskers” neuron and the “cat-iris” neuron. 
The cat neuron learns to give greater weight to neurons like “whiskers” and “cat-iris.”
(Okay, there aren’t actually “big wet nose” or “whiskers” neurons. 
But the detected features do have distinctive features of the specific class.)


Once the network has been trained, you can pass in an image and the neural network will be able to determine the image class probability for that image with a great deal of certainty.

The fully connected layer is a traditional Multi-Layer Perceptron. 
It uses a classifier in the output layer. 
The classifier is usually a softmax activation function. 
Fully connected means every neuron in the previous layer connects to every neuron in the next layer. 
What’s the purpose of this layer? To use the features from the output of the previous layer to classify the input image based on the training data.

Once your network is up and running you can see, for example, that you have a 95% probability that your image is a dog and a 5% probability that your image is a cat.


Why do these numbers add up to 1.0? (0.95 + 0.05)

There isn’t anything that says that these two outputs are connected to each other. 
What is it that makes them relate to each other? 
Essentially, they wouldn’t, but they do when we introduce the **softmax function**.
This brings the values between 0 and 1 and makes them add up to 1 (100%). 
(You can read all about this on Wikipedia.) 
The softmax function takes a vector of scores and squashes it to a vector of values between 0 and 1 that add up to 1.

After you apply a softmax function, you can apply the loss function.
Cross entropy often goes hand in hand with softmax. 
We want to minimize the loss function so we can maximize the performance of our
network.

At the beginning of backpropagation, your output values would be tiny.
That’s why you might choose cross entropy loss. 
The gradient would be very low and it would be hard for the neural network to start adjusting in the right direction. 
Using cross entropy helps the network assess even a tiny error and get to the optimal state faster.


#### Want more? Check out
-->

<ul>
<li><a href="https://www.youtube.com/watch?v=mlaLLQofmR8">video by Geoffrey Hinton</a> on the softmax function</li>
<li><a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/">A Friendly Introduction to Cross Entropy Loss</a> by Rob DiPietro</li>
<li><a href="https://peterroelants.github.io/posts/cross-entropy-softmax/">How to Implement a Neural Network Intermezzo 2</a> by Peter Roelants</li>
<li>
<p><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">画像分類の基礎</a>)</p>
</li>
<li>
<p><a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu </p>
</li>
<li>Yann LeCun’s original article, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-Based Learning Applied to Document Recognition</a></li>
<li><a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The Nine Deep Learning Papers You Need to Know About</a> (Understanding CNNs part 3) by Adit Deshpande</li>
</ul>
<h1 id="dimensionality-reduction">次元削減，次元圧縮 dimensionality reduction<a class="headerlink" href="#dimensionality-reduction" title="Permanent link">&para;</a></h1>
<p><a href="https://lvdmaaten.github.io/images/laurens.png">Van der Maaten</a> の<a href="https://lvdmaaten.github.io/drtoolbox/">ページ</a>によれば<strong>次元圧縮</strong> dimensionality reduction には 34 種類の方法があります。PCA, FA, MDS などが心理学では伝統的<strike><strong>無批判</strong></strike>に使われてきました。多くの心理学者は因子分析を好む <strike><strong>因子分析偏愛者</strong>，<strong>因子分析フェチ</strong>，<strong>factor analysis-pheria</strong></strike> ようですが，そのことを支持する理論的根拠は存在しません</p>
<h2 id="pca">PCA<a class="headerlink" href="#pca" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>主成分分析</strong> PCA: principal component analysis は一番最初に提案された手法で，<strong>固有値分解</strong> に基づきます。</li>
<li>特に 2 次元へのマッピングは皮質地図 cortial map との対応が考えられるので興味深い</li>
<li>条件付き最大値を求める一般的方法でラグランジアン Lagrangian，あるいはラグランジェの未定乗数法 Lagrange multiplier が定義される。</li>
</ul>
<h3 id="_7">最初は<strike>知らない方が良い</strike>知らなくても良いラグランジアンの説明<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction">Khan アカデミーのラグランジアンの説明</a>, <a href="https://youtu.be/vwUV2IDLP8Q">YouTube</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction">Constrained optimization introduction</a>, <a href="https://youtu.be/yuqB-d5MjZA">YouTube</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/lagrange-multipliers-using-tangency-to-solve-constrained-optimization">Lagrange multipliers, using tangency to solve constrained optimization</a>, <a href="https://youtu.be/yuqB-d5MjZA">YouTube</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/finishing-the-intro-lagrange-multiplier-example">Finishing the intro lagrange multiplier example</a>, <a href="https://youtu.be/aep6lwPqm6I">YouTube</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/the-lagrangian">The Lagrangian</a>, <a href="https://youtu.be/hQ4UNu1P2kw">YouTube</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/the-lagrangian">Meaning of the Lagrange multiplier</a>, <a href="https://youtu.be/m-G3K2GPmEQ">YouTube</a></li>
</ul>
<!--
## ラグランジアン
- [オイラーラグランジェ方程式](https://youtu.be/sFqp2lCEvwM)
-->

<p>
<script type="math/tex; mode=display">\begin{equation}
\mathcal{L}=\mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw}+\lambda\left({\mathbf{ww}^\top-1}\right)
\end{equation}</script>
</p>
<div class="codehilite" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span>

math.gamma(<span style="color: #B452CD">1</span>)
math.gamma(<span style="color: #B452CD">2</span>)
math.gamma(<span style="color: #B452CD">3</span>)
</pre></div>


<h2 id="t-sne">t-SNE<a class="headerlink" href="#t-sne" title="Permanent link">&para;</a></h2>
<ul>
<li>t-SNE は「ティーズニー」と発音します。<ul>
<li>ちなみに <script type="math/tex">f(x)</script> はどのように発音するか知っていますか？あるいは <script type="math/tex">f\left(x\vert y\right)</script> は？</li>
</ul>
</li>
<li>心理学者以外では支配的(かも)</li>
<li>t: **<script type="math/tex">t</script>-分布</li>
<li>S: <strong>S</strong>tochastic 確率的</li>
<li>N: <strong>N</strong>eigbor 隣接(隣人)</li>
<li>E: <strong>E</strong>mbedding 埋め込み</li>
<li>PCA, FA, 古典的 MDS (Torgerson) が固有値に基づくのに対して，t-SNE は多次元空間と低次元空有間への写像について確率的な仮定を考え，両者の分布が近づくように学習を行う。<ul>
<li>ここで，2 つの分布の距離を考える。距離の定義には様々が提案がなされているが <strong>カルバック=ライブラー</strong> ダイバージェンス(あるいは KL 距離)が用いられる。<a href="https://www.cis.twcu.ac.jp/~asakawa/2019komazawa/lect06supp/">本日の付録</a>参照</li>
</ul>
</li>
<li>以下は van der Maaten and Hinton (2008) のオリジナル論文に掲載された結果である</li>
</ul>
<p><center>
<img src='../assets/2008vanderMaaten_tSNE_Fig2.svg' style="width:74%"><br>
van der Maaten and Hinton (2008) Fig.2 </p>
<p><img src='../assets/2008vanderMaaten_tSNE_Fig3.svg' style="width:74%">
van der Maaten and Hinton (2008) Fig.3 
</center></p>
<p><center>
<img src='../assets/t-and-norm-dists.svg' style='width:49%'></br>
t 分布(<script type="math/tex">\nu=1</script>)と標準正規分布の確率密度分布 pdf
</center></p>
<ul>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_pca_tsne_fashion_mnist.ipynb">PCA と t-SNE の比較実験</a></li>
<li>スチューデントの <script type="math/tex">t</script> 分布
<a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><script type="math/tex">t</script> 分布</a> の確率密度関数 pdf は以下のとおり: <font size='+2' color='green'>数学愛好者<strike><font color='red' style='bold'>数学恐怖症 For all math-phobia</font></strike>の皆様へ</font></li>
</ul>
<p>
<script type="math/tex; mode=display">\begin{equation}
p(x,\nu)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}.
\end{equation}</script>
</p>
<ul>
<li>おそろしい形をしていますが，ポイントは <script type="math/tex">\Gamma</script> 関数(がんまかんすう)であり，内部で使われている
<script type="math/tex">\nu</script> (「にゅう」と読むギリシャアルファベット)は自由でデータ数 <script type="math/tex">-1</script> です。</li>
<li>
<p>
<script type="math/tex">\Gamma</script> は<a href="https://simple.wikipedia.org/wiki/Gamma_function">ガンマ関数</a>であり，
階乗の連続量への拡張とみなすことができます。</p>
</li>
<li>
<p>最も簡単な場合 <script type="math/tex">\nu=1</script> を考えれば，上式は以下のようになります。</p>
</li>
</ul>
<p>
<script type="math/tex; mode=display">\begin{equation}
p(x,\nu=1)=\frac{\Gamma\left(1\right)}{\sqrt{\nu\pi}\,\Gamma\left(\frac{1}{2}\right)}\left(1+x^2\right)^{-1}.
\end{equation}</script>
</p>
<ul>
<li>さらに <script type="math/tex">\Gamma(1)=1</script>, <script type="math/tex">\displaystyle\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}</script> を考慮すれば，以下の式を得ます。</li>
</ul>
<p>
<script type="math/tex; mode=display">\begin{equation}
p(x,\nu=1)=\frac{1}{\pi}\frac{1}{1+x^2},
\end{equation}</script>
</p>
<ul>
<li>
<p>
<script type="math/tex">\pi</script> は円周率で定数ですから，グラフの形を考えるときには無視して構いません。従って <script type="math/tex">t</script> 分布の本質は <script type="math/tex">\displaystyle\frac{1}{1+x^2}</script> であることになります。</p>
</li>
<li>
<p><a href="https://colab.research.google.com/notebook?hl=ja#create=true&language=python3" target="_blank">codolab</a> で確認してみましょう。</p>
</li>
</ul>
<div class="codehilite" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span>

<span style="color: #658b00">print</span>(math.gamma(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>))

<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,<span style="color: #B452CD">11</span>):
    <span style="color: #658b00">print</span>(i, math.gamma(i))

math.sqrt(math.pi) == math.gamma(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)
</pre></div>


<ul>
<li>ガンマ関数の概形を描いてみましょう</li>
</ul>
<div class="codehilite" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
x = np.linspace(<span style="color: #B452CD">0.25</span>,<span style="color: #B452CD">4</span>)
y = [math.gamma(xi) <span style="color: #8B008B; font-weight: bold">for</span> xi <span style="color: #8B008B">in</span> x] 
plt.plot(x,y)
</pre></div>


<ul>
<li>つづいて正規分布と <script type="math/tex">t</script>-分布とを比較してみましょう</li>
</ul>
<div class="codehilite" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">scipy.stats</span> <span style="color: #8B008B; font-weight: bold">import</span> norm

x = np.linspace(norm.ppf(<span style="color: #B452CD">0.001</span>), norm.ppf(<span style="color: #B452CD">0.999</span>), <span style="color: #B452CD">100</span>)
nu = <span style="color: #B452CD">1</span>

plt.plot(x, t.pdf(x, nu), <span style="color: #CD5555">&#39;b-&#39;</span>, lw=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">&#39;t&#39;</span>)
plt.plot(x, norm.pdf(x),  <span style="color: #CD5555">&#39;r-&#39;</span>, lw=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">&#39;norm&#39;</span>)
plt.legend()
</pre></div>


<ul>
<li><a href="https://lvdmaaten.github.io/drtoolbox/">t-SNE</a></li>
<li><a href="https://cs.stanford.edu/people/karpathy/tsnejs/">tSNEJS demo</a>, <a href="http://karpathy.github.io/2014/07/02/visualizing-top-tweeps-with-t-sne-in-Javascript/">blog</a></li>
<li><a href="https://harveyslash.github.io/TSNE-Embedding-Visualisation">Embedding Projector</a></li>
<li><a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively</a></li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../lect04/" class="btn btn-neutral float-right" title="第 4 回">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../lect02/" class="btn btn-neutral" title="第 2 回"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright (c) 2020</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../lect02/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../lect04/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
