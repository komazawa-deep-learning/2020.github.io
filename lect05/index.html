<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Shin Asakawa">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>第 5 回 - 2020駒澤大学心理学特講IIIA</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet">
  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet">
  <link href="../css/specific.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u7b2c 5 \u56de";
    var mkdocs_page_input_path = "lect05.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> 2020駒澤大学心理学特講IIIA</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00check_meet/">第 0 回 事前確認</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00guidance/">第 0 回 ガイダンス</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect01/">第 1 回 05 月 08 日</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect02/">第 2 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect03/">第 3 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect04/">第 4 回</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">第 5 回</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a></li>
    

    <li class="toctree-l2"><a href="#_1">連絡事項</a></li>
    

    <li class="toctree-l2"><a href="#_2">復習を兼ねてもう一度歴史</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#lenet5-lecun-1998">LeNet5 (LeCun, 1998)</a></li>
        
            <li><a class="toctree-l3" href="#alexnet-krizensky-et-al-2012">AlexNet (Krizensky, et al., 2012)</a></li>
        
            <li><a class="toctree-l3" href="#goolenet-inception-szegedy-et-al-2014">GooLeNet (Inception) (Szegedy et. al, 2014)</a></li>
        
            <li><a class="toctree-l3" href="#r-cnn-2015">R-CNN (2015)</a></li>
        
            <li><a class="toctree-l3" href="#he-et-al-2015">残渣ネット (He et. al, 2015)</a></li>
        
            <li><a class="toctree-l3" href="#fast-r-cnn-faster-r-cnn-2014">Fast R-CNN と Faster R-CNN (2014)</a></li>
        
            <li><a class="toctree-l3" href="#_3">セマンティックセグメンテーションとインスタンスセグメンテーション</a></li>
        
            <li><a class="toctree-l3" href="#_4">画像変換</a></li>
        
            <li><a class="toctree-l3" href="#_5">まんがの画風変換</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect06/">第 6 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect07/">第 7 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect08/">第 8 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect09/">第 9 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">付録</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../colaboratory_intro/">Colab 事始め</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_faq/">Colaboratory FAQ</a>
                </li>
                <li class="">
                    
    <a class="" href="../eco/">エコシステム</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_numpy_intro_ja/">Python の基礎</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_modules/">Python modules</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">2020駒澤大学心理学特講IIIA</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>第 5 回</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="iiia"><a href="https://komazawa-deep-learning.github.io/">ディープラーニングの心理学的解釈 (心理学特講IIIA)</a><a class="headerlink" href="#iiia" title="Permanent link">&para;</a></h1>
<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: XX/XX/2020<br/>
Appache 2.0 license<br/>
</div>

<p>
<script type="math/tex">\ldots</script> 工事中 <script type="math/tex">\ldots</script>
</p>
<h1 id="_1">連絡事項<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<ul>
<li>
<p>今回の資料の <a href="https://drive.google.com/drive/folders/1qEgY6JVXc9CeCvMmdMxGKWoWIeLFC4GD" target="_blank">PDF ファイル</a></p>
</li>
<li>
<p>ReLU, tanh に触れていない</p>
</li>
<li>memo: バッチ正則化をやっていない</li>
<li>本日の次元圧縮でも，SGD, dropout と同じく</li>
</ul>
<hr />
<h1 id="_2">復習を兼ねてもう一度歴史<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<h2 id="lenet5-lecun-1998">LeNet5 (LeCun, 1998)<a class="headerlink" href="#lenet5-lecun-1998" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/1998LeCun_Fig2_CNN.svg' style='width:94%'><br>
LeCun (1998) より
</center></p>
<h2 id="alexnet-krizensky-et-al-2012">AlexNet (Krizensky, et al., 2012)<a class="headerlink" href="#alexnet-krizensky-et-al-2012" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2012AlexNet.svg' style='width:94%'><br>
Krzensky et al (2012) より
</center></p>
<h2 id="goolenet-inception-szegedy-et-al-2014">GooLeNet (Inception) (Szegedy et. al, 2014)<a class="headerlink" href="#goolenet-inception-szegedy-et-al-2014" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2014Szegedy_GoogLeNet.svg' style='width:99%'><br>
</center></p>
<p><center>
<img src='../assets/2013Uijings_Selective_Search_Fig1.svg' style='width:94%'><br>
空間ピラミッド (2015) より
</center></p>
<hr />
<h2 id="r-cnn-2015">R-CNN (2015)<a class="headerlink" href="#r-cnn-2015" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2013Girshick_RCNN_Fig1.svg' style='width:74%'><br>
Girshick (2013) より</p>
<p><img src='../assets/2014SPP.svg' style='width:74%'><br>
Girshick (2013) より
</center></p>
<hr />
<h2 id="he-et-al-2015">残渣ネット (He et. al, 2015)<a class="headerlink" href="#he-et-al-2015" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/ResNet_Fig2.svg' style='width:39%'><br>
<img src='../assets/2015ResNet30.svg' style='width:94%'><br>
He (2015) より
</center></p>
<hr />
<h2 id="fast-r-cnn-faster-r-cnn-2014">Fast R-CNN と Faster R-CNN (2014)<a class="headerlink" href="#fast-r-cnn-faster-r-cnn-2014" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2015Fast_R-CNN_Fig1.svg' style='width:74%'><br>
Fast R-CNN</p>
<p><img src='../assets/2015Faster_RCNN_RPN.svg' style='width:74%'><br>
Faster R-CNN
</center></p>
<hr />
<h2 id="_3">セマンティックセグメンテーションとインスタンスセグメンテーション<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<ul>
<li>完全畳み込みネットワーク(Fully Convolutional Network:FCN) と呼ばれるセマンティックセグメンテー
ションを実現するネットワーク</li>
<li>FCN とは文字通り全ての層が畳込み層であるモデル</li>
</ul>
<p><center>
<img src='../assets/2015Long_FCN.svg' style='width:94%'></br>
Long (2017) FCN
</center></p>
<ul>
<li>通常のCNN は，出力層のユニット数が識別すべきカテゴリー数であった。一方 FCN では入力画像の画素数だけ
出力層が必要になる。</li>
<li>すなわち各画素がそれぞれどのカテゴリーに属するのかを出力する必要があるため出力層には，縦画素数 <script type="math/tex">\times</script> 横画素数 <script type="math/tex">\times</script> カテゴリー数の出力ニューロンが用意される。</li>
<li>
<p>図 では，識別すべきカテゴリー数 が 20 であったたま，どのカテゴリーにも属さない，すなわち背景を指示するもう1 つのカテゴリーを加えた計 21 カテゴリーの分類を行うことになる。</p>
</li>
<li>
<p>CNN では畳込演算によって畳込みのカーネル幅(受容野) だけ近傍の入力刺激を加えて計算することになるため，
上位層では下位層に比べて受容野が大きくなることの影響で画像サイズは小さく(あるいは粗く) なってしまう</p>
</li>
<li>このため，最終出力層に入力層と同じ解像度の画素数を得るためには，畳込みと反対方向の解像度を細かくする工夫が必要となる。</li>
<li>これを解決する一つの方法がアンサンプリング(unsampling) と呼ばれる方法</li>
<li>下位のプーリング層の情報を用いて詳細な解像度を得る</li>
<li>図 はアンサンプリングにより詳細な画像，すなわち最終的には入力画像と等解像度の出力を得る仕組みを示している。</li>
</ul>
<p><center>
<img src='../assets/2015Long_FCN2J.svg' style='width:94%'><br>
</center></p>
<ul>
<li>同様の仕組みがセグネット Segnet でも取り入れられている</li>
</ul>
<p><center>
<img src='../assets/SegNet.svg' style='width:94%'><br>
</center></p>
<p><center>
<img src='../assets/2016Radford_deconv.svg' style='width:94%'><br>
</center></p>
<hr />
<p><center>
<img src='../assets/2017He_MaskRCNN_02Oject.svg' style='width:74%'><br>
<img src='../assets/2017He_MaskRCNN_02SemSeg.svg' style='width:74%'><br>
<img src='../assets/2017He_MaskRCNN_08.svg' style='width:94%'><br>
<img src='../assets/2017He_MaskRCNN_41.svg' style='width:94%'><br>
</center></p>
<p><center>
<img src='../assets/yolo-and-ssd.jpg' style='width:94%'><br>
</center></p>
<p>— <a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a>, 2015.</p>
<blockquote>
<p>A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.</p>
</blockquote>
<p><center>
<iframe width="600" height="300" src="https://www.youtube.com/embed/lxLyLIL7OsU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center></p>
<hr />
<h2 id="_4">画像変換<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2017Taigman_fig.svg' style='width:94%'><br>
</center></p>
<h3 id="1">1<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_1.svg' style='width:94%'>
</center></p>
<h3 id="2">2<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_2.svg' style='width:94%'>
</center></p>
<h3 id="3">3<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_3.svg' style='width:94%'>
</center></p>
<h3 id="4">4<a class="headerlink" href="#4" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_4.svg' style='width:94%'>
</center></p>
<h3 id="5">5<a class="headerlink" href="#5" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_5.svg' style='width:94%'>
</center></p>
<h3 id="6">6<a class="headerlink" href="#6" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_6.svg' style='width:94%'>
</center></p>
<h3 id="7">7<a class="headerlink" href="#7" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp_7.svg' style='width:94%'>
</center></p>
<h3 id="8">8<a class="headerlink" href="#8" title="Permanent link">&para;</a></h3>
<p><center>
<img src='../assets/2017Reed_tmp8.svg' style='width:94%'>
</center></p>
<p><center>
<img src='../assets/2017Zhu_cycleGAN1.svg' style='width:94%'><br>
<img src='../assets/2017Zhu_cycleGAN2.svg' style='width:94%'><br>
<img src='../assets/2017Zhu_cycleGAN3.svg' style='width:94%'><br>
</center></p>
<hr />
<h2 id="_5">まんがの画風変換<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>
<p><center>
<img src='../assets/2018Chen_CartoonGAN.svg' style="width:94%"></br>
``CartoonGAN: Generative Adversarial Networks for Photo Cartoonization'' CVPR 2018 (Conference on Computer Vision and Pattern Recognition)
</center></p>
<p><center>
<img src='https://cdn-images-1.medium.com/max/1800/1*GS5_bEgpy00cotNRFvAPyA.png' style='width:46%'>
<img src="https://d2l930y2yx77uc.cloudfront.net/production/uploads/images/7291694/rectangle_large_type_2_86d2e2a52336624f98ed8aa3163a1865.jpg" style='width:49%'><br></p>
<blockquote>
<p>左: 君の名は。右: 風の谷のナウシカ，より
</center></p>
</blockquote>
<hr />
<p><center>
<iframe width="600" height="300" src="https://www.youtube.com/embed/pW6nZXeWlGM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
Realtime Multi-Person 2D Human Pose Estimation using Part Affinity Fields, CVPR 2017 Oral
</center></p>
<p><center>
<iframe width="600" height="300" src="https://www.youtube.com/embed/PCBTZh41Ris" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
Paper: <a href="https://arxiv.org/pdf/1808.07371.pdf">https://arxiv.org/pdf/1808.07371.pdf</a><br>
Web site: <a href="https://carolineec.github.io/everybody_dance_now/">https://carolineec.github.io/everybody_dance_now/</a>
</center></p>
<hr />
<p><center>
<img src='../assets/2014Imgur_Saddle_point.gif' style='width:74%'><br>
<img src='../assets/2014Imgur_Beales_function.gif' style='width:74%'><br>
<img src='../assets/2014Imgur_Long_Valley.gif' style='width:74%'><br>
</center></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../lect06/" class="btn btn-neutral float-right" title="第 6 回">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../lect04/" class="btn btn-neutral" title="第 4 回"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright (c) 2020</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../lect04/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../lect06/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
