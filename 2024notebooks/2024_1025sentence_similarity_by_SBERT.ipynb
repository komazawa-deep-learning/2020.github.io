{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPlItuUR5wz/XjCGRkip+YI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_1025sentence_similarity_by_SBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f'device:{device}')\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "if isColab:\n",
        "\n",
        "    # GPU 情報を表示\n",
        "    !nvidia-smi -L\n",
        "\n",
        "    # `import bit` する前に termcolor を downgrade しないと colab ではテキストに色がつかない\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "    import termcolor\n",
        "\n",
        "    !pip install --upgrade openpyxl\n",
        "    !pip install --upgrade pandas\n",
        "    #!pip install --upgrade fugashi[unidic-lite]\n",
        "    !pip install --upgrade 'fugashi[unidic]'\n",
        "    !pip install --upgrade ipadic\n",
        "    !python -m unidic download\n",
        "    !pip install transformers\n",
        "    !pip install --upgrade jaconv\n",
        "\n",
        "    !pip install --upgrade xlrd\n",
        "\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "from termcolor import colored\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "try:\n",
        "    import ipynb_path\n",
        "except ImportError:\n",
        "    !pip install ipynb-path\n",
        "    import ipynb_path\n",
        "__file__ = os.path.basename(ipynb_path.get())\n",
        "print(f'__file__:{__file__}')\n",
        "print(f'torch.__version__:{torch.__version__}')"
      ],
      "metadata": {
        "id": "GS45h1g75HGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ngQrePV2EhM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import json\n",
        "\n",
        "if not os.path.exists('STAIR-captions/stair_captions_v1.2.tar.gz'):\n",
        "    !git clone https://github.com/STAIR-Lab-CIT/STAIR-captions.git\n",
        "    tgz = tarfile.open('STAIR-captions/stair_captions_v1.2.tar.gz')\n",
        "    tgz.extractall()\n",
        "\n",
        "STAIR_dir = '.'\n",
        "data_ = {'val': {  'fname':os.path.join(STAIR_dir, 'stair_captions_v1.2_val.json')},\n",
        "        # 'train': {'fname':os.path.join(STAIR_dir, 'stair_captions_v1.2_train.json')},\n",
        "        }\n",
        "data_\n",
        "\n",
        "for _k in data_.keys():\n",
        "    fname = data_[_k]['fname']\n",
        "    with open(fname, 'r') as fp:\n",
        "        data_[_k]['dic'] = json.load(fp)\n",
        "    print(f\"fname:{fname}, data_[_k]['dic']:{len(data_[_k]['dic'])}\")\n",
        "\n",
        "ann = {}\n",
        "for _k in data_.keys():\n",
        "    a = data_[_k]['dic']\n",
        "    for x in a['annotations']:\n",
        "        imageid, anno_id, caption = x['image_id'], x['id'], x['caption']\n",
        "        if not imageid in ann:\n",
        "            ann[imageid] = []\n",
        "        ann[imageid].append({'anno_id':anno_id,'caption':caption})\n",
        "\n",
        "image_ids_ = sorted(list(ann.keys()))\n",
        "\n",
        "# n_samples の数だけデータを印字\n",
        "n_samples = 5\n",
        "\n",
        "for imageid in image_ids_[:n_samples]:\n",
        "    print(ann[imageid])\n",
        "\n",
        "for imageid in list(ann.keys())[:n_samples]:\n",
        "    for s in ann[imageid]:\n",
        "        print(f'画像インデックス(imageid):{imageid}, アノテーション:{s}') # [1])\n",
        "print(f'len(ann):{len(ann)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 更にデータを印字\n",
        "idx2flickr_url = {}\n",
        "for _k in data_.keys():\n",
        "    a = data_[_k]['dic']\n",
        "    for x in a['images']:\n",
        "        flickr_url, idx = x['flickr_url'], x['id']\n",
        "        if idx in idx2flickr_url:\n",
        "            print(f'idx:{idx} duplicated!')\n",
        "        else:\n",
        "            idx2flickr_url[idx] = []\n",
        "        idx2flickr_url[idx] = flickr_url\n",
        "\n",
        "keys_ = sorted(list(ann.keys()))\n",
        "print(data_['val']['dic']['images'][:3])\n",
        "image_id = 391895\n",
        "print(ann[image_id])\n",
        "print(len(ann.keys()))"
      ],
      "metadata": {
        "id": "1n3yCw632O_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "\n",
        "image_id = np.random.choice(list(idx2flickr_url.keys()))\n",
        "print(f'image_id:{image_id}, ann[{image_id}]')\n",
        "\n",
        "for x in ann[image_id]:\n",
        "    print(x['caption'])\n",
        "\n",
        "print(idx2flickr_url[image_id])\n",
        "Image(url=idx2flickr_url[image_id])\n"
      ],
      "metadata": {
        "id": "gV97Hwqm2UYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "class SentenceBertJapanese:\n",
        "    def __init__(self,\n",
        "                 model_name_or_path:str = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\",\n",
        "                 device=device):\n",
        "                 #device=None):\n",
        "\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def _mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "    def encode(self, sentences, batch_size=8):\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(sentences), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
        "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self.model(**encoded_input)\n",
        "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
        "\n",
        "            all_embeddings.extend(sentence_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)\n",
        "\n",
        "\n",
        "MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
        "sbert_model = SentenceBertJapanese(MODEL_NAME)\n",
        "\n",
        "sentences = [\"暴走したAI\", \"暴走した人工知能\"]\n",
        "sentence_embeddings = sbert_model.encode(sentences, batch_size=8)\n",
        "\n",
        "print(\"Sentence embeddings:\", sentence_embeddings)"
      ],
      "metadata": {
        "id": "6H_ktgfG41-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = []\n",
        "for x in list(ann.keys()):\n",
        "    for _x in ann[x]:\n",
        "        sents.append(_x['caption'])\n",
        "print(f'総文数 len(sents):{len(sents)}')\n",
        "\n",
        "top_n = 100\n",
        "coco_anno = sbert_model.encode(sents[:top_n])\n",
        "print(coco_anno.size(), len(sents))"
      ],
      "metadata": {
        "id": "VXmjvBae4947"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm.notebook import tqdm\n",
        "coco = sents\n",
        "_coco = {}\n",
        "n_coco = len(coco)\n",
        "for i in tqdm(range(n_coco)):\n",
        "    #sent = coco[i]['caption']\n",
        "    sent = coco[i]\n",
        "    _coco[i] = {}\n",
        "    _coco[i]['tokens'] = sbert_model.tokenizer.tokenize(sent)\n",
        "    _coco[i]['input_ids'] = sbert_model.tokenizer(sent)['input_ids']\n"
      ],
      "metadata": {
        "id": "S_STJDfw5uBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import jaconv\n",
        "#coco_sents = [jaconv.normalize(x['caption']) for x in coco]\n",
        "coco_sents = sents\n",
        "#coco_vects = sbert_model.encode(coco_sents)\n",
        "coco_vects = coco_anno.detach().numpy()"
      ],
      "metadata": {
        "id": "_novchPP50-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import scipy.spatial\n",
        "\n",
        "def search_sim_sents(queries:list,\n",
        "                     answers:list,\n",
        "                     model:BertModel,\n",
        "                     vectors:torch.Tensor,\n",
        "                     top_n:int = 5,\n",
        "                     verbose:bool=False,\n",
        "                    ):\n",
        "    # 文埋め込みベクトルの中から類似する文を検索する関数\n",
        "    if answers == None:\n",
        "        answers = queries\n",
        "    ret = {}\n",
        "    query_embeddings = sbert_model.encode(queries).detach().numpy()\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding],\n",
        "                                                 vectors,\n",
        "                                                 metric=\"cosine\")[0]\n",
        "\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        results = sorted(results, key=lambda x: x[1])\n",
        "        ret[query] = []\n",
        "        for idx, distance in results[1:top_n+1]:\n",
        "            print(f'{query}, {answers[idx]}, {1-distance/2:.3f}') if verbose else None\n",
        "            ret[query].append((answers[idx], 1 - distance/2, idx))\n",
        "            #ret[query].append((answers[idx], 1 - distance/2))\n",
        "    return ret\n",
        "\n",
        "\n",
        "# 検証のため，最初の top_n 文について\n",
        "top_n = 3\n",
        "ret = search_sim_sents(queries=coco_sents[:top_n],\n",
        "                       answers=coco_sents,\n",
        "                       model=sbert_model,\n",
        "                       vectors=coco_vects,\n",
        "                       #verbose=True,\n",
        "                       top_n=10,\n",
        "                      )\n",
        "for i, (k, v) in enumerate(ret.items()):\n",
        "    print(colored(f'文番号 {i:3d} {k}', 'grey', attrs=['bold']))\n",
        "    for j, _v in enumerate(v):\n",
        "        print(f'\\t近接文 {_v[2]:4d}:{_v[0]} ({_v[1]:.3f})')\n",
        "        #print(f'\\t近接文{j:2d}:{_v[0]}: {_v[2]:5d}({_v[1]:.3f})')"
      ],
      "metadata": {
        "id": "Q3eidHq454qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# tSNE 原著者の公開しているソースコードを Python 3 対応に書き換えた版を読み込む\n",
        "try:\n",
        "    from ccap import tsne\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "    from ccap import tsne"
      ],
      "metadata": {
        "id": "iwUNz9yH59il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_and_plot_tsne_pca(\n",
        "    # sbert_model.model もしくは _bert_model.model を指定可能\n",
        "    model:BertModel=sbert_model.model,\n",
        "\n",
        "    # どのベクトルを計算するかを指定: sbert_snow_vectors または _bert_snow_vectors を指定可能\n",
        "    vectors:torch.Tensor=coco_anno,\n",
        "\n",
        "    perplexity=30.0,         # 錯綜度を指定，原著論文によれば 5 から 50 程度\n",
        "    pca = False,             # 主成分分析を行うか否か\n",
        "    no_dims = 50,            # 主成分分析で求める固有値数\n",
        "\n",
        "    figsize:tuple=(20,20),   # 表示する図の大きさを指定\n",
        "    tag:list=sents,          # 散布図中に表示する記号や文字を指定\n",
        "    fontsize:int=5,          # 散布図中に表示する記号や文字のサイズ\n",
        "    title:str=None,          # 図のタイトル\n",
        "    fig_fname:str=None,      # 図を保存するファイル名\n",
        "    excel_fname:str=None,    # 図の座標を保存するエクセルファイル名\n",
        "    mod_size:int=5,          # 同じ色で描画する範囲 5\n",
        "    marker_size:int=20,      # marker の大きさ\n",
        "    ):\n",
        "\n",
        "    _X = vectors.detach().numpy()\n",
        "    #_X = vectors.clone().numpy()\n",
        "    (n, d) = _X.shape\n",
        "    #colors = np.array([\"orange\", \"purple\", \"beige\", \"brown\",\n",
        "    colors = [\"orange\", #\"purple\",\n",
        "              \"brown\", \"black\", \"cyan\",\n",
        "              \"magenta\", \"red\", \"green\", \"blue\", \"yellow\",\n",
        "              \"pink\"]\n",
        "    markers = ['o','v','^', '<', '>', '+','*','x', 'D','8','s', 'D','8','s','_']\n",
        "\n",
        "    if pca == True:\n",
        "        _X = _X - np.tile(np.mean(_X, 0), (n, 1))\n",
        "        (l, M) = np.linalg.eig(np.dot(_X.T, _X))\n",
        "        X = np.dot(_X, M[:,0:no_dims])\n",
        "    else:\n",
        "        X = tsne.tsne(_X, perplexity=perplexity)  # tSNE の実施\n",
        "    plt.figure(figsize=figsize)                   # 図のサイズ指定，単位インチ\n",
        "\n",
        "    half = n >> 1\n",
        "    change_col = False\n",
        "    _i = 0\n",
        "    _j = -1\n",
        "\n",
        "    for i in range(n):\n",
        "        if i % mod_size == 0:\n",
        "            _i = 0\n",
        "            _j += 1\n",
        "        else:\n",
        "            _i += 1\n",
        "\n",
        "        c = colors[_j % len(colors)]\n",
        "        m = markers[_i]\n",
        "        plt.scatter(X[i,0],X[i,1],\n",
        "                    marker=m,\n",
        "                    color=c,\n",
        "                    s=marker_size)\n",
        "\n",
        "    if tag != None:\n",
        "        for i, txt in enumerate(tag):           # 図内にアノテーションを書き込む\n",
        "            plt.annotate(tag[i], (X[i,0], X[i,1]),\n",
        "                         alpha=0.7,\n",
        "                         ha='center',\n",
        "                         fontsize=fontsize)\n",
        "    plt.title(title) if title != None else None  # 図の表題\n",
        "\n",
        "    # 図のファイル書き出し\n",
        "    if fig_fname != None:\n",
        "        if os.path.exists(fig_fname):\n",
        "            print(f'File: {fig_fname} exists.',end=\" \")\n",
        "            yn = input('delete[Y/n]')\n",
        "            if yn == 'Y':\n",
        "                os.remove(fig_fname)\n",
        "        plt.savefig(fig_fname)\n",
        "    plt.show()\n",
        "\n",
        "    # 図のデータをエクセルファイルに書き出し\n",
        "    if excel_fname != None:\n",
        "        if os.path.exists(excel_fname):\n",
        "            print(f'File: {excel_fname} exists.', end=\" \")\n",
        "            yn = input('delete[Y/n]')\n",
        "            if yn == 'Y':\n",
        "                os.remove(excel_fname)\n",
        "        _dict = {}\n",
        "        for i, (tag, _X) in enumerate(zip(tag, X)):\n",
        "            _dict[i] = {'tag':tag,\n",
        "                        'x': _X[0],\n",
        "                        'y': _X[1]\n",
        "                       }\n",
        "        df = pd.DataFrame(_dict).T\n",
        "        df.to_excel(excel_fname)\n",
        "\n",
        "    return X     # tSNE の結果を返す。\n",
        ""
      ],
      "metadata": {
        "id": "dLNXb1nq5-8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "N = 100\n",
        "_Xvects = coco_anno[:N,:]\n",
        "_Xsents = coco_sents[:N]\n",
        "\n",
        "np.random.seed(42)\n",
        "for _model, title in [(sbert_model, 'センテンスBERT')]:\n",
        "    calc_and_plot_tsne_pca(\n",
        "        model=_model,\n",
        "        vectors=_Xvects,\n",
        "        tag=_Xsents,\n",
        "        #tag = None,\n",
        "        fontsize=8,\n",
        "        #fontsize=4,\n",
        "        figsize=(12,12),\n",
        "        perplexity=30,\n",
        "        pca=False,\n",
        "        title=title,\n",
        "        #marker_size=50,\n",
        "        marker_size=100,\n",
        "        fig_fname=None, #'2022_0915sbert_staircoco500.pdf',\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qudAlk5w6BqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}