{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0517opencv_Object_Detection_with_Haar_Cascades.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Object Detection with Haar Cascades in Python](https://towardsdatascience.com/object-detection-with-haar-cascades-in-python-ad9e70ed50aa)\n",
        "\n",
        "## 理論<!-- ## Theory lesson-->\n",
        "\n",
        "Haar 特徴に基づくカスケード分類器を用いて，顔，目，笑顔，眼鏡を検出する。\n",
        "この手法は 2001 年に P. Viola と M. Jones によって提案された[1]。\n",
        "要するに，カスケード関数と呼ばれるものを，大量の正画像と負画像（正画像には目的の物体が含まれ，負画像にはそれが含まれていないことを意味する）に対して学習させ，それを物体検出に利用する機械学習手法である。\n",
        "<!-- We are going to use Haar Feature-based Cascade Classifiers to detect faces, eyes, smiles as well as eyeglasses.\n",
        "The method was proposed by P. Viola and M. Jones in 2001 [1].\n",
        "In short, it is a machine learning method where a so-called cascade function is trained on a large amount of positive and negative images (positive meaning it includes the desired object and negative images lack it), which in turn can be used for object detection.-->\n",
        "\n",
        "実際に説明するために，Haar 特徴量の概念を紹介する。\n",
        "Haar 特徴量は，畳み込みカーネルとして働く下の黒と白のボックスによって得られる。\n",
        "特徴量は，より具体的には，黒い四角形の下の画素の合計から白い四角形の下の画素の合計を引くことで得られる単一の値である。[2]\n",
        "<!-- To actually explain it we introduce the concept of Haar features, they are obtained by the black and white boxes below which act as convolutional kernels.\n",
        "The features are more specifically single values received by subtracting the sum of pixels under the white rectangles from the sum of pixels under the black rectangles. [2] -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*hbFPsfsCqV8rf1MV8b8p5w.jpeg\">\n",
        "</center>\n",
        "\n",
        "畳み込みという言葉に不安を感じるなら，ウィキペディアの画像を使って簡単に説明しよう。\n",
        "畳み込みとは，基本的に 2 つの関数が互いに影響を及ぼし合った結果である。\n",
        "つまり，この場合は，ボックスの白と黒の部分の画素の合計がどのように相互作用するか，つまり，微分されて1つの値が生成されることを意味する。もちろん，領域内の画素の和を効率的に計算するアルゴリズムもあり，それは Summed-Area Table と呼ばれ，1984 年にF. Crow によって発表された[3]。\n",
        "これは後に画像処理の分野で一般化され，積分画像という名前で呼ばれるようになったが，ここではそれについて深く掘り下げることはしない。\n",
        "<!-- If you get uneasy about the word convolution, let me simplify it with the help of an image from Wikipedia.\n",
        "It is essentially the result of two functions affecting one another.\n",
        "So in our case, it signifies how the sum of pixels in the white and black part of the boxes interact, i.e. are differentiated to produce a single value. Of course, there is also an algorithm for calculating the sum of pixels efficiently inside an area, it’s called summed-area table and was published by F. Crow in 1984 [3].\n",
        "It was later popularized in the image processing domain and goes under the name integral image, but I won’t delve deeper into that here. -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*CbOUB2WgVOzVRx8iDv5APQ.png\">\n",
        "</center>\n",
        "\n",
        "次に、想像したとおりに特徴選択が行われる。\n",
        "さまざまな Haar 特徴を試し，黒と白の矩形間の画素の総和の差について，どれが最大の値を出すかを見る。\n",
        "最適な Haar 特徴が見つかった例を以下に示す。\n",
        "通常，目は少し暗く，その下は明るいため，上部が黒，下部が白の横長の長方形が適している。\n",
        "次に，鼻筋は目よりも明るいことが多いので，真ん中に縦長の白いボックスがある Haar 特徴が適している。\n",
        "<!-- Next, the feature selection happens as you imagine it would.\n",
        "You try the different Haar features and see which of those produce the largest value for the difference between the sums of pixels between the black and white rectangles.\n",
        "We have below an example where optimal Haar features have been found.\n",
        "The eyes are usually a bit darker whereas the area below likely is lighter, and thus a horizontal rectangle with black up top and white below is suitable.\n",
        "Secondly, the bridge of the nose is often lighter than the eyes and as such a Haar feature with a vertical white box in the middle is the way to go. -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*64MTUF8nuEvSgBvYmOfhKA.png\">\n",
        "</center>\n",
        "\n",
        "Haar 特徴という名前は少し奇妙に聞こえるが，実は Haar ウェーブレットと直感的に似ていることに由来する：\n",
        "<!-- The name Haar features sounds a bit odd, but it actually originates from the intuitive similarity with Haar wavelets which are these bad boys: -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:440/format:webp/1*MUeF9CIalU87NC-6T7mNWw.png\">\n",
        "</center>\n",
        "\n",
        "これらは直接使われることはないが，特徴（黒と白のボックス）は Haar ライクと呼べるものだ。\n",
        "このような Haar のような特徴をたくさん画像に適用し，学習画像を正しく分類するための最適な閾値を見つける Adaboost アルゴリズムを使うことができる。\n",
        "しかし，そのうちの 1 つをどこかに配置すると，たとえ最適な位置に配置したとしても，正事例と負事例内の画像はすべて互いに異なるため，やはり多少の誤差が生じる。\n",
        "最終的に，最も誤差の少ない Haar 特徴が分類子として選ばれる。\n",
        "<!-- They are not directly used but the features (black and white boxes) are what we can call Haar-like in the sense that, well you get it by now.\n",
        "Lots of these Haar-like features can be applied to an image and using the Adaboost algorithm which finds an optimal threshold for classifying the training images correctly.\n",
        "But placing one of those somewhere, even on the best possible position, will result in some error still since all images within the positive and negative sets differ from each other.\n",
        "In the end, the Haar features with the smallest error rates are chosen as classifiers.-->\n",
        "\n",
        "最終的な特徴数は，対策を講じたにもかかわらず非常に多くなる可能性があるため，発明者たちは分類器のカスケードという概念を導入した (これで Haar Feature-based Cascade Classifiers という名前全体がわかった)。\n",
        "これは，検出を行うときに使われるもので，たくさんの特徴量を使って検出を行うと時間がかかるため，代わりに検出を行うときに，分類器は特徴量のカスケードで構成される。\n",
        "つまり，最初の Haar 特徴量は (顔検出の場合) 画像が顔の可能性があるかどうかをチェックするだけで，次の段階では，さらにいくつかの最も本質的な Haar 特徴量を持つ。\n",
        "これは好ましい方法である。\n",
        "なぜなら，目的の物体を含まない画像は早い段階で破棄され，それ以上処理されないからである。\n",
        "画像にすべての特徴を一度に投入するのと比較すれば，その利点がわかるだろう。[2]\n",
        "<!--The final number of features can be quite large despite the measures taken, so the inventors introduced the concept of Cascade of Classifiers (now we have got the whole name Haar Feature-based Cascade Classifiers down by the way).\n",
        "This is used when doing the detection since it would be slow to do it with lots of features, instead, the classifier consists of a cascade of features when detecting.\n",
        "So the initial Haar feature might just check if the image could possibly be a face (in the case of face detection), then the following stages have a few more of the most essential Haar features.\n",
        "This is a favorable method since early on images not containing the desired object are discarded and not processed anymore.\n",
        "Compare this to throwing all features at the image at once and you see the gain. [2] -->\n",
        "\n",
        "\n",
        "## 実装<!-- ## Implementation-->\n",
        "\n",
        "Python で実装する部分に移ろう。\n",
        "もし，自分のマシンで一緒に追ったり，クールなものを試してみたいなら，[GitHubリポジトリ](https://github.com/deinal/opencv-recognition-demo)や[Google Colab](https://colab.research.google.com/drive/1H2XBOI31j4idgAqyY-brunwOlIajeFRt#scrollTo=KjxUh493oYf-) で公開されている。\n",
        "後者の方は，残念ながら，ビデオの物体検出はできないが。\n",
        "とにかく，まずはいくつかのインポートを行い，準備は万端だ。\n",
        "<!--On to the part where we implement it in Python.\n",
        "If you want to follow along on your own machine or try out any of the cool stuff it is publicly available in a [GitHub repository](https://github.com/deinal/opencv-recognition-demo) or at [Google Colab](https://colab.research.google.com/drive/1H2XBOI31j4idgAqyY-brunwOlIajeFRt#scrollTo=KjxUh493oYf-), although the latter one can’t, unfortunately, do object detection in videos…\n",
        "Anyway, we start by doing a few imports so we are all set to go. -->\n"
      ],
      "metadata": {
        "id": "QcJJ-igPSg3y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOlItAp-uD8W"
      },
      "source": [
        "**OpenCV Demo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-xZ_JkefGw"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "plt.rcParams['figure.figsize'] = [10, 20]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZUcDVpSpEqE"
      },
      "source": [
        "# Load image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uozkltQTd4nr"
      },
      "source": [
        "#!wget \"https://raw.githubusercontent.com/hd4niel/opencv-recognition-demo/master/people-taking-group-picture.jpg\" -O group.jpg\n",
        "#!wget \"https://github.com/deinal/opencv-recognition-demo/images/people-taking-group-picture.jpg\" -O group.jpg\n",
        "#!wget https://github.com/deinal/opencv-recognition-demo/blob/master/images/people-taking-group-picture.jpg -O group.jpg\n",
        "!wget -O group.jpg https://mingle.jp/wp-content/uploads/2021/03/5060Tokyo.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsRPJT1ebi1"
      },
      "source": [
        "#from PIL import Image\n",
        "#img = Image.open('group.jpg')\n",
        "img = cv2.imread('group.jpg')\n",
        "#plt.imshow(img)\n",
        "#plt.show()\n",
        "\n",
        "imgrgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(imgrgb)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-331AMpo0jL"
      },
      "source": [
        "# Load Haar cascades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IxwaXBifq6w"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\" -O haarcascade_frontalface_default.xml\n",
        "!wget \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\" -O haarcascade_eye.xml\n",
        "!wget \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml\" -O haarcascade_eye_tree_eyeglasses.xml\n",
        "!wget \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_smile.xml\" -O haarcascade_smile.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqumah0Po98X"
      },
      "source": [
        "# Do object detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGvS3PLucbuQ"
      },
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "glasses_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
        "smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')\n",
        "\n",
        "img = cv2.imread('group.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここからが本題だ。\n",
        "まず最初に、分類はグレースケール・モードで行われる。\n",
        "次に，物体検出を行い，矩形のリストとして返す detectMultiScale を見てみよう。\n",
        "このメソッドにはいくつかのパラメータがあり，好みに応じて調整できる。\n",
        "<!-- So here is the juicy part I suppose.\n",
        "First off, classification happens in grayscale mode, and then we can take a look at detectMultiScale which performs the object detection and return them as a list of rectangles.\n",
        "The method can take a few parameters which can be tweaked to one's liking. -->\n",
        "\n",
        "* **scaleFactor**： 各画像スケールで画像サイズをどれだけ縮小するかを指定するパラメータ。\n",
        "この値を大きくすると，いくつかの物体を見逃すリスクはあるが，検出が速くなる。\n",
        "* **minNeighbors**： 各候補矩形を保持するために，いくつの近傍を持つべきかを指定するパラメータ。\n",
        "値が高いほど検出は少なくなるが，品質は高くなる。\n",
        "* **minSize**： 最小物体サイズ。これより小さい物体は無視される。\n",
        "\n",
        "<!-- * **scaleFactor**: Parameter specifying how much the image size is reduced at each image scale.\n",
        "Increasing it leads to faster detection with the risk of missing some objects, whereas a small value might sometimes be too thorough.\n",
        "* **minNeighbors**: Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
        "Higher value results in less detection but with higher quality.\n",
        "* **minSize**: Minimum possible object size. Objects smaller than that are ignored.\n",
        "* **maxSize**: Maximum possible object size. Objects larger than that are ignored. -->\n",
        "\n",
        "私のコードでは、scaleFactorとminNeighborsをいじっている。\n",
        "例えば、眼鏡は検出しにくかった。おそらく、私が使っている写真に写っている人たちは上を向いていて、入力カスケードが学習したものとは似ていないからだろう。\n",
        "そのため、scaleFactorとminNeighborsを下げて精度を上げた。\n",
        "それとは別に、笑顔が多すぎたので、両方のパラメーターを増やした。\n",
        "<!-- I fiddle with the scaleFactor and minNeighbors in my code.\n",
        "Eyeglasses, for example, were hard to detect, maybe because the people in the picture I am using are looking upwards and it does not resemble what the input cascades are trained for.\n",
        "I, therefore, lowered the scaleFactor as well as minNeighbors for more precision.\n",
        "Apart from that, I detected too many smiles — so I increased both of the parameters there. -->"
      ],
      "metadata": {
        "id": "O9ykABasV5V1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzC2upkF6g4Z"
      },
      "source": [
        "# default MultiScale paramters: scaleFactor=1.1, minNeighbors=3\n",
        "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "for(x, y, w, h) in faces:\n",
        "    img = cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 3)\n",
        "    roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_color = img[y:y+h, x:x+w]\n",
        "\n",
        "    smiles = smile_cascade.detectMultiScale(roi_gray, minNeighbors=20)\n",
        "    for(sx, sy, sw, sh) in smiles:\n",
        "        cv2.rectangle(roi_color, (sx,sy), (sx+sw,sy+sh), (0,0,255), 3)\n",
        "\n",
        "    glasses = glasses_cascade.detectMultiScale(roi_gray, scaleFactor=1.04, minNeighbors=1)\n",
        "    for(gx, gy, gw, gh) in glasses:\n",
        "        cv2.rectangle(roi_color, (gx,gy), (gx+gw,gy+gh), (0,255,0), 2)\n",
        "\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "    for(ex, ey, ew, eh) in eyes:\n",
        "        cv2.rectangle(roi_color, (ex,ey), (ex+ew,ey+eh), (0,255,255), 3)\n",
        "\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "face_patch = mpatches.Patch(color='blue', label='Faces')\n",
        "smile_patch = mpatches.Patch(color='red', label='Smiles')\n",
        "eye_patch = mpatches.Patch(color='yellow', label='Eyes')\n",
        "glass_patch = mpatches.Patch(color='green', label='Glasses')\n",
        "plt.legend(handles=[face_patch, smile_patch, eye_patch, glass_patch],\n",
        "           loc='lower right', fontsize=12)\n",
        "\n",
        "imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjxUh493oYf-"
      },
      "source": [
        "# Try on a webcam photo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsMY0Hq9Cgt"
      },
      "source": [
        "# Source: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwAehvFq9DRQ"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvkGEAZZ_IUT"
      },
      "source": [
        "img = cv2.imread(filename)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byCow_Cl8dHg"
      },
      "source": [
        "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_color = img[y:y+h, x:x+w]\n",
        "\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "    for (ex,ey,ew,eh) in eyes:\n",
        "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
        "\n",
        "    smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)\n",
        "    for (sx,sy,sw,sh) in smiles:\n",
        "        cv2.rectangle(roi_color,(sx,sy),(sx+sw,sy+sh),(0,0,255),2)\n",
        "\n",
        "plt.grid(None)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}