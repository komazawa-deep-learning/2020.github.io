{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0419Olivetti_face_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742",
      "metadata": {
        "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35000912-ec57-4778-ae33-6c4610eb1d1e",
      "metadata": {
        "id": "35000912-ec57-4778-ae33-6c4610eb1d1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = fetch_olivetti_faces()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# split_ratio = 0.2 としているので，訓練データ対テストデータが 8:2 になります\n",
        "split_ratio = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=split_ratio,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=0)\n",
        "print(f'オリジナル X サイズ:{X.shape}',\n",
        "      f'X_train 訓練画像のサイズ: {X_train.shape}')\n",
        "print(f'オリジナル y サイズ:{y.shape}',\n",
        "      f'y_train 教師信号データのサイズ: {y_train.shape}')\n",
        "print(\"目標とするクラス(画像中の人物の数) :\", np.unique(y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#N = 100\n",
        "# plt.figure(figsize=(3,3))\n",
        "# plt.imshow(X[N].reshape(64,64),cmap='gray')\n",
        "# plt.xticks([])\n",
        "# plt.yticks([])\n",
        "# plt.title(f'{y[N]}')\n",
        "# plt.show()\n",
        "\n",
        "nrows=32\n",
        "ncols=10\n",
        "figs, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 1.7, nrows * 1.7))\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        N = j + i * ncols + 0\n",
        "        axes[i,j].imshow(X[N].reshape(64,64), cmap='gray')\n",
        "        axes[i,j].set_xticks([],[])\n",
        "        axes[i,j].set_yticks([],[])\n",
        "        axes[i,j].set_title(f'num:{N}, ID:{y[N]}')"
      ],
      "metadata": {
        "id": "vfqlQjIvXX9-"
      },
      "id": "vfqlQjIvXX9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.linear_model\n",
        "# cls = sklearn.linear_model.LogisticRegression(penalty='l2',\n",
        "#                                               dual=False,\n",
        "#                                               tol=0.0001,\n",
        "#                                               #C=1.0,\n",
        "#                                               C=0.01,\n",
        "#                                               fit_intercept=True,\n",
        "#                                               intercept_scaling=1,\n",
        "#                                               class_weight=None,\n",
        "#                                               random_state=None,\n",
        "#                                               solver='lbfgs',\n",
        "#                                               max_iter=400,\n",
        "#                                               #max_iter=100,\n",
        "#                                               multi_class='auto',\n",
        "#                                               verbose=0,\n",
        "#                                               warm_start=False,\n",
        "#                                               n_jobs=None,\n",
        "#                                               l1_ratio=None).fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "PIj3WKATWWYR"
      },
      "id": "PIj3WKATWWYR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "logreg = sklearn.linear_model.LogisticRegression(C=1e5, verbose=10, max_iter=500)\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6wNRhm2yo6Uh",
        "outputId": "c981654f-f8da-49b7-de95-59b2676dda79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "id": "6wNRhm2yo6Uh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.7 s, sys: 19.9 s, total: 1min 4s\n",
            "Wall time: 4min 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  5.0min\n",
            "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:  5.0min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, max_iter=500, verbose=10)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100000.0, max_iter=500, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100000.0, max_iter=500, verbose=10)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジスティック回帰による予測\n",
        "y_hat = logreg.predict(X_test)  # テストデータを使って予測を行い結果を y_hat に格納\n",
        "print(f\"ロジスティック回帰を用いた分類精度: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "\n",
        "# 混同行列の表示\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ],
      "metadata": {
        "id": "5YD2w5EcPbj_"
      },
      "id": "5YD2w5EcPbj_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "model = LogisticRegression(max_iter= 10 ** 3, C=1e4)\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "print(f\"平均交差検証得点: {cv_scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "_ZYCEeUPPx6R",
        "outputId": "989aa8de-cade-441b-e6ea-ded72ec57cac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_ZYCEeUPPx6R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "平均交差検証得点: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "loo_cv = LeaveOneOut()\n",
        "model = LogisticRegression(max_iter= 10 ** 3, C=1e4)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv = loo_cv)\n",
        "print(f\"{model.__class__.__name__} リーブ・ワン・アウト交差検証法による平均得点:{cv_scores.mean():.3f}\")"
      ],
      "metadata": {
        "id": "VG_L2TDvQU_Z"
      },
      "id": "VG_L2TDvQU_Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "params={'penalty':['l1', 'l2'],\n",
        "        'C':np.logspace(0, 4, 10) }\n",
        "model = LogisticRegression(max_iter=10 ** 3, C=1e4)\n",
        "loo_cv = LeaveOneOut()\n",
        "gridSearchCV = GridSearchCV(model, params, cv=loo_cv)\n",
        "gridSearchCV.fit(X_train, y_train)\n",
        "print(\"Grid search fitted..\")\n",
        "print(gridSearchCV.best_params_)\n",
        "print(gridSearchCV.best_score_)\n",
        "print(f\"グリッドサーチによる交差妥当性得点:{gridSearchCV.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "R4_B8jqZQk3h"
      },
      "id": "R4_B8jqZQk3h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from funcsigs import signature\n",
        "except ImportError:\n",
        "    !pip install funcsigs\n",
        "    from funcsigs import signature"
      ],
      "metadata": {
        "id": "4NctgXlWRDYp"
      },
      "id": "4NctgXlWRDYp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.figure(1, figsize=(12,8))\n",
        "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2, where='post')\n",
        "plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(f'Average precision score, micro-averaged over all classes: AP={average_precision[\"micro\"]:0.2f}')"
      ],
      "metadata": {
        "id": "2vMFKnjORBio"
      },
      "id": "2vMFKnjORBio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "Target = label_binarize(target, classes=range(40))\n",
        "n_classes = Target.shape[1]\n",
        "X_train_multiclass, X_test_multiclass, \\\n",
        "y_train_multiclass, y_test_multiclass = train_test_split(X,\n",
        "                                                         Target,\n",
        "                                                         test_size=0.2,\n",
        "                                                         stratify=Target,\n",
        "                                                         random_state=0)\n",
        "\n",
        "oneRestClassifier = OneVsRestClassifier(clf)\n",
        "oneRestClassifier.fit(X_train_multiclass, y_train_multiclass)\n",
        "y_score = oneRestClassifier.decision_function(X_test_multiclass)\n",
        "\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "average_precision = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = metrics.precision_recall_curve(y_test_multiclass[:, i], y_score[:, i])\n",
        "    average_precision[i] = metrics.average_precision_score(y_test_multiclass[:, i], y_score[:, i])\n",
        "\n",
        "precision[\"micro\"], recall[\"micro\"], _ = metrics.precision_recall_curve(y_test_multiclass.ravel(), y_score.ravel())\n",
        "average_precision[\"micro\"] = metrics.average_precision_score(y_test_multiclass, y_score, average=\"micro\")\n",
        "print(f'平均精度得点, 全クラスの平均: {average_precision[\"micro\"]:0.3f}')"
      ],
      "metadata": {
        "id": "ZU2hVPoiQ-U4"
      },
      "id": "ZU2hVPoiQ-U4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "\n",
        "# test_size = 0.2 としているので，訓練データ対テストデータが 8:2 になります\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.2, stratify=target, random_state=0)\n",
        "print(f'X_train 訓練画像のサイズ: {X_train.shape}')\n",
        "print(f'y_train 教師信号データのサイズ: {y_train.shape}')"
      ],
      "metadata": {
        "id": "0zhJfSUJOJqA"
      },
      "id": "0zhJfSUJOJqA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "\n",
        "y_hat = logreg.predict(X_test)  # テストデータを使って予測を行い結果を y_hat に格納\n",
        "print(f\"ロジスティック回帰を用いた分類精度: {metrics.accuracy_score(y_test, y_hat):.3f}\")\n",
        "\n",
        "# 混同行列の表示\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(metrics.confusion_matrix(y_test, y_hat))"
      ],
      "metadata": {
        "id": "RgBAUoVIqjbP"
      },
      "id": "RgBAUoVIqjbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "for name, model in [\n",
        "    ['ロジスティック回帰',  sklearn.linear_model.LogisticRegression(max_iter= 10 ** 4)],\n",
        "    #['サポートベクターマシン', SVC()],\n",
        "    #['線形判別分析', LinearDiscriminantAnalysis()],\n",
        "    ]:\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "    print(f\"{name} 平均交差検証得点: {cv_scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "eGQ61hZBsuqW",
        "outputId": "ce3a74e9-f709-4a27-dd3e-2e3aa461ba58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eGQ61hZBsuqW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ロジスティック回帰 平均交差検証得点: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181",
      "metadata": {
        "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "\n",
        "# データローダーのサブプロセスの乱数の seed 固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "\n",
        "print(worker_init_fn(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7",
      "metadata": {
        "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7"
      },
      "outputs": [],
      "source": [
        "# データセットの作成\n",
        "class _dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feature = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return feature, label\n",
        "\n",
        "\n",
        "X_ = torch.tensor(X_train).float().reshape(-1,1,64,64)\n",
        "#X_ = torch.reshape(torch.tensor(X_train).float(), (-1,1,64,64))\n",
        "y_ = torch.tensor(y_train).long()\n",
        "Xtest_ = torch.tensor(X_test).float().reshape(-1,1,64,64)\n",
        "ytest_ = torch.tensor(y_test).long()\n",
        "\n",
        "train_dataset = _dataset(X_, y_)\n",
        "test_dataset = _dataset(Xtest_, ytest_)\n",
        "\n",
        "# データローダーの作成\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=64,    # バッチサイズ\n",
        "                                               shuffle=True,     # データシャッフル\n",
        "                                               num_workers=0,    # 高速化\n",
        "                                               pin_memory=True,  # 高速化\n",
        "                                               worker_init_fn=worker_init_fn\n",
        "                                              )\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                              batch_size=64,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=0,\n",
        "                                              pin_memory=True,\n",
        "                                              worker_init_fn=worker_init_fn\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c74c6ed-5286-4fd3-be20-855a211449ab",
      "metadata": {
        "id": "9c74c6ed-5286-4fd3-be20-855a211449ab"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_features, out_features, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_features, out_features,\n",
        "                     kernel_size=3,\n",
        "                     stride=stride,\n",
        "                     padding=dilation,\n",
        "                     groups=groups,\n",
        "                     bias=False,\n",
        "                     dilation=dilation,\n",
        "                    )\n",
        "\n",
        "\n",
        "def conv1x1(in_features, out_features, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_features, out_features, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class ResNet_BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 stride=1,\n",
        "                 downsample=None,\n",
        "                 groups=1,\n",
        "                 base_width=64,\n",
        "                 dilation=1,\n",
        "                 norm_layer=None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = conv3x3(in_features, out_features, stride=stride)\n",
        "        self.bn1 = norm_layer(out_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_features, out_features)\n",
        "        self.bn2 = norm_layer(out_features)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eefe4fe-822c-4627-8250-35a581d11779",
      "metadata": {
        "id": "2eefe4fe-822c-4627-8250-35a581d11779"
      },
      "outputs": [],
      "source": [
        "class ResNet_Bottleneck(nn.Module):\n",
        "    # ボトルネックは、ダウンサンプリングのストライドを 3x3 convolution(self.conv2) に置くのに対し、\n",
        "    # オリジナルの実装では最初の 1x1 convolution(self.conv1) にしています。\n",
        "    # [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) によると、\n",
        "    # ダウンサンプリングのストライドを 3x3 convolution(self.conv2) にしています。\n",
        "    # このバージョンは ResNet V1.5 としても知られており、精度が向上しています。\n",
        "    # (https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch)\n",
        "\n",
        "    #expansion: int = 4\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 stride=1,\n",
        "                 downsample=None,\n",
        "                 groups=1,\n",
        "                 base_width=64,\n",
        "                 dilation=1,\n",
        "                 norm_layer= None):\n",
        "\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(out_featuress * (base_width / 64.0)) * groups\n",
        "\n",
        "        self.expansion = 4\n",
        "\n",
        "        self.conv1 = conv1x1(in_features, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, otu_features * self.expansion)\n",
        "        self.bn3 = norm_layer(out_features * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, inp):\n",
        "        identity = inp\n",
        "\n",
        "        out = self.conv1(inp)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 in_channels = 3,\n",
        "                 num_classes = 40, # 1000,\n",
        "                 zero_init_residual=False,\n",
        "                 groups=1,\n",
        "                 width_per_group=64,\n",
        "                 #width_per_group=8,\n",
        "                 norm_layer=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.in_features = 64\n",
        "        self.dilation = 1\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.in_features, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.in_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # 各残差ブランチの最後のバッチ正規化をゼロ初期化し，残差ブランチがゼロで始まり，各残差ブロックが恒等写像のように振る舞うようにします。\n",
        "        # https://arxiv.org/abs/1706.02677 によると，これによりモデルが 0.2~0.3 %改善されます。\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self,\n",
        "                    block,\n",
        "                    out_features,\n",
        "                    blocks,\n",
        "                    stride=1):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if stride != 1 or self.in_features != out_features * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_features, out_features * block.expansion, stride),\n",
        "                norm_layer(out_features * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.in_features, out_features, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.in_features = out_features * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.in_features,\n",
        "                    out_features,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    \"\"\"ResNet-18 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "    \"\"\"\n",
        "    return _resnet(\"resnet18\", ResNet_BasicBlock, [1, 1, 1, 1], **kwargs)\n",
        "    #return _resnet(\"resnet18\", ResNet_BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0",
      "metadata": {
        "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "in_channels=1\n",
        "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
        "#print(tmp_resnet_model)\n",
        "summary(model, input_size=(in_channels,64,64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876",
      "metadata": {
        "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876"
      },
      "source": [
        "$$\n",
        "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
        "l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
        "w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n",
        "$$\n",
        "\n",
        "where $x$ is the input, $y$ is the target, $w$ is the weight, and $N$ is the batch size.\n",
        "If `reduction` is not `none` (default `mean` ), then\n",
        "$$\n",
        "\\ell(x, y) = \\begin{cases}\n",
        "\\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n",
        "\\text{if reduction} = \\text{`mean';}\\\\\n",
        "\\sum_{n=1}^N l_n,  &\n",
        "\\text{if reduction} = \\text{`sum'.}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112",
      "metadata": {
        "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112"
      },
      "outputs": [],
      "source": [
        "# モデル\n",
        "in_channels=1\n",
        "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
        "\n",
        "#loss_f = nn.NLLLoss()\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optim_f = optim.Adam(model.parameters(), weight_decay=0.01)\n",
        "\n",
        "\n",
        "# 学習・評価\n",
        "def calc_loss(label, pred):\n",
        "    return loss_f(pred, label)\n",
        "\n",
        "def train_step(x, y):\n",
        "    model.train()\n",
        "    preds = model(x)\n",
        "    loss = calc_loss(y, preds)\n",
        "    optim_f.zero_grad()\n",
        "    loss.backward()\n",
        "    optim_f.step()\n",
        "\n",
        "    return loss, preds\n",
        "\n",
        "def test_step(x, y):\n",
        "    model.eval()\n",
        "    preds = model(x)\n",
        "    loss = calc_loss(y, preds)\n",
        "\n",
        "    return loss, preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a",
      "metadata": {
        "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af80fd68-d59c-43e9-a574-9959017ab2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "エポック:  1, 訓練損失: 2.900 テスト損失: 3.699, 訓練精度:0.403 検証精度: 0.023\n",
            "エポック:  2, 訓練損失: 0.885 テスト損失: 3.873, 訓練精度:0.966 検証精度: 0.016\n",
            "エポック:  3, 訓練損失: 0.231 テスト損失: 4.108, 訓練精度:0.997 検証精度: 0.031\n",
            "エポック:  4, 訓練損失: 0.063 テスト損失: 4.381, 訓練精度:1.000 検証精度: 0.031\n",
            "エポック:  5, 訓練損失: 0.027 テスト損失: 4.624, 訓練精度:1.000 検証精度: 0.023\n",
            "エポック:  6, 訓練損失: 0.014 テスト損失: 4.744, 訓練精度:1.000 検証精度: 0.016\n",
            "エポック:  7, 訓練損失: 0.010 テスト損失: 4.774, 訓練精度:1.000 検証精度: 0.016\n",
            "エポック:  8, 訓練損失: 0.007 テスト損失: 4.707, 訓練精度:1.000 検証精度: 0.047\n",
            "エポック:  9, 訓練損失: 0.006 テスト損失: 4.568, 訓練精度:1.000 検証精度: 0.055\n",
            "エポック: 10, 訓練損失: 0.005 テスト損失: 4.395, 訓練精度:1.000 検証精度: 0.070\n",
            "エポック: 11, 訓練損失: 0.005 テスト損失: 4.140, 訓練精度:1.000 検証精度: 0.078\n",
            "エポック: 12, 訓練損失: 0.006 テスト損失: 3.837, 訓練精度:1.000 検証精度: 0.086\n",
            "エポック: 13, 訓練損失: 0.006 テスト損失: 3.534, 訓練精度:1.000 検証精度: 0.125\n",
            "エポック: 14, 訓練損失: 0.006 テスト損失: 3.204, 訓練精度:1.000 検証精度: 0.227\n",
            "エポック: 15, 訓練損失: 0.005 テスト損失: 2.985, 訓練精度:1.000 検証精度: 0.227\n",
            "エポック: 16, 訓練損失: 0.005 テスト損失: 2.832, 訓練精度:1.000 検証精度: 0.227\n",
            "エポック: 17, 訓練損失: 0.006 テスト損失: 2.655, 訓練精度:1.000 検証精度: 0.273\n",
            "エポック: 18, 訓練損失: 0.009 テスト損失: 2.519, 訓練精度:1.000 検証精度: 0.211\n",
            "エポック: 19, 訓練損失: 0.252 テスト損失: 3.095, 訓練精度:0.978 検証精度: 0.203\n",
            "エポック: 20, 訓練損失: 0.359 テスト損失: 1.078, 訓練精度:0.969 検証精度: 0.641\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "epochs = 20\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
        "for epoch in range(epochs):\n",
        "#for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    train_loss, test_loss, train_acc, test_acc = 0., 0., 0., 0.\n",
        "    for (x, y) in train_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        loss, preds = train_step(x, y)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_acc /= len(train_dataloader)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    for (x, y) in test_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        loss, preds = test_step(x, y)\n",
        "        test_loss += loss.item()\n",
        "        test_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_acc /= len(test_dataloader)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f'エポック: {epoch + 1:2d},',\n",
        "          f'訓練損失: {train_loss:.3f}',\n",
        "          f'テスト損失: {test_loss:.3f},',\n",
        "          f'訓練精度:{train_acc:.3f}',\n",
        "          f'検証精度: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa29be63-69a7-4bbf-9b51-5945f952052c",
      "metadata": {
        "id": "aa29be63-69a7-4bbf-9b51-5945f952052c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 学習進行状況の描画\n",
        "plt.plot(train_losses, label='訓練損失')\n",
        "plt.plot(test_losses, label='テスト損失')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(test_accs, label=\"テスト精度\")\n",
        "plt.plot(train_accs, label=\"訓練精度\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a"
      },
      "source": [
        "<center>\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/ResNet_Fig2.svg' width=\"49%\"><br/>\n",
        "\n",
        "\n",
        "<img src='https://komazawa-deep-learning.github.io/assets/2015ResNet30.svg' width=\"88%\"><br/>\n",
        "</center>"
      ],
      "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2021_1028Olivetti_face_ResNet_from_Pytorch_source.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}