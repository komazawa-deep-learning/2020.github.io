{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPrnze+YnSaDSUju+l8sFvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024notebooks/2024_0927pytorch_charRNN_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [青空文庫](https://www.aozora.gr.jp/) を用いた RNN のデモ\n"
      ],
      "metadata": {
        "id": "uW4z1BNtApfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.aozora.gr.jp/cards/000258/files/50326_35772.html\n",
        "txt = [ \"武蔵の国のある村に茂作、巳之吉と云う二人の木こりがいた。 \",\n",
        "       \"この話のあった時分には、茂作は老人であった。\",\n",
        "        \"そして、彼の年季奉公人であった巳之吉は、十八の少年であった。\",\n",
        "        \"毎日、彼等は村から約二里離れた森へ一緒に出かけた。\",\n",
        "        \"その森へ行く道に、越さねばならない大きな河がある。\",\n",
        "        \"そして、渡し船がある。\",\n",
        "        \"渡しのある処にたびたび、橋が架けられたが、その橋は洪水のあるたびごとに流された。\",\n",
        "        \"河の溢れる時には、普通の橋では、その急流を防ぐ事はできない。\",\n",
        "\n",
        "        \"　茂作と巳之吉はある大層寒い晩、帰り途で大吹雪に遇った。\",\n",
        "        \"渡し場に着いた、渡し守は船を河の向う側に残したままで、帰った事が分った。\",\n",
        "        \"泳がれるような日ではなかった。\",\n",
        "        \"それで木こりは渡し守の小屋に避難した――避難処の見つかった事を僥倖に思いながら。\",\n",
        "        \"小屋には火鉢はなかった。\",\n",
        "        \"火をたくべき場処もなかった。\",\n",
        "        \"窓のない一方口の、二畳敷の小屋であった。\",\n",
        "        \"茂作と巳之吉は戸をしめて、蓑をきて、休息するために横になった。\",\n",
        "        \"初めのうちはさほど寒いとも感じなかった。\",\n",
        "        \"そして、嵐はじきに止むと思った。\",\n",
        "\n",
        "        \"　老人はじきに眠りについた。\",\n",
        "        \"しかし、少年巳之吉は長い間、目をさましていて、恐ろしい風や戸にあたる雪のたえない音を聴いていた。\",\n",
        "        \"河はゴウゴウと鳴っていた。\",\n",
        "        \"小屋は海上の和船のようにゆれて、ミシミシ音がした。\",\n",
        "        \"恐ろしい大吹雪であった。\",\n",
        "        \"空気は一刻一刻、寒くなって来た、そして、巳之吉は蓑の下でふるえていた。\",\n",
        "        \"しかし、とうとう寒さにも拘らず、彼もまた寝込んだ。\",\n",
        "\n",
        "        \"　彼は顔に夕立のように雪がかかるので眼がさめた。\",\n",
        "        \"小屋の戸は無理押しに開かれていた。\",\n",
        "        \"そして雪明かりで、部屋のうちに女、――全く白装束の女、――を見た。\",\n",
        "        \"その女は茂作の上に屈んで、彼に彼女の息をふきかけていた、――そして彼女の息はあかるい白い煙のようであった。\",\n",
        "        \"ほとんど同時に巳之吉の方へ振り向いて、彼の上に屈んだ。\",\n",
        "        \"彼は叫ぼうとしたが何の音も発する事ができなかった。\",\n",
        "        \"白衣の女は、彼の上に段々低く屈んで、しまいに彼女の顔はほとんど彼にふれるようになった、そして彼は――彼女の眼は恐ろしかったが――彼女が大層綺麗である事を見た。\",\n",
        "        \"しばらく彼女は彼を見続けていた、――それから彼女は微笑した、そしてささやいた、――『私は今ひとりの人のように、あなたをしようかと思った。\",\n",
        "        \"しかし、あなたを気の毒だと思わずにはいられない、――あなたは若いのだから。\",\n",
        "        \"……あなたは美少年ね、巳之吉さん、もう私はあなたを害しはしません。\",\n",
        "        \"しかし、もしあなたが今夜見た事を誰かに――あなたの母さんにでも――云ったら、私に分ります、そして私、あなたを殺します。\",\n",
        "        \"……覚えていらっしゃい、私の云う事を』　そう云って、向き直って、彼女は戸口から出て行った。\",\n",
        "        \"その時、彼は自分の動ける事を知って、飛び起きて、外を見た。しかし、女はどこにも見えなかった。\",\n",
        "        \"そして、雪は小屋の中へ烈しく吹きつけていた。巳之吉は戸をしめて、それに木の棒をいくつか立てかけてそれを支えた。\",\n",
        "        \"彼は風が戸を吹きとばしたのかと思ってみた、――彼はただ夢を見ていたかもしれないと思った。\",\n",
        "        \"それで入口の雪あかりの閃きを、白い女の形と思い違いしたのかもしれないと思った。\",\n",
        "        \"しかもそれもたしかではなかった。彼は茂作を呼んでみた。\",\n",
        "        \"そして、老人が返事をしなかったので驚いた。\",\n",
        "        \"彼は暗がりへ手をやって茂作の顔にさわってみた。\",\n",
        "        \"そして、それが氷である事が分った。茂作は固くなって死んでいた。……\"]"
      ],
      "metadata": {
        "id": "E2Vb_wJYi7tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 表示精度桁数の設定\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "#chars = list(sorted(set([ch for ch in txt])))\n",
        "chars = list(sorted(set(\"\".join(txt))))\n",
        "n_vocab = len(chars)\n",
        "chr2idx = { ch:i for i,ch in enumerate(chars) }\n",
        "idx2chr = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "maxlen = len(max(txt, key=len))\n",
        "for i in range(len(txt)):\n",
        "    while len(txt[i]) < maxlen:\n",
        "        txt[i] += \" \"\n",
        "\n",
        "# Creating lists that will hold our input and target sequences\n",
        "inp_seq = []\n",
        "tgt_seq = []\n",
        "\n",
        "for i in range(len(txt)):\n",
        "    # Remove last character for input sequence\n",
        "    inp_seq.append(txt[i][:-1])\n",
        "\n",
        "    # Remove first character for target sequence\n",
        "    tgt_seq.append(txt[i][1:])\n",
        "    print(f\"{i:3d}: Input Sequence: {inp_seq[i]}\",\n",
        "          f\"Target Sequence: {tgt_seq[i]}\")\n",
        "\n",
        "for i in range(len(txt)):\n",
        "    inp_seq[i] = [chr2idx[ch] for ch in inp_seq[i]]\n",
        "    tgt_seq[i] = [chr2idx[ch] for ch in tgt_seq[i]]\n"
      ],
      "metadata": {
        "id": "0h3NcRr-0HJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_size = len(chr2idx)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(txt)\n",
        "\n",
        "def one_hot_encode(seq, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "\n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, seq[i][u]] = 1\n",
        "    return features\n",
        "\n",
        "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
        "input_seq = one_hot_encode(inp_seq, dict_size, seq_len, batch_size)\n",
        "target_seq = torch.Tensor(tgt_seq)\n",
        "#target_seq = one_hot_encode(tgt_seq, dict_size, seq_len, batch_size)\n",
        "\n",
        "input_seq = torch.from_numpy(input_seq)\n",
        "#target_seq = torch.from_numpy(target_seq)\n",
        "#target_seq = torch.Tensor(target_seq)\n"
      ],
      "metadata": {
        "id": "CpjzcCnL3N1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(target_seq)"
      ],
      "metadata": {
        "id": "saJdj4WB5Eyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "id": "bJcIHjXK4S-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "CNvUZeqH6N2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 1000\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "WSvoXiLc6hZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch: {epoch}/{n_epochs}.............', end=' ')\n",
        "        print(f\"Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "XcAvJFce6ka2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
        "def predict(model, ch):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    chs = np.array([[chr2idx[c] for c in ch]])\n",
        "    chs = one_hot_encode(chs, dict_size, chs.shape[1], 1)\n",
        "    chs = torch.from_numpy(chs)\n",
        "    chs.to(device)\n",
        "    out, hid = model(chs)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    chr_idx = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return idx2chr[chr_idx], hid\n",
        "\n",
        "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
        "def sample(model, out_len, start='老'):\n",
        "    model.eval() # eval mode\n",
        "    #start = start # .lower()\n",
        "    # First off, run through the starting characters\n",
        "    chs = [ch for ch in start]\n",
        "    size = out_len - len(chs)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        ch, h = predict(model, chs)\n",
        "        chs.append(ch)\n",
        "\n",
        "    return ''.join(chs)"
      ],
      "metadata": {
        "id": "FxBEY4ZR64wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model, 15, 'の')"
      ],
      "metadata": {
        "id": "zfzx1h0g7MUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# hyperparameters\n",
        "n_hid = 128 # size of hidden layer of neurons\n",
        "n_seq = 25 # number of steps to unroll the RNN for\n",
        "lr = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(n_hid, len(chars)) * 0.01 # input to hidden\n",
        "Whh = np.random.randn(n_hid, n_hid) * 0.01 # hidden to hidden\n",
        "Why = np.random.randn(len(chars), n_hid) * 0.01 # hidden to output\n",
        "bh = np.zeros((n_hid, 1)) # hidden bias\n",
        "by = np.zeros((n_vocab, 1)) # output bias\n",
        "\n",
        "def train(inputs:np.array,\n",
        "          targets:np.array,\n",
        "          H_init:np.array):\n",
        "    \"\"\"\n",
        "        inputs, targets are both list of integers.\n",
        "        h_init is Hx1 array of initial hidden state\n",
        "        returns the loss, gradients on model parameters, and last hidden state\n",
        "    \"\"\"\n",
        "    X, H, Y, Prob = {}, {}, {}, {}\n",
        "    H[-1] = np.copy(H_init)\n",
        "    loss = 0\n",
        "    # forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        X[t] = np.zeros((n_vocab,1)) # encode in 1-of-k representation\n",
        "        X[t][inputs[t]] = 1\n",
        "        H[t] = np.tanh(np.dot(Wxh, X[t]) + np.dot(Whh, H[t-1]) + bh) # hidden state\n",
        "        Y[t] = np.dot(Why, H[t]) + by # unnormalized log probabilities for next chars\n",
        "        Prob[t] = np.exp(Y[t]) / np.sum(np.exp(Y[t])) # probabilities for next chars\n",
        "        loss += -np.log(Prob[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dh_init = np.zeros_like(H[0])\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dY = np.copy(Prob[t])\n",
        "        dY[targets[t]] -= 1 # backprop into y\n",
        "        dWhy += np.dot(dY, H[t].T)\n",
        "        dby += dY\n",
        "        dh = np.dot(Why.T, dY) + dh_init # backprop into h\n",
        "        dh_ = (1 - H[t] * H[t]) * dh # backprop through tanh nonlinearity\n",
        "        dbh += dh_\n",
        "        dWxh += np.dot(dh_, X[t].T)\n",
        "        dWhh += np.dot(dh_, H[t-1].T)\n",
        "        dh_next = np.dot(Whh.T, dh_)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, H[len(inputs)-1]\n",
        "\n",
        "\n",
        "def sample(H:np.array,\n",
        "           seed_idx:int,\n",
        "           n:int):\n",
        "    \"\"\"\n",
        "        sample a sequence of integers from the model\n",
        "        h is memory state, seed_idx is seed letter for first time step\n",
        "    \"\"\"\n",
        "    X = np.zeros((n_vocab, 1))\n",
        "    X[seed_idx] = 1\n",
        "    ids = []\n",
        "    for t in range(n):\n",
        "        H = np.tanh(np.dot(Wxh, X) + np.dot(Whh, H) + bh)\n",
        "        Y = np.dot(Why, H) + by\n",
        "        Prob = np.exp(Y) / np.sum(np.exp(Y))\n",
        "        _idx = np.random.choice(range(n_vocab), p=Prob.ravel())\n",
        "        X = np.zeros((n_vocab, 1))\n",
        "        X[_idx] = 1\n",
        "        ids.append(_idx)\n",
        "    return ids"
      ],
      "metadata": {
        "id": "hqRGpIcNFEFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(chars)\n",
        "#_chars = set(\"\".join(txt))\n",
        "print(chars)"
      ],
      "metadata": {
        "id": "obtO6_uBzfDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/n_vocab) * n_seq # loss at iteration 0\n",
        "n, p = 0, 0\n",
        "\n",
        "epochs = 30000\n",
        "for n in range(epochs):\n",
        "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "    if p+n_seq+1 >= len(txt) or n == 0:\n",
        "        h_init = np.zeros((n_hid,1)) # reset RNN memory\n",
        "        p = 0 # go from start of data\n",
        "    inputs = [chr2idx[ch] for ch in txt[p:p+n_seq]]\n",
        "    targets = [chr2idx[ch] for ch in txt[p+1:p+n_seq+1]]\n",
        "\n",
        "    # forward seq_length characters through the net and fetch gradient\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = train(inputs, targets, h_init)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "    # sample from the model now and then\n",
        "    if n % 100 == 0:\n",
        "        print(f'反復回数: {n:6d}, 損失: {smooth_loss:8.3f}', end=\": \")\n",
        "\n",
        "        sample_ids = sample(h_init, inputs[0], 50)\n",
        "        out_txt = ''.join(idx2chr[idx] for idx in sample_ids)\n",
        "        print(f'サンプリング： {out_txt}')\n",
        "\n",
        "    # perform parameter update with Adagrad\n",
        "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -lr * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "    p += n_seq # move data pointer\n",
        "    #n += 1 # iteration counter\n"
      ],
      "metadata": {
        "id": "AJxhBjWHvEkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class rnn(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 rnn_type:str='LSTM',\n",
        "                 ntoken:int=None,\n",
        "                 ninp:int=None,\n",
        "                 nhid:int=None,\n",
        "                 nlayers:int=None,\n",
        "                 dropout:float=0.5,\n",
        "                 tie_weights:bool=False):\n",
        "        super().__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\") from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "metadata": {
        "id": "BSZElji5lhqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKz6KdoiozE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CSB-Ug5tPbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}