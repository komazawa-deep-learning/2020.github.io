{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_1213royabel_BU_TD_multi_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Abel&Ullman (2023) Top-Down Network Combines Back-Propagation with Attention](https://arxiv.org/abs/2306.02415)\n",
        "\n",
        "あるいは，[Roy Abel, Shimon Ullman (2023) Biologically-Motivated Learning Model for Instructed Visual Processing](https://arxiv.org/abs/2306.02415)\n",
        "\n",
        "* [https://github.com/royabel/Top-Down-Networks](https://github.com/royabel/Top-Down-Networks)\n",
        "\n",
        "本論文では，生物学に着想を得たインストラクションモデルの学習法を提案する。\n",
        "ボトムアップ (BU)-トップダウン (TD) モデルを用い，1 つの TD ネットワークを学習と注意誘導の両方に使用する。\n",
        "本論文の主な貢献は以下の通り\n",
        "<!-- The paper propose a biologically-inspired learning method for instruction-models.\n",
        "It uses a bottom-up (BU) - top-down (TD) model, in which a single TD network is used for both learning and guiding attention.\n",
        "The key contributions of the paper are: -->\n",
        "\n",
        "* 誤差信号からの学習とトップダウンの注意を組み合わせた新しいトップダウン機構を提案\n",
        "* 従来研究を拡張し，より生物学的に妥当な学習モデルへの新たなステップを提供\n",
        "* 生物学的学習のためのカウンター Hebb 学習機構の提案\n",
        "* 従来のネットワークの中に，課題依存した独自の部分ネットワークを動的作成。生物学に着想を得た新しい Multi Task Learning (MTL) アルゴリズムの提示\n",
        "\n",
        "<!-- * Propose a novel top-down mechanism that combines learning from error signals with top-down attention.\n",
        "* Extending earlier work, offering a new step toward a more biologically plausible learning model.\n",
        "* Suggest a Counter-Hebbian mechanism for biological learning.\n",
        "* Present a novel biologically-inspired MTL algorithm that dynamically creates unique task-dependent sub-networks within conventional networks. -->"
      ],
      "metadata": {
        "id": "zXBeUSmf8gYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備作業"
      ],
      "metadata": {
        "id": "-lMsc5Hy8moi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tYI7LXTZVbY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f'device:{device}')\n",
        "\n",
        "import os\n",
        "try:\n",
        "    import ipynb_path\n",
        "except ImportError:\n",
        "    !pip install ipynb-path\n",
        "    import ipynb_path\n",
        "__file__ = os.path.basename(ipynb_path.get())\n",
        "print(f'__file__:{__file__}')\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "print(f'isColab:{isColab}')\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    basedir = '/content/drive/Shareddrives/#2024認知心理学研究(1)b/浅川先生/MNIST'\n",
        "\n",
        "    #!cp '/content/drive/Shareddrives/#2024認知心理学研究(1)b/浅川先生/MNIST/names_utils.py' .\n",
        "    !git clone https://github.com/royabel/Top-Down-Networks.git\n",
        "    !mv Top-Down-Networks/* .\n",
        "else:\n",
        "    HOME = os.environ['HOME']\n",
        "    basedir = os.path.join(HOME, 'study/data/MNIST')\n",
        "\n",
        "import json\n",
        "with open(os.path.join(basedir, 'multi_mnist_config.json')) as config_params:\n",
        "    configs_ = json.load(config_params)\n",
        "\n",
        "if isColab:\n",
        "    configs_['data set parameters']['data_path'] = os.path.join(basedir, 'processed')\n",
        "else:\n",
        "    configs_['data set parameters']['data_path'] = '../data/MNIST/processed'\n",
        "#print(configs_['learning settings'])\n",
        "\n",
        "configs_['learning settings'].update(configs_['architecture parameters'])\n",
        "for k, v in configs_.items():\n",
        "    #print(k, v)\n",
        "    if isinstance(v, dict):\n",
        "        for kk, vv in v.items():\n",
        "            print(f'\\t{k}:{kk}->{vv}')\n",
        "    else:\n",
        "        print(f'{k}:{v}')\n",
        "\n",
        "# import numpy as np\n",
        "# import sys\n",
        "# import zipfile\n",
        "# import glob\n",
        "# import matplotlib.pyplot as plt\n",
        "# import PIL\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 設定ファイルの読み込み"
      ],
      "metadata": {
        "id": "J5mwwtWMC6eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader の設定"
      ],
      "metadata": {
        "id": "54C4LUWlC9oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "WeightDecay = False\n",
        "Analyze = False\n",
        "aveLastModel = True\n",
        "SaveIntermediateModels = True\n",
        "EvalIntermediate = True\n",
        "SavedModelsFrequency = 5\n",
        "SaveForResuming = True\n",
        "SavedModelDirPath = ''\n",
        "EvalInterFrequency = 5\n",
        "SaveLastModel = True\n",
        "\n",
        "import json\n",
        "\n",
        "with open('multi_mnist_config.json') as config_params:\n",
        "    configs_ = json.load(config_params)\n",
        "\n",
        "configs_['data set parameters']['data_path'] = '../data/MNIST/processed'\n",
        "#print(configs_['learning settings'])\n",
        "\n",
        "configs_['learning settings'].update(configs_['architecture parameters'])\n",
        "for k, v in configs_.items():\n",
        "    #print(k, v)\n",
        "    if isinstance(v, dict):\n",
        "        for kk, vv in v.items():\n",
        "            print(f'\\t{k}:{kk}->{vv}')\n",
        "    else:\n",
        "        print(f'{k}:{v}')"
      ],
      "metadata": {
        "id": "lNTJbQ26Ao8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 設定ファイルの読み込み"
      ],
      "metadata": {
        "id": "gwstjVN78vNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def global_transformer():\n",
        "    return transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "class MNIST(data.Dataset):\n",
        "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where ``processed/training.pt``\n",
        "            and  ``processed/test.pt`` exist.\n",
        "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
        "            otherwise from ``test.pt``.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "    \"\"\"\n",
        "    urls = [\n",
        "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
        "    ]\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'processed'\n",
        "    training_file = 'training.pt'\n",
        "    test_file = 'test.pt'\n",
        "    multi_training_file = 'multi_training.pt'\n",
        "    multi_validation_file = 'multi_validation.pt'\n",
        "    multi_test_file = 'multi_test.pt'\n",
        "\n",
        "    def __init__(self,\n",
        "                 root=os.path.join(HOME, 'study/data/MNIST'),\n",
        "                 split=\"train\",\n",
        "                 transform=global_transformer(),\n",
        "                 download=False):\n",
        "        assert split in [\"train\", \"val\", \"test\"]\n",
        "\n",
        "        #self.root = os.path.expanduser(root)\n",
        "\n",
        "        isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "        if isColab:\n",
        "            root='/content/drive/Shareddrives/#2024認知心理学研究(1)b/浅川先生/MNIST'\n",
        "\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        if not self._check_multi_exists():\n",
        "            raise RuntimeError('Dataset not found.' +\n",
        "                               ' You can use download=True to download MNIST and generate a random MultiMNIST')\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            self.train_data, self.train_labels_l, self.train_labels_r = torch.load(\n",
        "                os.path.join(self.root, self.processed_folder, self.multi_training_file),\n",
        "                weights_only=False)\n",
        "        elif self.split == \"val\":\n",
        "            self.validation_data, self.validation_labels_l, self.validation_labels_r = torch.load(\n",
        "                os.path.join(self.root, self.processed_folder, self.multi_validation_file),\n",
        "                weights_only=False)\n",
        "        else:\n",
        "            self.test_data, self.test_labels_l, self.test_labels_r = torch.load(\n",
        "                os.path.join(self.root, self.processed_folder, self.multi_test_file),\n",
        "                weights_only=False)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.split == \"train\":\n",
        "            img, target_l, target_r = self.train_data[index], self.train_labels_l[index], self.train_labels_r[index]\n",
        "        elif self.split == \"val\":\n",
        "            img, target_l, target_r = self.validation_data[index], self.validation_labels_l[index], \\\n",
        "                                      self.validation_labels_r[index]\n",
        "        else:\n",
        "            img, target_l, target_r = self.test_data[index], self.test_labels_l[index], self.test_labels_r[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets to return a PIL Image\n",
        "        img = Image.fromarray(img.numpy().astype(np.uint8), mode='L')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target_l, target_r\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == \"train\":\n",
        "            return len(self.train_data)\n",
        "        elif self.split == \"val\":\n",
        "            return len(self.validation_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_multi_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.multi_training_file)) and \\\n",
        "            os.path.exists(os.path.join(self.root, self.processed_folder, self.multi_test_file)) and \\\n",
        "            os.path.exists(os.path.join(self.root, self.processed_folder, self.multi_validation_file))\n",
        "\n",
        "\n",
        "train_dataset = MNIST(split='train')\n",
        "test_dataset = MNIST(split='test')\n",
        "val_dataset = MNIST(split='val')"
      ],
      "metadata": {
        "id": "ztF0uM4X4fCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ncols = 10\n",
        "nrows = 10\n",
        "fig, axes = plt.subplots(ncols=ncols,nrows=nrows,figsize=(12,14))\n",
        "Ns = np.random.permutation(train_dataset.__len__())\n",
        "\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        img, left_lbl, right_lbl = train_dataset.__getitem__(Ns[j*nrows+i])\n",
        "        axes[i][j].imshow(img.detach().numpy()[0], cmap='gray')\n",
        "        axes[i][j].set_title(f'L:{int(left_lbl.detach())}, R:{int(right_lbl.detach())}')\n",
        "        axes[i][j].set_xticks([])\n",
        "        axes[i][j].set_yticks([])"
      ],
      "metadata": {
        "id": "912Uc5HvLgBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def update_in_shape_and_n_classes(params, data_loader):\n",
        "    # auto adjust data set input size and number of classes\n",
        "    data_sample = data_loader.dataset[0]\n",
        "\n",
        "    if hasattr(data_loader.dataset, 'n_classes'):\n",
        "        params['n_classes'] = data_loader.dataset.n_classes\n",
        "    elif hasattr(data_loader.dataset, 'classes'):\n",
        "        params['n_classes'] = len(data_loader.dataset.classes)\n",
        "    elif len(data_sample[1].shape) > 1:\n",
        "        params['n_classes'] = data_sample[1].shape[-1]\n",
        "    elif hasattr(data_loader.dataset, 'train_labels_l'):\n",
        "        params['n_classes'] = data_loader.dataset.train_labels_l.max() + 1\n",
        "    else:\n",
        "        raise ValueError('Cannot infer the number of classes from the data loader object')\n",
        "\n",
        "    print(f'type(data_sample[0]):{type(data_sample[0])}')\n",
        "    print(f'data_sample[0].size():{data_sample[0].size()}')\n",
        "    if isinstance(data_sample[0], list):\n",
        "        in_sample_ = data_sample[0][0]\n",
        "    else:\n",
        "        in_sample_ = data_sample[0]\n",
        "\n",
        "    params['in_shape'] = in_sample_.shape\n",
        "\n",
        "    return params\n",
        "\n",
        "update_in_shape_and_n_classes(configs_['learning settings'], train_dataloader)\n",
        "print(configs_['learning settings']['in_shape'])"
      ],
      "metadata": {
        "id": "edj_3Nav4e-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BU-TD ネットワークの定義"
      ],
      "metadata": {
        "id": "jIT-B7Xm8c8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from names_utils import name2loss, name2metric, name2optim, name2network_module\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "from butd_modules.butd_architectures import BUTDSimpleNet, BUTDTinyResNet\n",
        "#from butd_modules.butd_core_networks import BUTDSimpleNet, BUTDTinyResNet\n",
        "from butd_modules.butd_architectures import get_conv_net, butd_resnet18\n",
        "from utils import galu\n",
        "\n",
        "##from butd_modules.network_models import ClassificationBUTDNet, TaskBUTDNet\n",
        "## 以下は butd_modules.netword_models.py より全文引用\n",
        "from butd_modules.butd_layers import *\n",
        "\n",
        "class ClassificationBUTDNet(nn.Module):\n",
        "    def __init__(self, n_classes: int, shared_weights, core_network, **kwargs):\n",
        "        super(ClassificationBUTDNet, self).__init__()\n",
        "        self.shared_weights = shared_weights\n",
        "        self.n_classes = n_classes\n",
        "        self.core_net = core_network\n",
        "\n",
        "        self.head_layer = BUTDLinear(in_features=self.core_net.out_shape, out_features=n_classes,\n",
        "                                     shared_weights=shared_weights, **kwargs)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor, non_linear=True, lateral=False,\n",
        "                      head_non_linear=False, head_lateral=False, **kwargs):\n",
        "        x = self.core_net(x, non_linear=non_linear, lateral=lateral, **kwargs)\n",
        "\n",
        "        x = self.head_layer(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor, non_linear=True, lateral=False,\n",
        "                head_non_linear=False, head_lateral=False, **kwargs):\n",
        "        return self._forward_impl(x, non_linear=non_linear, lateral=lateral,\n",
        "                                  head_non_linear=head_non_linear, head_lateral=head_lateral, **kwargs)\n",
        "\n",
        "    def back_forward(self, x: Tensor, non_linear=False, lateral=True,\n",
        "                     head_non_linear=False, head_lateral=True, **kwargs):\n",
        "        x = self.head_layer.back_forward(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "\n",
        "        x = self.core_net.back_forward(x, non_linear=non_linear, lateral=lateral, **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def counter_hebbian_back_prop(self, grads, **kwargs):\n",
        "        # propagate gradients (back propagation)\n",
        "        self.back_forward(grads, non_linear=False, lateral=True, head_non_linear=False, head_lateral=True,\n",
        "                          bias_blocking=True, **kwargs)\n",
        "\n",
        "        self.counter_hebbian_update_value(**kwargs)\n",
        "\n",
        "    def counter_hebbian_update_value(self, update_forward_bias=True, update_backward_bias=True):\n",
        "        self.core_net.counter_hebbian_update_value(\n",
        "            update_forward_bias=update_forward_bias, update_backward_bias=update_backward_bias)\n",
        "        self.head_layer.counter_hebbian_update_value(\n",
        "            update_forward_bias=update_forward_bias, update_backward_bias=update_backward_bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def probs(network_outputs):\n",
        "        return F.softmax(network_outputs, axis=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def multi_label_probs(network_outputs):\n",
        "        return torch.sigmoid(network_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def predict(network_outputs):\n",
        "        if len(network_outputs.shape) == 1 or (len(network_outputs.shape) == 2 and network_outputs.shape[1] == 1):\n",
        "            if len(network_outputs.shape) == 2:\n",
        "                network_outputs = network_outputs[:, 0]\n",
        "            return (network_outputs > 0).int()\n",
        "        return torch.argmax(network_outputs, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "class TaskBUTDNet(ClassificationBUTDNet):\n",
        "    def __init__(self, task_vector_size, n_tasks, task_embedding_size=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.task_vector_size = task_vector_size\n",
        "        self.n_tasks = n_tasks\n",
        "        self.multi_decoders = kwargs.get('multi_decoders', False)\n",
        "\n",
        "        if self.multi_decoders:\n",
        "            self.head_layer = BUTDLinear(in_features=self.core_net.out_shape, out_features=self.n_classes * self.n_tasks,\n",
        "                                         **kwargs)\n",
        "\n",
        "        task_layers = [self.core_net.out_shape]\n",
        "        if task_embedding_size is not None:\n",
        "            task_layers.append(task_embedding_size)\n",
        "        task_layers.append(task_vector_size)\n",
        "\n",
        "        self.task_head = BUTDSequential(*[\n",
        "            BUTDLinear(in_features=task_layers[i], out_features=task_layers[i+1], **kwargs)\n",
        "            for i in range(len(task_layers)-1)\n",
        "        ])\n",
        "\n",
        "    def _forward_impl(self, x: Tensor, non_linear=True, lateral=False, task_head=False,\n",
        "                      head_non_linear=False, head_lateral=False, **kwargs):\n",
        "        x = self.core_net(x, non_linear=non_linear, lateral=lateral, **kwargs)\n",
        "\n",
        "        if task_head:\n",
        "            x = self.task_head(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "        else:\n",
        "            x = self.head_layer(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def back_forward(self, x: Tensor, non_linear=False, lateral=True, task_head=False,\n",
        "                     head_non_linear=False, head_lateral=True, **kwargs):\n",
        "        if task_head:\n",
        "            x = self.task_head.back_forward(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "        else:\n",
        "            x = self.head_layer.back_forward(x, non_linear=head_non_linear, lateral=head_lateral, **kwargs)\n",
        "\n",
        "        x = self.core_net.back_forward(x, non_linear=non_linear, lateral=lateral, **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor, task=None, non_linear=True, lateral=False, task_head=False,\n",
        "                head_non_linear=False, head_lateral=False, pure_bu=False, **kwargs):\n",
        "        if task is None:\n",
        "            return self._forward_impl(x, non_linear=non_linear, lateral=lateral, task_head=task_head,\n",
        "                                      head_non_linear=head_non_linear, head_lateral=head_lateral, **kwargs)\n",
        "\n",
        "        # Task guidance forward. It includes two steps:\n",
        "        # 1) a backward pass with task as input to select the task-dependent sub-network\n",
        "        # 2) a forward pass made on the selected sub-network\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.back_forward(task, non_linear=True, lateral=False, task_head=True,\n",
        "                              head_non_linear=True, head_lateral=False)\n",
        "\n",
        "        x = self._forward_impl(x, non_linear=True, lateral=True, task_head=False, **kwargs)\n",
        "\n",
        "        if self.multi_decoders:\n",
        "            if self.n_classes > 1:\n",
        "                x = x.reshape(x.shape[0], -1, self.n_classes)\n",
        "\n",
        "            # Select the output neurons correspond with the requested task\n",
        "            x = x[task == 1]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_network(net_params):\n",
        "    core_net_arch = name2network_module(net_params['network_name'])\n",
        "    core_network = core_net_arch(**net_params)\n",
        "\n",
        "    if net_params.get('mtl', False):\n",
        "        net = TaskBUTDNet(core_network=core_network, **net_params)\n",
        "    else:\n",
        "        net = ClassificationBUTDNet(core_network=core_network, **net_params)\n",
        "\n",
        "    return net\n",
        "\n",
        "net_ = create_network(configs_['learning settings'])\n",
        "net_.eval()"
      ],
      "metadata": {
        "id": "V3XGfeJd72Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練と評価のための関数定義"
      ],
      "metadata": {
        "id": "70xdEqCtBmfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import datetime\n",
        "\n",
        "#Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "Device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f'Device:{Device}')\n",
        "\n",
        "# from utils.py\n",
        "def loss_grads(outputs, labels, loss_name=None, n_classes=2):\n",
        "    loss_name = 'BCE' if loss_name is None else loss_name\n",
        "    if loss_name == 'BCE':\n",
        "        # BCE gradients\n",
        "        return (labels * (torch.sigmoid(outputs) - 1) + (1 - labels) * torch.sigmoid(outputs)) / labels.shape[-1]\n",
        "    if loss_name == 'CrossEntropy':\n",
        "        if len(labels.shape) > 1:\n",
        "            return F.softmax(outputs, dim=1) - labels\n",
        "        return F.softmax(outputs, dim=1) - F.one_hot(labels, n_classes)\n",
        "    if loss_name == 'MSE':\n",
        "        # MSE gradients\n",
        "        return (2 / n_classes) * (outputs - labels)\n",
        "    raise ValueError(f\"loss {loss_name} was not implemented\")\n",
        "\n",
        "\n",
        "class ModelCTL:\n",
        "    \"\"\"\n",
        "    The main model object for training and evaluating a network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net, benchmark='Multi MNIST', **kwargs):\n",
        "        #self.logger = logging.getLogger(__name__)\n",
        "        #self.writer_name = None\n",
        "\n",
        "        self.benchmark = benchmark\n",
        "        self.analysis_dict = {}\n",
        "\n",
        "        # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = Device\n",
        "        if torch.cuda.device_count() > 1 and self.device != 'cpu':\n",
        "            self.logger.info(f\"{torch.cuda.device_count()} GPUs were detected and will be used\")\n",
        "            # net = torch.nn.DataParallel(net)\n",
        "            net = MyDataParallel(net)\n",
        "        self.net = net.to(self.device)\n",
        "\n",
        "        self.n_classes = net.n_classes\n",
        "        self.task = kwargs.get('task', None)\n",
        "        self.n_tasks = kwargs.get('n_tasks', None)\n",
        "\n",
        "        self.loss_name = kwargs.get('loss_name', 'BCE')\n",
        "        self.criterion = name2loss(self.loss_name)\n",
        "        self.metric_name = kwargs.get('metric_name', 'Accuracy')\n",
        "        self.metric = name2metric(self.metric_name)\n",
        "\n",
        "        self.trainloader = None\n",
        "        self.testloader = None\n",
        "        self.dataset_name = None\n",
        "\n",
        "    def _analyze(self, epoch):\n",
        "        with torch.no_grad():\n",
        "            if self.benchmark == \"Multi MNIST\" and isinstance(self.net.core_net, BUTDSimpleNet):\n",
        "                if 'td_activations' not in self.analysis_dict:\n",
        "                    self.analysis_dict['td_activations'] = {}\n",
        "\n",
        "                td_activations = {t: {} for t in range(self.n_tasks)}\n",
        "\n",
        "                # Sub-Networks Analysis\n",
        "\n",
        "                # Calculate the task-dependent sub-networks for all tasks\n",
        "                tasks = torch.eye(self.n_tasks, device=self.device)\n",
        "                self.net.back_forward(tasks, non_linear=True, lateral=False, task_head=True,\n",
        "                                      head_non_linear=True, head_lateral=False)\n",
        "\n",
        "                layer_i = 0\n",
        "                for td_layer in self.net.core_net.layers:\n",
        "                    if not hasattr(td_layer, 'td_neurons'):\n",
        "                        continue\n",
        "                    for t in range(self.n_tasks):\n",
        "                        td_activations[t][f\"{layer_i} - {td_layer._get_name()}\"] = td_layer.td_neurons[t].tolist()\n",
        "                    layer_i += 1\n",
        "\n",
        "                self.analysis_dict['td_activations'][epoch] = td_activations\n",
        "        return None\n",
        "\n",
        "    def train(self,\n",
        "              dataloader=None,\n",
        "              lr=0.001,\n",
        "              epochs=10,\n",
        "              ch_learning=False,\n",
        "              train_all_tasks=False,\n",
        "              **kwargs):\n",
        "        \"\"\"\n",
        "        Train the model's network (self.net)\n",
        "\n",
        "        Args:\n",
        "            dataloader (DataLoader): a data loader to train on\n",
        "            lr (float): the learning rate\n",
        "            epochs (int): number of epochs to train the model\n",
        "            ch_learning (bool): whether to use Counter Hebbian Learning or the standard optimizer.\n",
        "            mtl (bool): whether it is multi-task learning or a single task\n",
        "        \"\"\"\n",
        "        if dataloader is not None:\n",
        "            self.trainloader = dataloader\n",
        "\n",
        "        if self.trainloader is None:\n",
        "            raise ValueError(\"Needs to set a train data loader first\")\n",
        "\n",
        "        # learning algorithm parameters\n",
        "        self.loss_name = kwargs.get('loss_name', self.loss_name)\n",
        "        self.criterion = name2loss(self.loss_name)\n",
        "        if WeightDecay:\n",
        "            optimizer = name2optim(kwargs.get('optimizer_name', 'SGD'))(self.net.parameters(), lr=lr, weight_decay=0.05)\n",
        "        else:\n",
        "            optimizer = name2optim(kwargs.get('optimizer_name', 'SGD'))(self.net.parameters(), lr=lr)\n",
        "\n",
        "        if kwargs.get('lr_decay', False):\n",
        "            if isinstance(kwargs['lr_decay'], bool):\n",
        "                decay_rate = 0.95\n",
        "            else:\n",
        "                decay_rate = kwargs['lr_decay']\n",
        "            lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, decay_rate)\n",
        "\n",
        "        calc_loss_grads = partial(loss_grads, loss_name=self.loss_name, n_classes=self.n_classes)\n",
        "\n",
        "        mtl = kwargs.get('mtl', False)\n",
        "\n",
        "        if kwargs.get('writer_name', ''):\n",
        "            self.writer_name = (datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\") + '_' +\n",
        "                                kwargs.get('writer_name', ''))\n",
        "        else:\n",
        "            self.writer_name = (datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\") +\n",
        "                                \"_sw\" * self.net.shared_weights +\n",
        "                                \"_chl\" * ch_learning +\n",
        "                                \"_mtl\" * mtl +\n",
        "                                (self.net.multi_decoders * \"_multi_d\"))\n",
        "\n",
        "        #writer = SummaryWriter(f\"runs/{self.writer_name}\")\n",
        "        model_name = (\n",
        "                f\"model_{self.dataset_name}_{self.writer_name}\"\n",
        "        )\n",
        "\n",
        "        if kwargs.get('resume_training', False):\n",
        "            # loaded_model_path = os.path.join('Saved_Models', kwargs['saved_model_path'])\n",
        "            #\n",
        "            # checkpoint = torch.load(loaded_model_path)\n",
        "            checkpoint = self.load_model(kwargs['saved_model_path'], get_ckpt_data=True)\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            if kwargs.get('lr_decay', False):\n",
        "                lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "            last_epoch = checkpoint['epoch']\n",
        "            last_loss = checkpoint['loss']\n",
        "\n",
        "            self.logger.info(f\"resuming training, starting from epoch {last_epoch} and loss {last_loss:.3f}\")\n",
        "        else:\n",
        "            last_epoch = 0\n",
        "\n",
        "        # if EvalIntermediate:\n",
        "        #     with torch.inference_mode():\n",
        "        #         self._write_metrics(epoch=last_epoch, writer=writer, **kwargs)\n",
        "\n",
        "        self.net.train()\n",
        "        if Analyze:\n",
        "            self._analyze(epoch=0)\n",
        "\n",
        "        epoch_loss = torch.Tensor([0])\n",
        "        for epoch in range(last_epoch, epochs):  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for i, data in enumerate(self.trainloader):\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if train_all_tasks:\n",
        "                    tasks_loss = []\n",
        "                    for task_i in range(self.n_tasks):\n",
        "                        tasks = (F.one_hot(torch.ones(data[0].shape[0]).long() * task_i, self.n_tasks) * 1.0).to(\n",
        "                            self.device)\n",
        "                        inputs, outputs, labels, tasks = self._inner_op(data, mtl=mtl, tasks=tasks, train=True)\n",
        "\n",
        "                        tasks_loss.append(self.criterion(outputs, labels))\n",
        "\n",
        "                    loss = torch.mean(torch.stack(tasks_loss))\n",
        "                else:\n",
        "                    inputs, outputs, labels, tasks = self._inner_op(data, mtl=mtl, train=True)\n",
        "\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # backward + update\n",
        "                if ch_learning:\n",
        "                    # calculate the gradients of the loss with respect to the network outputs\n",
        "                    # Then propagate it using the TD network and apply the Counter Hebbian learning rule\n",
        "                    d_l_d_outputs = calc_loss_grads(outputs, labels)\n",
        "                    self.net.counter_hebbian_back_prop(d_l_d_outputs)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print(f\"iteration {i + 1} running loss: {running_loss / (i+1):.3f}\")\n",
        "                    #self.logger.info(f\"iteration {i + 1} running loss: {running_loss / (i+1):.3f}\")\n",
        "\n",
        "            if Analyze and (epoch + 1) % AnalyzeFrequency == 0:\n",
        "                self._analyze(epoch=epoch + 1)\n",
        "\n",
        "            if kwargs.get('lr_decay', False):\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(self.trainloader)\n",
        "            print(f\"Epoch {epoch + 1} running loss: {epoch_loss:.3f}\")\n",
        "            print(f\"{self.loss_name} loss/running\", epoch_loss, epoch + 1)\n",
        "            # self.logger.info(f\"Epoch {epoch + 1} running loss: {epoch_loss:.3f}\")\n",
        "            # writer.add_scalar(f\"{self.loss_name} loss/running\", epoch_loss, epoch + 1)\n",
        "\n",
        "            if SaveIntermediateModels and epoch % SavedModelsFrequency == 0:\n",
        "                if SaveForResuming:\n",
        "                    checkpoint_dict = {\n",
        "                        'epoch': epoch + 1,\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'loss': epoch_loss,\n",
        "                    }\n",
        "                    if kwargs.get('lr_decay', False):\n",
        "                        checkpoint_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
        "\n",
        "                    self.save_model(f\"{model_name}_epoch_{epoch+1}.tar\", checkpoint_dict=checkpoint_dict)\n",
        "\n",
        "                else:\n",
        "                    self.save_model(f\"{model_name}_epoch_{epoch+1}.pth\")\n",
        "\n",
        "            if EvalIntermediate and (epoch + 1) % EvalInterFrequency == 0:\n",
        "                # with torch.inference_mode():\n",
        "                #     self._write_metrics(epoch + 1, writer=writer, **kwargs)\n",
        "                self.net.train()\n",
        "\n",
        "        self.net.eval()\n",
        "        #self.logger.info('Finished Training')\n",
        "\n",
        "        # Evaluate the learned model\n",
        "        # with torch.inference_mode():\n",
        "        #     self._write_metrics(epoch=epochs, writer=writer, **kwargs)\n",
        "\n",
        "        if SaveLastModel:\n",
        "            self.save_model(f\"{model_name}.pth\")\n",
        "            if SaveForResuming:\n",
        "                checkpoint_dict = {\n",
        "                    'epoch': last_epoch + epochs,\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': epoch_loss,\n",
        "                }\n",
        "                if kwargs.get('lr_decay', False):\n",
        "                    checkpoint_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
        "\n",
        "                self.save_model(f\"{model_name}tar\", checkpoint_dict=checkpoint_dict)\n",
        "        print(\"model is saved\")\n",
        "        #self.logger.info(\"model is saved\")\n",
        "\n",
        "    def test(self, testloader=None, data_name=\"Test\", test_all_tasks=False, **kwargs):\n",
        "        \"\"\"\n",
        "        evaluate the model's network\n",
        "\n",
        "        Args:\n",
        "            testloader (DataLoader): the data that will be evaluated. if None, will use self.testloader.\n",
        "            data_name (str): the name of the data which will be used for the logger.\n",
        "\n",
        "        Returns:\n",
        "            scores (dict[str, float]): the scores on the data. keys: loss, metric (for example f1)\n",
        "        \"\"\"\n",
        "        if testloader is None and self.testloader is not None:\n",
        "            testloader = self.testloader\n",
        "\n",
        "        if testloader is None:\n",
        "            raise ValueError(\"Needs to set a test data loader first\")\n",
        "\n",
        "        self.metric_name = kwargs.get('metric_name', self.metric_name)\n",
        "        self.metric = name2metric(self.metric_name)\n",
        "\n",
        "        mtl = kwargs.get('mtl', False)\n",
        "\n",
        "        self.net.eval()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        y_gt = []\n",
        "        y_preds = []\n",
        "\n",
        "        if test_all_tasks:\n",
        "            for data in testloader:\n",
        "                for task_i in range(self.n_tasks):\n",
        "                    tasks = (F.one_hot(torch.ones(data[0].shape[0]).long()*task_i, self.n_tasks) * 1.0).to(self.device)\n",
        "                    inputs, outputs, labels, tasks = self._inner_op(data, mtl=mtl, tasks=tasks, train=False)\n",
        "\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    predictions = self.net.predict(outputs)\n",
        "                    y_gt.append(labels.detach().cpu())\n",
        "                    y_preds.append(predictions.detach().cpu())\n",
        "        else:\n",
        "            for data in testloader:\n",
        "                inputs, outputs, labels, tasks = self._inner_op(data, mtl=mtl, train=False)\n",
        "\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                predictions = self.net.predict(outputs)\n",
        "                y_gt.append(labels.detach().cpu())\n",
        "                y_preds.append(predictions.detach().cpu())\n",
        "\n",
        "        if test_all_tasks:\n",
        "            total_samples = len(testloader) * self.n_tasks\n",
        "        else:\n",
        "            total_samples = len(testloader)\n",
        "\n",
        "        final_loss = running_loss / total_samples\n",
        "\n",
        "        if len(y_gt[0].shape) == 1:\n",
        "            y_gt = torch.cat([x for x in y_gt])\n",
        "        elif len(y_gt[0].shape) == 2 and y_gt[0].shape[1] == 1:\n",
        "            y_gt = torch.cat([x[:, 0] for x in y_gt])\n",
        "        else:\n",
        "            y_gt = torch.argmax(torch.cat([x for x in y_gt]), axis=1)\n",
        "\n",
        "        y_preds = torch.cat([x for x in y_preds])\n",
        "\n",
        "        metric_score = self.metric(y_gt.numpy(), y_preds.numpy())\n",
        "        self.logger.info(f\"{data_name} Loss: {final_loss:.4f}\")\n",
        "        self.logger.info(f\"{data_name} {kwargs.get('metric', self.metric_name)}: {metric_score:.5f}\")\n",
        "\n",
        "        results_dict = {f\"{self.loss_name} loss\": final_loss, kwargs.get('metric', self.metric_name): metric_score}\n",
        "\n",
        "        return results_dict\n",
        "\n",
        "    def _inner_op(self, batch_data, mtl=False, tasks=None, train=True):\n",
        "        inputs = batch_data[0].to(self.device)\n",
        "        if self.benchmark == \"Multi MNIST\":\n",
        "            labels = torch.stack([batch_data[1], batch_data[2]], -1).to(self.device)\n",
        "        else:\n",
        "            labels = batch_data[1].to(self.device)\n",
        "\n",
        "        # forward\n",
        "        if mtl:\n",
        "            tasks, labels = self._get_tasks_and_labels(all_labels=labels, tasks=tasks)\n",
        "            outputs = self.net(inputs, task=tasks)\n",
        "        else:\n",
        "            tasks = None\n",
        "            outputs = self.net(inputs)\n",
        "\n",
        "        if self.loss_name == 'BCE':\n",
        "            if len(labels.shape) <= 1 and (not self.net.multi_decoders):\n",
        "                if self.n_classes == 1:\n",
        "                    labels = labels.unsqueeze(-1)\n",
        "                else:\n",
        "                    labels = F.one_hot(labels, self.n_classes)\n",
        "            labels = labels.float()\n",
        "\n",
        "        return inputs, outputs, labels, tasks\n",
        "\n",
        "    def _get_tasks_and_labels(self, all_labels, tasks=None):\n",
        "        if self.task == \"left/right of\":\n",
        "            # if task is not specified, generate a random task\n",
        "            if tasks is None:\n",
        "                # Chose randomly whether to the left or right to an object\n",
        "                tasks = (F.one_hot(torch.randint(0, 2, [all_labels.shape[0]]), 2) * 1.0).to(self.device)\n",
        "\n",
        "                # Given all labels that appear in the input ordered according to their position\n",
        "                # Extract all the indices of all labels that appear in the input\n",
        "                if len(all_labels.shape) > 2:\n",
        "                    # If all_labels is a one-hot vector, get the class indices\n",
        "                    all_labels = torch.argmax(all_labels, -1)\n",
        "                # Pick a random label.\n",
        "                # If the task is to predict the object to the left, don't pick the first,\n",
        "                # If it is to the right, don't pick the last\n",
        "                chosen_locations = (torch.randint(\n",
        "                    0, all_labels.shape[-1] - 1,\n",
        "                    [all_labels.shape[0]]).to(self.device) + tasks[:, 0].type(torch.int64)\n",
        "                                    ).unsqueeze(1)\n",
        "                chosen_labels = torch.gather(all_labels, 1, chosen_locations)[:, 0]\n",
        "\n",
        "                # Concatenate the chosen label to the task\n",
        "                tasks = torch.cat([tasks, F.one_hot(chosen_labels, self.n_classes)], -1)\n",
        "\n",
        "            # calculate the ground truth labels of that task\n",
        "            # Find the instance requested in the task\n",
        "            instance_location = (tasks[:, 2:].argmax(1).unsqueeze(1) == all_labels).nonzero()[:, 1]\n",
        "\n",
        "            # Find the target instance location - to the left/right of the instance mentioned in the task\n",
        "            target_instance_location = instance_location - tasks[:, 0] + tasks[:, 1]\n",
        "\n",
        "            # Get the label at that location\n",
        "            task_labels = torch.gather(all_labels, 1, target_instance_location.type(torch.int64).unsqueeze(1))[:, 0]\n",
        "        elif self.task == \"left/right\":\n",
        "            # if task is not specified, generate a random task\n",
        "            if tasks is None:\n",
        "                # Chose randomly whether to the left or right to an object\n",
        "                tasks = (F.one_hot(torch.randint(0, 2, [all_labels.shape[0]]), 2) * 1.0).to(self.device)\n",
        "\n",
        "            task_labels = torch.gather(all_labels, 1, tasks.argmax(1).unsqueeze(1))[:, 0]\n",
        "        elif self.task == \"binary attribute\":\n",
        "            if tasks is None:\n",
        "                # Chose randomly whether to the left or right to an object\n",
        "                tasks = (F.one_hot(torch.randint(0, self.n_tasks, [all_labels.shape[0]]), self.n_tasks) * 1.0).to(self.device)\n",
        "\n",
        "            task_labels = torch.gather(all_labels, 1, tasks.argmax(1).unsqueeze(1))[:, 0]\n",
        "        else:\n",
        "            raise ValueError(f\"The following task: {self.task} is not supported for multi-task learning\")\n",
        "\n",
        "        return tasks, task_labels\n",
        "\n",
        "    def save_model(self, model_name, checkpoint_dict=None):\n",
        "        \"\"\"\n",
        "        save the model's parameters\n",
        "\n",
        "        Args:\n",
        "            model_name (str): the model will be saved at `./Saved_Models/model_name`\n",
        "            checkpoint_dict (dict): a dictionary contain relevant information for resuming training.\n",
        "        \"\"\"\n",
        "        if os.path.exists(SavedModelDirPath) and SavedModelDirPath != '':\n",
        "            dir_path = SavedModelDirPath\n",
        "        else:\n",
        "            Path('Saved_Models').mkdir(parents=True, exist_ok=True)\n",
        "            dir_path = 'Saved_Models'\n",
        "\n",
        "        if checkpoint_dict is None:\n",
        "            torch.save(self.net.state_dict(), os.path.join(dir_path, model_name))\n",
        "        else:\n",
        "            checkpoint_dict.update({'model_state_dict': self.net.state_dict()})\n",
        "\n",
        "            torch.save(checkpoint_dict, os.path.join(dir_path, model_name))\n",
        "\n",
        "    def load_model(self, model_name, get_ckpt_data=False):\n",
        "        \"\"\"\n",
        "        load model's parameters from a file, and move them to the device\n",
        "\n",
        "        Args:\n",
        "            model_name (str): the model will be loaded from `./Saved_Models/model_name`\n",
        "        \"\"\"\n",
        "        if os.path.exists(SavedModelDirPath) and SavedModelDirPath != '':\n",
        "            dir_path = SavedModelDirPath\n",
        "        else:\n",
        "            Path('Saved_Models').mkdir(parents=True, exist_ok=True)\n",
        "            dir_path = 'Saved_Models'\n",
        "\n",
        "        model_path = os.path.join(dir_path, model_name)\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise ValueError(f\"try to load {model_name}, but it was not found\")\n",
        "\n",
        "        loaded_data = torch.load(model_path)\n",
        "\n",
        "        if '.tar' in model_name:\n",
        "            self.net.load_state_dict(loaded_data['model_state_dict'])\n",
        "        else:\n",
        "            self.net.load_state_dict(loaded_data)\n",
        "\n",
        "        self.net.to(self.device)\n",
        "        self.net.eval()\n",
        "\n",
        "        if get_ckpt_data:\n",
        "            del loaded_data['model_state_dict']\n",
        "            return loaded_data\n",
        "\n",
        "    def _write_metrics(self, epoch, writer, **kwargs):\n",
        "        \"\"\"\n",
        "        write the metrics for tensorboard\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Evaluating epoch {epoch}:\")\n",
        "        res = self.test(self.trainloader, data_name='Train', **kwargs)\n",
        "        for k, v in res.items():\n",
        "            writer.add_scalar(k + '/train', v, epoch)\n",
        "\n",
        "        if WriteCSV:\n",
        "            csv_out = [self.dataset_name, self.writer_name, str(epoch)] + [res[k] for k in sorted(res.keys())]\n",
        "\n",
        "        if self.testloader is not None:\n",
        "            res = self.test(**kwargs)\n",
        "            for k, v in res.items():\n",
        "                writer.add_scalar(k + '/test', v, epoch)\n",
        "\n",
        "            if WriteCSV:\n",
        "                csv_out.extend([res[k] for k in sorted(res.keys())])\n",
        "\n",
        "        if WriteCSV:\n",
        "            csv_writer.writerow(csv_out)\n",
        "\n",
        "    def set_dataloaders(self, train_loader, test_loader, dataset_name):\n",
        "        \"\"\"\n",
        "        set the default train data\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): a train data for the model to be trained on\n",
        "            test_loader (DataLoader): a test data for the model to be evaluated on\n",
        "            dataset_name (str): the name of the dataset\n",
        "        \"\"\"\n",
        "        self.trainloader = train_loader\n",
        "        self.testloader = test_loader\n",
        "        self.dataset_name = dataset_name"
      ],
      "metadata": {
        "id": "uMMbLukrBHrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練の実施"
      ],
      "metadata": {
        "id": "b18K7cm6Buf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModelCTL(net=net_, benchmark='multi mnist')\n",
        "model.train(dataloader=train_dataloader, epochs=2)"
      ],
      "metadata": {
        "id": "pczq9bUxBoZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練データの視覚化"
      ],
      "metadata": {
        "id": "j119mXjXDksU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = next(iter(train_dataloader))\n",
        "#data = next(iter(train_dataloader_))\n",
        "img = data[0][0].detach().numpy().transpose(1,2,0)\n",
        "label = data[1][0]\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.title(f'{label}')\n",
        "plt.axis('off')\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "print(label, data[2:])"
      ],
      "metadata": {
        "id": "c16HoBA_BxLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02c72B2gB3Q2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}