---
title: ディープラーニング用語集
layout: home
---


# 用語集

* **GAN generative adversarial networks**: 敵対的生成ネットワーク。 画像，言語を生成するニューラルネットワークモデルの一つ。 GAN 内部には，生成器と識別器と 2 つのネットワークが存在し， 互いに競合関係の中で学習が行われる。
* **GPU Graphical Processing Units**: グラフィック処理ユニット。コンピュータの中央演算装置 CPU に対してグラフック処理に特化した演算装置のこと。 PC 上でゲームをするために開発されたが，ディープラーニングや暗号通貨の計算でもゲームのグラフィック処理と同様の並列計算が行われる。 このため GPU を使ってディープラーニングモデルの学習を高速化することが一般的となっている。
* **LSTM 長-短期記憶 long short-term memory**: リカレントニューラルネットワークモデルの一つで，ゲートを内部に持つ。 注意機構を実装したトランスフォーマー以前は LSTM が支配的な地位を占めていた。
* **R-CNN** (what/where or ventral/dorsal pathway: 領域切り出しと， 切り出された領域に対して認識を行うモデル。 このモデルの提案により， ほぼ実時間で領域切り出しと物体認識が可能になった。
* **SOTA**: State of the arts 現時点での最高性能の意。
* **U-net**: 画像中の領域切り出しにおいて，高精度を得ることができるモデル。X 線画像から，関心領域 (腫瘍や変性) を切り出すことができる。
* **one convolution**: 畳み込みニューラルネットワークにおいて，カーネル幅が 1 である畳み込みを指す。特徴方向には畳み込みが行われることに注意。
* **おばあちゃん細胞仮説 grandmother hypothesis**: 脳の情報表現について，分散か局在かの論争の中で提唱された仮説。 自身のおばあちゃんを見たときにだけ応答を示す神経細胞が存在するという考え方を指す。 ニューラルネットワークとの関連では，疎性表現，あるいはワンホット表現と関連する。
* **アレックスネット AlexNet**: 2012 年の ImageNet コンテンストにおいて，ディープラーニングアルゴリズムが初めて大規模画像認識で 1 位を獲得したときのディープラーニングモデルのこと。 第 1 著者のファーストネームから アレックスネットと呼ばれる。畳み込みニューラルネットワークとデータ拡張を特徴とする。 GPU が本格的に使用されるきっかけともなった。
* **エンドツーエンド**: 複雑で職人芸的な前処理，や後処理を必要としないで， （ほぼ）生データから一気に結論までを実行する処理や手順のこと。 エンドツーエンドを可能としたことがディープラーニングの特徴の一つである。 このエンドツーエンドにより， より高次で複雑な仕事や処理への発展，ビジネス展開が可能となる。
* **カーネル kernel**: 畳み込み演算において，核 (kernel) となる関数のこと。畳み込みニューラルネットワークに置いては推定すべき結合係数である。
* **シグモイド関数 sigmoid functions**: 直訳すれば S 字状の曲線の意味である。ニューラルネットワークの活性化関数としても用いられてきた。 近年では勾配消失問題の回避のため別の活性化関数が採用されることが多い。 ただし，注意機構や LSTM のゲートの開閉にはシグモイド関数が用いられている。
* **スキップ結合 skip connections**: 階層型ニューラルネットワークにおいて，層をまたぐ結合のこと。
* **ストライド stride**: 畳み込みニューラルネットワークにおいて，カーネルの移動幅のこと
* **ソフトマックス関数 softmax function**: 多値分類を行う最終層において，候補選択の際に用いられる出力関数の一つ。対比学習や注意においても重要な役割を果たす。
* **ダイレーション dilation**: 畳み込み演算において，データのサンプル点の間隔を表す。 dilation=1 であれば通常の畳み込みである。dilation=2 であれば， 一画素飛ばしで畳み込みを行うことになる。
* **ディープニューラルネットワーク deep neural networks**: または ディープラーニング，深層学習，とも呼ばれる。 1980 年代の第二次ニューロブームまでは 3 層のニューラルネットワークが主流であった。 今世紀に入って，多層のニューラルネットワークを構築するための要因が整い多層化した。 その結果ニューラルネットワークの大規模化，性能の向上が可能となり，第 3 次ブームとなった。
* **データセット（訓練，検証，テスト） dataset**: ニューラルネットワークに限らず，機械学習におけるモデルの学習，パラメータのチューニングにもちいるデータのこと。 パラメータの設定に用いるデータを訓練データ。 妥当性の検証に用いるデータを検証データと呼ぶ。 モデルの性能評価に用いるデータをテストデータと呼ぶ。 公開されているデータセットにおいては， 訓練データとテストデータに分けられていることが多い。 検証データは，訓練データを分割して用いることがある。
* **データ拡張 data augmentation**: 限られたデータを部分的に変更して， データを増やす手法のこと。 画像や音声データでは， 拡大， 縮小， 回転， 反転， 色調変換， 周波数変調などを行う場合がこれに該当する。
* **ドロップアウト dropout**: ニューラルネットワークにおける般化性能向上手法の一つ。 ニューロン間の結合をランダムに 0 にすることで実現される。 ドロップアウトにより，入出力関係が確率的に変動するので， 決定論的な推論をするよりも，確率的な変動を学習することになる。
* **ニューラルネットワーク neural networks**: 神経細胞の機能を模したモデル一般を指す。 実際の神経系のニューラルネットワークを 生物学的ニューラルネットワーク (BNN: biological neural networks) と呼ぶ。一方，BNN を簡略化，単純化，抽象化したモデルを人工ニューラルネットワーク （ANN: artifitial neural networks) と呼ぶ。実際のニューロンの抽象化の程度により，複数に分割される。最も単純な人工的ニューラルネットワークは，下位層ニューロンの信号を重み付けして足し合わせる $\sum_{i} w_{i} x_{i}$ ような場合も含まれる。
* **バイアス bias**: ニューラルネットワークにおける定数項のこと。バイアス項がない場合もある。
* **パディング padding**: 畳み込みニューラルネットワークにおいて， カーネルがデータ領域の外に出る場合に埋め草として用いる量あるいはその手法のこと。ゼロパディングであれば，埋め草として 0 を用いる，`same` パディングであれば連接データと同じ値を埋め草の値として用いることを表す。
* **パーセプトロン perceptron**: 第 1 次ニューラルネットワークブームの際にフランク・ローゼンブラットによって提案されたニューラルネットワークモデル。出力層と直下層との間の結合係数を調整するための学習機能を有する初期のモデルの一つ。
* **ファインチューニング fine tuning**: 詳細微調整とも訳される。 訓練済のモデルに対して， 解くべき課題に特化した追加学習を行う際の学習を指す。 しばしば全結合係数を再学習することを指す。 乱数を用いた初期値から学習するよりも学習時間が短縮される。再学習時に，全層のパラメータを再調整する。 最終層と最終直下層との間野結合係数のみを調整する転移学習の反対語。
* **プーリング pooling**: 畳み込み層の上位にあって，下位層の演算結果を併せて出力とする技法。手法により，最大値プーリングや平均値プーリングなどの違いがある。
* **メタ学習 meta learning**: 複数の課題や領域について学習を汎化させる試み 
* **リカレントニューラルネットワーク recurrent neural networks**: 時系列予測，自然言語，音声，制御で用いられるニューラルネットワークの一つ。 入力信号として， 外部入力に加えて， 前時刻の自己状態を入力とする。
* **ワン・アルゴリズム仮説 one algorithm hypothesis**: 脳内では，同一のアルゴリズムが用いられているという仮説。 [@1988Sur] は， 西洋イタチ(フェレット)の新生児に対して外側膝状体 (LGN) の視覚経路を視覚野につなぎ替える実験などを通して，考えられるようになった。
* **一撃学習, ゼロ撃学習, 少数学習 one/zero/few shot(s) learning**: 少数事例で学習を成立させる仕組み，または試みのこと。  
* **交差エントロピー cross entropy**: 
* **交差検証 (k-hold, leave-one-out) cross validation**:
* **勾配消失問題 gradient descent problem**: 多層ニューラルネットワークにおいて，上位層から誤差を逆伝播して学習させる際に，活性化関数の微分によってはその値が 0 に近づくことを指す。 勾配消失問題を解消することが， 多層化への障害となっていた。 現在では整流線形化関数を用いるなどして，この問題を回避する。畳み込みを用いる
* **半教師あり学習 semi-supervised learning**: 部分的に教師信号が与えられない場合の学習を言う。 正解データが与えられている場合でも，意図的に正解データを隠蔽して用いる場合もある。 こうすることによって，中間表現，潜在変数の表現を豊富にできる場合がある。 変分自己符号化器 VAE など半教師あり学習に該当する。
* **完全結合層 fully connected layers**: ニューラルネットワークに認識，識別を行う場合の最終層において，下位層からの情報全てが結合している層を指す。一方，畳み込み層は部分結合である。
* **対比予測符号化 contrastive predictive coding**: 自己回帰モデルを用いて潜在空間の予測を行う自己教師付きの表現を学習するモデル。 このモデルは将来のサンプルを予測するのに最大限役立つ情報を潜在空間に誘導する確率的な対比損失を用いる。
* **強化学習**: 教師信号として報酬をとり，報酬を最大化するモデルの総称。囲碁やテレビゲーム，ロボット制御，ドローン，自動運転などに応用されている。reinforcement learning
* **損失関数 (誤差関数, 目的関数) loss function, error function, objective function**: 学習目標として設定される量，あるいはその関数。損失， 誤差， 目標 と言い方に變動があるが， 同じ意味で用いられる場合もある。 特に区別するという注意が無い限り， 同じ概念を表すと考えて良い場合が多い。 ニューラルネットワークや機械学習においてモデルのパラメータを調整するときに用いられる，最適化 (最小,最大) するための関数。 loss function,  目的関数 objective function，誤差関数 error function と区別せずに用いられることも多い。 畳み込みとは，カーネルと呼ばれるパラメータの組を入力データについて掛け合わせて総和を計算したもの。
* **教師あり学習 supervised learning**: 入力データと教師データとが対になったデータを用いて，与えられた入力データを正しい教師データを出力するように学習する枠組のこと。
* **教師なし学習 unsupervised learning**: 入力データに対して， 正解データ， すなわち教師信号が与えれていない場合の学習を指す。 教師なし学習では， 入力データの統計的性質を学習することになる。
* **最終直下層 penultimate layers**: 転移学習においては， 最終直下層には豊富な情報が含まれていることから，転移学習では重視される。
* **最適化手法 optimization methods**: ニューラルネットワークや機械学習においてモデルのパラメータを最適化 (最小, 最大) するための関数。 loss function,  目的関数 objective function，誤差関数 error function と区別せずに用いられることも多い。
* **正則化 (L1, L2, エラスティック) regularization**:
* **残差ネット ResNet**: 2015 年の ImageNet コンテストで人間の性能を超えたことで話題となったモデル。スキップ結合， バッチ正則化， などを特徴とする
* **注意 attention**: 画像の注目部位，言語翻訳モデルに使われる機構。言語処理では，マルチヘッド注意という， 同時に複数の注意を持つトランスフォーマーが採用されている。
* **活性化関数 activation functions**: ニューラルネットワークにおいて，入力値を出力値へ変換するために用いられる。整流線形化関数 ReLU， ハイパータンジェント， シグモイドロジスティック関数などがある。 
* **畳み込みニューラルネットワーク**: 主に画像処理で用いられるディープラーニングの標準的なネットワーク。層ごとに， 畳み込み演算を行う。畳み込みとは，カーネルと呼ばれるパラメータの組を入力データについて掛け合わせて総和を計算したもの。
* **結合係数 weights, connections, or weight connections とも**。 ニューラルネットワークにおいて，ニューロン間を結びつける重みのこと。 通常，実数として扱われる。 複素数の場合もある。
* **蒸留 distillation**: より小さなモデルに知識を転移する転移学習に用いる手法のこと。 エッジ実装の際より小さく軽量のモデルが求められることが必要な技術である。
* **誤差逆伝播法 error back-propagation**: ニューラルネットワークに限らず，目的とする関数の最小値 (または最大値) を求めるために用いられる手法。 合成関数の微分を用いてパラメータを調整する。
* **責任割当問題 credit assignment problems**:
* **転移学習 transfer learning**: 学習済のモデルを別の課題に対して適用する再学習の試み。最終層と最終直下層との間の結合係数のみを学習させる場合を指す場合もある。 このときには ファインチューニングの反対語となる。 解くべき課題が類似していれば学習時間が短縮される。
* **領域分割 semantic, object, instance segmentations**: 画像の領域分割の手法。セマンティック，オブジェクト，インスタンス分離，


