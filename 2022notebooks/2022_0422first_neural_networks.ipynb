{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0422first_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05208395-1ccb-4d3a-a3ae-456260a89f15",
      "metadata": {
        "id": "05208395-1ccb-4d3a-a3ae-456260a89f15"
      },
      "outputs": [],
      "source": [
        "# HAD データの取得\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade xlrd\n",
        "    !pip install --upgrade pandas\n",
        "\n",
        "    # HAD サンプルデータをダウンロード\n",
        "    !wget 'https://files.au-1.osf.io/v1/resources/32cyp/providers/osfstorage/5fb340145502ac018d8c86ab/?zip=' -O had.zip\n",
        "    !unzip had.zip\n",
        "    #from google.colab import files\n",
        "    #files.upload()  # ご自身の PC からファイルをアップロードして下さい"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8de19d57-e873-44c6-9d87-a560f2baeafd",
      "metadata": {
        "id": "8de19d57-e873-44c6-9d87-a560f2baeafd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn.datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#X, y = sklearn.datasets.load_iris(return_X_y=True,as_frame=True)\n",
        "#_X, _y = X.to_numpy(), y.to_numpy()\n",
        "\n",
        "had_dir = '.'  if isColab else '/Users/asakawa/study/2022HAD'\n",
        "had_samples = pd.read_excel(os.path.join(had_dir, 'HAD_sample_data.xls'),sheet_name='iris')\n",
        "X = had_samples[['Sepal.L','Sepal.W', 'Petal.L', 'Petal.W']].to_numpy()\n",
        "# sepal:萼片, petal:花弁\n",
        "# `花は内側から外に向かって、雌しべ、雄しべ、花弁、萼片、そして場合によっては苞という順番で器官が並んでいます。`\n",
        "# `ですので、雄しべのすぐ外にある器官が花弁で、そのさらに外にある器官が萼片ということになります。`\n",
        "# https://jspp.org/hiroba/q_and_a/detail.html?id=1219 より\n",
        "y = had_samples[['Species']].to_numpy()\n",
        "_y = np.zeros((y.shape[0],3))\n",
        "for i, __y in enumerate(y):\n",
        "     _y[i,__y[0]-1] = 1\n",
        "        \n",
        "y = np.copy(_y)        \n",
        "#y.shape[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e448a228-dffa-4cfe-a7e1-f4d494349e0f",
      "metadata": {
        "id": "e448a228-dffa-4cfe-a7e1-f4d494349e0f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = torch.Tensor(X)\n",
        "y = torch.Tensor(y)\n",
        "lr = 0.01\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_inp, n_hid, n_out=1):\n",
        "        super().__init__()\n",
        "        self.n_inp = n_inp\n",
        "        self.n_hid = n_hid\n",
        "        self.n_out = n_out\n",
        "        self.hid_layer = nn.Linear(in_features =self.n_inp, \n",
        "                                   out_features=self.n_hid,\n",
        "                                   bias=True)\n",
        "        self.out_layer = nn.Linear(self.n_hid, self.n_out)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.hid_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.out_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "    \n",
        "mlp = MLP(n_inp=X.shape[1], n_hid=8, n_out=y.shape[1])\n",
        "#loss_f = nn.MSELoss()\n",
        "#loss_f = nn.CrossEntropyLoss()\n",
        "loss_f = nn.BCELoss()\n",
        "#optim_f = torch.optim.SGD(mlp.parameters(),lr=lr)\n",
        "optim_f = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
        "\n",
        "mlp.eval()\n",
        "_y = mlp(X)\n",
        "_pre_loss = loss_f(_y, y)\n",
        "print(f'訓練開始前の損失値:    {_pre_loss.item():.3f}')\n",
        "\n",
        "mlp.train()\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optim_f.zero_grad()\n",
        "    \n",
        "    _y = mlp(X)                  # モデルに処理させて出力を得る\n",
        "    loss = loss_f(_y, y)         # 損失値の計算\n",
        "\n",
        "    if epoch % (epochs>>2) == 0: # 途中結果の出力\n",
        "        print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n",
        "\n",
        "    loss.backward()              # 誤差逆伝播\n",
        "    optim_f.step()               # 重み更新。すなわち学習\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KeK1HVUsKv1h",
      "metadata": {
        "id": "KeK1HVUsKv1h"
      },
      "outputs": [],
      "source": [
        "mlp.eval()\n",
        "_y = mlp(X)\n",
        "for i,z in enumerate(_y):\n",
        "    print(f'{i:2d} {z.detach().numpy()}')     # 結果の出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b9eac7-3a61-49b2-a1ae-1b2892c27184",
      "metadata": {
        "id": "42b9eac7-3a61-49b2-a1ae-1b2892c27184"
      },
      "outputs": [],
      "source": [
        "import numpy as np                                # 必要となるライブラリの輸入\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "had_dir = '.'  if isColab else '/Users/asakawa/study/2022HAD'\n",
        "had_samples = pd.read_excel(os.path.join(had_dir, 'HAD_sample_data.xls'),sheet_name='iris')\n",
        "X = had_samples[['Sepal.L','Sepal.W', 'Petal.L', 'Petal.W']].to_numpy()\n",
        "y = had_samples[['Species']].to_numpy()\n",
        "_y = np.zeros((y.shape[0],3))\n",
        "for i, y1 in enumerate(y):\n",
        "     _y[i,y1[0]-1] = 1\n",
        "\n",
        "y = np.copy(_y)        \n",
        "\n",
        "lr, N_hid = 0.002, 8                              # lr: 学習率, N_hid: 中間層数\n",
        "Wh = np.random.random((X.shape[1], N_hid)) - 1/2  # 入力層から中間層への結合係数の初期化\n",
        "Wo = np.random.random((N_hid, y.shape[1])) - 1/2  # 中間層から出力層への結合係数の初期化\n",
        "epochs = 500\n",
        "for t in range(epochs):                           # 繰り返し\n",
        "    H  = np.tanh(np.dot(X, Wh))                   # 入力層から中間層への計算。ハイパータンジェント関数\n",
        "    _y = 1/(1. + np.exp(-(np.dot(H, Wo))))        # 中間層から出力層への計算。シグモイド関数\n",
        "    Dy = (y - _y) * (_y * (1. - _y))              # 誤差の微分\n",
        "    DH = Dy.dot(Wo.T) * (1. - H ** 2)             # 誤差逆伝播\n",
        "    Wo += lr * H.T @ Dy\n",
        "    Wh += lr * X.T @ DH                           # 中間層から入力層への重み更新\n",
        "    if t % (epochs>>2) == 0:\n",
        "        print(np.array((y-_y)**2).mean())\n",
        "\n",
        "for i,z in enumerate(_y):\n",
        "    print(f'{i:2d} {z}')                          # 結果の出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VyxbhNgRae7C",
      "metadata": {
        "id": "VyxbhNgRae7C"
      },
      "source": [
        "#### 積分発火モデル\n",
        "\n",
        "ニューロンが振動数符号化法のみを利用している --- すなわちニューラルネットワークにおけるすべての情報はニューロンの発火頻度によって伝達される --- ことを仮定する。\n",
        "すなわち，このニューロンに到達する信号を単位時間あたりで等しく貢献するものと考える。\n",
        "このような記述の仕方は integrate--and--fire (積分発火) モデルと呼ばれる。\n",
        "\n",
        "一つのニューロンの振る舞いは，n 個のニューロンから入力を受け取って出力を計算する多入力，1 出力の情報処理素子である (下図 1)。\n",
        "\n",
        "$n$ 個の入力信号を $x_1, x_2, \\cdots, x_n$ とし、$i$ 番目の軸索に信号が与えられたとき、この信号 1 単位によって変化する膜電位の量をシナプス荷重(または結合荷重, 結合強度とも呼ばれる) といい $w_i$ と表記する。\n",
        "抑制性のシナプス結合については $w_i < 0$, 興奮性の結合については $w_i > 0$ である。\n",
        "このニューロンのしきい値を $h$, 膜電位の変化を $u$, 出力信号を $z$ とする。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/formal_neuron.svg\"><br/>\n",
        "<!-- <img src=\"figures/formal_neuron.svg\"><br/> -->\n",
        "図 1. ニューロンの模式図\n",
        "</center>\n",
        "\n",
        "出力信号 $z$ は次式で表される:\n",
        "$$\n",
        "z=f(\\mu)=f\\left(\\sum_{i=1}^{n}w_{i}x_{i}-h\\right).\\tag{1}\n",
        "$$\n",
        "$f(\\mu)$ は出力関数であり、$0\\le f(\\mu)\\le1$ の連続量を許す場合や、0 または 1 の値しかとらない場合などがある。\n",
        "連想記憶などを扱うときなどは $-1\\le f(\\mu)\\le 1$ とする場合もある。\n",
        "\n",
        "- 上図で，$f$ がなければ，$z=\\sum wx - h$ となり，回帰と同じである。\n",
        "- すなわちニューラルネットワークのもっとも簡単な形は，統計学の用語では，**回帰 regression** である。\n",
        "- 統計学においては，回帰，この場合，線形回帰になる，にかかわる変数 $x_{i}$ \n",
        "- 回帰の中を複雑にしていくことで，データに適合させようとする努力が，データサイエンスであり，機械学習であり，ニューラルネットワークであると言える。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yW5FjWxvaqVT",
      "metadata": {
        "id": "yW5FjWxvaqVT"
      },
      "source": [
        "## マッカロック・ピッツの形式ニューロン\n",
        "\n",
        "信号入力の荷重和 \n",
        "$$\n",
        "\\mu = \\sum_{i=1}^{n}w_{i}x_{i},\\tag{2}\n",
        "$$\n",
        "に対して、出力 $z$ は $u$ がしきい値 $h$ を越えたときに $1$, そうでなければ 0 を出力するモデルのことをマッカロックとピッツの形式ニューロン (formal neuron) と呼ぶ。\n",
        "マッカロック・ピッツの形式ニューロンは神経細胞の振る舞いを記述するもっとも古く、単純な神経細胞のモデルであるが、現在でも用いられることがある。\n",
        "\n",
        "$$\n",
        "z = \\left\\{ \\begin{array}{ll}\n",
        " 1, & \\mbox{$u > 0$ のとき} \\\\\n",
        " 0, & \\mbox{それ以外}\n",
        " \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "とすればマッカロック・ピッツのモデルは次式で表すことができる\n",
        "\n",
        "$$\n",
        "z=\\mathbb{1}\\left(\\sum_{i=1}^nw_ix_i -h\\right)\\tag{3}\n",
        "$$\n",
        "\n",
        "式中の $\\mathbb{1}$ は数字ではなく 上式で表される関数の意味である。\n",
        "このモデルは、単一ニューロンのモデルとしてではなく、ひとまとまりのニューロン群の動作を示すモデルとしても用いることがある。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pkYkUZ1ta2CR",
      "metadata": {
        "id": "pkYkUZ1ta2CR"
      },
      "source": [
        "### 出力が連続関数の場合\n",
        "\n",
        "時間 $t$ における入力信号は $x_i(t)$ は $i$ 番目のシナプスの興奮伝達の時間 $t$ 付近での平均ととらえることができる。\n",
        "すると、最高頻度の出力を 1, 最低(興奮無し)を 0 と規格化できると考えて $0\\le f(\\mu) \\le1$ とする。\n",
        "入出力関係は $f$ を用いて\n",
        "\n",
        "$$\n",
        "z = f(\\mu) = f\\left(\\sum w_ix_i(t)-h\\right)\\tag{4}\n",
        "$$\n",
        "\n",
        "のように表現される。\n",
        "このモデルは、ニューロン集団の平均活動率ととらえることもできる。\n",
        "\n",
        "良く用いられる出力関数 $f$ の形としては、$\\mu = \\sum w_ix_i -h$ として、\n",
        "\n",
        "$$\n",
        "f(\\mu) = \\frac{1}{1+e^{-\\mu}},\\tag{5}\n",
        "$$\n",
        "\n",
        "や\n",
        "\n",
        "$$\n",
        "f(\\mu) = \\tanh(\\mu)\\tag{6}, \n",
        "$$\n",
        "\n",
        "ただし $\\mu = \\sum w_{i}x_{i} -h$ などが使われることが多い。\n",
        "(5) 式および (6) 式は、入力信号の重みつき荷重和 $\\mu$ としてニューロンの活動が定まることを示している。\n",
        "後述するバックプロパゲーション則で必要となるので、$\\mu$ の微小な変化がニューロンの活動どのような影響を与えるか調べるために (5) 式 および (6) 式 を $\\mu$ で微分することを考える。\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{1+e^{-x}},\n",
        "$$\n",
        "\n",
        "を $x$ について微分すると\n",
        "\n",
        "$$\n",
        "\\frac{df}{dx} = f(x)\\left(1- f(x)\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(x) = \\tanh{x} = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
        "$$\n",
        "\n",
        "を $x$ について微分すると\n",
        "\n",
        "$$\n",
        "\\frac{df}{dx} = \n",
        "1 - \\tanh^2x = 1 - \\left(f(x)\\right)^2\n",
        "$$\n",
        "\n",
        "$\\tanh$ は双曲線関数である。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n_yfcUpCa9cu",
      "metadata": {
        "id": "n_yfcUpCa9cu"
      },
      "source": [
        "以降では表記を簡単にするために線形数学の表記、すなわちベクトルと行列による\n",
        "表記方法を導入する。$n$ 個の入力信号の組 $\\left(x_1, x_2, \\cdots, x_n\\right)$ をまと\n",
        "めて $\\mathbf{x}$ のようにボールド体で表す。本章では一貫してベクトル表記には小\n",
        "文字のボールド体を、行列には大文字のボールド体を用いることにする。例えば \n",
        "$n$ 個の入力信号の組 $(x_1, x_2, \\cdots, x_n)=\\mathbf{x}$ に対して、同数の結合\n",
        "荷重 $\\left(w_1, w_2, \\cdots, w_n\\right)=\\mathbf{w}$ が存在するので、加算記号 $\\sum$ を使っ\n",
        "て表現していた任意のニューロンへの全入力 $\\mu=\\sum w_{i}x_{i}$ はベクトルの内\n",
        "積を用いて $\\mu=(\\mathbf{w}\\cdot\\mathbf{x})$ と表現される。\n",
        "\n",
        "なお、横一行のベクトルを行ベクトル、縦一列のベクトルを列ベクトルと呼ぶことがある。\n",
        "ここでは行ベクトルと混乱しないように、必要に応じて列ベクトルを表現する際には $\\{x_1, x_2, \\cdots, x_n\\}^{\\top}=\\mathbf{x}$ とベクトルの肩に $\\top$ を使って表現することもある。\n",
        "\n",
        "そして、これらをベクトル表現や行列表現で表せば、表記も簡単になり、行列の演算法則を活用することもできる。\n",
        "そのため、ニューラルネットワークに関する文献でも行列表現が用いられることが多い。\n",
        "\n",
        "図のような単純な 2  層の回路を例に説明する。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/matrix-notation.svg\"><br/>\n",
        "<!-- <img src=\"figures/matrix-notation.svg\"><br/> -->\n",
        "ネットワークの行列表現\n",
        "</center>\n",
        "\n",
        "わかりやすいように図と対応させながら、対応する行列表現とシグマ記号による表記を併記するので、よく理解した上で先に読み進んでいただきたい。\n",
        "なお、どうしても行列表現にはなじめないという読者は、行列表現の箇所だけをとばして読んでもある程度はわかるよう記述するつもりである。\n",
        "\n",
        "それでは、まず図 4 のような単純な 2 層のネットワークを例に説明しよう。\n",
        "図には、３つの入力素子 (ユニットと呼ばれることもある) \n",
        "と２つの出力素子の活性値（ニューロンの膜電位に相当する）\n",
        "\n",
        "$x_{1}, x_{2}, x_{3}$ と $y_{1}, y_{2}$，および入力素子と出力素子の結合強度を表す $w_{11}, w_{12}, \\cdots, w_{32}$ が示されている。\n",
        "これらの記号をベクトル $\\mathbf{x}$, $\\mathbf{y}$ と行列 $\\mathbf{w}$ を使って表すと $\\mathbf{y}=\\mathbf{Wx}$ となる。\n",
        "\n",
        "図\\ref{fig:matrix-notation.eps}の場合、ベクトルと行列の各要素を書き下せば、\n",
        "$$\n",
        "\\left(\\begin{array}{l}y_{1}\\\\\n",
        "y_2\\\\\n",
        "\\end{array}\\right)\n",
        "=\\left(\n",
        "\\begin{array}{lll}\n",
        "w_{11}&w_{12}&w_{13}\\\\\n",
        "w_{21}&w_{22}&w_{23}\n",
        "\\end{array}\n",
        "\\right)\n",
        "\\left(\n",
        "\\begin{array}{l}\n",
        "x_1\\\\\n",
        "x_2\\\\\n",
        "x_3\\\\\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "のようになる。　　\n",
        "\n",
        "---\n",
        "\n",
        "行列の積は、左側の行列の $i$ 行目の各要素と右側の行列（ベクトルは１列の行列でもある）の $i$ 列目の各要素とを掛け合わせて合計することなので、以下のような、加算記号を用いた表記と同じである。\n",
        "\n",
        "$$\n",
        "\\begin{array}{lllll}\n",
        "y_1 &=& w_{11}x_1 + w_{12}x_2 + w_{13}x_3 &=&\\sum_i w_{1i} x_i\\\\\n",
        "y_2 &=& w_{21}x_1 + w_{22}x_2 + w_{23}x_3 &=&\\sum_i w_{2i} x_i\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "これを、$m$ 個の入力ユニットと $n$ 個の出力ユニットの場合に一般化すれば、\n",
        "以下のようになる。\n",
        "\n",
        "\n",
        "単純な 2 層のネットワークを考える。\n",
        "$i$ 番目の出力層の各ニューロンの膜電位 $y_i,(i=1,2,\\cdots,n)$ をまとめて $\\mathbf{y}$ とベクトル表現し、同様に入力層も $\\mathbf{x}$ とする。\n",
        "ニューロン $x_{j}$ から ニューロン $y_{i}$ へのシナプス結合強度を $w_{ij}$ と表現すれば、入力層から出力層への関係は\n",
        "\n",
        "$$\n",
        "\\left(\\begin{array}{l}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right)  = \n",
        "\\left(\\begin{array}{llll}\n",
        "w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
        "w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
        "\\vdots &        & \\ddots & \\vdots \\\\\n",
        "w_{n1} & w_{n2} & \\cdots & w_{nm} \n",
        "\\end{array}\\right)\n",
        "\\left(\\begin{array}{l}\n",
        "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{array}\\right) \\\\\n",
        "\\mathbf{y} = \\mathbf{Wx}\n",
        "$$\n",
        "\n",
        "と表現できる。\n",
        "しきい値の扱いについては、常に 1 を出力する仮想的なニューロン$x_0=1$を\n",
        "考えて $W$ に組み込むことも可能である。\n",
        "\n",
        "\n",
        "実際の出力は $\\mathbf{y}$ の各要素に対して \n",
        "\n",
        "$$\n",
        "f(y) = \\frac{1}{1+e^{-y}},\n",
        "$$\n",
        "\n",
        "のような非線型変換を施すことがある。\n",
        "\n",
        "階層型のネットワークにとっては上式，非線型変換が本質的な役割を果たす。\n",
        "なぜならば、こうした非線形変換がなされない場合には、ネットワークの構造が何層になったとしても、この単純なシナプス結合強度を表す行列を $\\mathbf{W}_{i}$\n",
        "($i=1,\\cdots, p$) としたとき、$\\mathbf{W} = \\prod_{i=1}^p \\mathbf{W}_{i}$ と置くことによって本質的には 1 層のネットワークと等価になるからである。\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W}_{p}\\mathbf{W}_{p-1}\\cdots\\mathbf{W}_{1}\\mathbf{y}=\\left(\\prod_{i=1}^p\\mathbf{W}_{i}\\right)\\mathbf{y}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zbXCu8GtbByp",
      "metadata": {
        "id": "zbXCu8GtbByp"
      },
      "source": [
        "<img src=\"https://komazawa-deep-learning.github.io/assets/xor.svg\"><br/>\n",
        "\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/xor-graph.svg\"><br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ao3qyb7a21M",
      "metadata": {
        "id": "9ao3qyb7a21M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0422first_neural_networks.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}