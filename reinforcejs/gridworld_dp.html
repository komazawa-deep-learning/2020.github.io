<!doctype html>
<html lang="en">
 <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>REINFORCEjs: Gridworld with Dynamic Programming</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- jquery and jqueryui -->
  <script src="./jquery-2.1.3.min.js"></script>
  <link href="./external/jquery-ui.min.css" rel="stylesheet">
  <script src="./external/jquery-ui.min.js"></script>

  <!-- bootstrap -->
  <script src="./bootstrap.min.js"></script>
  <link href="./bootstrap.min.css" rel="stylesheet">

  <!-- d3js -->
  <script type="text/javascript" src="./external/d3.min.js"></script>

  <!-- markdown -->
  <script type="text/javascript" src="./external/marked.js"></script>
  <script type="text/javascript" src="./external/highlight.pack.js"></script>
  <link rel="stylesheet" href="./external/highlight_default.css">
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- mathjax: nvm now loading dynamically
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
-->
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS_CHTML"></script>
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
  </script>


  <!-- rljs -->
  <script type="text/javascript" src="./lib/rl.js"></script>

  <!-- GA -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-3698471-24', 'auto');
  ga('send', 'pageview');
  </script>
  
  <style>
  #wrap {
    width:900px;
    margin-left: auto;
    margin-right: auto;
  }
  body {
    font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;
  }
  #draw {
    margin-left: 100px;
  }
  #exp {
    margin-top: 20px;
    font-size: 16px;
  }
  svg {
    cursor: pointer;
  }
  h2 {
    text-align: center;
    font-size: 30px;
  }
  #rewardui {
    font-weight: bold;
    font-size: 16px;
  }
  </style>
  
  <script type="application/javascript">

    // Gridworld
    var Gridworld = function(){
      this.Rarr = null; // reward array
      this.T = null; // cell types, 0 = normal, 1 = cliff
      this.reset()
    }
    Gridworld.prototype = {
      reset: function() {

        // hardcoding one gridworld for now
        this.gh = 10;
        this.gw = 10;
        this.gs = this.gh * this.gw; // number of states

        // specify some rewards
        var Rarr = R.zeros(this.gs);
        var T = R.zeros(this.gs);
        Rarr[55] = 1;

        Rarr[54] = -1;
        //Rarr[63] = -1;
        Rarr[64] = -1;
        Rarr[65] = -1;
        Rarr[85] = -1;
        Rarr[86] = -1;

        Rarr[37] = -1;
        Rarr[33] = -1;
        //Rarr[77] = -1;
        Rarr[67] = -1;
        Rarr[57] = -1;

        // make some cliffs
        for(q=0;q<8;q++) { var off = (q+1)*this.gh+2; T[off] = 1; Rarr[off] = 0; }
        for(q=0;q<6;q++) { var off = 4*this.gh+q+2; T[off] = 1; Rarr[off] = 0; }
        T[5*this.gh+2] = 0; Rarr[5*this.gh+2] = 0; // make a hole
        this.Rarr = Rarr;
        this.T = T;
      },
      reward: function(s,a,ns) {
        // reward of being in s, taking action a, and ending up in ns
        return this.Rarr[s];
      },
      nextStateDistribution: function(s,a) {
        // given (s,a) return distribution over s' (in sparse form)
        if(this.T[s] === 1) {
          // cliff! oh no!
          // var ns = 0; // reset to state zero (start)
        } else if(s === 55) {
          // agent wins! teleport to start
          var ns = this.startState();
          while(this.T[ns] === 1) {
            var ns = this.randomState();
          }
        } else {
          // ordinary space
          var nx, ny;
          var x = this.stox(s);
          var y = this.stoy(s);
          if(a === 0) {nx=x-1; ny=y;}
          if(a === 1) {nx=x; ny=y-1;}
          if(a === 2) {nx=x; ny=y+1;}
          if(a === 3) {nx=x+1; ny=y;}
          var ns = nx*this.gh+ny;
          if(this.T[ns] === 1) {
            // actually never mind, this is a wall. reset the agent
            var ns = s;
          }
        }
        // gridworld is deterministic, so return only a single next state
        return ns;
      },
      sampleNextState: function(s,a) {
        // gridworld is deterministic, so this is easy
        var ns = this.nextStateDistribution(s,a);
        var r = this.Rarr[s]; // observe the raw reward of being in s, taking a, and ending up in ns
        r -= 0.01; // every step takes a bit of negative reward
        var out = {'ns':ns, 'r':r};
        if(s === 55 && ns === 0) {
          // episode is over
          out.reset_episode = true;
        }
        return out;
      },
      allowedActions: function(s) {
        var x = this.stox(s);
        var y = this.stoy(s);
        var as = [];
        if(x > 0) { as.push(0); }
        if(y > 0) { as.push(1); }
        if(y < this.gh-1) { as.push(2); }
        if(x < this.gw-1) { as.push(3); }
        return as;
      },
      randomState: function() { return Math.floor(Math.random()*this.gs); },
      startState: function() { return 0; },
      getNumStates: function() { return this.gs; },
      getMaxNumActions: function() { return 4; },

      // private functions
      stox: function(s) { return Math.floor(s/this.gh); },
      stoy: function(s) { return s % this.gh; },
      xytos: function(x,y) { return x*this.gh + y; },
    }

    // ------
    // UI
    // ------
    var rs = {};
    var trs = {};
    var tvs = {};
    var pas = {};
    var cs = 60;  // cell size
    var initGrid = function() {
      var d3elt = d3.select('#draw');
      d3elt.html('');
      rs = {};
      trs = {};
      tvs = {};
      pas = {};

      var gh= env.gh; // height in cells
      var gw = env.gw; // width in cells
      var gs = env.gs; // total number of cells

      var w = 600;
      var h = 600;
      svg = d3elt.append('svg').attr('width', w).attr('height', h)
        .append('g').attr('transform', 'scale(1)');

      // define a marker for drawing arrowheads
      svg.append("defs").append("marker")
        .attr("id", "arrowhead")
        .attr("refX", 3)
        .attr("refY", 2)
        .attr("markerWidth", 3)
        .attr("markerHeight", 4)
        .attr("orient", "auto")
        .append("path")
          .attr("d", "M 0,0 V 4 L3,2 Z");

      for(var y=0;y<gh;y++) {
        for(var x=0;x<gw;x++) {
          var xcoord = x*cs;
          var ycoord = y*cs;
          var s = env.xytos(x,y);

          var g = svg.append('g');
          // click callbackfor group
          g.on('click', function(ss) {
            return function() { cellClicked(ss); } // close over s
          }(s));

          // set up cell rectangles
          var r = g.append('rect')
            .attr('x', xcoord)
            .attr('y', ycoord)
            .attr('height', cs)
            .attr('width', cs)
            .attr('fill', '#FFF')
            .attr('stroke', 'black')
            .attr('stroke-width', 2);
          rs[s] = r;

          // reward text
          var tr = g.append('text')
            .attr('x', xcoord + 5)
            .attr('y', ycoord + 55)
            .attr('font-size', 10)
            .text('');
          trs[s] = tr;

          // skip rest for cliffs
          if(env.T[s] === 1) { continue; }

          // value text
          var tv = g.append('text')
            .attr('x', xcoord + 5)
            .attr('y', ycoord + 20)
            .text('');
          tvs[s] = tv;
        
          // policy arrows
          pas[s] = []
          for(var a=0;a<4;a++) {
            var pa = g.append('line')
              .attr('x1', xcoord)
              .attr('y1', ycoord)
              .attr('x2', xcoord)
              .attr('y2', ycoord)
              .attr('stroke', 'black')
              .attr('stroke-width', '2')
              .attr("marker-end", "url(#arrowhead)");
            pas[s].push(pa);
          }
        }
      }

    }

    var drawGrid = function() {
      var gh= env.gh; // height in cells
      var gw = env.gw; // width in cells
      var gs = env.gs; // total number of cells

      // updates the grid with current state of world/agent
      for(var y=0;y<gh;y++) {
        for(var x=0;x<gw;x++) {
          var xcoord = x*cs;
          var ycoord = y*cs;
          var r=255,g=255,b=255;
          var s = env.xytos(x,y);

          var vv = agent.V[s];
          var ms = 100;
          if(vv > 0) { g = 255; r = 255 - vv*ms; b = 255 - vv*ms; }
          if(vv < 0) { g = 255 + vv*ms; r = 255; b = 255 + vv*ms; }
          var vcol = 'rgb('+Math.floor(r)+','+Math.floor(g)+','+Math.floor(b)+')';
          if(env.T[s] === 1) { vcol = "#AAA"; rcol = "#AAA"; }

          // update colors of rectangles based on value
          var r = rs[s];
          if(s === selected) {
            // highlight selected cell
            r.attr('fill', '#FF0');
          } else {
            r.attr('fill', vcol); 
          }

          // write reward texts
          var rv = env.Rarr[s];
          var tr = trs[s];
          if(rv !== 0) {
            tr.text('R ' + rv.toFixed(1))
          }

          // skip rest for cliff
          if(env.T[s] === 1) continue; 

          // write value
          var tv = tvs[s];
          tv.text(agent.V[s].toFixed(2));
          
          // update policy arrows
          var paa = pas[s];
          for(var a=0;a<4;a++) {
            var pa = paa[a];
            var prob = agent.P[a*gs+s];
            if(prob === 0) { pa.attr('visibility', 'hidden'); }
            else { pa.attr('visibility', 'visible'); }
            var ss = cs/2 * prob * 0.9;
            if(a === 0) {nx=-ss; ny=0;}
            if(a === 1) {nx=0; ny=-ss;}
            if(a === 2) {nx=0; ny=ss;}
            if(a === 3) {nx=ss; ny=0;}
            pa.attr('x1', xcoord+cs/2)
              .attr('y1', ycoord+cs/2)
              .attr('x2', xcoord+cs/2+nx)
              .attr('y2', ycoord+cs/2+ny);
          }
        }
      }
    }

    var selected = -1;
    var cellClicked = function(s) {
      if(s === selected) {
        selected = -1; // toggle off
        $("#creward").html('(select a cell)');
      } else {
        selected = s;
        $("#creward").html(env.Rarr[s].toFixed(2));
        $("#rewardslider").slider('value', env.Rarr[s]);
      }
      drawGrid(); // redraw
    }

    var updatePolicy = function() {
      agent.updatePolicy();
      drawGrid();
    }

    var evaluatePolicy = function() {
      agent.evaluatePolicy();
      drawGrid();
    }

    var sid = -1;
    var runValueIteration = function() {
      if(sid === -1) {
        sid = setInterval(function(){
          agent.evaluatePolicy();
          agent.updatePolicy();
          drawGrid();
        }, 100);
      } else { 
        clearInterval(sid); 
        sid = -1;
      }
    }

    function resetAll() {
      env.reset();
      agent.reset();
      drawGrid();
    }

    var agent, env;
    function start() {
      env = new Gridworld(); // create environment
      agent = new RL.DPAgent(env, {'gamma':0.9}); // create an agent, yay!
      initGrid();
      drawGrid();

      $("#rewardslider").slider({
        min: -5,
        max: 5.1,
        value: 0,
        step: 0.1,
        slide: function(event, ui) {
          if(selected >= 0) {
            env.Rarr[selected] = ui.value;
            $("#creward").html(ui.value.toFixed(2));
            drawGrid();
          } else {
            $("#creward").html('(select a cell)');
          }
        }
      });

      // suntax highlighting
      //marked.setOptions({highlight:function(code){ return hljs.highlightAuto(code).value; }});
      $(".md").each(function(){
        $(this).html(marked($(this).html()));
      });
      renderJax();
    }

    var jaxrendered = false;
    function renderJax() {
      if(jaxrendered) { return; }
      (function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src  = "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
        document.getElementsByTagName("head")[0].appendChild(script);
        jaxrendered = true;
      })();
    }
  </script>

 </head>
 <body onload="start();">

<!--
  <a href="https://github.com/karpathy/reinforcejs"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
-->

  <!-- content -->

  <div id="wrap">
   
   <div id="mynav" style="border-bottom:1px solid #999; padding-bottom: 10px; margin-bottom:50px;">
    <div>
      <img src="loop.svg" style="width:50px;height:50px;float:left;">
      <h1 style="font-size:50px;">REINFORCE <span style="color:#058;">js</span></h1>
    </div>
    <ul class="nav nav-pills">
      <li role="presentation" class="active"><a href="index.html">About</a></li>
      <li role="presentation"><a href="gridworld_dp.html">グリッドワールド:DP</a></li>
      <li role="presentation"><a href="gridworld_td.html">グリッドワールド:TD</a></li>
      <li role="presentation"><a href="puckworld.html">パックマンワールド:DQN</a></li>
      <li role="presentation"><a href="waterworld.html">ウォーターワールド:DQN</a></li>
    </ul>
   </div>


   <h2>グリッドワールド: 動的プログラミング法 デモ</h2>
   
   <div style="text-align:center;">
     <button class="btn btn-warning" onclick="evaluatePolicy()" style="width:200px;height:50px;margin-bottom:5px;">方策評価(1スイープ)</button>
     <button class="btn btn-warning" onclick="updatePolicy()" style="width:160px;height:50px;margin-bottom:5px;">方策更新</button>
     <button class="btn btn-success" onclick="runValueIteration()" style="width:160px;height:50px;margin-bottom:5px;">価値反復の切り替え</button>
     <button class="btn btn-danger" onclick="resetAll()" style="width:120px;height:50px;margin-bottom:5px;">リセット</button>
  </div>

   <br>

   <div id="draw"></div>
   <br>
   <div id="rewardui">セルの報酬変更:<span id="creward">(セルを1つ選択)</span> <div id="rewardslider"></div></div>

   <hr>

   <div id="exp" class="md">

### セットアップ

<!-- This is a toy environment called **Gridworld** that is often used as a toy model in the Reinforcement Learning literature. In this particular case:

- **State space**: GridWorld has 10x10 = 100 distinct states. The start state is the top left cell. The gray cells are walls and cannot be moved to.
- **Actions**: The agent can choose from up to 4 actions to move around. In this example 
- **Environment Dynamics**: GridWorld is deterministic, leading to the same new state given each state and action
- **Rewards**: The agent receives +1 reward when it is in the center square (the one that shows R 1.0), and -1 reward in a few states (R -1.0 is shown for these). The state with +1.0 reward is the goal state and resets the agent back to start.

In other words, this is a deterministic, finite Markov Decision Process (MDP) and as always the goal is to find an agent policy (shown here by arrows) that maximizes the future discounted reward. My favorite part is letting Value iteration converge, then change the cell rewards and watch the policy adjust.

**Interface**. The color of the cells (initially all white) shows the current estimate of the Value (discounted reward) of that state, with the current policy. Note that you can select any cell and change its reward with the *Cell reward* slider.
-->

このページは 強化学習の文献では 「おもちゃモデル」 としてしばしば使用される **グリッドワールド** という おもちゃの仮想環境です。
ここでは以下のような仮定が置かれます:

- **状態空間** グリッドワールド には 10x10=100 の状態があります。開始状態は左上のセルです。 灰色のセルは壁です。壁を突き抜けて移動することはできません。
- **行為** エージェントは 最大 4 つの行為 を選択することができます。上例では エージェントは最大 4 つの行為から一つの行為を選択して移動することができます。 
- **環境ダイナミクス**  グリッドワールド は 決定論的です 各状態と行動が与えられるごとに，同じ新しい状態になります。
- **報酬** エージェントは 中央のマス （R 1.0を示すマス） にいるときに +1 の報酬を受け取ります。 いくつかの状態では -1 の報酬を受け取ります（R -1.0 と表示）。 報酬が +1.0 の状態が ゴール状態です。ゴールするとエージェントをスタート状態に戻してリセットします。

この例は 決定論的有限マルコフ決定過程 (MDP) です。
エージェントの目標は常に 将来の割引された報酬 を最大化する エージェントの方策 (矢印で示されている)を見つけることです。
私のお気に入りの部分は 価値反復を収束させ、 セルの報酬を変更して、方策が調整されるのを見ることです。

**インターフェース** セルの色（最初はすべて白）は 現在の方策で その状態の 価値（割引報酬）の現在の推定値を示しています。
任意のセルを選択して **セルの報酬変更** スライダーを使って報酬を変更することができます。


### 動的プログラミング法 (Dynamic Programming)

<!-- An interested reader should refer to **Richard Sutton's Free Online Book on Reinforcement Learning**, in this particular case [Chapter 4](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node40.html). Briefly, an agent interacts with the environment based on its policy \\(\pi(a \mid s)\\). This is a function from states \\(s\\) to an action \\(a\\), or more generally to a distribution over the possible actions. After every action, the agent also receives a reward \\(r\\) from the environment. The interaction between the agent and the environment hence takes the form \\(s\_t, a\_t, r\_t, s\_{t+1}, a\_{t+1}, r\_{t+1}, s\_{t+2}, \ldots \\), indexed by t (time) and our goal is to find the policy \\(\pi^\*\\) that maximizes the amount of reward over time. In the DP approach, it is helpful to define a **Value function** \\(V^\pi(s)\\) that gives the expected reward when starting from the state \\(s\\) and then interacting with the environment according to \\(\pi\\):
-->

興味のある方は **リチャード=スットン Richard Sutton** の 強化学習の教科書 （オンライン版は無料）の
[第 4 章](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node40.html) を参考にしてください。
簡潔に言うと エージェントは その 方策(policy) $\pi(a \mid s)$ に基づいて 環境と対話します。
方策とは 状態 $s$ から 行為 $a$ を導く関数です。
ある行為が行われた後 えージェーンとは 環境から報酬 $r$ を受取ります。
それゆえ エージェントと環境との相互作用は $s\_t, a\_t, r\_t, s\_{t+1}, a\_{t+1}, r\_{t+1}, s\_{t+2} \dots$ ここで  $t$ (時刻) を指標とした形になります。
動的プログラミング (DP) のアプローチでは 方策  $\pi^*$ とは「価値関数」 $\pi$ に従って，環境と相互作用し，状態  $s$ から始まって期待報酬の最適値
時間を指標とした最大値を与え得る最適方略です。

$$
V^\pi(s) = E \left\{ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots \mid s_t = s \right\}
$$

<!-- Note that the expectation is over both 1. the agent's (in general) stochastic policy and 2. the environment dynamics; That is, how the environment evolves when the agent takes an action \\(a\\) in state \\(s\\) and what the obtained reward is in that case. The constant \\(\gamma\\) is a discount factor that gives more weight to earlier rewards than the later ones. We can start writing out the expectation. To get the value of state \\(s\\) we sum over all the actions the agent would take, then over all ways the environment could respond, and so on back and forth. When you do this, you'll find that:
 -->

期待値は 1. 確率的な方策と 2. 環境のダイナミクスの両方を超えていることに注意してください。
定数 $\Gamma$ は 早い方の報酬の方が 重視される割引係数です。
状態 $s$ で行動 $a$ を起こしたときに 環境がどのように変化し そのときに得られる報酬がどのようなものになるのか ということです。
定数 ($\gamma$) は、後のものよりも先の報酬に重みを与える割引係数です。

では、期待値を書き出してみましょう。状態を得るために  
期待値を書き出してみると，
態を得るために  
そうすると、次のようなことがわかります。

$$
\pi(s) = \sum\_{a} \pi(s,a) \sum\_{s'} \mathcal{P}_{ss'}^{a} \left[ \mathcal{R}_{ss'}^{a} + \gamma V^\pi(s') \right]
$$

上式  $\mathcal{P}_{ss'}^{a}, \mathcal{R}_{ss'}^{a}$ は特定の環境については定数です。
次の状態 $s'$ は，状態 $s$ で行動 $a$ をとることで繰り返し与えられます。
この方程式は **ベルマン方程式** と呼ばれ、与えられた方策の価値関数が満たすべき再帰的な関係を記述しています。

<!-- 
In the above equation \\(\mathcal{P}\_{ss'}, \mathcal{R}\_{ss'}^{a}\\) are fixed constants specific to the environment, and give the probability of the next state s′ given that the agent took action a in state s, and the expected reward for that transition, respectively. 
This equation is called the Bellman Equation and interestingly, it describes a recursive relationship that the value function for a given policy should satisfy.
and give the probability of the next state \\(s'\\) given that the agent took action \\(a\\) in state \\(s\\), and the expected reward for that transition, respectively. This equation is called the **Bellman Equation** and interestingly, it describes a recursive relationship that the value function for a given policy should satisfy.
 -->
<!-- With our goal of finding the optimal policy \\(\pi^\*(s,a)\\) that gets the most Value from all states, our strategy will be to follow the **Policy Iteration** scheme: We start out with some diffuse initial policy and evaluate the Value function of every state under that policy by turning the Bellman equation into an update. The value function effectively diffuses the rewards backwards through the environment dynamics and the agent's expected actions, as given by its current policy. Once we estimate the values of all states we will update the policy to be greedy with respect the Value function. We then re-estimate the Value of each state, and continue iterating until we converge to the optimal policy (the process can be shown to converge). These two steps can be controlled by the buttons in the interface:

- The **Policy Evaluation (one sweep)** button turns the Bellman equation into a synchronous update and performs a single step of Value Function estimation. Intuitively, this update is diffusing the raw Rewards at each state to other nearby states through 1. the the dynamics of the environment and 2. the current policy.
- The **Policy Update** button iterates over all states and updates the policy at each state to take the action that leads to the state with the best Value (integrating over the next state distribution of the environment for each action).
- The **Value Iteration** button starts a timer that presses the two buttons in turns. In particular, note that Value Iteration doesn't wait for the Value function to be fully estimates, but only a single synchronous sweep of Bellman update is carried out. In full policy iteration there would be many sweeps (until convergence) of backups before the policy is updated.
 -->

目標は、すべての状態から最も多くの価値を得る最適な政策 \\(\pi^\*(s,a))\\) を見つけることです。
戦略は **方略反復 policy Iteration** スキームに従うことになります。
初期方策から始めて その方策下で ベルマン方程式 を更新して 各状態の価値関数を評価します。
価値関数は 現在の方策 によって与えられ、環境のダイナミクス と エージェントの期待される行動 とを通して 効果的に後方に報酬を拡散させます。
一旦 すべての状態の値を推定したら、価値関数に関して貪欲に方策を更新します。
その後 各状態の値を再推定し 最適な方策に収束するまで反復を繰り返します
このプロセスは収束することを示すことができます。
この 2 つのステップは，インタフェースのボタンで制御できます。

- **方策評価（1回のスィープ）**  ボタンは ベルマン方程式を同期更新に変え 価値関数推定の 1 つのステップを実行します。直感的には この更新では 1. 環境のダイナミクスと 2. 現在のポリシー とを介して、 各状態での生の報酬を他の近くの状態に拡散します。
- **方策更新** ボタンは、 すべての状態を反復処理し、 各状態で方策を更新します。最高の価値を持つ状態につながる行動を取るようにします。各行動のための環境の次の状態分布を積分します。
- **価値反復** ボタンは 2 つのボタンを交互に押すタイマーを開始します。特に 価値反復 は 価値関数 が完全に推定されるのを待たずに ベルマン更新 の単一の同期スイープのみが 実行されることに注意してください。 完全な方策反復では、方策が更新される前にバックアップのスイープ（収束まで）が何度も行われます。


### コードの概要 <!--Sketch of the Code-->

<!-- The goal of **Policy Evaluation** is to update the value of every state by diffusing the rewards backwards through the dynamics of the world and current policy (this is called a *backup*). The code looks like this:
-->

**ポリシー評価** の目的は 世界 と 現在の方策 の ダイナミクスを介して 報酬を後方に拡散させることで すべての状態の値を更新することです（これを **バックアップ**と呼びます）。 コードは以下のようになっています。

<pre><code class="js">
evaluatePolicy: function() {
  // 価値関数の同期的更新の実行 <!--perform a synchronous update of the value function-->
  var Vnew = zeros(this.ns); // initialize new value function array for each state
  for(var s=0;s < this.ns;s++) {
    var v = 0.0;
    var poss = this.env.allowedActions(s); // 可能な全行動について<!--fetch all possible actions-->
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      var prob = this.P[a*this.ns+s]; // 現時点での方策の元での行動確率 <!--probability of taking action under current policy-->
      var ns = this.env.nextStateDistribution(s,a); // 次の状態をルックアップ<!--look up the next state-->
      var rs = this.env.reward(s,a,ns); // s->a->ns 遷移の報酬を取得 <!--get reward for s->a->ns transition-->
      v += prob * (rs + this.gamma * this.V[ns]);
    }
    Vnew[s] = v;
  }
  this.V = Vnew; // swap
},
</code></pre>

<!-- Here, we see `this.ns` (number of states), `this.P` which stores the current policy, and `this.env`, which is a pointer to the Environment object. The policy array is one-dimensional in this implementation, but stores the probability of taking any action in any state, so I'm using funny indexing (`this.P[a*this.ns+s]`) to not have to deal with 2D arrays in Javascript. The code implements the synchronous Value backup for each state:
-->

ここでは `this.ns`  (状態の数)、 現在のポリシーを格納する `this.P`、 環境オブジェクトへのポインタ `this.env` があります。
この実装では 方策の配列は一次元です。
ですが、任意の状態で何かの行動を起こす確率を格納しています。
このコードは 各状態に対する同期的な値のバックアップを実装しています。


\begin{align}
V^\pi(s) & = E_\pi \left\{ r_t + \\gamma r_{t+1} +  \gamma^2 r_{t+2} + \ldots \mid s_t = s  \right\} \\
& = E_\pi \left\{ r_t + \gamma V^\pi(s_{t+1}) \mid s_t = s  \left\} \\
& = \sum_a \pi(s,a) \sum_{s'} \mathcal{P}_{ss'}^a \left[ \mathcal{R}_{ss'}^a + \\gamma V(s') \right]
\end{align}

<!-- However, note that in the code we only took expectation over the policy actions (the first sum above), while the second sum above was not evaluated because this Gridworld is deterministic (i.e. `ns` in the code is just a single integer indicating the next state). Hence, the entire probability mass is on a single next state and no expectation is needed.

Once the Value function was re-estimated, we can perform a **Policy Update**, making the policy greedy with respect to the estimate Value in each state. This is a very simple process, but the code below is a little bloated because we're handling ties between actions carefully and distributing the policy probabilities equally over all winning actions:
 -->

このコードでは 方策行動に対する期待値（上の第一の和）のみを取っています。
ですが このグリッドワールドは決定論的であるため、上の第二の和は評価されていないことに注意です。
すなわち、コード中の `ns` は次の状態を示す単一の整数です。
したがって 全確率量 は単一の次の状態にあり、期待値は不要です。

価値関数 が再推定されると ***方策更新 (policy update)** を実行して、各状態での推定値に対して方策を貪欲にすることができます。
これは非常に簡単なプロセスです。
以下のコードは少し拡張しています。
これは、アクション間の結びつきを慎重に処理し、
すべての勝利アクションに均等にポリシーの確率を配分しているからです。


<pre><code class="js">
updatePolicy: function() {
  // 学習した各関数について貪欲に方策を更新する <!--update policy to be greedy w.r.t. learned Value function-->
  // すべての状態について繰り返し... <!--iterate over all states...-->
  for(var s=0;s < this.ns;s++) {
    var poss = this.env.allowedActions(s);
    // 可能な行動をとったときの価値を計算 <!--compute value of taking each allowed action-->
    var vmax, nmax;
    var vs = [];
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      // 行動 a をとったときの価値 を計算 <!--compute the value of taking action a-->
      var ns = this.env.nextStateDistribution(s,a);
      var rs = this.env.reward(s,a,ns);
      var v = rs + this.gamma * this.V[ns];
      // ブックキーピング: 最大値を保存 <!--bookeeping: store it and maintain max-->
      vs.push(v);
      if(i === 0 || v > vmax) { vmax = v; nmax = 1; }
      else if(v === vmax) { nmax += 1; }
    }
    // すべての行動に渡って方策を更新 <!--update policy smoothly across all argmaxy actions-->
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      this.P[a*this.ns+s] = (vs[i] === vmax) ? 1.0/nmax : 0.0;
    }
  }
},
</code></pre>

<!-- Here, we are computing the action value function at each state \\(Q(s,a)\\), which measures how much expected reward we would get by taking the action \\(a\\) in the state \\(s\\). The function has the form:
 -->

ここでは、各状態 $Q(s,a)$ での行動価値関数を 計算しています。
Q とは，ある行動 $a$ をとることで，どの程度期待報酬が得られるかを表す関数です。
Q 関数は以下のようになります:

\begin{align}
Q^\pi (s,a) &= E_\pi \left\{ r_t + \gamma V^\pi(s_{t+1}) \mid s_t = s, a_t = a \left\} \\
&= \sum_{s'} \mathcal{P}_{ss'}^a \left[ \mathcal{R}\_{ss'}^a + \gamma V^\pi(s') \right]
\end{align}

<!-- In our Gridworld example, we are looping over all states and evaluating the Q function for each of the (up to) four possible actions. Then we update the policy to take the argmaxy actions at each state. That is, 
 -->

グリッドワールド では すべての状態をループして 最大 4 つの 行為のそれぞれについて Q関数 を評価します。
各状態で最大値を与える行動を実行するように方策を更新します。以下のようになります:


$$
\pi'(s) = \arg\max_a Q(s,a)
$$

<!-- It can be shown (see Richard Sutton's book) that performing these updates over and over again is guaranteed to converge to the optimal policy. In this Gridworld example, this corresponds to arrows that perfectly guide the agent to the terminal state where it gets reward +1.
-->

このような更新を何度も何度も行うことで、 最適な政策に収束することが保証されています。（リチャード・サットンの本を参照）。
このグリッドワールドの例では、エージェントが 報酬+1 を得る終末状態 へと完璧に導く矢印に対応しています。


### REINFORCE js API の 動的プログラミングの使用 <!--REINFORCEjs API use of DP-->

<!-- If you'd like to use the REINFORCEjs Dynamic Programming for your MDP, you have to define an environment object `env` that has a few methods that the DP agent will need:

- `env.getNumStates()` returns an integer of total number of states
- `env.getMaxNumActions()` returns an integer with max number of actions in any state
- `env.allowedActions(s)` takes an integer `s` and returns a list of available actions, which should be integers from zero to `maxNumActions`
- `env.nextStateDistribution(s,a)`, which is a misnomer, since right now the library assumes deterministic MDPs that have a single unique new state for every (state, action) pair. Therefore, the function should return a single integer that identifies the next state of the world
- `env.reward(s,a,ns)`, which returns a float for the reward achieved by the agent for the `s`, `a`, `ns` transition. In the simplest case, the reward is usually based only the state `s`.

See the GridWorld environment in this demo's source code for an example. An example of creating and training the agent will then look something like:
 -->

マルコフ決定過程 MDP に REINFORCEjs ダイナミックプログラミング を使いたい場合 DP エージェントが必要とするいくつかのメソッドを持つ環境オブジェクト `env` を定義しなければなりません。

- 環境オブジェクト `env.getNumStates()` は状態の総数を整数で返します。
- `env.getMaxNumActions()` は、任意の状態におけるアクションの最大数を表す整数を返します。
- これは 0 から `maxNumActions` までの整数でなければなりません。
- これは誤記です。今のところライブラリは決定論的な MDP を想定しているので (状態、アクションの)ペアごとに一意の新しい状態を持つことになっています。 したがって この関数は 世界の次の状態を識別する単一の整数を返すべきです。
- これは エージェントが `s`, `a`, `ns` の遷移に対して達成した報酬を 浮動小数点 float で返します． 最も単純な場合では，報酬 は通常，状態 `s` のみに基づいています．

このデモのソースコードの `GridWorld` 環境を参照してください。エージェントの作成と訓練の例は次のようになります。


<pre><code class="js">
// 環境の生成 <!--create environment-->
env = new Gridworld(); 
// エージェントの生成，割引率は 0.9 <!--create the agent, yay! Discount factor 0.9-->
agent = new RL.DPAgent(env, {'gamma':0.9}); 

// 収束するまで，この関数を呼び出す <!--call this function repeatedly until convergence:-->
agent.learn();

// 一度実行した場合のエージェントの行動 <!--once trained, get the agent's behavior with:-->
var action = agent.act(); // 選択された行動のインデックスを返す <!--returns the index of the chosen action-->
</code></pre>

### 長所と短所 (Pros and Cons)

<!-- In practice you'll rarely see people use Dynamic Programming to solve Reinforcement Learning problems. There are numerous reasons for this, but the two biggest ones are probably that:

- It's not obvious how one can extend this to continuous actions and states
- To calculate these updates one must have access to the environment model \\(P\_{ss'}^a\\), which is in practice almost never available. The best we can usually hope for is to get samples from this distribution by having an agent interacting with the environment and collecting experience, which we can use to approximately evaluate the expectations by sampling. More on this in TD methods!

However, the nice parts are that:

- Everything is mathematically exact, expressible and analyzable. 
- If your problem is relatively small (maybe a few thousand states and up to few tens/hundreds actions), DP methods might be the best solution for you because they are the most stable, safest and come with simplest convergence guarantees.
 -->

実際には，強化学習の問題を解決するために動的プログラミングを使用している人をほとんど見かけません。
理由はいくつも挙げられます。おそらく以下の 2 理由が大きいと思われます。

- どうすれば，連続的な行動(動作) や 状態に拡張できるのかが不明である
- 動的計画法による更新を計算するためには、環境モデルへのアクセスが必要です。ですが 実際にはほとんど利用できません。 通常期待できる最善の方法としては エージェントが 環境と相互作用し， 経験を重ねることで，この分布からサンプルを得ることである。このことの詳細は TD 法で!

長所は次のとおり:

- 数学的に正確，表現可能，分析可能である
- 問題が比較的小さい場合 （数千の状態と数十/数百の行動くらい） 動的プログラミング法  は最も安定しています。
また，最も安全でもあり，最も単純な収束保証もあります。
このような場合には 動的プログラミング法 は最良の解決策かも知れません。

   </div>
   </div>

   <br><br><br><br><br>
 </body>


</html>
