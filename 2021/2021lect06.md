---
title: "第06回"
author: 浅川 伸一
layout: home
---


# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 21/May/2021<br/>
Appache 2.0 license<br/>
</div>

<!--
# [ディープラーニングの心理学的解釈 (心理学特講IIIA)](https://komazawa-deep-learning.github.io/)
-->

- [実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
<!-- - [本日の課題 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529komazawa_homework.ipynb){:target="_blank"}
 -->


<<<<<<< HEAD

- ところで知能とは何だろうか？ [心理学者の考えた知能](../psychological_intelligence/)


# 実習
- [もう少し詳しくステップバイステップで画像認識  <img src="https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/4c5e1c665109926508b3fa505914b60b7237bf62/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb){:target="_blank"}
- [第一層目の視覚化  <img src="https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/4c5e1c665109926508b3fa505914b60b7237bf62/assets/colab_icon.svg">](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0514komazawa_visualise_first_layers.ipynb){:target="_blank"} - 視覚特徴の視覚化。
[ガボールパッチ あるいは ガボールフィルタ](https://komazawa-deep-learning.github.io/gabor_filter/){:target="_blank"} とか，色ブロッブ。

<!--
- [○☓分類器](./2019OX_classifier.html)
- [pose](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html)
- [nothotdog](../nothotdog/)
-->

<div align="center">
<img src="https://miro.medium.com/max/2812/1*bD_DMBtKwveuzIkQTwjKQQ.png" style="width:88%"><br/>
</div>

from [Difference between AlexNet, VGGNet, ResNet, and Inception](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96){:target="_blank"}



----

# CNN の特徴

- 次の 7 つを上げることができます
<!--[@2017Asakawa_deep_jps][^2017Aakawa\_deep\_jps\]-->。

1.  非線形活性化関数 (nonlinear activation functions)
2.  畳込み演算 (convolutional operation)
3.  プーリング処理 (pooling)
4.  データ拡張 (data augmentation)
5.  バッチ正規化 (batch normalization)
6.  ショートカット(shortcut)
7.  GPU の使用

上記 7 つの特徴を説明するのは専門的になりすぎるので省略します。一つだけ説明するとすれば最後の
GPU とは高解像度でしかも処理速度を必要とするパソコンゲームで用いられるグラフィックボードのことです。
詳細な画像を高速に画面に表示する必要から開発されたグラフィックボードですが，大規模なニュールネットワークの計算でも用いられる数学が同じです。
そのため，ゲーム用に開発されたグラフィックボードがニューラルネットワークにも用いられるようになりました。


<!--
- 素粒子 Subatomic Particles
- 原子 Atom
- 分子 Molecules
- 遺伝子 Genes
- 細胞 Cells
- 神経回路 Circuits
- 生理学 Physiology
- 行動 Behavior
- 社会 Society

Morris and Cuthbert, (2012) Research Domain Criteria: cognitive systems,
neural circuits, and dimensions of behavior. Dialogues Clin Neurosci. 2012;14:29-37.
を一部改変

- Self-reports 
- Paradigms
n-->

<!--
- <https://openai.com/blog/language-unsupervised/>

- [ELMo](../ELMo_tab/)
- [BERT](../BERT/)

- <https://gluebenchmark.com/leaderboard>
- <http://www.msmarco.org/leaders.aspx>
- <https://paperswithcode.com/sota>
-->

<!--
- \citep{2018KriegesKorte}, 
- \citep{2001DayanAbbott}, 
- \citep{Poggio1985},
- \citep{1963Hubel,1959Hubel_Wiesel,1962HubelWiesel,1968HubelWiesel,LivingstoneHubel1988},
- \citep{1954Hartline,1957Hartline,1958Hartline},
- \citep{Poggio1985},
- \citep{1997Edelman}

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2017Marcus_CoverPage.png" style="width:49%"><br>
**Marcus (2017)より**
</center>
-->

## ディープラーニングの特徴

from Democratize AI slides

- データハングリー data hungry
- 計算資源ハングリー resource hungry
- 理論欠如 theory lagging
- 不透明 opacity

- ニューラルネットワークは素人の統計学である, Anderson et. al (1993)

... But Neural networks are not alchemy.



<!--
## 第四次産業革命，AIの自動化，AIの民主化，ソフトウェア 2.0

- [autoML](http://www.ml4aad.org/automl/)
- [auto-sklearn](https://automl.github.io/auto-sklearn/stable/)
- [AI democratization](https://news.microsoft.com/features/democratizing-ai/)
-->

<!---

## AI の自動化
`auto-ML`, `auto-sklearn`

## AI の民主化
Democratizing AI
-->

<!--
## ソフトウェア2.0
- Karpathy medium 'https://medium.com/@karpathy/software-2-0-a64152b37c35'
 
1. Software 2.0 is written in neural network weights.
2. No human is involved in writing this code.
-->

<!--
## Michael Jordan's view

- 機械学習は統計学の一部なのだから，機械学習の文献だけ読まずに，統計学一般の文献を熟読して貢献しなさい
(Machine learning is a part of statistics; don't just read the machine learning literature. - read, ponder and contribute to the broad statistical literature.)

__from Michael Jordan, Machine Learning Summer School, Cambridge 2009.__

<center>
<img src="https://komazawa-deep-learning.github.io/assets/c3-s4-jordan.jpg" style="width:39%"></br>
Michael Irvin Jordan
</center>

- ニューラルネットワーク $\subset$ ディープラーニング
- 統計学 $\supset$ 機械学習
-->

<!---
<center>
<img src="https://komazawa-deep-learning.github.io/assets/Deep_Learning_Icons_R5.png" style="width:84%"></br>
Nvidia のサイトより
</center>
-->

<!--
- <https://enterprisetechnologyconsultant.wordpress.com/2013/03/10/data-science-and-the-definition-and-role-of-a-data-scientist/>

- ウィリアム・クリーブランドの「データサイエンス:統計学の技術領野を拡張するための行動計画」が公刊されて以来データサイエンスという言葉が普及した。 
  - W. S. Cleveland. Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics. ISI Review, 69:21–26, 2001.

- <https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/#123e7cc355cf> Gil Press, CONTRIBUTOR I write about technology, entrepreneurs and innovation.
- 1962 John W. Tukey writes in “The Future of Data Analysis”:
- 1974 Peter Naur publishes ``Concise Survey of Computer Methods'' in Sweden and the United States.
- 1977 The International Association for Statistical Computing (IASC) is established as a Section of the ISI. “It is the mission of the IASC to link traditional statistical methodology, modern computer technology, and the knowledge of domain experts in order to convert data into information and knowledge.”
- 1994 BusinessWeek publishes a cover story on “Database Marketing”:
- 1996 Members of the International Federation of Classification Societies (IFCS) meet in Kobe, Japan, for their biennial conference. For the first time, the term “data science” is included in the title of the conference
- 1996 Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth publish “From Data Mining to Knowledge Discovery in Databases.” 
- 2001 William S. Cleveland publishes “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” 

-->

---

# 1. 生理学

<center>
<img src='https://komazawa-deep-learning.github.io/assets/1994vanEssen_gallant_Fig2.jpg' style='width:74%'></br>
**Van Essen & Gallant (1994)**

<img src='https://komazawa-deep-learning.github.io/assets/thorpe_sj_01_260.jpg' style='width:49%'>
<img src='https://komazawa-deep-learning.github.io/assets/1991Felleman_VanEssen_Fig4.svg' style='width:49%'></br>
**Left: Thorpe et al.(2001), Right: Felleman & Van Essen (1992)**
</center>

<div align="center" style="width:88%">
  <img src="https://upload.wikimedia.org/wikipedia/commons/d/d2/Retinotopic_English.jpg" style="width:74%"><br/>
  <div align="left">
    source: https://en.wikipedia.org/wiki/Retinotopy
  </div>
</div>


## ヒューベルとウィーゼル Hubel and Wiesel (1969)
<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br/> source: https://youtu.be/4nwpU7GFYe8
<!--<iframe width="901" height="676" src="https://www.youtube.com/embed/4nwpU7GFYe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
</center>

- トポグラフックマッピング topographic mappings (地形図):  網膜や皮膚などの体制感覚，筋肉系のような効果器系を、中枢神経系の 1 つ以上の構造物に整然と投影した地図。
地形図は、すべての感覚系と多くの運動系で観察される。
- トノトピー tonotopy（ギリシャ語のtono=周波数、topos=場所）とは、異なる周波数の音が脳内で処理される場所の空間的配置のこと。
周波数が近い音は、脳内の場所的に隣接する領域で表現される。トノトピック地図とも呼ばれる <!--は 視覚系のレチノトピーに似た地形的な組織の特殊な例である。-->
- ソマトピー somatopy: 中枢神経系上の特定の点へ身体領域の照射，投影のこと。 身体領域は 第一次体性感覚皮質 (後腹回) に投影される。 典型的には 特定の身体部位と そのそれぞれの位置を ホムンクルス homunclus (小人) 
 上に配置する 感覚ホムンクルスとして表現される。
細かく制御されている部位 (指，舌) は体性感覚野の面積が大きい。一方 粗く制御されている部位 (体幹) は面積が小さい。内臓のような領域は体性感覚野の位置を持たない。
- レティノトピー retinotopy: 網膜からの視覚入力を神経細胞 にマッピングすること。哺乳類の脳に多く見られる。この地図の大きさ、数、空間的配置は種間で異なる。
視野の隣接する点 が 同じ領域 の 隣接する 領域で 表現される とは限らないという意味で、複雑な地図となる。
例えば 第 2 次視覚野 (V2) での視覚地図は 視野の上半分に応答する網膜の部分が 視野の下半分に応答する部分から分離されて表現されている。第３次視覚野 (V3) や 第4次視覚野 (V4) では下位の視覚野に比べて より複雑な表現がなされている

---

## ブレイクモア と クーパー Blackmore and Cooper (1970)

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--<iframe width="845" height="676" src="https://www.youtube.com/embed/QzkMo45pcUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
</br> source: https://youtu.be/QzkMo45pcUo
</center>

<center>
<img src='https://komazawa-deep-learning.github.io/assets/1970BlackmoreCooper_Fig1.svg' style='width:39%'>
<img src='https://komazawa-deep-learning.github.io/assets/1970BlackmoreCooper_Fig2.svg' style='width:49%'>
</center>

---

<center>
<iframe width="450" height="300" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br/>source: https://youtu.be/RSNofraG8ZE
<!--<iframe width="784" height="627" src="https://www.youtube.com/embed/RSNofraG8ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
</center>

---

<!--
<center>
<video style="width:84%" src="../assets/Style-based-GAN.mp4" type="video/mp4" />
</center>
-->

<!--
図 [fig:2014Szegedy](#fig:2014Szegedy){reference-type="ref"
reference="fig:2014Szegedy"}
は画像認識の性能向上の例を示しています[@2014GoogLeNet]。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Szegedy_Going_deeper.jpg" style="width:74%">
<img src="https://komazawa-deep-learning.github.io/assets/2014Szegedy_Going_deeper.jpg" style="width:74%">
</center>
-->


# 2. 心理学
## セルフリッジ Selfridge のパンデモニウム pandemonium モデル

<center>
<img src='https://komazawa-deep-learning.github.io/assets/1958Selfridge_fig3.svg' style='width:74%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より

<img src='https://komazawa-deep-learning.github.io/assets/1958Selfridge_fig7.svg' style='width:84%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center>


<center>
<img src='https://komazawa-deep-learning.github.io/assets/1958Selfridge_fig4.svg' style='width:49%'><br>
<!--
<img src='https://komazawa-deep-learning.github.io/assets/1958Selfridge_fig5.svg' style='width:74%'><br>
-->
<img src='https://komazawa-deep-learning.github.io/assets/1958Selfridge_fig6.svg' style='width:49%'><br>
セルフリッジ (1958) ``Mechanisation of Thought Processes'' より
</center>


# 生理学，視覚心理学との対応

- Julesz
  - Julesz (1981) Textons, the elements of texture perception, and their interactions, Nature

<center>
<img src="https://komazawa-deep-learning.github.io/assets/1981Julesz-texton-Fig2.svg" style="width:84%"></br>
Julesz (1981) Fig. 2 より
</center>

- Marr
    - Computational approach: Vision (1908) 
- Poggio
    - Poggio, Torre, and Koch (1985) Computational vision and regularization theory

## 心理学モデル Psycholosophical consideration
- Epistemology 思念的，観念的
- Empirical Episitemology 実証的 = psychology
- Constructive epipstemology 構成論的 = computer vision, neural networks

- Marr
    - Computational approach: Vision (1908) 
- Poggio
    - Poggio, Torre, and Koch (1985) Computational vision and regularization theory



## 生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン) 
- 層間の結合の仕方, アーキテクチャ
- forward/backward 役割，機能，実現方法
- 側抑制 lateral inhibition (これについては多層化して回避できる可能性あり)
- shape from X は正しかったのか？ ただし発達心理学におけるシェイプバアスは言語発達において重要な意味を持つはず。だからと言って乳幼児はそのように強制(脅迫？)，矯正されて育つわけではないだろう。

    - Ritter (2017) Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study
    - Landau, Smith, Jones (1992) Syntactic Context and the Shape Bias in Childrens and Adults Lexical Learning
    - Yamins (2016) Using goal-driven deep learning models to understand sensory cortex

    - Julez のアプローチは視覚研究者 Haar, SIFT, DoG などのアルゴリズム開発者と対応
    - Poggio (1985) Computational Vision and Regularization Theory

## Marr and Poggio (1976)

中枢神経系は 4 つのほぼ独立したレベルの記述で理解される必要がある。
1. 計算の性質が表現されるレベル、
2. 計算を実装するアルゴリズムが特徴づけられるレベル、
3. アルゴリズムが特定のメカニズムにコミットされるレベル、
4. メカニズムがハードウェアで実現されるレベル

である。
一般的に、計算の性質は解決すべき問題によって決定され、使用されるメカニズムは利用可能なハードウェアに依存し、選択される特定のアルゴリズムは問題と利用可能なメカニズムに依存する。

のちに 3 つにまとめられ，それが有名になりました。ですが，出始めは 4 つでした。他の授業では Marr の 3 水準と呼ばれるかも知れません。
その有名になった Marr の 3 水準とは
1. 計算論
2. アルゴリズム
3. 実装
にあたります。脳を理解する上で，3 つを区別して考えましょうというスローガンです。


## 現代的ニューラルネットワークモデルと生理学との対応

<div align="center">
<img src='https://komazawa-deep-learning.github.io/assets/2012Zeiler_Deconvolution.png' style='width:77%'></br>
<br>Zeiller 2012 より
</div>

<div align="center">
<img src="https://komazawa-deep-learning.github.io/assets/2010ZeilerKrishnanTaylorFerguss_fig.svg" style="width:88%"><br/>
Zeiler et. al. (2010) Fig. 7, and 8
</div>



<center>
<img src='https://komazawa-deep-learning.github.io/assets/2016Yamins_fig1s_ja.svg' style='width:94%'>
<!--<img src='https://komazawa-deep-learning.github.io/assets/2016Yamins_Fig1.svg' style='width:94%'>-->
<img src='https://komazawa-deep-learning.github.io/assets/2016Yamins_fig2ja.svg' style='width:94%'>
</center>

<center>
<img src='https://komazawa-deep-learning.github.io/assets/2016Yamins_fig3s_ja.svg' style='width:84%'><br/>
<div align="left" style="width:84%">
目標駆動型モデリングの構成要素。
内側の円は、与えられた数のレイヤーのモデルを含む完全なモデルクラスの部分空間を表す。
目標駆動モデルは、特に最適なモデルを発見するために、モデル・クラス内の軌道（実線）に沿ってシステムを駆動する学習アルゴリズム（黒の点線矢印）を使用して構築される。
各ゴールは、そのゴールを解くのに特に適したパラメータを含むモデルクラス（太い黒の等高線）内の引き寄せの盆地に対応していると考えることができる。
計算結果は、課題がモデルのパラメータ設定に強い制約を与えることを示す。
すなわち、与えられた課題の最適化パラメータのセットは、元の空間に比べて非常に小さいということである。
これらの目標駆動モデルは、与えられた課題領域での行動の根底にあると考えられている脳領域のニューロンの応答特性をどの程度予測できるかについて評価することができる。
例えば、単語認識のために最適化されたモデルのユニット、聴覚皮質の一次領域、ベルト領域、パラベルト領域の応答特性と比較することができる。
また、モデルを互いに比較して、異なるタイプの課題がどの程度の神経構造の共有につながるかを判断することもできる。
また、様々な構成要素のルール（教師あり、教師なし、半教師あり）を研究して、それらが生後の発達や専門知識学習の間にどのように異なるダイナミクスにつながるかを判断することもできる（緑の破線のパス）。
</div>
</center>

<div align="center">
<!--<img src='https://komazawa-deep-learning.github.io/assets/2014Yamins_fig2Aja.svg' style='width:74%'>-->
<!--<img src='https://komazawa-deep-learning.github.io/assets/2014Yamins_FigS2.svg' style='width:74%'>-->
<img src="https://komazawa-deep-learning.github.io/assets/2020Schrimpf_fig1.svg" style="width:84%"><br/>
</div>

<!--<a href="../assets/2018asakawa_jdlaGtestChapt7.pdf">G 検定公式テキスト7章より</a>-->


---

# 3. 畳込みニューラルネット(CNN)

深層学習 (ディープラーニング) の中で **畳み込みニューラルネットワーク** CNN と呼ばれるニューラルネットワークについて解説します。

最初に画像処理の概略を述べる CNN が，それまで主流であった従来の手法の性能を凌駕したことはすでに述べました。
CNN の特徴の一つに **エンドツーエンド** と呼ばれる考え方があります。
エンドツーエンドとは，従来手法によるパターン認識システムでは，専門家による手の込んだ詳細な作り込みを必要としていたことと異なり，面倒な作り込みをせずとも性能が向上したことを指します。

エンドツーエンドなニューラルネットワークにより，次のことが実現しました。

- ニューラルネットワークの層ごとに，特徴抽出が行われ，抽出された特徴がより高次の層へと伝達される
- ニューラルネットワークの各層では，比較的単純な特徴から次第に複雑な特徴へと段階的に変化する
- 高次層にみられる特徴は低次層の特徴より大域的，普遍的である
- 高次層のニューロンは，低次層で抽出された特徴を共有している

このことを簡単に説明してみます。

我々人間は，外界を認識するために必要な計算を，生物種としての発生の過程と，個人の発達を通しての経験に基づく認識システムを保持していると見ることができます。
従って我々の視覚認識には化石時代に始まる光の受容器としての眼の進化の歴史と発達を通じた個人の視覚経験が反映された結果でもあります。
人工知能の目標は，この複雑な特徴検出過程をどうやったらコンピュータが獲得できるかということでもあります。
外界を認識するために今日まで考案されてきたモデル（例えば，ニューラルネットワークサポートベクターマシンなどは）は複雑です。
ですがモデルを訓練するための学習方法はそれほど難しくありません。
この意味で画像認識課題が正しく動作するためのポイントは，認識システムが問題を解く事が可能なほど複雑であるかどうかではなく，十分に複雑が視覚環境，すなわち画像認識の場合，外部の艦橋を反映するために十分な量の像データを容易すことができるか否かにあります。
今日の CNN による画像認識性能の向上は，簡単な計算方法を用いて複雑な外部環境に適応できる認識システムを構築する方法が確立したからであると言うことが可能です。

下図 <!--[fig:2012Ng_01](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"} -->
に画像処理の例を挙げました。

<!--
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_01.png" style="width:94%">
</center>
-->

<!--図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"}
-->
<!--
<center>
<img src='https://komazawa-deep-learning.github.io/assets/2013LeCun-tutorial-icml_15.svg' style="width:84%"></br>
**LeCun (2013) より**
</center>
-->

<center>
<img src='https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI.svg' style="width:84%"></br>
**Goodfellow et al. (2017) Fig.1 を改変**
</center>


<!--では入力画像がネコであるか否かを判断する画像認識であるとしました。-->
我々は<!--ネコの--> 画像をほぼ瞬時に判断できます。
ですが画像認識の難しさは，入力画像が上図 <!--[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref"
reference="fig:2012Ng_01"}-->
に示されているように入力信号の数字の集まりでしか無いことです。
このようなデータを何度も経験することでネコを識別できるようにする必要があります。
<!-- [[fig:2012Ng_01]]{#fig:2012Ng_01 label="fig:2012Ng_01"}-->
<!--図[[fig:2012Ng_01]](#fig:2012Ng_01){reference-type="ref" reference="fig:2012Ng_01"}-->
<!--に示したように-->コンピュータに入力される画像は数字の塊に過ぎません。

状況ごとにとるべき操作を命令として逐一コンピュータに与える指示する手順の集まりのことをコンピュータプログラムと呼びます。人間がコンピュータに与えることができる操作や命令によって画像認識システムを作る場合，命令そのものが膨大になったり，そもそも説明することが難しかったりします。
例を挙げれば，お母さんを思い浮かべてくださいと言われれば誰でも，それぞれ異なるイメージであれ思い浮かべることができます。また，提示された画像が自分の母親のものであるか，別の女性であるかの判断は人間であれば簡単です。ところがコンピュータには難しい課題となります。
加えて母親の特徴をコンピュータに理解できる命令としてプログラムすることも難しい課題です。つまり自分の母親の特徴を曖昧な言葉でなく明確に説明するとなるととても難しい課題となります。
というのは，女性の顔写真であればどの写真も似ていると言えるからです。顔の造形や輪郭，髪の位置などはどの画像も類似していることでしょう。ところがコンピュータにはこの似ている，似ていいないの区別が難しいのです。

加えて，同一ネコの画像であっても，被写体の向き視線の方向や光源の位置や撮影条件が異なれば画像としては異なります。
下図に示したように入力画像の中の特定の値だけを調べてみても，入力画像がネコであ
る，そうではないかを判断することは難しい課題になります。

<!--[fig:2012Ng_02](#fig:2012Ng_02){reference-type="ref"
reference="fig:2012Ng_02"}
-->
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_02.png" style="width:94%">
</center>

<!--[fig:2012Ng_02]{#fig:2012Ng_02 label="fig:2012Ng_02"}-->

現在の画像認識では，特定の画素の情報に依存せずに，入力画像が持っている特徴
をとらえるように設計されます。たとえば，ネコを認識するために必要ことは，ネ
コに特徴的な「ネコ目」や「ネコ足」を検出することであると考えます。入力画像
から，ネコの持つ特徴を抽出することができれば，それらの特徴を持っている入力
画像はネコであると判断して良いことになります(下図
<!--[[fig:2012Ng_03]](#fig:2012Ng_03){reference-type="ref"
reference="fig:2012Ng_03"}-->)。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_03.png" style="width:94%">
</center>

<!-- [[fig:2012Ng_03]]{#fig:2012Ng_03 label="fig:2012Ng_03"}-->

下図
<!--[[fig:2013LeCun_9]](#fig:2013LeCun_9){reference-type="ref"
reference="fig:2013LeCun_9"} -->
は，音声認識と画像認識の両分野において CNN が用いられる以前の従来手法 をまとめたものです。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2013LeCun-tutorial-icml_9.png" style="width:94%">
</center>
<!--[[fig:2013LeCun_9]]{#fig:2013LeCun_9 label="fig:2013LeCun_9"}-->
上図<!--[[fig:2013LeCun_9]](#fig:2013LeCun_9){reference-type="ref"
reference="fig:2013LeCun_9"} -->
のような従来手法に対して，CNN
ではエンドツーエンドな特徴抽出を多層多段に重ねることによって複雑な特徴を抽出(検出)しようとしています
(下図<!-- [[fig:2013LeCun_10]](#fig:2013LeCun_10){reference-type="ref"
reference="fig:2013LeCun_10"}-->)。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2013LeCun-tutorial-icml_10.png" style="width:94%">
</center>
<!--
[fig:2013LeCun_10]{#fig:2013LeCun_10 label="fig:2013LeCun_10"}
-->

コンピュータにはネコ目特徴検出器，ネコ足特徴検出器は備わっていません。そこで画像認識研究では，画像の統計的性質に基づいて特徴検出器を算出する方法を探す努力が行われてきました。
しかし，コンピュータにネコ目特徴やネコ足特徴を教えるは容易なことではありません。
このことは画像処理の分野だけに限りません，音声認識でも言語情報処理でもそれぞれの特徴器を一つ一つ定義し，チューニングするのは時間がかかり，専門的な知識も必要で困難な作業でした。

<!--
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_05.png" style="width:94%">
</center>
-->
<!--[fig:2012Ng_05]{#fig:2012Ng_05 label="fig:2012Ng_05"}-->

<!--
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_04.png" style="width:94%">
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_06.png" style="width:94%">
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_07.png" style="width:94%">
<img src="https://komazawa-deep-learning.github.io/assets/2012Ng_ML_and_AI_08.png" style="width:94%">
-->


## 領域切り出し

<div align="center">
<img src="https://komazawa-deep-learning.github.io/assets/HistoryOfObjectRecognition_head.svg" style="wdith:94%">
</div>

## 画像処理と言語処理との融合

<center>
<img src='https://komazawa-deep-learning.github.io/assets/2015Xu_Show_Attend_and_Tell.svg' style="width:84%"></br>
</center>



まとめると，1950 年代後半以来:固定的，手工芸的特徴抽出器と学習可能な分類器を用いた認識システムを作ることが試みられてきたといえます。
これに対して CNN が主流となった現在はエンドツーエンドで学習可能な特徴抽出器を多数重ね合わせることで性能が向上しました。

夢のような話が続きましたが，本節の最後に逆に CNN は簡単に騙すことができる例
を挙げておきます<!--(図[fig:GAN](#fig:GAN){reference-type="ref"
reference="fig:GAN"})-->。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Goodfellow_Fig1.png" style="width:94%">
</center>

図では，左の画像が入力画像で，CNN は確信度 57.7パーセントでパンダである認識しました。 ところがこの画像に 0.007だけ意味のない画像(図中央)を加えるた画像(図右)を CNN は 99.3パーセントの確信度でテナガザル
(gibbon)と判断しました。この例はここでは詳しく触れることはしませんが **敵対的学習**と呼ぶ訓練手法を説明する際に用いられた例です <!--[@2014Goodfellow_GANHarness]-->。

この例からも分かることは以下のようにまとめられるでしょう。
すなわち，人間の脳を模したニューラルネットワークである CNN が大規模化像認識チャレンジにおいて人間の認識性能を越えたと報道されました。
ですが，人間の視覚認識を完全に実現したと考えるのは早計で，解くべき課題は未だ多数あるということです。
この状況は，音声認識や言語情報処理でも同様であると言えます。



# 認知計算論的神経科学 Cognitive computational neuroscience

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2018Kriegeskorte_Fig2.jpg" style="width:84%"><br>
**Kriegeskorte and Doglas (2018) Fig. 2より**
</center>

脳の機能を理解することはどういうことか？ 認知計算神経科学の目的は実世界の認知課題を遂行可能で，生物学的妥当性を持つ計算モデルを用いて動物や人間の神経活動と行動の多くの観測結果を説明することである。
歴史的には各分野（円）はこれら課題の回問題（白ラベル）に取り組んできた。<!--認知計算神経科学は、同時にすべての基準を満たすよう努めるています。-->
<!--
Figure 2 | What does it mean to understand how the brain works? The goal of cognitive computational neuroscience is to explain rich measurements of neuronal activity and behavior in animals and humans by means of biologically plausible computational models that perform real-world cognitive tasks. Historically, each of the disciplines (circles) has tackled a subset of these challenges (white labels). Cognitive computational neuroscience strives to meet all the criteria simultaneously.
-->


<!--
---

###  表記

<b>深層学習</b> 文献では MLP: Multi-Layered Perceptrons と表記される場合が多い

- \(x\) をニューロンの活性値とし，入力層から数えて \(i\) 層目の \(j\) 番目のニューロンの活性値を \(x_i^{(j)}\) と表記する。
- 入力層は \(0\) 層目とみなせば \(x_1^{(0)}\)　は入力データのうち \(1\) 番目のニューロンに与える数値を表している。
- 下付き添字は番号付けされたニューロンの指す場合に使われる。
- 上付き添字はカッコを付けて階層的ニューラルネットワークの層を表すとする。このとき，べき乗と区別するためにカッコをつける。\(x_1^{2}\) は第 \(2\) 層の \(1\) 番目のニューロンの出力値を表している。
- 活性化関数は多値入力一出力 (many to one) である場合が多い。プログラミング言語に多く見られる関数の定義と同じように引数が複数個存在し，戻り値が一つである関数と酷似している。
- 活性化関数への引数は前層のニューロンとの結びつきを表現する結合強度 connection legth(結合係数 connection coefficient，重み weight とも呼ぶ)とによる荷重総和である。
- 荷重総和は総和記号 \(\sum\) を用いることで \(\sum_{i=1}^{m}w_ix_i\) と表記される。\(m\) は関与する下位層のニューロンの総数である。
- 各ニューロンの出力値を計算するためには荷重総和とバイアス bias を合わせてから，非線形写像 \(f\) によって変換されるので
\(f\left(\sum_{i}w_ix_i+b\right)\) と表記される。

---

- 写像 $f$ を **活性化関数 activation function** とする
- 第 $k$ 層の $i$ 番目のニューロンの出力値 $x_i^{(k)}$ は次のように表記できる：
$$ x_i^{(k)} = f\left(\sum_{i=1}^{m}w_{i}x_{i}^{(k-1)}+b_{i}^{(k)}\right) $$
- バイアス項は常に $1$ を出力する特別な入力値であるとみなして表記を簡単にするために $0$ 番目の入力として扱うと：
$$ x_i^{(k)} = f\left(\sum_{i=0}^{m}w_{i}x_{i}^{(k-1)}+b_{i}^{(k)}\right) $$
とする表記も行われてきた。
- C や Python のメモリ配置ではメモリ先頭番地を $0$ とする場合が多いので繰り返しは $n$ 回の繰り返しを $0$　から $n-1$ 回までのカウンタとする場合が多いので上記のようにバイアス項を $0$ 番目の要素として組み込むより，別の項として扱うことが最近は多いようである。()
- ３層パーセプトロン(実際には入力層は値をセットするだけなので演算は２層しか行われない)を表記すれば

\begin{align}
y_i=x_i^{(2)}&=f\left(\sum_{j}w_{j}^{(1)}x_{j}^{(1)}+b_{i}^{(2)}\right)\\
&=f\left(\sum_{j}w_{j}^{(1)}\left(f\left(\sum_{k}w_{k}^{(0)}x_{k}^{(0)}+b_{j}\right)\right)+b_{i}^{(2)}\right)\\
\end{align}

```python
# Define the neural network function y = x * w + b
def total_input(x, w, b):
    return x * w + b

# Define the cost function
def cost(y, t): 
    return ((t - y)**2).sum()
```

-->

<!--
---

### 出力関数
学習可能な重み係数とバイアスがを持つニューロンで \(f=\left(\sum_i w_ix_i + b\right)\) 構成される。<br>
複数入力，１出力の非線形出力関数を持つ。

出力関数としては：<br>

- $ReLU\left(x\right)=\max\left(0,x\right)$
- $ReLU6\left(x\right)=\min\left(\max\left(0,x\right),6\right)$
- $crelu\left(x\right)=\left(\left[x\right]_{+},\left[-x\right]_{-}\right)$
- \[elu\left(x\right)=\left\{
\begin{align}
x       & \hspace{3em}if x>0\\
e^{x}-1 & \hspace{3em}otherwise
\end{align}\right.\]
- $softplus\left(x\right)=\log\left(1+e^{x}\right)$
- $softsign\left(x\right)=\frac{x}{\Vert x\Vert+1}$

- ドロップアウト
- バイアス付加

- $\sigma(x)=\frac{1}{1+e^{-x}}$
- $\phi\left(x\right)=\tanh\left(x\right)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$

がある
-->


---


<!-- [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)より -->

## CNN の詳細

通常のニューラルネットワークでは，直下層のニューロンとそのすぐ上の層の全ニューロンと結合を有する。一方 CNN ではその結合が部分的である。
各ニューロンは多入力一出力の信号変換機とみなすことができ，活性化関数に非線形な関数を用いる点は通常のニューラルネットワークと同様。

画像処理を考える場合，典型的には一枚の入力静止画画像は 3 次元データである。次元は幅w，高さh，奥行きd であり，入力画像では奥行きが３次元，すなわち赤緑青の三原色。出力ニューロンへの入力は局所結合から小領域に限局される。

### 1. CNNの構成

CNN は以下のいずれかの層から構成される：

1. **畳込み層**
2. **プーリング層**
3. **完全結合層**（通常のニューラルネットワークと正確に同じもの，CNN では最終 1 層または最終 1,2 層に用いる）

入力信号はパラメータの値が異なる活性化関数によって非線形変換される。
畳込み層とプーリング層と複数積み重ねることで多層化を実現し，深層ニューラルネットワークとなる。


---

#### 例：
- <!--CNNの最も単純な場合。--> 画像データを出力信号へ変換
- 各層は別々の役割（畳込み，全結合，ReLU, プーリング）
- 入力信号は 3 次元データ，出力信号も 3 次元データ
- 学習すべきパラメータを持つ層は畳込み層，全結合層
- 学習すべきパラメータを持たない層は ReLU 層とプーリング層
- ハイパーパラメータを持つ層は畳込み層, 全結合層, プーリング層
- ハイパーパラメータを持たない層は ReLU層

---

<center>
  <img src="https://komazawa-deep-learning.github.io/assets/cnn/convnet.jpeg" width="94%">
  <!--
  <div class="figcaption">
  -->
  <div>
  CNN アーキテクチャ: 入力層は生画像の画素値(左)を格納、最後層は分類確率(右)を出力。処理経路に沿った活性の各ボリュームは列として示されている。3Dボリュームを視覚化することは難しいため、各ボリュームのスライスを行ごとに配置してある。最終層のボリュームは各クラスのスコアを保持するが、ソートされた上位5スコアだけを視覚化し、それぞれのラベルを印刷してある。
  <!--
  <a href="http://cs231n.stanford.edu/">ウェブベースのデモ</a>は、ウェブサイトのヘッダーに表示されています。ここに示されているアーキテクチャは、あとで説明する小さなVGG Netです。
  -->
</div>
</center>

<!--
The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full <a href="http://cs231n.stanford.edu/">web-based demo</a> is shown in the header of our website. The architecture shown here is a tiny VGG Net, which we will discuss later.
-->
<!--
We now describe the individual layers and the details of their hyperparameters and their connectivities.
-->

---

<!--
*Example Architecture: Overview*. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:

*アーキテクチャ例* CIFAR-10の CNN は [入力層-畳込み層-ReLLU層-プーリング層-全結合層] という構成である。
-->

- 入力層[32x32x3]: 信号は画像の生データ（画素値）幅w(32)，高さh(32)、色チャネル3(R, G, B)
- 畳込み層: 下位層の限局された小領域のニューロンの出力の荷重付き総和を計算(内積，ドット積）。12個のフィルタを使用すると[32x32x12]となる。
- ReLU層の活性化関数は ReLU (Recutified Linear Unit) $max(0,x)$<!--入力範囲は変更されない([32x32x12])。-->
- プーリング層: 空間次元（幅,高さ）に沿ってダウンサンプリングを実行。[16x16x12]のようになる。
- 全結合層はクラスに属する確率を計算: 10 の数字のそれぞれが CIFAR-10 の 10 カテゴリーの分類確率に対応するサイズ[1x1x10]に変換。通常のニューラルネットワーク同様、全結合層のニューロンは前層の全ニューロンと結合する。

<!--
In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don't. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image. 
-->

CNN は元画像（入力層）から分類確率（出力層）へ変換。学習すべきパラメータを持つ層（畳込み層，全結合層）とパラメータを持たない層（ReLU層）が存在。畳込み層と全結合層のパラメータは勾配降下法で訓練

---

### 2. 畳込層

<center>
<!--<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%">-->
<img src="https://komazawa-deep-learning.github.io/assets/Neocognitron.svg" style="width:74%">
<img src="https://komazawa-deep-learning.github.io/assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br>

<img src="https://komazawa-deep-learning.github.io/assets/1998LeCun_Fig2_CNN.svg" style="width:94%"><br>
LeNet5 (LeCun, 1998)<br>

<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNet_2.svg" style="width:94%"><br>
アレックスネット(Krizensky et al. 2012)<br>
<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNet_Result.svg" style="width:94%">
アレックスネットの出力例(Krizensky et al. 2012)<br>
</center>

- 畳込み層のパラメータは学習可能なフィルタの組
- 全フィルタは空間的に（幅と高さに沿って）小さくなる
- フィルタは入力信号の深さと同一
- 第1層のフィルタサイズは例えば 5×5×3（5 画素分の幅，高さ，と深さ 3（３原色の色チャンネル）
- 各層の順方向の計算は入力信号の幅と高さに沿って各フィルタを水平または垂直方向へスライド
- フィルタの各値と入力信号の特定の位置の信号との内積（ドット積）。
- 入力信号に沿って水平，垂直方向にフィルタをスライド
- 各空間位置でフィルタの応答を定める 2 次元の活性化地図が生成される
- 学習の結果獲得されるフィルタの形状には、方位検出器，色ブロッブ，生理学的には視覚野のニューロンの応答特性に類似
- 上位層のフィルタには複雑な視覚パタンに対応する表象が獲得される
- 各畳込み層全体では学習すべき入力信号をすべて網羅するフィルタの集合が形成される
- 各フィルタは相異なる 2 次元の活性化地図を形成
- 各フィルタの応答特性とみなすことが可能な活性化地図
- フィルタの奥行き次元に沿って荷重総和を計算し、出力信号を生成

---

- [畳込み演算のデモ](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_scipy_convolve.ipynb)
- [フィルタ演算のデモ 2019komazawa_kitten_filters_demo.ipynb](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_kitten_filters_demo.ipynb)
- [畳込み演算のデモビデオ](/conv-demo/)

<center>
<video controls loop>
  <source src="../assets/2019si_conv-demo.mp4" type="video/mp4" style="width:84%">
</video>
</center>


- ビオラ，ジョーンズアルゴリズム (2001)，富士フィルムによる実装は2006年頃

- [実際にどのようなフィルタが構成されたのか](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/How_to_Visualize_Filters_and_Feature_Maps_in_Convolutional_Neural_Networks.ipynb)

---

**局所結合**: 画像のような高次元の入力を処理する場合，下位層の全ニューロンと上位層の全ニューロンとを接続することは **責任割当問題回避** の観点からもパラメータ数の増加は現実的ではない。<br>
代わりに各ニューロンを入力ボリュームのローカル領域のみに接続。空間的領域はニューロンの **受容野** と呼ばれるハイパーパラメータ（フィルタサイズとも言う）。<font color="blue">深さ次元に沿った接続性＝入力層の深さ次元</font>。
空間次元（幅と高さ）と深さ次元をどのように扱うかにより，この非対称性を再び強調することが重要です。ニューロン間の結合は空間次元（幅と高さ）にそって限局的。入力次元の深さ全体を常にカバーする。

- 例1: 入力層のサイズが[32x32x3]（RGB CIFAR-10画像データセットなど）であれば受容野（フィルタサイズ）が 5x5 とすれば，畳込み層内の各ニューロンは入力層の [5x5x3] 小領域への結合係数を持つ。各小領域毎に 5x5x3=75 の重み係数と 1 つのバイアス項が必要である。深さ次元に沿った上層のニューロンから下位層のニューロンへの結合は下位層の深さ(色チャンネル数)と等しく 3 である。

- 例2: 入力ボリュームのサイズが[16x16x20]であるとすると 3x3 の受容野サイズで畳込層の全ニューロンの合計は 3x3x20=180 接続。接続性は空間的に局在する（3x3）が，入力深度（20）に沿っては完全結合
    
**空間配置**: 出力層ニューロンの数と配置については 3 つのハイパーパラメータで出力ニューロン数が定まる。

1. **深さ数(フィルタ数)**
2. **ストライド幅**
3. **ゼロパディング**
   
* 1 出力層ニューロン数のことを出力層の **深さ** 数と呼ぶハイパーパラメータである。深さ数とはフィルタ数（カーネル数）とも呼ばれる。第 1 畳込み層が生画像であれば，奥行き次元を構成する各ニューロンによって種々の方位を持つ線分(エッジ検出細胞)や色ブロッブのような特徴表現を獲得可能となる。入力の同じ領域を **深さ列** とするニューロン集団を **ファイバ** ともいう。

* 2 フィルタを上下左右にずらす幅を **ストライド幅** と呼ぶ。ストライド幅が 1 ならフィルタを 1 画素ずつ移動することを意味する。ストライドが 2ならフィルタは一度に 2 画素ずつジャンプさせる。ストライド幅が大きければ入力信号のサンプリング間隔が大きく広がることを意味する。ストライド幅が大きくなれば上位層のニューロン数は減少する。

* 3 入力の境界上の値をゼロで埋め込むことがある。これを **ゼロパディング** という。ゼロパディングの量はハイパーパラメータである。ゼロパディングにより出力層ニューロンの数を制御できる。下位層の空間情報を正確に保存するには入力と出力の幅，高さは同じである必要がある。

  入力層のニューロン数を $W$，上位にある畳込み層のニューロン数を $F$，とすれば出力層に必要なニューロン数 $S$ は，周辺のゼロパディング を $P$ とすれば $(W-F+2P)/S+1$ で算出できる。
  たとえば下図でストライド $1$ とゼロパディング $0$ であれば入力 $7\times7$ でフィルタサイズが $3\times3$ であれば 5x5(=S=(7-3+2x0)/1+1=5) の出力である。
  ストライド $2$ ならば $3\times 3=(S=(7-3+2x\times )/2+1=3)$ となる。

<!--
We can compute the spatial size of the output volume as a function of the input volume size ($W$), the receptive field size of the Conv Layer neurons ($F$), the stride with which they are applied ($S$), and the amount of zero padding used ($P$) on the border. You can convince yourself that the correct formula for calculating how many neurons "fit" is given by $(W - F + 2P)/S + 1$. For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more graphical example:
-->

<div align="center" style="width:94%">
  <img src="https://komazawa-deep-learning.github.io/assets/cnn/stride.jpeg"><br/>
  <div align="left" sytle="width:88%">
空間配置の例：入力空間の次元（x軸）が1つで受容野サイズ F=3 の場合，入力サイズ W=5, ゼロパディング P=1 であれば，<br>
<b>左図：</b>出力層ニューロン数は (5-3+2)/1+1=5 の出力層ニューロン数となる。ストライド数 S=1 の場合。<br>
<b>右図：</b>s=2，出力層ニューロン数 (5-3+2)/2+1=3 となる。ストライド S=3 ならばボリューム全体にきちんと収まらない場合もでてくる。数式で表現すれば  $(5-3+2)=4$ は 3 で割り切れないので、整数の値として一意に決定はできない。<br>
ニューロン結合係数は（右端に示されている）[1,0,-1]でありバイアスはゼロ。この重みはすべての黄色ニューロンで共有される。
  </div>
</div>


*ゼロパディング*: 上例では入力次元が 5，出力次元が 5 であった。
これは受容野が 3 でゼロ埋め込みを1としたためである。ゼロ埋め込みが使用されていない場合、出力ボリュームは、どれだけの数のニューロンが元の入力に「フィット」するのであろうかという理由で、空間次元がわずか 3 であったであろう。
ストライドが $S=1$ のとき、ゼロ埋め込みを $P=(F-1)/2$ に設定すると、入力ボリュームと出力ボリュームが空間的に同じサイズになる。
このようにゼロパディングを使用することは一般的である。CNNについて詳しく説明している完全な理由について説明する。

*ストライドの制約*: 空間配置ハイパーパラメータには相互の制約があることに注意。たとえば入力に $W=10$ というサイズがあり、ゼロパディングは $P=0$ ではなく、フィルタサイズは$F=3$, $(W-F+2P)/S+1=(10-3+0)/2+1=4.5$ よりストライド $S=2$ を使用することは不可能である。
すなわち整数ではなくニューロンが入力にわたってきれいにかつ対称的に "適合" しないことを示す。

*AlexNet* の論文では，第一畳込層は受容野サイズ $F=11$，ストライド$S=4$，ゼロパディングなし$P=0$。<br>
畳込層 $K=96$ の深さ $(227-11)/4+1=55$。畳込層の出力サイズは [55x55x96]。55x55x96 ニューロンは入力領域 [11x11x3] と連結。全深度列 96 個のニューロンは同じ入力領域[11×11×3]に繋がる。論文中には(224-11)/4+1 となっている。パディングについての記載はない。

**パラメータ共有** パラメータ数を制御するために畳み込み層で使用される。上記の実世界の例を使用すると、最初の畳故意層には 55x55x96=290,400のニューロンがあり、それぞれ 11x11x3=363 の重みと1のバイアスがある。これにより CNN 単独の第 1 層に最大 290400x364=105,705,600 のパラメータが追加される。<!--この数は非常に高いです。-->

<!--
**Parameter Sharing.** Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55\*55\*96 = 290,400 neurons in the first Conv Layer, and each has 11\*11\*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.
-->

<!--
It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a **depth slice** (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96\*11\*11\*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55\*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.
-->

**パラメータ共有** により学習すべきパラメータ数が減少する。
例えば [55x55x96] のフィルタでは深さ次元は 96 個のニューロンで，各深さで同じ結合係数を使うことにすれば
ユニークな結合係数は計 96x11x11x3=34,848 となるので総パラメータ数は 34,944 となる(バイアス項 +96)。各深さで全ニューロン(55x55)は同じパラメータを使用する。逆伝播での学習では，全ニューロンの全結合係数の勾配を計算する必要がある。各勾配は各深さごとに加算され 1 つの深さあたり一つの結合係数集合を用いる。

ある深さの全ニューロンが同じ重み係数ベクトルを共有する場合，畳込み層の順方向パスは各深さスライス内で入力ボリュームとのニューロンの重みの **畳み込み** として計算できることに注意。結合荷重係数集合のことを **フィルタ** または **カーネル** と呼ぶ。入力信号との間で畳込み演算を行うこととなる。

<!--
Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a **convolution** of the neuron's weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a **filter** (or a **kernel**), that is convolved with the input.
-->


<div align="center" style="width:88%">
  <img src="https://komazawa-deep-learning.github.io/assets/cnn/weights.jpeg" style="width:94%"><br/>
<div align="left" style="width:88%">
AlexNet の学習済フィルタ例: 図の 96 個のフィルタは サイズ [11x11x3]。
それぞれが 1 つの深さ内の $55\times55$ ニューロンで共有されている。
画像の任意の位置で水平エッジ検出が必要な場合，画像の並進不変構造 translationall-invariant structure 仮定により画像中の他の場所でも有効である。
畳込み層の出力ニューロン数は $55\times55$ 個の異なる位置すべてで水平エッジの検出を再学習する必要はない。
<!--
Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.
-->
</div>
</div>


### 3. プーリング層

CNN では，連続する畳込み層間にプーリング層を挿入するのが一般的。プーリング層の役割は，空間次元の大きさに減少させることである。パラメータ，すなわち計算量を減らし，過学習を制御できる。プーリング層は入力の各深さ毎に独立して動作する。最大値のみをとり他の値を捨てることを **マックスプーリング** と呼ぶ。サイズが 2x2 のフィルタによるプーリング層では，入力の深さごとに $2$ つのダウンサンプルを適用し、幅と高さに沿って2ずつ増やして75％の情報を破棄する。この場合 4 つの数値のうち最大値を採用することになる。

<!--
It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:
-->

<div align="center">
  <img src="https://komazawa-deep-learning.github.io/assets/cnn/maxpool.jpeg" width="74%"><br/>
<div align="left">
一般的なダウンサンプリング演算は <b>マックスプーリング</b> である。図では ストライド 2 すなわち 4 つの数値の中の最大値
<!--
The most common downsampling operation is max, giving rise to <b>max pooling</b>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).
-->
</div>
</div>

**平均プーリング**. マックスプーリングではなく *L2正則化プーリング* を行う場合もある。平均プーリングは歴史的な意味あいがあるがマックスプーリングの方が性能が良いとの報告がある。ある画像位置には物理的に一つの値だけが存在するという視覚情報処理が仮定すべき外界の物理的制約を反映していると文学的に解釈することも可能である。

<center>
<div>
  <img src="https://komazawa-deep-learning.github.io/assets/cnn/pool.jpeg" width="74%"><br/>
<div>
プーリング層では，入力層ニューロン数の各深さについて空間的ダウンサンプリングを行う。この例は サイズ[224x224x64]の入力層ニューロン数がフィルタサイズ 2 でプールされ，サイズ 2 の出力ニューロン数 [112x112x64] は 2 倍である。奥行き数が保持されている。
<!--
Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved.    
-->
</div>
</div>
</center>

### 4. 全結合層

全結合層のニューロンは、通常のニューラルネットワークと同じ<br>
前層の全ニューロンと結合を持つ<br>

### 5. CNN アーキテクチャ

1. 畳込層
2. プーリング層
3. 全結合層

層は以上 3 種類が一般的。

### 6. CNN の層構造

入力層 $\rightarrow$ [[畳込層 $\rightarrow$ ReLU]$\times N\rightarrow$ プーリング(?)]$\times$ M $\rightarrow$ [全結合層 $\rightarrow$ ReLU] $\times$ K $\rightarrow$ 全結合層

最近のトレンドとしては大きなフィルタより小さなフィルタが好まれる傾向にある。<br>
[3x3] が好まれる理由はど真ん中がある奇関数を暗黙に仮定しているためだと思われる（浅川の妄想）。
その代わり多段にすれば [3x3] が２層で ［5x5]，３層で[7x7]の受容野を形成できるから受容野の広さを層の深さとして実装しているとも解釈できる。１層で[7x7]の受容野より３層で[7x7]の受容野を実現した方が the simpler, the better の原則に沿っているとも（文学的）解釈が可能である（またしても浅川妄想）。

バックプロパゲーションの計算時に広い受容野を作るより層を分けた方が GPU のメモリに乗せやすいと言う計算上の利点もある。


<div align="center">
<img src="https://komazawa-deep-learning.github.io/assets/2020Lillicrap_fig7.svg" style="width:94%"><br/>
Lillicrap et al. (2020) Fig. 7 を改変
</div>

- [video by Geoffrey Hinton](https://www.youtube.com/watch?v=mlaLLQofmR8){:target="_blank"} on the softmax function
- [A Friendly Introduction to Cross Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/){:target="_blank"} by Rob DiPietro
- [How to Implement a Neural Network Intermezzo 2](https://peterroelants.github.io/posts/cross-entropy-softmax/){:target="_blank"} by Peter Roelants
- [画像分類の基礎](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb){:target="_blank"}

- [Introduction to Convolutional Neural Networks](https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf){:target="_blank"} by Jianxin Wu 
- Yann LeCun’s original article, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf){:target="_blank"}
- [The Nine Deep Learning Papers You Need to Know About](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html){:target="_blank"} (Understanding CNNs part 3) by Adit Deshpande


## 前回の復習
=======
## 用語
>>>>>>> 4a7765a869c2d07b7a5e3b661e5f9c7368508eef

- パーセプトロン perceptron
- 正則化 regularization
- 誤差逆伝播法

### 補足的用語

- 勾配降下法
- バッチ学習，オンライ学習，ミニバッチ，確率的勾配降下法，
- ソフトマックス関数，交差エントロピー誤差

<!-- ## 前回の復習

- **ディープラーニング** 深層学習, deep learning とは **信用割当問題** credit assignment problem を回避するために **畳込み** convolution 演算(処理)を用いて多層にしたニューラルネットワークのこと
	- 畳込み演算って何？
	- カーネルサイズ
	- プーリング
	- ストライド
	- パディング-->
 <!-- ハイパーパラメータとしてのカーネル(特徴)サイズ，ストライド，パディング
\[
\left[\text{畳込み}\left(\ge1\right) \rightarrow \text{プーリング}\left(\ge0\right)\right]
\times \left(\ge1\right)\rightarrow\text{全結合層}\left(\ge1\right)
\]
-->


# 本日の目標
- 最小二乗法から誤差逆伝播法へ。誤差関数，損失関数，目的関数，勾配降下法 (ブラインド ハイカー アナロジー)。 信用割当問題。勾配消失問題。
- 標準正則化理論。制約付き最適化。変分原理。[オイラー=ラグランジェ方程式](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%82%A4%E3%83%A9%E3%83%BC%EF%BC%9D%E3%83%A9%E3%82%B0%E3%83%A9%E3%83%B3%E3%82%B8%E3%83%A5%E6%96%B9%E7%A8%8B%E5%BC%8F){:target="_blank"} 
- 画像切り分け

<!--- 画像切り分け
- ニューラルネットワーク，機械学習の分野で頻繁に用いられている性能向上のための技法を紹介
- この授業の目標は深層学習の心理学的な意味付けを考えることであるので，紹介する上記の技法は無関係のように思われる
- だがそうではないことを理解することが目的
-->

## 多分本日は行わない実習

<!-- - [kminst による CNN](https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019keras_kmnist_demo.ipynb){:target="_blank"} -->
- [転移学習](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529transfer_learning.ipynb){:target="_blank"}

- [MaskR-CNN によるインスタンス画像領域分割](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529Mask_R_CNN_Image_Segmentation.ipynb){:target="_blank"}
- [Deeplab のデモによる画像の意味的画像切り分け](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529Semantic_segmantation_DeepLab_Demo.ipynb){:target="_blank"}


<!--
# デモ

- [グーグルによるニューラルネットワークの遊び場 (プレイグランド)](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"}

- [Scavenger hunt](https://emojiscavengerhunt.withgoogle.com/){target="_blank"}
- [https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/){target="_blank"}
- [姿勢推定デモ](https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html){target="_blank"}
- [Style-based GAN](https://youtu.be/kSLJriaOumA)
- [foodly による唐揚げもりつけロボット](https://rt-net.jp/service/foodly/), [YouTube](https://youtu.be/KiT_DrDjdDE)
-->


## 案件紹介 夏季インターン

<!-- 機械学習系だと自然言語処理、Computer Vision、構造化データ（テーブルデータ）の3種があり、
興味があるものに応募するイメージです(メール最下部)。
-->

```
1. 自然言語処理：

SNSデータを利用したドメイン判定モデルの開発
SNSデータを利用したドメイン転用化な観点別極性分析モデルの開発

2. Computer Vision

異常検知に対する事前学習モデル検討
自然災害防止のため画像による異常検知

3. 構造化データ

顧客のビジネス要件にマッチしたモデル構築検討
構造化案件におけるEDA検討

顧客のビジネスに貢献できるモデル可視化手法の活用方法検討
```

## 勾配降下法 Gradient descent methods

<center>
<img src="https://miro.medium.com/max/814/1*kmmjFBP5vRkKOM1SP4URpA.png" style="width:68%"><br/>

出典: [The Complete Beginner’s Guide to Deep Learning: Artificial Neural Networks](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)
</center>


### コスト関数

- コスト関数 cost function
- 損失関数 loss function
- 誤差関数 error function
- 目的関数 objective function

$$
p(\mathbf{y}\vert \mathbf{x};\mathbf{\theta})
$$

**最小二乗誤差**（下式）, あるいは**負の対数尤度** negative log likelifood ($-\log(x)$) など

$$
J(\mathbf{\theta})=\frac{1}{2}\mathbb{E}_{\mathbf{x,y}\sim\hat{p}_{data}}
\left\|\mathbf{y}-f(\mathbf{x};\mathbf{\theta})\right\|^2+\mbox{const.}
$$


### 交差エントロピー損失関数
ニューラルネットワークや機械学習において，予測すべき値が2値化された量，たとえば真偽値真であれば $1$ をとり，偽
であれば $0$ であったり，確率である場合には，最小化すべき目標関数(正則化項を含めて損失関数でもよい)は平均二乗
誤差 Mean Square Errors ではなく **交差エントロピー cross-entropy 損失**，あるいは交差エントロピー誤差と呼ぶ関
数が用いられる。

自乗誤差に比べて交差エントロピーを用いると学習が高速化される。
<!-- 理由は以下で説明する-->
文献的にはニューラルネットワークに交差エントロピーが導入されたのは Hinton(1989) など

交差エントロピーは次式で表される:

$$
\mathcal{L}=-t\log(y)-(1-t)\log(1-y),
$$<!-- {#eq:def-cross-entropy}-->

ここで $t$ は教師信号すなわち $1$ または $0$ をとり，$y$ はニューラルネットワークから出力された予測値。

上式は （確率とみなせる）出力 $y$ が $t$ 回起こった と解釈できる $y^t$ このときの $t$ の値はは $0$ か $1$ しか取らないので，
上式右辺は，もし $t$ が 1 であれば右辺第一項を計算し，$t$ が $0$ であれば 右辺第2項を計算することになる。

右辺第一項と右辺第二項とを別曲線として描いた下図。

<center>
<img src="/assets/cross-entropy.svg" style="width:39%"><br/>
<!--      https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/e69ca10d8b2a4e9f34943fc302e5eafc7dbd934d/assets/cross-entropy.svg-->
交差エントロピーのグラフ
</center>

ここで対数 $\log$ の底は $e$ や $2$ が用いられる。

## エントロピー
エントロピーには熱力学エントロピーと情報論的エントロピーと $2$ 種類存在するがどちらも同じ形式をしている。情報
論的には平均エントロピー $H$ を以下のように定義する

$$
H[X]=-\sum_i X_i\log(X_i)
$$ 

上式 は 平均情報量 [@Shannon1948] とも呼ばれる。連続変量の場合には総和記号 $\sum$ が積分記号 $\int$ となって 
$$
H[x]=-\int x\log(x)\;dx
$$

<center>
<img src="/assets/shannon-entropy.svg" style="width:29%"><br/>
シャノンのエントロピー
</center>

### まとめ

- コスト関数，損失関数，誤差関数，目的関数，はほぼ同じような意味で用いられる
- 代表的なコスト関数として，最小自乗誤差，交差エントロピー誤差，などがある
- 出力が確率で与えられるような問題，たとえば，分類問題などでは交差エントロピー誤差関数が用いられる



## 一般化とオーバーフィッティング，アンダーフィッティング
<!--Generalization, Overfitting and Under-fitting-->

- データへの当てはまりが良いことが良いモデルではない
- 未知のデータに対してどれほど当てはまるのかがモデルの性能を決める
<!--
* 訓練データ training data 実際に学習に用いたデータ
* テストデータ test data 未知のデータ，訓練時には使用していないデータ
-->
* オーバーフィッティング 訓練データへの過剰適合
* アンダーフィッティング 訓練データを十分に学習できない場合
<!--
* データ数(*小*) アンダーフィットする可能性**大**
-->

<center>
<img src="/assets/04_07underOverFittings.svg" style="width:59%"><br/>
</center>

- [多項回帰による過剰適合，デモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020Sight_Visit_polynomilal_fittings_demo.ipynb)

<!--
It's not a good idea to test a machine learning model on a dataset which we used to train it, since it won't g
ive any indication of how well our model performs on unseen data. The ability to perform well on unseen data i
s called generalization, and is the desirable characteristic we want in a model.
When a model performs well on training data (the data on which the algorithm was trained) but does not perform
 well on test data (new or unseen data), we say that it has overfit the training data or that the model is ove
rfitting. This happens because the model learns the noise present in the training data as if it was a reliable
 pattern. 
Conversely, when a model does not perform well on training data (i.e. it fails to capture patterns present in 
the training data) as well as unseen data then it is said to be under-fitting. That is, the model is unable to
 capture patterns present in the training data. 
A smaller dataset can significantly increase the chance of overfitting. This is because it is much tougher to 
separate reliable patterns from noise when the dataset is small. [1]
Examples of overfitting and under-fitting
-->

$y = w_0 + w_1 x$, 

$y = w_0 + w_1 x_1 + w_2 x_2$, 

$y = w_0 + w_1 x_1 +\cdots + x_nx_n$


<!--
Suppose we have the following dataset (red points in the figure), where we have only one input variable x and 
one output variable y. 

If we fit y = w0 + w1x to the above dataset, we get the straight line fit as shown above. Note that this is no
t a good fit since it is quite far from many data points. This is an example of under-fitting. 

Now, if we add another feature x2 and fit y = w0 + w1x1 + w2x2 then we'll get a curve fit as shown above. (Sid
e note: This is still a linear model. x2 is a feature, i.e. input. The weights are w's and they are interactin
g linearly with the features x and x2. The curve we are fitting is a quadratic curve). As you can see, this is
 slightly better since it passes much closer to the data points above. 

If we keep adding more features we'll get a curve that is more and more complex and that passes through more a
nd more data points. Above figure shows an example. This is an example of overfitting. In this case, we are pe
rforming polynomial fitting y = w0 + w1x1 + w2x2 + ... + wdxd.
Even though the fitted curve passes through almost all points, it won't perform well on unseen data. 
-->

### オーバーフィッティングの回避
<!-- Strategies to Avoid Overfitting

One way to avoid overfitting it to collect more data. However, that is not always feasible. Below are some oth
er strategies to overcome the problem of overfitting - regularization and cross-validation. -->
### 正則化 Regularization

モデルの複雑さを調整する

<!--
In regularization, we combat overfitting by controlling the model's complexity, i.e. by introducing an additio
nal term in our cost function in-order to penalize large weights. This biases our model to be simpler, where s
impler is weights of smaller magnitude (or even zero). We want to make the weights smaller, because complex mo
dels and overfitting are characterized by large weights. Recall the mean-squared error cost function, 
J(w)=1nn∑i=1(y(xi)−yit)2
-->

### L2 正則化 リッジ回帰 
<!--Regularization or Ridge Regression-->

$$
\text{目的関数} = \text{誤差} + \lambda \left|w\right|^2
$$

<!--
In L2 regularization, a commonly used regularization technique, we add a penalty proportional to the squared m
agnitude of each weight. Our new cost function with L2 regularization is as follows:-
J(w)=1nn∑i=1(y(xi)−yit)2+λ||w2||
where, the first term is the same as in regular linear regression (without any regularization), and the second
 term is the regularization term. λ is a hyper-parameter that we choose and decides the regularization strengh. Larger values of λ imply more regularization, i.e. smaller values for the model parameters. ||w2|| is w12  w22 + ... wd2. 
-->
- L2 正則化はパラメータの絶対値が大きくなると罰則項 pernalty term として作用

<!--
L2 regularization penalizes the larger weights more (since the penalty is proportional to the weight squared).
 For example, reducing w = 10 to w = 9 has a larger effect on the penalty term (102-92) than reducing w = 3 to
 w = 2 (32-22).  
-->
### L1 正則化 Lasso 回帰 <!--Regularization or Lasso Regression-->

$$
\text{目的関数} = \text{誤差} + \lambda\left|w\right|
$$

<!--
In L1 regularization, we the penalty term is λ ||w||. That is, our cost function is:
J(w)=1nn∑i=1(y(xi)−yit)2+λ||w||
-->
<!--
An interesting property of L1 regularization is that model's parameters become sparse during optimization, i.e
. it promotes a larger number of parameters w to be zero. This is because smaller weights are equally penalize
d as larger weights, whereas in L2 regularizations, larger weights are being penalized much more. This sparse 
property is often quite useful. For example, it might help us identify which features are more important for m
aking predictions, or it might help us reduce the size of a model (the zero values don't need to be stored). 
Ordinary least square (which we saw earlier in linear regression) with L2 regularization is known as Ridge Reg
ression and with L1 regularization it is known as Lasso Regression.
Cross Validation and Validation Datasets
-->

### 正則化項

- 簡潔さ原理 simplicity principle L1
- 滑らかさ原理 smoothness principle L2
- 疎性原理 sparseness principle L0

<center>
<img src="/assets/Regularization.svg" style="width:44%"><br/>
</center>

#### 正則化項の影響

<center>
<img src="/assets/2001Hastie_p84.png" style="width:66%"><br/>
<img src="/assets/2001Hastie_p89.png" style="width:66%"><br/>
<img src="/assets/2001Hastie_p91.png" style="width:69%"><br/>
</center>
Hastie (2001) より

### まとめ

- アンダーフィッテイングとオーバーフィッティング
- データ数に比べて，推定すべきパラメータが多過ぎ = オーバーフィッティング
- データ数に比べて，推定すべきパラメータが少な過ぎ = アンダーフィッティング
- 正則化 L1, L2, L0, エラスティック
- 正則化項の大きさ $\lambda$ はハイパーパラメータと呼ぶ


## 交差妥当性 cross validation

<!--
is a method for finding the best hyper-parameters of a model. 
For example, in gradient descent, we need to choose a stopping criteria. 
The simplest stopping criteria is to check whether our accuracy is improving on the training dataset. 
However, this is prone to overfitting since the model might be capturing noise present in the training data as reliable patterns. -->

## ホールド・アウト法 Holdout method

データを訓練データと検証データに分割 
<!--
We can overcome this problem by not using the entire training data while training a model. 
Instead we will hold out some data (validation dataset) and we'll train only on remaining data. 
For example, we can split our training dataset into 70/30 and use 70% data for training and 30% data for validation. 
In the above example of gradient descent, now we train our algorithm on the training data, but check whether or not our model is getting better on the validation dataset. 
This is known as the holdout method and it is one of the simplest cross validation methods. 
We can also use the validation data for other types of experimentation. Such as if we want to run multiple experiments where we choose different features to use to train our machine learning model. 
-->

- kホールド法 K-fold Cross Validation

データを k 個に分割して, k-1 データで訓練，残りの 1 で検証
<!--
In K-fold cross validation, the dataset is divided into k separate parts. We repeat training process k times. 
Each time, one part is used as validation data, and the rest is used for training a model. 
Then we average the error to evaluate a model. Note that k-fold cross validation increases the computational requirements for training our model by a factor of k.
-->

<!--
The main advantages of k-fold cross validation are that 
1. It is more robust to over-fitting than the holdout method when performing large number of experiments. 
2. It is better to use when the dataset size is small. This is because when performing k-fold cross-validation, we can use a much smaller validation split (say 10% instead of 30%) since we are testing the model on various subsamples of the data being in the 10%.
Leave-one-out cross validation is a special instance of k-fold cross validation in which k is equal to the number of data points in the dataset. 
Each time, we hold out a single data point and train a model on rest of the data. 
We use the single data point to test our model. Then we calculate the average error to evaluate a model.
-->


- 初期停止 early stopping

オーバーフィッティングを避ける方法の一つ: 学習打ち切り基準

<center>
<img src="/assets/04_07earlyStopping.svg" style="width:66%"><br/>
</center>



## SGD は SDG に貢献できるのか？

報道などで昨今耳にする SDG 持続可能な成長目標 ですが，大変紛らわしいことに，ニューラルネットワーク，機械学習の分野では SGD があります。

同じ ３ 文字で同じ文字で，順番が異なるだけでややこしいですが， SGD は 確率的勾配降下法 Stochastic Gradient Descent methods のことです。
レオン・ボットーらを中心に，

前回までと同様に，この授業では，損失関数，目標関数，誤差関数，を区別せずに用います。
ニューラルネットワークに限らず最適化手法として，これら関数の最大化，もしくは最小化を行うことを学習と呼びます。



<center>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Saddle_point.gif' style='width:74%'><br>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Beales_function.gif' style='width:74%'><br>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Long_Valley.gif' style='width:74%'><br>
</center>


## 整流線型ユニット ReLU (Recutified Linear Unit)

**整流線型ユニット ReLU** とは，ニューラルネットワークの活性化関数の一つです。
シグモイド関数や，ハイパータンジェント関数に比べて，極端に単純な形をしています。
駄菓子菓子，生理学との対応についても根拠を持っています。

<!-- The **ReLU** (rectified linear unit) layer is another step to our convolution layer. 
You’re applying an activation function onto your feature maps to increase non-linearity in the network. 
This is because images themselves are highly non-linear! 
It removes negative values from an activation map by setting them to zero.

Convolution is a linear operation with things like element wise matrix
multiplication and addition. 
The real-world data we want our CNN to learn will be non-linear. 
We can account for that with an operation like ReLU. 
You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy.

Want to dig deeper? Try Kaiming He, et al. [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).

If you need a little more info about [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!


Here’s how our little buddy is looking after a ReLU activation function turns all of the negative pixel values black


```python
viz_layer(activated_layer)
```

<center>
<img src="https://komazawa-deep-learning.github.io/assets/output2.jpg" style="width:84%">
</center>
-->



# 標準正則化理論と条件付き最適化

視覚情報処理の分野では，ディビッド・マー David Marr や トマソ・ポッジオ らによって視覚情報処理を定式化する研究が行われました。
以下に論文を引用します。

<div align="center" style="width:94%">
<img src="https://komazawa-deep-learning.github.io/assets/1985Poggio_2.svg" style="width:98%">
</div>

以下に上記引用部分の拙訳を付けます:

データ $y$ から $z$ を見つけ出す不良設定問題の正則化
$$
Az = y
$$
では，正則化項 $\left\|\cdot\right\|$ の選択と汎関数の安定化項 $\left\|Pz\right\|$ が必要となる。
標準正則化理論においては，$A$ は線形演算子，ノルムは 2 次，$P$ は線形である。
2 種類の方法が適用可能である。
すなわち 
1. $\left\|Az-y\right\|\leqslant\epsilon$ を満たし，次式を最小化する $z$ を探す
$$
\left\|Pz\right\|^2
$$

2. 次式を最小化する $z$ を探す
$$
\left\|Az-y\right\|+\lambda\left\|Pz\right\|^2,
$$
ここで $\lambda$ はいわゆる正則化パラメータである。

最初の方法は，十分にデータを近似し，かつ，「基準」$\left\|Pz\right\|$ を最小化するという意味で「正則」な $z$ を探す方法である。
二番目の方法は，$\lambda$ が正則化の程度と解のデータへの近似とをコントロールする。
標準正則化理論は，最良の $\lambda$ を決定する手法を提供する。
標準正則化の手法は，上式に制約を導入することで変分原理の問題としている。
最小化するコストは物理的制約条件を満たす良い解を反映している。
すなわち，データへの近似もよく，かつ，正則化項 $\left\|Pz\right\|^2$ も小さいことを意味する。
$P$ は問題の物理的制約を表しており，2 次の変分原理であり，解空間内での唯一解が存在する。
標準正則化手法は，不良設定問題に対して注意深い分析が必要であることを注記しておく。
ノルム $\left\|\cdot\right\|$，正則化関数 $\left\|Pz\right\|$, および，汎関数空間の選択は数学的性質と，物理的説得性を有する必要がある。
これらにより，正しい正則化の詳細条件が定まる。

変分原理は物理学，経済学，工学，で幅広く用いられている。例えば物理学における基本法則は変分原理を用いて，
エネルギーやラグランジェ関数を用いて簡潔に表現されている。

<!--
- [上を訳してみました。github.io だと数式が表示されない場合があるため colab にしています](https://github.com/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0529Poggios_standard_regularization_translation.ipynb){:target="_blank"}
-->

様々な視覚課題に適用されていて，以下のようなリストが挙げられています。

<div align="center" style="width:94%">
<img src="https://komazawa-deep-learning.github.io/assets/1985Poggio_1.svg" style="width:44%">
<img src="https://komazawa-deep-learning.github.io/assets/1985Poggio_3math.svg" style="width:44%">
<!-- <div align="left" sytle="width:49%">-->
<br/>
</div>

1. 縁検出 Edge detection $\int[(Sf-i)^2 +\lambda(f_{xx}^2)]dx$ 
1. 光学フローの計算 Computation of optical flow $\int[(V\cdot N - V^N )^2+\lambda(\partial/\partial_x)v^2]dx$
1. 表面の再構成 $\int[(S\cdot f - d)^2+\lambda(f_{xx}^2+2f_{xy}^2+f_{yy}^2)^2]dxdy$
1. 時空間近似 spatiotemporal approximation: $\int[(S\cdot i)^2+\lambda(\nabla fV+f_t)^2]dxdydt$
1. 色: $\|I^v-Az\|^2 +\lambda\|Pz\|^2$
1. 陰影からの形状復元 shape from shading: $\int[(E-R(f,g))^2+\lambda(f_x^2+f_y^2+g_x^2+g_y^2)]dxdy$
1. 立体視: $\int\{[
\nabla^2 G * (L(x,y)-R(x+d(x,y),y))]^2
+\lambda(\nabla d)^2]\} dxdy$

<!--
1. 時空間内挿，近似 Spatio-temporal interpolation and approximation $\int[i_x+i,v+i)^2+\lambda(u_x^2+u_y^2+v_x^2+v_y^2)]dxdy$
1. 明度，環境光の計算 Computation of lightness and albedo
1. 輪郭線からの形状復元 Shape from contours
1. キメからの形状復元 Shape from texture
1. 陰影からの形状復元 Shape from shading
1. 両眼立体視 Binocular stereo matching
1. 運動からの形状復元 Structure from motion
1. 両眼立体視 Structure from stereo
1. 表面復元 Surface reconstruction
1. 表面色の計算 Computation of surface colour
</div>
</div>
-->

<!--
The regularization of the ill-posed problem of finding $z$ from
the 'data' $y$

\begin{equation}
Az=y \;\;\;\;\;\;\;\;\;\;(1)
\end{equation}

requires the choice of norms $||\cdot||$ and of a stabilizing functional
$|Pz|$.  In standard regularization theory, $A$ is a linear operator, the
norms are quadratic and $P$ is linear.  Two methods that can be applied
are: (1) among $z$ that satisfy $|Az-y|<\epsilon$ find $z$ that minimizes
$\epsilon$ depends on the estimated measurement errors and is zero if the
data are noiseless

\begin{equation}
|Pz|^{2} \;\;\;\;\;\;\;\;\;\;(2)
\end{equation}

p(2) find $z$ minimizes

\begin{equation}
|Az-y|^2+\lambda|Pz|^2 \;\;\;\;\;\;\;\;\;\;(3)
\end{equation}

where $\lambda$ is a so-call regualarization parameter.

- Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex by Qianli Liao and Tomaso Poggio は注目すべき？ 
- ResNet の解釈

- Hinton, Deep Learning, (Rumelhart backprop also) は Sutton の Bitter lesson の具現化である。end-to-end 一気通貫学習は，特徴抽出(特徴分析)，表現学習(内部表象)，分類器(意思決定)を含む。
-->

<!--
Roe et. al (1992) Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex
-->

<!-- How does cortex that normally processes inputs from one sensory
modality respond when provided with input from a different modality? We
have addressed such a question with an experimental preparation in which
retinal input is routed to the auditory pathway in ferrets. Following
neonatal surgical manipulations, a specific population of retinal ganglion
cells is induced to innervate the auditory thalamus and provides visual
input to cells in auditory cortex (Sur et al., 1988).  We have now examined
in detail the visual response properties of single cells in primary
auditory cortex (A 1) of these rewired animals and compared the responses
to those in primary visual cortex (V1) of normal animals. Cells in A 1 of
rewired animals differed from cells in normal V1: they exhibited larger
receptive field sizes and poorer visual responsivity, and responded with
longer latencies to electrical stimulation of their inputs. However,
striking similarities were also found. Like cells in normal V1, A 1 cells
in rewired animals exhibited orientation and direction selectivity and had
simple and complex receptive field organizations. Furthermore, the degree
of orientation and directional selectivity as well as the proportions of
simple, complex, and nonoriented cells found in A1 and V1 were very
similar. These results have significant implications for possible
commonalities in intracortical processing circuits between sensory
cortices, and for the role of inputs in specifying intracortical circuitry.

あるモダリティからの入力を通常処理する皮質は、異なるモダリティからの入力を与えられたときにどのように反応するのだろうか？網膜入力が西洋イタチ，フェレットの聴覚経路にルーティングされる実験でそのような状況を作り出した。新生児外科手術に続いて、網膜神経節細胞の特定の集団が聴覚視床を神経支配するように誘導し、聴覚皮質の細胞に視覚的な入力を提供した（Sur et al 1988）。
今回、これらの再配線された動物の一次聴覚皮質（A1）における単細胞の視覚反応特性を詳細に調べ、正常な動物の一次視覚皮質（V1）におけるそれらとの反応を比較した。
再配線された動物の A1 細胞は、正常な V1 細胞とは異なっていた：それらはより大きい受容野の大きさと劣った視覚的反応性を示し、入力電気刺激に対してより長い潜時で反応した。
だが、驚くほどの類似点も見つかった。正常な V1 の細胞と同様、再配線された動物の A1 細胞は、方向選択性と方位選択性を示し、単純型，複雑型の受容野組織を有していた。
さらに、方位選択性および方向選択性、ならびに A1および V1 に見られる単純、複雑、および無配向のセルの割合は非常に類似していた。
これらの結果は、皮質内処理回路における知覚皮質間の可能な共通性、および皮質内回路の指定における入力の役割に対して重要な意味を持つ。
-->

<!--
- Metin and Frost (1988) Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus
-->

<!--
These experiments investigate the capacity of thalamic and cortical
structures in a sensory system to proces..  information of a modality
normally associated with another system. Retinal ganglion ceUs in newborn
Syrian hamsters were made to project permanently to the main thalamic
somatosensory (ventrobasal) nucleus. When the animals were adults, single
unit recordings were made in the somatosensory cortices, the principal
targets of the ventrobasal nucleus. The somatosensory neurons responded to
visual stimulation of distinct receptive fields, and their response
properties resembled, in several characteristic features, those of normal
visual cortical neurons. In the visual cortex of normal animals and the
somatosensory cortex of operated animals, the same functional categories of
neurons occurred in similar proportions, and the neurons' selectivity for
the orientation or direction of movement of visual stimuli was
comparable. These results suggest that thalamic nuclei or cortical areas at
corresponding levels in the visual and somatosensory pathways perform
similar transformations on their inputs.

実験で視床と皮質の能力を調べた。モダリティの情報を処理するための感覚システムの構造
通常は他のシステムと関連付けられています。
新生児シリアンハムスターの網膜神経節細胞は、主に視床の体性感覚（腹側基底核）核に永久的に突出するように作られた。
動物が成体のときは、腹側基底核の主な標的である体性感覚皮質において単一単位の記録が行われた。
体性感覚ニューロンは、異なる受容野の視覚刺激に応答し、そしてそれらの応答特性は、いくつかの特徴的な特徴において、正常な視覚皮質ニューロンのそれらに似ていた。
正常な動物の視覚皮質および手術された動物の体性感覚皮質において、同じ機能範疇のニューロンが同様の割合で生じ、そしてニューロンの選択性は
視覚刺激の運動の方向または方向は
同程度の。
これらの結果は、視経路および体性感覚経路における対応するレベルの視床核または皮質領域がそれらの入力に対して同様の変換を実行することを示唆している。
-->


## 畳み込み演算を利用したニューラルネットワーク

<div align="center">
<!--<img src='https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%">-->
<img src="https://komazawa-deep-learning.github.io/assets/Neocognitron.svg" style="width:74%">
<img src="https://komazawa-deep-learning.github.io/assets/Fukushima.jpeg" style="width:24%"><br>
ネオコグニトロンの概略図(Fukushima, 1979)<br>
</div>


## LeNet5 (LeCun, 1998)
<center>
<img src="https://komazawa-deep-learning.github.io/assets/1998LeCun_Fig2_CNN.svg" style='width:94%'><br>
LeCun (1998) より
</center>

## AlexNet (Krizensky, et al., 2012)

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNet.svg" style="width:94%"><br/>
Krzensky et al (2012) より
</center>

## GooLeNet (Inception) (Szegedy et. al, 2014)

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2014Szegedy_GoogLeNet.svg" style='width:99%'><br/>
</center>

<!-- <center>
<img src='https://komazawa-deep-learning.github.io/assets/2013Uijings_Selective_Search_Fig1.svg' style='width:94%'><br>
空間ピラミッド (2015) より
</center>



<div align="center" style="width:94%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/full_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1.
		右:入力層 5x5青，出力層緑，カーネルサイズ3x3, フルパディング，ストライド=1. トランスポーズド畳み込み
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/numerical_max_pooling.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/numerical_average_pooling.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左: 最大値プーリング。
		右: 平均値プーリング
	</div>
	<div align="left" style="width:66%">
		Dmoulin and Visin (2020) より
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides_odd.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/padding_strides_odd_transposed.gif" style="width:33%"><br/>
	<div align="left" style="width:66%">
		左: padding_strides, 中:padding_strides_odd, 右:padding_stride_transposed
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/same_padding_no_strides_transposed.gif" style="width:33%">
    <div align="left" style="width:66%">
	 右:same_padding_no_strides, 左: same_padding_no_strides_transposed
	</div>
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/arbitrary_padding_no_strides.gif" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assetsdmoulin_gif/arbitrary_padding_no_strides_transposed.gif" style="width:33%">
    <div align="left" style="width:66%">
	 右:arbitrary padding no strides, 左: artibtrary padding no stride transposed
	</div>
</div>
-->


### イメージネットコンテスト，アレックスネットの出力にみる問題点

<div align="center" style="width:89%">
	<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNetResult0.svg" style="width:33%">
	<img src="https://komazawa-deep-learning.github.io/assets/2012AlexNetResult.svg" style="width:33%">
	<div align="left" style="width:66%">
	アレックスネットの結果: 画像のすぐ下の英単語は正解ラベルを表す。Krizensky et. al (2012) Fig. 4 より。
	ピンク色は正解ラベルの確率を表す。ブルーは不正解ラベル判断確率を表している。
	チェリーが正解であるが，画像を見る限り，第一回答候補のダルマチアンを正解だと考えても問題は無いと考えられる。
</div>
</div>

### 画像切り出し

1. 物体位置
3. 物体認識 object recognition
2. 意味的切り出し semantic segmentation
4. 対象切り出し instance segmentation
5. 特徴点抽出 keypoint
6. パノプティック切り出し

<div align="center">
	<img src="https://komazawa-deep-learning.github.io/assets/2017DangHa_History_Of_Object_Recognition_ja.svg" style="width:99%"><br/>
	Dang and Ha (2017) より
</div>


# 転移学習

<div align="center" style="width:99%">
<img src="https://komazawa-deep-learning.github.io/assets/2017Li_Deeper_Broader_fig1ja.svg" style="width:84%"><br/>
</div>

<!---
- [活性化関数](../activation_functions/)
-->

<!-- 
<div align="center">
<img src='https://komazawa-deep-learning.github.io/assets2019Inception_screenshot.png' style='width:84%'><br>
<div align="left"  style="width:69%">
映画インセプションのスクリーンショット。
 
[Netflix](https://www.netflix.com/watch/70131314?trackId=14170286&tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT){target="_blank"} <br/>
<https://www.netflix.com/watch/70131314?trackId=14170286&tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT><br/>

『インセプション』（原題: Inception）は、クリストファー・ノーラン監督・脚本・製作による2010年のアメリカのSFアクション映画。第83回アカデミー賞では作品賞、脚本賞、撮影賞、視覚効果賞、美術賞、作曲賞、音響編集賞、録音賞の8部門にノミネートされ、撮影賞、視覚効果賞、音響編集賞、録音賞を受賞した。全米脚本家組合賞ではオリジナル脚本賞を受賞した。
[日本語ウィキペデイアより](https://ja.wikipedia.org/wiki/%E3%82%A4%E3%83%B3%E3%82%BB%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3){target="_blank"}

</div>
</div>

<div align="center" style="width:94%">
	<img src='https://komazawa-deep-learning.github.io/assetsInception3.svg' style="width:94%"></br>
	<img src='https://komazawa-deep-learning.github.io/assets2015GoogLeNet_Inception.svg' style="width:74%"></br>
	<div align="left">
Inception モジュール
</div>
</div>
 -->

<!-- 
</center>
<center>
<img src='https://komazawa-deep-learning.github.io/assets2014Cadieu_Fig3.svg' style='width:74%'>
</center>
-->

