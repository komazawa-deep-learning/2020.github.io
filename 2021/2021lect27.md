---
title: 第27回
author: 浅川 伸一
date: 2022_0107
layout: home
---

# 課題一覧

## 09 月 17 日

日本語訳付きの Mtichell (2008) 論文を斜め読みして，簡潔にまとめてください。
分量は，長くても A4 半ページ程度とします。
念の為，論文の URL を以下に掲載します。
[名詞の意味に関連した⼈間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}

提出期限は，9月23日 23:59 とさせていただきます。
このメールに返信する形でお送りください。
ワードで作成する必要はありません。メールに直接書いていただくことでも構いません。

##  09 月 24 日

本日は 2 点あります。

1. CAM の colab を時分で集めた画像を用いて評価せよ。最低 3 枚の画像を試してください。
その結果が表示されている colab ファイルを自身の Google drive に保存して，共有設定をし
て浅川のアカウント educ0233@komazawa-u.ac.jp とファイルの共有設定をしてください。
その共有設定された colab ファイルの URL を本メールに返信する形で送付してくだい。
CAM の実習ファイルは，以下の通りです:

- [CAM 実習ファイル https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}

2. 「⼼理学，神経科学，機械学習における注意, Lindsay, 2019」 論文を読み，自分にとって
未知の専門用語を抜き出して記せ。必ずしも，意味を調べる必要は無いが，リストアップして，そのリストをメールに書いて送信してください。
論文の URL は以下の通りです。
[https://project-ccap.github.io/2020Lindsay_Attention_in_Psychology_Neuroscience_and_Machine_Learning.pdf](
https://project-ccap.github.io/2020Lindsay_Attention_in_Psychology_Neuroscience_and_Machine_Learning.pdf)

## 10 月 01 日

今週金曜日 10月1日ですが，休講とさせてください。
次回の授業は 来週 10月8日となります。

## 10 月 08 日

1. は全員必修課題です。
2. は選択課題であり，提出の義務はありません。
提出されれば，ボーナスポイントが加算されます。

課題 1. 授業で取り上げた seq2seq モデルは，翻訳，対話，チャットボット，
などに応用されるという話題の続きです。
授業中に検討された以外の，応用を一つ考えてメールで送ってください。

課題 2. 注意つきの翻訳モデルの実習ファイル，もしくは，注意なしの翻訳モデルの実習
ファイルのどちらかを，英語から日本語に翻訳するのではなく，反対に，日本語から英語
に翻訳するように書き換えてください。

- [注意つき英日翻訳モデル実習ファイル](
https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)

- [注意なしの英日翻訳モデル実習ファイル](
https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb)

課題は，以上です。
来週 10月15日は，開校記念日ということで休講のようです。
ですので次回は10月22日ということになるようです。
従いまして，今回の課題提出期限は，10月21日23:59となります。


## 10 月 22 日

今回話題としてとりあげた BERT とそのもとになったトランスフォーマーは，それまでのリカレントニューラルネットワークモデルを注意を使って置き換えた手法です。
BERT は自然言語処理課題において，人間超えしたことで話題になりました。

このことは，注意機構が知的情報処理において重要であることを示していると考えられます。
そこで今回の課題は以下になります。

BERT やトランスフォーマーで用いられた，多頭自己注意機構と人間の注意とで同じ点と異なる点とを例を挙げて考察してください。
同じ点，異なる点，それぞれ最低 1 点を挙げ，1, 2 行の考察を加える。

加えて，ボーナスポイント課題として，文字逆唱課題のコードを順唱課題のように修正して加算型注意と積算型注意との違いを観察してください。
こちらの課題の提出は任意です。提出された場合ボーナスポイントとして加算されます。

以上です。提出期限は，次回の授業前日の 23:59 とさせていただきます。

ところで，ご存知でしたら教えていただきたいことがあります。
それは次回の授業日についてです。
駒澤大学のサイトも見ていると，オータムフェスティバルで休講のようにも読めます。
どなたか，10月29日に授業があるかどうか，教えていただけないでしょうか。

## 11月06日

課題をお送りしいます。
提出期限は，日本時間11月11日23:59 とさせていただきます。
本メールに返信する形で提出をお願いいたします。

課題は以下のとおりです。
11月06日は，強化学習の導入を行いました。

1. 心理学における古典的条件づけ（パブロフ）やオペランド条件付け（スキナー）と比較して，
スットンとバルトーが定義した現代的強化学習の相違を短く説明し，今後の研究方向を考えよ。

参考文献としては，スットンが 2019 年に自身のブログに書いた「苦い教訓」と題する短いエッセイ
があります。エッセイの日本語訳が次の URL にあります
- [https://komazawa-deep-learning.github.io/2021cogpsy/2019Sutton_Bitter_Lesson.pdf](https://komazawa-deep-learning.github.io/2021cogpsy/2019Sutton_Bitter_Lesson.pdf)

分量としましては，A4 1 ページを超えてはいけません。1 から 2 段落程度にまとめてください。

以上です。

https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb


## 11月12日

本日の課題をお送りします。
本日実習で使ったスーパーマリオブラザーズでハイスコアを叩き出してください。実習で使用した colab ファイルは以下のとおりです。

[https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb)

このファイルで学習を重ねたり，パラメータをチューニングしたりしてハイスコア狙ってください。
最高得点者には，ボーナスポイント （出席点 1 回分相当）を差し上げます。

従いまして，今回の課題提出は，浅川個人宛に提出するのではなく，
このメールに返信する形で，全員に送ってください。
何度メールを送信されても良いですが，常識的な回数に留めるようにしてください。
例えば，一日の投稿回数は 3 回以内にしてください。

ちなみに，訓練して，評価して，を繰り返すことになりますが，以下のようなコード最終部分がポイントです

```
# For training
run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=1, exploration_max = 1)

# For Testing
run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, exploration_max = 0.05)
 ```

run の第1引数で training_mode = True であれば訓練を行い学習が進行します。
一方で，train_mode = False であれば，学習結果の評価が実行されます。スコアは，training_mode=False の場合のスコア
が高い人が優勝です。
このとき，レギュレーションとして，評価方法を固定します。かならず次のコマンドで評価してください。

```
run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, exploration_max = 0.05)
```

つまり 3 エピソード分の平均報酬が競技に用いられる得点です。
復習になりますが，その他の引数を簡単に説明しておきます。

pretrained=True であれば，前回までに学習した内容が読み込まれて，前回の続きから学習が進行します。
逆に pretrained=False であれば，新しい学習が開始されます。もちろん，何度も学習したほうがスコア
は高くなるでしょう。

double_dqn=True であれば，ダブル DQN モデルで学習が行われ，double_dqn = False であれば，通常の DQN
モデルで学習が行なわれます。

num_epsodes は，訓練に要するエピソードの回数を指定します。
exploration_max は，探察と活用のジレンマにおいて，イプシロン貪欲な探索を行う割合を指定します。
この値が低いと探索は行なわれません。経験再生を行うごとに探索率 イプシロンは減衰するようにしてあります。
したがって，テスト試行では 0.05 ですが 5 ％程度の割合で探索を行います。イプシロンの下限は， 0.02 に設定
してあります。

上記が簡単に試すことができるパラメータです。ですが，詳細にパラメータチューニングをするのであれば，
他のパラメータをいじることができます。
例えば，DQN を定義してあるセルの 312 行目以降
```
                     max_memory_size=30000,  #  経験再生を行う際のエピソードにためこむメモリ容量
                     batch_size=32,                     # バッチサイズ
                     gamma=0.90,                       # 割引率，将来の報酬にどれだけ価値をおくか
                     lr=0.00025,                           # 学習率
                     dropout=0.2,                         # ドリップアウト率
                     exploration_max=1.0,           # イプシロン探索を行う際の上限
                     exploration_min=0.02,          # 同下限
                     exploration_decay=0.99,      # イプシロン探索の減衰率
```

などです。
スコアと自分が試したパラメータを報告してください。

ただし，学習結果を保存して，後日，学習を再開させる方法は，示していませんが，途中から学習再開させる
場合には，各自調べるか，相談してください。

以上になります。締切は，来週の授業開始，1 分前とさせていただきます。
すなわち，11月19日午前 8 時59分が締め切りです。
どうぞよろろ敷くお願いいたします。


## 11月26日

以下に示すキーワードの中から，3 つ以上を使って一段落から３段落程度の文章を作成してください。
文章のテーマは，最近の時事問題，過去に観たアニメや映画などから選んでください。
テーマの選定が難しそうであれば，下記のキーワードの解説でも構いません。

提出期限は，次回授業日の前日中，すなわち12月2日23:59 とさせてください。
どうぞよろしくお願いいたします。

環境, 行動, 報酬, 方針(ポリシー), モデルフリー, モデルベース, 価値関数, Q 学習, 割引率, マルコフ性, 探索と活用のジレンマ,  アドバンテージ関数, 決闘(ドュエリング)ネットワーク, 二重 Q 学習, アクター・クリティック法, 優先度付き経験再生, 方針勾配法,  内発的動機づけ, 好奇心

## 12月03日

提出期限は，日本時間12月09日23;59 とさせていただきます。
本メールに返信する形で提出をお願いいたします。

課題は以下のとおりです。

ベイズ則、または、EM アルゴリズムおける推定について、
以下に挙げる現象の一つを取り上げて、
何が観測可能なデータ $x$ であり、何が観測不可能な潜在変数 $z$ であり、
何が推定すべきモデルのパラメータ $\theta$ であるのか、各々一行程度に簡潔に述べなさい。

現象: ストループ効果、ラバーハンド錯視、マガーク効果、腹話術、二重フラッシュ錯視

以上です。

## 12月10日

本日分の課題をお送りいたします。

本日実習で用いたパレイドリアの実習ファイル: 
[https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb) 
を用いて，適当な画像を用いてパレイドリア画像を生成してください。
異なるパラメータを用いて，複数の画像を生成し，
その結果を考察 (一行程度)して，ファイル中に+コメントとして書き込んでください。
そのファイルを自身のグーグルドライブに保存してください。
保存したファイルを，浅川 (educ0233@komazawa-u,ac.jp)と共有してください。
そのファイルの共有情報を，このメールに対する返信としてお送りください。

課題は以上になります。
提出は，来週の授業日前日 12月16日木曜日 23:59 とさせていただきます。

## 12月17日

本日分の課題をお送りいたします。
本日の授業では，神経心理学検査のうち，TLPA 失語症検査の課題の一つ，絵画命名検査を取り上げ，ディープラーニングモデルの転移学習を用いて実習しました。
授業中にもコメントしましたとおり，画像認識モデルを転移学習しているので，実習に用いたモデルでは，意味性の失語症患者の示すであろう，意味的な誤りに対応できません。
そこで，今回の課題は，その他にも，失語症モデルとして足りない要因を考察する，というものです。
複数個の不足が挙げられますが，一つ以上指摘してください。
解決案も提示できれば，なお良いです。

課題は以上になります。
提出は，来週の授業日前日 12月23日木曜日 23:59 とさせていただきます。


## 12月24日

今回は，Dell の相互活性化モデルのシミュレーションを紹介しました。
このモデルと先週紹介した絵画命名検査と合わせた考察をお願いします。
分量は 1 段落程度でお願いします。
このメールに返信する形でお願いいたします。

加えて，この授業の改善提案も記してください。どのような内容でも構いません。こちらは評価に影響しません。

提出期限は，2021年12月31日23:59 とさせてください。
すなわち今年のうちにお願いいたします。

# 用語集

- [用語集](/2021cogpsy/glossary){:target="_blank"}

* **word2vec** 単語埋め込みモデルの一つ。単語の意味を，前後の単語によって表現する スキップグラムと CBOW という 2 つのモデルがある。
* **トランスフォーマー** 多頭自己注意 muliti-head self attention を用いた言語モデル。
* **BERT** 多頭自己注意機構に基づく，言語モデル。入力情報を 3 重化し，それぞれ Q, K, V, として用いる。Q と K との積の結果と V とから出力を得る。学習時には，マスク化言語モデルと次文予測課題とで事前訓練を行い，各下流課題に対して微調整を行うことで精度向上が示された。
従来モデルであるリカレントニューラルネットワークモデルに基づく言語処理モデルを置き換える動きが主流となっている。
BERT は **B**idirectional **E**ncoder **R**epresentations from **T**ransformers の略だとされる。
それまで，リカレントニューラルネットワークが主流であった自然言語処理課題のほとんどにおいて，BERT に基づくモデルが性能を凌駕している。
* **ソフトマックス関数** 複数の候補から，もっとも値の大きい項目一つ選び出す機構として頻用される関数。このため，注意を表現sるためにも用いられる。
* **CAM** 画像認識課題において，入力画像中のどの領域が，当該カテゴリーと判断するために貢献したかを示すための手法，またはアルゴリズムを指す。**C**lass **A**ssociation **M**apping の略
* **Seq2seq** 入力系列を出力系列へと変換するための機構。系列から系列へ (sequence-to-sequnce) の略称である。系列処理を司るリカレントニューラルネットワークを 2 つ連結させた形をとるニューラルネットワークモデルである。
入力側の中間層状態を出力側の中間層の初期状態とするのが，原型である。このとき中間層の状態に対して注意機構を用いることが行われる
* **強化学習** アルファ碁やアタリのボードゲームなどを解くことで注目された枠組み，または理論。
経済学，ゲーム理論，学習心理学などを背景として成立した。
動作主 (エージェント) と呼ばれる主体が，環境を探索し，相互作用することで行動を変容させ，報酬を最大化するように振る舞うようプログラムされる。
神経科学における脳内対応物についての研究などから，行動異常との関連が指摘され，精神医学的症状の計算モデルとして提案される場合がある。
* **古典的条件づけ**
* **オペランド条件づけ**
* **報酬**
* **Q 学習**
* **ベルマン方程式**
* **環境**
* **報酬**
* **方針(ポリシー)**, 
* **モデルフリー**
* **モデルベース**
* **価値関数**
* **Q 学習**
* **割引率**
* **マルコフ性**
* **探索と実用のジレンマ** 過去の経験から報酬を最大にする行動を選ぶ実用 (「活用」と訳す場合もある) と，その実用によって選択されるべき行動を捨てて，新たな行動を探索する方が将来的に獲得される報酬が大きい場合もある。
この行動を探索と呼ぶ。これら探索と実用という 2 つ行動の選択において葛藤が生じるため，ジレンマと称される。Exploration and exploitation dilemma 
* **アドバンテージ関数**
* **決闘(ドュエリング)ネットワーク**
* **二重 Q 学習**
* **アクター・クリティック法**
* **優先度付き経験再生**
* **方針勾配法**
* **内発的動機づけ**
* **好奇心**
* **神経心理学**
* **EM アルゴリズム**
* **ストループ効果**
* **ラバーハンド錯視**
* **マガーク効果**
* **二重フラッシュ錯視**
* **転移学習** 深層学習においては，事前訓練済のモデルに対して，標的とする課題に対して学習させる際に，最上位層である全結合層，あるいは，最終直下層付近のパラメータのみを更新し，それより下層のパラメータは凍結させることを指す。transfer learning 
* **微調整** 深層学習においては，事前訓練済のモデルに対して，標的とする課題に対して学習させる際に，全パラメータにわたる更新を行うことを指す。fine tuning
* **パレイドリア** 視覚的妄想の一つ。初期視覚情報だけでなく，高次視覚情報に基づく見誤り，幻視を言う。
* **TLPA**
* **相互活性化モデル** 入力情報と中間段階，出力情報との間に相互作用を仮定するモデル。
* **絵画命名課題** 認知心理学，神経心理学などで用いられる検査または課題の名称。
被検査者に対して，刺激図版 (近年では電子化された図版をディスプレイに提示する場合もある) を提示し，その刺激の視覚情報に基づく判断を求める。
失語症患者の中には，視覚的な誤りの他，意味的な誤り，あるいは単語の音や綴りに類似した誤りを表出する場合があるため，言語処理の神経機構を推定することが期待される。


