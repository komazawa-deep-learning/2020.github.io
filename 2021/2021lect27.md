---
title: 第27回
author: 浅川 伸一
date: 2022_0107
layout: home
---

# 課題一覧

## 09 月 17 日

日本語訳付きの Mtichell (2008) 論文を斜め読みして，簡潔にまとめてください。
分量は，長くても A4 半ページ程度とします。
念の為，論文の URL を以下に掲載します。
[名詞の意味に関連した⼈間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}

提出期限は，9月23日 23:59 とさせていただきます。
このメールに返信する形でお送りください。
ワードで作成する必要はありません。メールに直接書いていただくことでも構いません。

##  09 月 24 日

本日は 2 点あります。

1. CAM の colab を時分で集めた画像を用いて評価せよ。最低 3 枚の画像を試してください。
その結果が表示されている colab ファイルを自身の Google drive に保存して，共有設定をし
て浅川のアカウント educ0233@komazawa-u.ac.jp とファイルの共有設定をしてください。
その共有設定された colab ファイルの URL を本メールに返信する形で送付してくだい。
CAM の実習ファイルは，以下の通りです:

- [CAM 実習ファイル https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}

2. 「⼼理学，神経科学，機械学習における注意, Lindsay, 2019」 論文を読み，自分にとって
未知の専門用語を抜き出して記せ。必ずしも，意味を調べる必要は無いが，リストアップして，そのリストをメールに書いて送信してください。
論文の URL は以下の通りです。
[https://project-ccap.github.io/2020Lindsay_Attention_in_Psychology_Neuroscience_and_Machine_Learning.pdf](
https://project-ccap.github.io/2020Lindsay_Attention_in_Psychology_Neuroscience_and_Machine_Learning.pdf)

## 10 月 01 日

今週金曜日 10月1日ですが，休講とさせてください。
次回の授業は 来週 10月8日となります。

## 10 月 08 日

1. は全員必修課題です。
2. は選択課題であり，提出の義務はありません。
提出されれば，ボーナスポイントが加算されます。

課題 1. 授業で取り上げた seq2seq モデルは，翻訳，対話，チャットボット，
などに応用されるという話題の続きです。
授業中に検討された以外の，応用を一つ考えてメールで送ってください。

課題 2. 注意つきの翻訳モデルの実習ファイル，もしくは，注意なしの翻訳モデルの実習
ファイルのどちらかを，英語から日本語に翻訳するのではなく，反対に，日本語から英語
に翻訳するように書き換えてください。

- [注意つき英日翻訳モデル実習ファイル](
https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1008seq2seq_attention_demo.ipynb)

- [注意なしの英日翻訳モデル実習ファイル](
https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1003vanilla_seq2seq2.ipynb)

課題は，以上です。
来週 10月15日は，開校記念日ということで休講のようです。
ですので次回は10月22日ということになるようです。
従いまして，今回の課題提出期限は，10月21日23:59となります。


## 10 月 22 日

今回話題としてとりあげた BERT とそのもとになったトランスフォーマーは，それまでのリカレントニューラルネットワークモデルを注意を使って置き換えた手法です。
BERT は自然言語処理課題において，人間超えしたことで話題になりました。

このことは，注意機構が知的情報処理において重要であることを示していると考えられます。
そこで今回の課題は以下になります。

BERT やトランスフォーマーで用いられた，多頭自己注意機構と人間の注意とで同じ点と異なる点とを例を挙げて考察してください。
同じ点，異なる点，それぞれ最低 1 点を挙げ，1, 2 行の考察を加える。

加えて，ボーナスポイント課題として，文字逆唱課題のコードを順唱課題のように修正して加算型注意と積算型注意との違いを観察してください。
こちらの課題の提出は任意です。提出された場合ボーナスポイントとして加算されます。

以上です。提出期限は，次回の授業前日の 23:59 とさせていただきます。

ところで，ご存知でしたら教えていただきたいことがあります。
それは次回の授業日についてです。
駒澤大学のサイトも見ていると，オータムフェスティバルで休講のようにも読めます。
どなたか，10月29日に授業があるかどうか，教えていただけないでしょうか。

## 11月06日

課題をお送りしいます。
提出期限は，日本時間11月11日23:59 とさせていただきます。
本メールに返信する形で提出をお願いいたします。

課題は以下のとおりです。
11月06日は，強化学習の導入を行いました。

1. 心理学における古典的条件づけ（パブロフ）やオペランド条件付け（スキナー）と比較して，
スットンとバルトーが定義した現代的強化学習の相違を短く説明し，今後の研究方向を考えよ。

参考文献としては，スットンが 2019 年に自身のブログに書いた「苦い教訓」と題する短いエッセイ
があります。エッセイの日本語訳が次の URL にあります
- [https://komazawa-deep-learning.github.io/2021cogpsy/2019Sutton_Bitter_Lesson.pdf](https://komazawa-deep-learning.github.io/2021cogpsy/2019Sutton_Bitter_Lesson.pdf)

分量としましては，A4 1 ページを超えてはいけません。1 から 2 段落程度にまとめてください。

以上です。

https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb


## 11月12日

本日の課題をお送りします。
本日実習で使ったスーパーマリオブラザーズでハイスコアを叩き出してください。実習で使用した colab ファイルは以下のとおりです。

[https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1108super_mario_dqn.ipynb)

このファイルで学習を重ねたり，パラメータをチューニングしたりしてハイスコア狙ってください。
最高得点者には，ボーナスポイント （出席点 1 回分相当）を差し上げます。

従いまして，今回の課題提出は，浅川個人宛に提出するのではなく，
このメールに返信する形で，全員に送ってください。
何度メールを送信されても良いですが，常識的な回数に留めるようにしてください。
例えば，一日の投稿回数は 3 回以内にしてください。

ちなみに，訓練して，評価して，を繰り返すことになりますが，以下のようなコード最終部分がポイントです

```
# For training
run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=1, exploration_max = 1)

# For Testing
run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, exploration_max = 0.05)
 ```

run の第1引数で training_mode = True であれば訓練を行い学習が進行します。
一方で，train_mode = False であれば，学習結果の評価が実行されます。スコアは，training_mode=False の場合のスコア
が高い人が優勝です。
このとき，レギュレーションとして，評価方法を固定します。かならず次のコマンドで評価してください。

```
run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=1, exploration_max = 0.05)
```

つまり 3 エピソード分の平均報酬が競技に用いられる得点です。
復習になりますが，その他の引数を簡単に説明しておきます。

pretrained=True であれば，前回までに学習した内容が読み込まれて，前回の続きから学習が進行します。
逆に pretrained=False であれば，新しい学習が開始されます。もちろん，何度も学習したほうがスコア
は高くなるでしょう。

double_dqn=True であれば，ダブル DQN モデルで学習が行われ，double_dqn = False であれば，通常の DQN
モデルで学習が行なわれます。

num_epsodes は，訓練に要するエピソードの回数を指定します。
exploration_max は，探察と活用のジレンマにおいて，イプシロン貪欲な探索を行う割合を指定します。
この値が低いと探索は行なわれません。経験再生を行うごとに探索率 イプシロンは減衰するようにしてあります。
したがって，テスト試行では 0.05 ですが 5 ％程度の割合で探索を行います。イプシロンの下限は， 0.02 に設定
してあります。

上記が簡単に試すことができるパラメータです。ですが，詳細にパラメータチューニングをするのであれば，
他のパラメータをいじることができます。
例えば，DQN を定義してあるセルの 312 行目以降
```
                     max_memory_size=30000,  #  経験再生を行う際のエピソードにためこむメモリ容量
                     batch_size=32,                     # バッチサイズ
                     gamma=0.90,                       # 割引率，将来の報酬にどれだけ価値をおくか
                     lr=0.00025,                           # 学習率
                     dropout=0.2,                         # ドリップアウト率
                     exploration_max=1.0,           # イプシロン探索を行う際の上限
                     exploration_min=0.02,          # 同下限
                     exploration_decay=0.99,      # イプシロン探索の減衰率
```

などです。
スコアと自分が試したパラメータを報告してください。

ただし，学習結果を保存して，後日，学習を再開させる方法は，示していませんが，途中から学習再開させる
場合には，各自調べるか，相談してください。

以上になります。締切は，来週の授業開始，1 分前とさせていただきます。
すなわち，11月19日午前 8 時59分が締め切りです。
どうぞよろろ敷くお願いいたします。


## 11月26日

以下に示すキーワードの中から，3 つ以上を使って一段落から３段落程度の文章を作成してください。
文章のテーマは，最近の時事問題，過去に観たアニメや映画などから選んでください。
テーマの選定が難しそうであれば，下記のキーワードの解説でも構いません。

提出期限は，次回授業日の前日中，すなわち12月2日23:59 とさせてください。
どうぞよろしくお願いいたします。

環境, 行動, 報酬, 方針(ポリシー), モデルフリー, モデルベース, 価値関数, Q 学習, 割引率, マルコフ性, 探索と活用のジレンマ,  アドバンテージ関数, 決闘(ドュエリング)ネットワーク, 二重 Q 学習, アクター・クリティック法, 優先度付き経験再生, 方針勾配法,  内発的動機づけ, 好奇心

## 12月03日

提出期限は，日本時間12月09日23;59 とさせていただきます。
本メールに返信する形で提出をお願いいたします。

課題は以下のとおりです。

ベイズ則、または、EM アルゴリズムおける推定について、
以下に挙げる現象の一つを取り上げて、
何が観測可能なデータ $x$ であり、何が観測不可能な潜在変数 $z$ であり、
何が推定すべきモデルのパラメータ $\theta$ であるのか、各々一行程度に簡潔に述べなさい。

現象: ストループ効果、ラバーハンド錯視、マガーク効果、腹話術、二重フラッシュ錯視

以上です。

## 12月10日

本日分の課題をお送りいたします。

本日実習で用いたパレイドリアの実習ファイル: 
[https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb) 
を用いて，適当な画像を用いてパレイドリア画像を生成してください。
異なるパラメータを用いて，複数の画像を生成し，
その結果を考察 (一行程度)して，ファイル中に+コメントとして書き込んでください。
そのファイルを自身のグーグルドライブに保存してください。
保存したファイルを，浅川 (educ0233@komazawa-u,ac.jp)と共有してください。
そのファイルの共有情報を，このメールに対する返信としてお送りください。

課題は以上になります。
提出は，来週の授業日前日 12月16日木曜日 23:59 とさせていただきます。

## 12月17日

本日分の課題をお送りいたします。
本日の授業では，神経心理学検査のうち，TLPA 失語症検査の課題の一つ，絵画命名検査を取り上げ，ディープラーニングモデルの転移学習を用いて実習しました。
授業中にもコメントしましたとおり，画像認識モデルを転移学習しているので，実習に用いたモデルでは，意味性の失語症患者の示すであろう，意味的な誤りに対応できません。
そこで，今回の課題は，その他にも，失語症モデルとして足りない要因を考察する，というものです。
複数個の不足が挙げられますが，一つ以上指摘してください。
解決案も提示できれば，なお良いです。

課題は以上になります。
提出は，来週の授業日前日 12月23日木曜日 23:59 とさせていただきます。


## 12月24日

今回は，Dell の相互活性化モデルのシミュレーションを紹介しました。
このモデルと先週紹介した絵画命名検査と合わせた考察をお願いします。
分量は 1 段落程度でお願いします。
このメールに返信する形でお願いいたします。

加えて，この授業の改善提案も記してください。どのような内容でも構いません。こちらは評価に影響しません。

提出期限は，2021年12月31日23:59 とさせてください。
すなわち今年のうちにお願いいたします。

# 用語集

- [前期の用語集](/2021cogpsy/glossary){:target="_blank"}

* **word2vec** 単語埋め込みモデルの一つ。単語の意味を，前後の単語によって表現する スキップグラムと CBOW という 2 つのモデルがある。
* **トランスフォーマー** 多頭自己注意 muliti-head self attention を用いた言語モデル。
* **BERT** 多頭自己注意機構に基づく，言語モデル。入力情報を 3 重化し，それぞれ Q, K, V, として用いる。Q と K との積の結果と V とから出力を得る。学習時には，マスク化言語モデルと次文予測課題とで事前訓練を行い，各下流課題に対して微調整を行うことで精度向上が示された。
従来モデルであるリカレントニューラルネットワークモデルに基づく言語処理モデルを置き換える動きが主流となっている。
BERT は **B**idirectional **E**ncoder **R**epresentations from **T**ransformers の略だとされる。
それまで，リカレントニューラルネットワークが主流であった自然言語処理課題のほとんどにおいて，BERT に基づくモデルが性能を凌駕している。
* **ソフトマックス関数** 複数の候補から，もっとも値の大きい項目一つ選び出す機構として頻用される関数。このため，注意を表現するためにも用いられる。
たとえば $a=1,b=2,c=3$ のとき，ソフトマックス関数を用いて確率に変換する場合と，単純に総和 $a+b+c$ で除して確率化する場合とを比較すると
ソフトマックス関数によれば $p(a)\simeq0.09,p(b)\simeq0.24,p(c)\simeq0.67$ であるのに対して，総和による確率化では $p(a)\simeq0.16,p(b)\simeq0.33,p(c)=0.5$ となる。このように，ソフトマックス関数による確率化では，最大値 (本例の場合 c の値) がより強調される結果となる。
ソフトマックスの反対語としてハードマックスが挙げられる。ハードマックスでは，最大値をとる項目を 1 とし，他の全ての項目を 0 とする。
ハードマックスの場合は，微分が不可能となる。このためハードマックスは，ニューラルネットワークの学習には不向きと言える。
* **CAM** 画像認識課題において，入力画像中のどの領域が，当該カテゴリーと判断するために貢献したかを示すための手法，またはアルゴリズムを指す。**C**lass **A**ssociation **M**apping の略
* **Seq2seq** 入力系列を出力系列へと変換するための機構。系列から系列へ (sequence-to-sequnce) の略称である。系列処理を司るリカレントニューラルネットワークを 2 つ連結させた形をとるニューラルネットワークモデルである。
入力側の中間層状態を出力側の中間層の初期状態とするのが，原型である。このとき中間層の状態に対して注意機構を用いることが行われる
* **強化学習** アルファ碁やアタリのボードゲームなどを解くことで注目された枠組み，または理論。
経済学，ゲーム理論，学習心理学などを背景として成立した。
動作主 (エージェント) と呼ばれる主体が，環境を探索し，相互作用することで行動を変容させ，報酬を最大化するように振る舞うようプログラムされる。
神経科学における脳内対応物についての研究などから，行動異常との関連が指摘され，精神医学的症状の計算モデルとして提案される場合がある。
* **古典的条件づけ** 無条件刺激 (無関連な刺激) と条件刺激 (生体にとって好ましい刺激)  とが随伴して生起することで行動変容が起こることを指す概念。
イアン・パブロフによって提唱された。
* **オペランド条件づけ** 環境に働きかけることで，報酬を獲得し，行動が変容することを指す概念。バーハス・F・スキナーによって提唱された。
* **報酬** 強化学習において，動作主が受け取る利得。動物では餌，人間では金銭や名誉などである。強化学習では，将来に渡る報酬を最大化することが目的関数となる。
* **Q 学習** 動作主が，ある状況において，ある動作を行った際に期待される報酬を定義する関数を Q 関数と呼ぶ。
すなわち Q 関数には 2 つの引数 状態 s と，この s という状態で行った行動 a によって定まる関数である。
このため $Q(s,a)$ のように表記される。Q 関数を最大化するような学習を Q 学習と呼ぶ。
より正確には，Q の値は，状態 s のときに行動 a を採択した場合の将来に渡る報酬，すなわち期待報酬で定義される。
$Q(s,a)=\mathcal{E}\left[R+\gamma R+\gamma^{2} R+\cdots\vert s,a\right]$
* **ベルマン方程式** ベルマンによって提案された，将来に渡って得られる報酬を定義する式。現在の報酬と次の時刻に得られる報酬に割引率を掛けた値を将来に渡って合算した形である。
* **環境** 強化学習において，動作主と相互作用する世界を指す。動作主は環境に働きかけて，報酬を得る。環境は動作主の行動を受けて，環境自身の状態を偏移させ，かつ，動作主に報酬を与える。
* **方針 (ポリシー)** 強化学習において，ある状況に対して，特定の行動を起こす傾向のこと。方針によって Q が異なる。
強化学習の文脈では，方針はギリシャアルファベット $\pi$ で表記される場合が多い (このため，本授業でも一貫して $\pi$ と表記した)。
方針 $\pi$ に基づく Q 関数を $Q_{\pi}(s,a)$ のように表記することがある。
* **モデルフリー** 強化学習において，規則に基づく意思決定がなされる場合を指す。行動選択の根拠となる規則が正しければ，学習あるいは経験が不要である。
* **モデルベース** 強化学習において，経験則に基づく意思決定がなされる場合を指す。経験に基づく必要があるため最適な行動選択のためには，データ量が要求される。
* **価値関数** 強化学習において，動作主 (エージェント) が評価する値を与える関数。付与される価値が大きいほど，その行動が選択されやすくなる。
* **割引率** 強化学習において，将来の報酬に対して適用される減衰係数である。しばしば $\gamma$ (ガンマ) と表記され，$t=0$ のときは $\gamma^{0}=1$ である。1 時刻先では $\gamma^{1}$ となり n 時刻先では $\gamma^{n}$ などと表記される。$0\le\gamma\le1$ の範囲である。このため，$\gamma=0$ であれば，即時報酬のみが考慮され，将来に報酬が得られることが予想されたとしても考慮されない。
一方 $\gamma\neq1$ であれば，即時報酬と将来得られる報酬との重みが同等となる。
このため，即時に報酬が得られなくとも将来的に得られる報酬が相対的に重要視されることとなる。
* **マルコフ性** 現在の状態が次の時刻の状態に影響を与えることを指す。換言すれば，将来の状態は現在の状態によって定まることを指す。逆言すれば，将来の状態は過去によって定まるのではなく，現在の状態にのみ依存する。未来予測する場合に，過去に起こった全ての出来事は現在の状況に反映されているので，現在の状況だけを考慮すれば良いことを指す。
* **探索と実用のジレンマ** 過去の経験から報酬を最大にする行動を選ぶ実用 (「活用」と訳す場合もある) と，その実用によって選択されるべき行動を捨てて，新たな行動を探索する方が将来的に獲得される報酬が大きい場合もある。
この行動を探索と呼ぶ。これら探索と実用という 2 つ行動の選択において葛藤が生じるため，ジレンマと称される。Exploration and exploitation dilemma 
* **アドバンテージ関数** 強化学習において，$Q(s,a)$ 値と $V(s)$ との差分をアドバンテージ advantage と呼ぶ。
* **決闘 (ドュエリング) ネットワーク** アドバンテージを計算するために，Q 関数と 価値関数とを別々に分岐させたネットワークで算出し，各出力の差からアドバンテージを求めるように設計されたネットワーク。
* **二重 Q 学習** Q 関数は得られる報酬を最大化するために用いられる。このとき Q を更新するために，Q 自身の評価が入っている。
このことは Q の評価を行う際に，自分自身の値が入ってしまっていることを避ける目的で，Q 関数を 2 つ用意することが行われる。
このことを **二重 Q 学習** double Q learning と呼ぶ。
* **アクター・クリティック法** 動作主の内部に，行動を選択する機構，すなわちアクターと，アクターが行動を行った場合の環境から与えられた報酬を用いて，アクターの行動を評価する機構，すなわちクリティク (批評家) の両者を併せ持つ強化学習の手法。
具体的には，アクターは方針，クリティックは Q 関数 (または価値関数) である。

<!-- REINFORCE-with-baseline 法は、方策と状態価値値関数の両方を学習するが，その状態価値関数はベースラインとしてのみ使用され，批判者としては使用されないため，アクター批判法とはみなさない。
つまり，ブートストラップ (後続の状態の推定値からある状態の値の推定値を更新すること) には使用されず，推定値が更新されている状態のベースラインとしてのみ使用される。
ブートストラップによってのみ，バイアスと，関数近似の品質に対する漸近的な依存性が導入されるため，これは有用な区別である。
これまで見てきたように，ブートストラップと状態表現への依存によって導入されたバイアスは，分散を減らし，学習を加速するため，しばしば有益である。
分散を減らし，学習を加速することができるからです。
ベースラインを用いた REINFORCE は偏りがなく，漸近的に局所的な最小値に収束するが，他のモンテカルロ法と同様に学習速度が遅く (分散の大きい推定値を生成)，オンラインでの実装や継続的な問題には不便な傾向がある。
本書の前半で見てきたように，TD 法ではこれらの不都合を解消することができ，マルチステップ法ではブートストラップの度合いを自由に選択することができる。
方策勾配法の場合，これらの利点を得るために、ブートストラップ法のクリティックを用いた actor-critic 法を使用する。
In this chapter we consider methods that instead learn a parameterized policy that can select actions without consulting a value function. 
A value function may still be used to learn the policy parameter, but is not required for action selection. 
We use the notation theta in mathcal{R}^{d} for the policy's parameter vector. 
Thus we write pi(a |s; theta)=Pr[A_{t}=a|S_{t}=s;theta_{t}=theta] for the probability that action a is taken at time t given that the environment is in state s at time t with parameter theta.
If a method uses a learned value function as well, then the value function's weight vector is denoted w in Rd as usual, as in ^v(s;w).

Actor は学習された方策を指し critic は学習された価値関数 (通常は状態-価値関数) を指します。
最初に，パラメータ化された方策の下で開始状態の値として成績が定義されるエピソード的場合を扱い，その後，10.3 節のように平均報酬率として成績が定義される継続的な場合を検討する。
`actor` is a reference to the learned policy, and `critic` refers to the learned value function, usually a state-value function. 
First we treat the episodic case, in which performance is defined as the value of the start state under the parameterized policy, before going on to consider the continuing case, in which performance is defined as the average reward rate, as in Section 10.3.

13.5 Actor-Critic Methods

Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. 
That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), but only as a baseline for the state whose estimate is being updated. 
This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence on the quality of the function approximation. 
As we have seen, the bias introduced through bootstrapping and reliance on the state representation is often beneficial because
it reduces variance and accelerates learning. 
REINFORCE with baseline is unbiased and will converge asymptotically to a local minimum, but like all Monte Carlo methods it tends to learn slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing problems. 
As we have seen earlier in this book, with temporal-difference methods we can eliminate these inconveniences, and through multi-step methods we can exibly choose the degree of bootstrapping. 
In order to gain these advantages in the case of policy gradient methods we use actor{critic methods with a bootstrapping critic.
-->
* **優先度付き経験再生** 強化学習において，以前に経験したエピソードを含めたデータを用いて学習を行うことを経験再生 experience replay と呼ぶ。
経験再生においては，過去のエピソードからランダムにエピソードをサンプリングするのではなく，重要なエピソードを重点的に再生した方が学習の効率が良い。
そこで，重要な経験の方が重要ではない経験よりも頻繁にサンプリングされるように，経験したエピソードの散布稟議頻度に優劣をつけてサンプリング率に優先順位をつけることを **優先度付き経験再生** prioritized experience replay と呼ぶ。優先度は，過去のエピソードの集まりの中で，誤差が大きかったエピソードに対して，誤差の大きさに比例した重み付きのサンプリングを行うことがある。
これにより，重要なエピソードは繰り返し，反復して学習される経験が多くなる。言い換えれば，過去に犯した過ちのうち，重大な過ちほど学習を繰り返し，過ちを繰り返さなくなることに相当する。
* **方針 (ポリシー) 勾配法** 行動価値関数 Q を学習するのではなく，Q を与える方針をニューラルネットワークなどによって学習させる手法のこと。

<!-- * **内発的動機づけ** 
* **好奇心** -->

* **神経心理学** 脳の障害によって生じる種々の認知機能の変化から，脳の機構，機能，役割を考察する心理学の一分野。
言語，思考，記憶，意欲，行為などの障害が生じる機構を解明することを目指す。
表出する障害によって，失語症，注意障害，物体失認，無視，失行，などの症状に分類される。
* **EM アルゴリズム** 観測データ，未観測データ (もしくは潜在変数) の 2 種類があるときのモデルの最尤推定量を算出するアルゴリズム。
E ステップでは，未観測データの推定を行い，M ステップでは，未観測データを推定した値であるとみなして最尤推定を行う。
これらの E ステップと M ステップとを交互に繰り返すことにより，モデルの推定と未観測データ (もしくは潜在変数) の推定を行う手法である。
* **ストループ効果** 色のついた文字を読み上げる課題で，文字音読条件と文字の色名呼称条件とを比較した場合，両条件の音読潜時に差異が認められる現象。
* **ラバーハンド錯視** 視覚，触覚，および固有受容感覚の 3 者で不一致が生じると触覚において錯覚が生じる現象。
被験者の手を隠し，代わりにゴム製の手を被験者から見えるようにし，ゴム製の手と実際の手をブラシで同じ強度，リズム，で刺激することを繰り返す。テスト試行で，ゴム製の手だけをブラシで刺激すると，実際の手にブラシが触れたと報告されることがある。
* **マガーク効果** 聴覚情報と視覚情報との不一致によって生じる感覚統合における視覚優位を示す現象を指す。
被験者に対して，ある音韻を発話している映像と別の音韻の音声を組み合わせて視聴させた場合，第三の音韻が知覚されることが報告される場合がある。
ガ（ga）と発話している動画と共に，バ（ba）の音声を同時に提示した場合，「ガ」でも「バ」でもなく「ダ（da）」と聞こえ場合がある。 
被検査は，この矛盾を自覚することが難しいとされている。
マガーク効果は言語処理が行われる前に，視覚情報処理と聴覚情報処理がなされるためであると説明されている。
* **二重フラッシュ錯視** 視覚情報処理が聴覚情報によって影響を受ける現象の一つ。通常の視覚優位効果とは逆と言える。
視覚対象 (実験では，黒いディスプレイを背景とした白丸) が 1 回点滅する間に，音声情報 (ビープ音) を複数回同時提示すると，被験者はビープ音の回数だけ点滅しているとの知覚が報告される。
<!-- [イリュージョンフォーラムは錯覚の情報を集めたウェブサイト](https://illusion-forum.ilab.ntt.co.jp/index.html) -->
* **転移学習** 深層学習においては，事前訓練済のモデルに対して，標的とする課題に対して学習させる際に，最上位層である全結合層，あるいは，最終直下層付近のパラメータのみを更新し，それより下層のパラメータは凍結させることを指す。transfer learning 
* **微調整** 深層学習においては，事前訓練済のモデルに対して，標的とする課題に対して学習させる際に，全パラメータにわたる更新を行うことを指す。fine tuning
* **パレイドリア** 視覚的妄想の一つ。幻視と呼ばれることもある。
偏頭痛に伴う視覚障害では，単純な線分や白黒チェッカーパタンのような単純な幾何学模様が視認されるのに対して，パレイドリアでは高次視覚特徴により視覚経験が報告される。
このため，初期視覚情報処理ではなく，高次視覚情報処理の障害であると考えられる。
授業では，ディープラーニングモデルの上位層からのフィードバックによるトップダウン信号と，下位層 (初期視覚情報処理) との間の差異を強調するアルゴリズムを持ちいたモデルを実習した。
* **TLPA** 日本で失語症の臨床診断に用いられている 失語症語彙検査 (**T**est of **L**exical **P**rocessing in **A**phasia) の一つである。
語彙判断検査, 名詞表出検査, 名詞理解検査，絵画命名検査などの下位検査から構成されている。
日本に失語症検査としては WAB (西洋失語症検査項目 Western Aphasia Battery) 日本語版 や SALA (上智大学失語症言語分析 **S**ophia **A**nalysis of **L**anguage in **A**phasia) などがある。
* **相互活性化モデル** 入力情報と中間段階，出力情報との間に相互作用を仮定するモデル。
* **絵画命名課題** 認知心理学，神経心理学などで用いられる検査または課題の名称。
被検査者に対して，刺激図版 (近年では電子化された図版をディスプレイに提示する場合もある) を提示し，その刺激の視覚情報に基づく判断を求める。
失語症患者の中には，視覚的な誤りの他，意味的な誤り，あるいは単語の音や綴りに類似した誤りを表出する場合があるため，言語処理の神経機構を推定することが期待される。


