---
title: 第24回
author: 浅川 伸一
date: 2021_1203
layout: home
---

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 10/Dec/2021<br/>
Appache 2.0 license<br/>
</div>


# 第 24 回 マルチモーダル統合(2)
## 実習ファイル

- [変分自己符号化器 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0628vae_demo.ipynb)
- [ディープドリーム <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb)

<center>
<div class="figure">
<img src="/assets/model_proposed.svg" width="66%">
<div class="figcaption">
潜在空間にバイアスを組み込むことで多様性を表現するモデルの一般的な枠組み
</div>
</div>
</center>  

上図は，VAE または エンコーダ・デコーダモデル [@2014Sutskever_Sequence_to_Sequence] の中間に潜在変数 $z$ を仮定したモデルである。
基本的なアイデアは [@2016GenSeq] に示されているとおりである。
このモデルに cycleGAN [@2017Zhu_cycleGAN] の機構と評価関数を組み合わせること，および，潜在変数の事前確率 (prior) をバイアスを見なすことで，多様性とバイアスとを意図的な要素を加味していくことを試みた。
加えて ヘルムホルツマシン Helmholtz machines[@1995DayanHinton_Helmholtz;@1995Hinton_wake_sleep;@1996DayanHinton_Variety_Helmholtz] の機構を用いることで，多様な出力結果を得ることが可能である。

##### 幻視の成立機序モデル

上記の応用は，日常的な生成のみならず，精神病理的な異常な知覚にも応用が可能である。
[@2020Asakawa_jnps] では，レビー小体症などの認知症患者では幻視 (paleidoria) の症状を呈する場合があることが報告されている [@2012UchiyamaNishio_Pareidolias]。

下図に，幻視のシミュレーション例を示した。

<center>
<div class="figure">
<img src="/assets/tenn895x671.jpg" width="44%">
<img src="/assets/2019-06-07deepdream_tennanmon.jpg" width="44%">
<div class="figcaption">
パレイドリアのシミュレーション。左:入力画像，右:幻視の例 [@2019Asakawa_jnps]
</div>
</div>
</center>



$x$ はデータ集合 $\mathbb{D}$ から独立同一に抽出されたと考える。
このとき $\theta$ をパラメータとする $x$ の確率を $p(x;\theta)$ または $p_{\theta}(x)$ のように表記する。
条件付き確率を $p(x\vert \theta)$ のように表記する。
この場合 $\theta$ が与えられたときの $x$ の条件付き確率である。
また $p(x,\theta)$ は $x$ と $\theta$ との同時確率である。
ベイズの定理を使って，同時確率と条件付き確率との間には次のような関係がある。

### ベイズの定理
$$
\begin{aligned}
p(x,\theta)      &= p(x\vert \theta) p(\theta) =p(\theta\vert x) p(x)\\
p(x\vert \theta) &= \frac{p(\theta\vert x) p(x)}{p(\theta)}\\
p(\theta\vert x) &= \frac{p(x\vert \theta)p(\theta)}{p(x)}
\end{aligned}
$$

## 変分推論
<!-- From \strong{2008Blei topic model summer school} -->

* Alternative to MCMC; replace sampling with \strong{optimization}.
* Deterministic approximation to posterior distribution.
* Uses established optimization methods (block coordinate ascent; Newton-Raphson; interior-point).
* Faster, more scalable than MCMC for large problems.
* Biased, whereas MCMC is not.
* Emerging as a useful framework for fully Bayesian and empirical  Bayesian inference problems.


<!-- From \url{www.courecera.org/learn/bayesian-methods-in-machine-learning/} -->
* Full inference: $p(z,\theta\vert X)$
* Mean field Approximation: $q(z)q(\theta)\sim p(z,\theta\vert X)$
* EM algorithm  $q(z),\theta=\theta_{MP}$
* Variational EM: $q_{i}(z_{i}) = \theta_{MP}$
* Crisp EM: $z=z_{MP}$, $\theta=\theta_{MP}$


# 対数尤度関数
<!-- 
% \begin{itembox}{貧者のベイズ推論, MAP 推定}
%   \begin{equation}
%     \Hat{\theta}_{\text{ML}}=\argmax_{\theta}p\of{x;\theta}
%   \end{equation}
%   \begin{equation}
%     \Hat{\theta}_{\text{MAP}}=\argmax_{\theta}p\of{x;\theta}
%   \end{equation}
%   \begin{equation}
%     \Hat{theta}_{\text{MAP}}=\argmax_{\theta}p\of{\theta\given{x}}
%   \end{equation}
%   \begin{equation}
%     p\of{\theta\given{x}}
%     =\frac{p\of{x\given{\theta}}}{p\of{\theta}}\int p\of{x\given{\theta}}p\of{\theta}\;d\theta,
%   \end{equation}
% \end{itembox}
-->

**尤度** とは確率と一対一対応が取れる関数と考えて良い。
任意の関数 $f$ が確率密度関数であるためには，$f(x)\ge0$, $\int f(x)dx=1$ を満たしていなければならない。
一方尤度にはこれらの性質は養成されない。
尤度は以下のようにデータ $x_1,x_2,\ldots,x_n$ が与えられたときのモデルのパラメータの関数として表される場合が多い。
すなわち尤度とは観測データが与えられたときのモデルを記述するパラメータ $\theta$ の関数である <!-- [@2008Tzikas_VaBayes] -->。

$$
\mathcal{L}(\theta) =\prod_{i=1}^{N}p\left(\theta\vert x_{1},\ldots,x_{n}\right)\tag{2.66}
$$

**ガウシアン Gaussian** 関数であれば **確率密度関数 pdf** は
$$
q(x\vert \mu,\sigma^2) = 
\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left(-\frac{1}{2}\frac{\left(x-\mu\right)^2}{\sigma^2}\right)
$$

で与えられるので推定すべきパラメータは $\theta=\left(\mu,\sigma^2\right)^\top$ である。
対数尤度の期待値は以下のように書くことができる:

$$
\begin{aligned}
\mathcal{E}\left[-\log\mathcal{L}\right] &= -\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}\log q(x^{n})\\
                                         &= -\int p(x)\log q(x) dx
\end{aligned}
$$

上式は観測データから推測されるモデルのパラメータと分布の持つ真のパラメータとの距離と解釈することができる。
このとき次式を $p(x)$ の **エントロピー entropy** と呼び $H[x]$ と表記する場合もある。
$$
H[x]=\int p(x)\log p(x)dx.
$$
また 2 つの分布 $q(x)$ と $p(x)$ との距離あるいは相違を次式で定義する:
$$
\mathcal{L}=-\int p(x)\log\frac{q(x)}{p(x)}dx
$$
上式は **カルバックライブラー Kullback--Leibler 距離** あるいは divergence と呼ばれる。

分布を記述するパラメータの数が増えれば予測の精度は向上するのは当然であるが，ニューラルネットワークにおいては，パラメータ数はデータサイズに依存して増加するのではなく，真のモデルの複雑さに依存して増加すると言われる<!--  [@Bishop1995] -->。

# EM アルゴリズムと変分ベイズとの関連

<!-- % ここは PRML chapt. 9 かな？ -->
**EM アルゴリズム** [@Dempster1977] は観測されない変数を含む確率モデルのパラメータ推定に用いられるアルゴリズムである。
観測変数 $x$，潜在変数 $z$，及び推定すべきパラメータを $\theta$ とする。
もし潜在変数がなければ，パラメータ推定のためには\strong{最尤推定法 maximum likelihood} が一般に用いられる。
最尤推定法とは観測データ $x$ が与えられたとき対数尤度 $\log p(x\vert \theta)$ を最大にするような $\theta$ を求める手法である。
EM アルゴリズムは観測可能な変数 $x$ と潜在変数 $z$ を併せた完全データの最尤推定を利用して， 観測できるデータからなる不完全データの最尤推定を行なうためのアルゴリズムである。観測可能なデータだけで構成されている不完全データを用いて最尤推定を行なうためには
$$
\log p(x\vert \theta)
$$
の最大化を行なえばよい。
完全データを用いて最尤推定を行なうことは簡単でも，不完全データの最尤推定を直接行なうのはできない場合が多い。

そこで，この対数尤度関数を近似することを考える。任意の確率分布 $q(z\vert x)$ について以下の関係が成立する。

$$
\begin{aligned}
\log p(x\vert\theta) &= \log\sum_{z}p(z,x\vert \theta)\\
                     &=\log\sum_{z}q(z\vert x)\frac{p(z,x\vert \theta)}{q(z\vert x)}
\end{aligned}
$$
**イェンセン の不等式 Jensen's inequality** から関数 $f$ が上に凸であれば次式が成り立つ:
$$
\mathbb{E}\left[f(x)\right]\le f(\mathbb{E}[x])
$$

ここで $\mathbb{E}[\cdot]$ は期待値をとる操作を表す。
上式で $Y=\frac{p(z,x\vert \theta)}{q(z\vert x)}$,
$f(\cdot) = \log(\cdot)$ とすると以下のようになる:

$$
\begin{aligned}
\mathbb{E}\left[f(Y)\right] &= \sum_{z} q(z\vert x) \log\frac{p(z,x\vert \theta)}{q(z\vert x)}\\
                            &  \le f(\mathbb{E}[Y])\\
                            &= \log\sum_{z}q(z\vert x) \frac{p(z,x\vert \theta)}{q(z\vert x)}
\end{aligned}
$$

この不等式を適用すると次式を得る:
$$
\begin{aligned}
\log p(x\vert\theta) &= \log\sum_{z} p(z,x\vert \theta)\\
                     &= \log\sum_{z} q(z\vert x) \frac{p(z,x\vert \theta)}{q(z\vert x)}\\
                     &\ge \sum_{z} q(z\vert x) \log\frac{p(z,x\vert \theta)}{q(z\vert x)}\\
                     &= \sum_{z}q(z\vert x)\log p(z,x\vert \theta) - \sum_{z}q(z\vert x) \log q(z\vert x)\\
                     &= \mathcal{L}(q,\theta)
\end{aligned}
$$

これは $\mathcal{L}(q;\theta)$ が $\log(x\vert \theta)$ の下界を与えることを示すものである。

さらに $\log p(x\vert \theta) = \sum_{z} q(z\vert x)\log p(x\vert \theta)$ すなわち $q(z\vert x)$ の全ての $z$ に対して加算すれば $z$ が消去できることと，
$p(z\vert x,\theta)=p(z,x\vert\theta)/p(x\vert\theta)$ であることから，

$$
\begin{aligned}
\log p(x\vert \theta) - \sum_{z} q(z\vert x) \log p(z,x\vert \theta)  &= \sum_{z} q(z\vert x)\log p(x\vert\theta) - \sum_{z}q(z\vert x)\log p(z,x\vert\theta)\\
&=  \sum_{z}q(z\vert x)\log\left(\frac{p(x\vert \theta)}{p(z,x\vert \theta)}\right)\\
&=-\sum_{z}q(z\vert x)\log\left(\frac{p(z,x\vert \theta)}{p(x\vert \theta)}\right)\\
&=-\sum_{z}q(z\vert x)\log\left(p(z\vert x,\theta)\right)
\end{aligned}
$$

式(asa2) と (asa1) が等しく，また (asa3) と (asa4) とが等しいことが分かる。
ここで (asa1) の第 1 項と (asa3) の第 2 項が等しいので

$$
\begin{aligned}
d &= \log p(x\vert \theta) -\mathcal{L}(q;\theta)\\
  &= -\sum_{z}q(z\vert x) \log p(z\vert x,\theta) + \sum_{z} q(z\vert x) \log q(z\vert x)\\
  &= \sum_{z}q(z\vert x)\log\frac{q(z\vert x)}{p(z\vert x,\theta)}\ge0
\end{aligned}
$$
となるから，$d$ は $q(z\vert x)$ と $p(z\vert x,\theta)$ のカルバックライブラー距離 (KL 距離) Kullbuck--Leibler divergence となることが分かる。
KL 距離の性質より$d$ が最小になるのは 
$ q(z\vert x) = p(z\vert x,\theta) $ のときである。
すなわち $\mathcal{L}(q;\theta)$ は固定されたパラメータ $\theta$ の元で $q=p(z\vert x,\theta)$ のときに最大になることを示している。
このとき以下が成立する:
$$
\log p(x\vert \theta) =\mathcal{L}(q;\theta) =\mathcal{L}(p(z\vert x,\theta)\theta
$$

また，式\eqref{asa1},\eqref{asa2}において
$$
\mathcal{L}(q;\theta) = \sum_{z}q(z\vert x) \log p(z,x\vert \theta) -\sum_{z}q(z\vert x) \log q(z\vert x)
$$
パラメータ $\theta$ に依存するのは
$$
\mathcal{L}(q;\theta) = \sum_{z}q(z\vert x)\log p(z,x\vert \theta)
$$
だけであるから，以下のようなアルゴリズムによって $\log p(x\vert \theta)$ の単調非減少列を得ることができる。

* 固定した $\theta^{(k)}$ と観測データ $x$ の下で $q\leftarrow p(z\vert x,\theta^{(k)})$ を計算する (E ステップ)
* 上で求めた $q$ の下で $\theta^{(k+1)} = \arg\max_{\theta}\sum_{z} q(z\vert x)\log p(z,x\vert \theta)$ を求める (M ステップ)


