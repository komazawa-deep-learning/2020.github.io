<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/assets/css/style.css"></head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    CommonHTML: { matchFontHeight: false },
    displayAlign: "left",
    displayIndent: "2em",
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
    }
  });
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS_CHTML"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
  <body>
<div class="header">
  <div class="wrap">
    
      <div class="header__inner header__inner--internal">
    
      <div class="header__content">
        <h1 class="header__title">
          
        </h1>
        <p class="header__tagline">
          
        </p>
      </div>
    </div>
  </div>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">第10回</h1><h1 id="ディープラーニングの心理学的解釈-心理学特講iiia">ディープラーニングの心理学的解釈 (心理学特講IIIA)</h1>

<div align="right">
<a href="mailto:educ0233@komazawa-u.ac.jp">Shin Aasakawa</a>, all rights reserved.<br />
Date: 18/Jun/2021<br />
Appache 2.0 license<br />
</div>

<h1 id="0-期末試験の相談">0. 期末試験の相談</h1>

<h1 id="1-キーワード">1. キーワード</h1>

<ul>
  <li>転移学習 Transfer Learning</li>
  <li>ファインチューニング Fine Turing</li>
  <li>蒸留 distillation</li>
  <li>教師 生徒 学習 teacher-student learning</li>
  <li>自己教師あり学習 SSL: Self Supervised Learning</li>
  <li>対比学習 CL: Contrastive Learning あるいは 対比予測符号化 contrasitive predictive coding</li>
  <li>CAM: Class Associated Memory</li>
</ul>

<h1 id="2-実習">2. 実習</h1>
<ul>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb" target="_blank">フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
  <li>
    <p><a href="https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb">データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></p>
  </li>
  <li><a href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb" target="_blank">CAM 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
  <li><a href="https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb" target="_blank">各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg" /></a></li>
  <li><a href="https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS">EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg" /></a></li>
</ul>

<h1 id="3-画像処理関係のトレンドと-sota">3. 画像処理関係のトレンドと SOTA</h1>

<ol>
  <li>R−CNN から位置情報と物体情報とを切り分けて，実時間処理が可能に</li>
  <li>YOLO, SSD</li>
  <li>Squeeze-and-Extention など分岐して結合する流れ</li>
  <li>EfficientNet</li>
  <li>対比学習 によりトップ 1 精度でも性能向上</li>
</ol>

<h2 id="31-noisystudent">3.1. NoisyStudent</h2>
<center>
    <img src="/assets/2020Xie_NoisyStudent_fig2ja.svg" style="width:66%" /><br />
    教師あり学習の SOTA (Xie 他, 2020, Fig. 2 を改変)
</center>

<h2 id="32-swav">3.2 SwAV</h2>
<center>
    <img src="/assets/2020Jaiswal_fig3ja.svg" style="width:88%" /><br />
    対比学習 の SOTA (Jaiswal 他, 2020, Fig. 3 を改変)
</center>

<h2 id="33-クラス活性マッピング-cam-class-activation-mapping">3.3 クラス活性マッピング CAM: Class Activation Mapping</h2>

<center>
    <img src="http://gradcam.cloudcv.org/static/images/network.png" /><br />
    Grad-CAM の概念図
</center>

<h3 id="331--cam-の例-正確には-grad-cam-の例">3.3.1  CAM の例 （正確には grad-CAM の例）</h3>

<center>
    <img src="/assets/2016Grad-CAM_boxer_tigercat.png" style="width:30%" />
    <img src="/assets/2016Grad-CAM_boxer.png" style="width:30%" />
    <img src="/assets/2016Grad-CAM_tigercat.png" style="width:30%" />
    <div align="left" style="width:88%">
        左:入力画像, 中:ボクサーに対応するヒートマップ, 右:トラ猫に対応するヒートマップ
    </div>
</center>

<h2 id="32-対比学習-contrastive-learning">3.2. 対比学習 contrastive learning</h2>

<center>
<img src="/assets/2020Jaiswal_fig1.svg" style="width:88%" /><br />
<img src="/assets/2020Jaiswal_fig2.svg" style="width:88%" /><br />
</center>

<h1 id="33-蒸留-distillation">3.3. 蒸留 distillation</h1>

<h1 id="34-教師-生徒-学習-teacher-student-learning">3.4. 教師 生徒 学習 teacher student learning</h1>

<h2 id="4-本日あまり触れない話題">4. 本日あまり触れない話題</h2>

<ul>
  <li>R-CNN によって，位置 where 情報と 物体 what 情報 とを多層畳み込みニューラルネットワークで表現する試みが，発展。実時間で物体の切り出しと認識とが行えるようになった。<a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank">Faster R-CNN</a>, <a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank">YOLO</a>, <a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank">SSD</a>,</li>
</ul>

<h3 id="41-nas">4.1. NAS</h3>

<p>NAS とは 最適なネットワーク構成の自動探索 Network Architecture Search です。</p>

<p>アレックスネット以来，CNN は精度向上したが巨大になった。</p>
<ul>
  <li>2014年 GoogLeNet 精度 74.8%，パラメータ数 6.8M</li>
  <li>2017年 SENet 精度 82.7% パラメータ数 145M</li>
  <li>
    <p>2018年 GPipe 精度 84.3% パラメータ数 557M</p>
  </li>
  <li>パラメータ最適化のみであればベイズ最適化や遺伝アルゴリズムやグリッドサーチで実現可能。</li>
  <li>NAS は，さらに
    <ol>
      <li>ネットワークの設計</li>
      <li>ハイパーパラメータのチューニング</li>
      <li>学習 （最適化）
を繰り返して最適化を目指します。</li>
    </ol>
  </li>
  <li>ただし，自動化によるネットワークの最適化だけでなく，ネットワークを小さくするための人力の努力が行われてきました。</li>
  <li>ネットワークを小さくすることで，貧弱な計算資源しか持たない端末でも実行可能となり，学習も高速化します。</li>
</ul>

<h3 id="42-mnasnet">4.2. MNasNet</h3>

<center>
<img src="/assets/2019Tan_MnasNet_fig7.svg" style="width:77%" /><br />
@2019Tan_MnasNet Fig. 7. MBConv: モバイルネット反転ボトルネック畳み込み。MBConv の後の数字は特徴数（チャンネル数）の何倍かを表す。
</center>

<p>MBconv とは mobile inverted bottleneck の意味。MBconv は MobileNetV2 で導入された</p>

<h3 id="43-モバイル反転ボトルネック畳み込み">4.3. モバイル反転ボトルネック畳み込み</h3>

<center>
<img src="/assets/2019MobileNetV2_regular_conv.svg" style="width:23%" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2019MobileNetV2_separable_conv.svg" style="width:33%" /><br />
左: 通常の畳み込み，右: 分離可能畳み込み 出典: MobileNetV2。
通常の畳み込み演算は，幅，高さ，深さ (チャンネル, 特徴) の 3 次元で行われる。この操作を，各チャンネル毎の畳み込みと，各点における 1x1 畳み込み の積に因子分解する。<br />

<img src="/assets/2019MobileNetV2_bottleneck_conv.svg" style="width:43%" />&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2019MobileNetV2_expansion_conv.svg" style="width:49%" /><br />
因子分解化畳み込みの最終操作である 点毎の畳み込み (1x1 pointwise) はチャンネル数だけ繰り返される。<br />
チャンネル数を一旦少なくしてしまえば，情報圧縮になる。ボトルネックはこれを実現したものである。
ボトルネック層には全ての情報が埋め込まれていると考えられる。従って，ボトルネック層を使ってビルディングブロックを構成しても良いことになる。
これが，<strong>反転ボトルネック因子分解可能畳み込み</strong>である。
出典: MobileNetV2
</center>

<h3 id="44-senet-squeeze-and-excitation-net">4.4. SENet (Squeeze-and-Excitation Net)</h3>

<center>

<img src="/assets/2017Hu_SENet_fig2.svg" style="width:66%" /><br />
Hu et. al (2017) Fig. 2 を改変
</center>

<center>

<img src="/assets/2017Hu_SENet_fig1.svg" style="width:88%" /><br />
出典: Hu et. al (2017) Fig. 1 を改変
</center>

<h3 id="45-yolo">4.5. YOLO</h3>

<center>
    <img src="/assets/2015Redmo_YOLO_fig2ja.jpg" style="width:77%" /><br />
    YOLO の概観
</center>

<h3 id="45-efficientnet-のパラメータ数">4.5. EfficientNet のパラメータ数</h3>

<center>
    <img src="/assets/2019EfficientNet_params_ja.svg" style="width:77%" /><br />
    EfficientNet のパラメータ数比較
</center>

<center>
    <img src="/assets/2019TanLVe_EfficientNet_fig1.svg" style="width:66%" /><br />
</center>

<!--
- MobileNet, EfficientNet

- `/study/2020Tessellate-Imaging_Pytorch_Tutorial.git/E) RoadMap 5 - Data 2 - Transformations (General).ipynb`
-->

<!-- 

- [本日の課題 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626homework.ipynb){:target="_blank"}

# 実習
- [word2vecの先週の再録 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"}
- [BERT の注意の視覚化 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626BERT_head_view.ipynb){:target="_blank"}
- [日本語 BERT の実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb){:target="_blank"}
-->

<h2 id="雑談余談">雑談，余談</h2>

<p>鳥の翼や羽の構造と空を飛ぶための仕組みの解明と飛行機との関係について。
鳥類や昆虫の翅と飛行機との対比は，人間の脳に宿る知性と，人工知能，あるいはニューラルネットワークモデルとの対比がなされます。
調べる限り サイエンティフィック・アメリカン に掲載された Ford と Hayes の記事が出典のようです。
この記事によれば，鳥の羽の構造の研究だけからは，飛行機は生まれなかった。飛行機を実用化するために必要な実験は「人工翼」の風洞実験でした。
飛行機の実現がもたらしたものは，空力学の理解，鳥の羽と飛行機と飛行することについての深い理解でした。
鳥の羽の解剖学は，つぎはぎ，付け足しから成る進化の産物である羽は，かえって飛行の本質を捉えにくかったと考えられます。
「鳥の羽」と「人工の翼」との関係を，「人間の脳」あるいは脳に宿る知性と「ニューラルネットワーク」に置き換えて考えれば，
人間の知性を，微に入り際に入り調べること，そこから一旦離れて，別の材料を用いた実験を行うことで，
人間と動物と機械の全てに共通する知性について深い理解が得られるだろう，と前述の Ford と Hayes は書いています。</p>

<p>Ford, K. and Hayes, P. (1998) On Computational Wings: Rethinking the Goals of Artificial Intelligence, Scientific American, 9(4), 79-83.</p>

<!-- 
## 復習


## 0.1 デモ
- 前回できなかった [GAN のデモ TL-GAN (transparent latent-space GAN)<img src="https://komazawa-deep-learning.github.io/assets/kaggle-site-logo.png" style="width:09%">](https://www.kaggle.com/summitkwan/tl-gan-demo){:target="_blank"}
- [漱石「こころ」冒頭部分を文字ベースリカレントニューラルネットワークで言語モデル javascript 版](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_SRN_simulator.ipynb">2019cnps_SRN_simulator<img src="../assets/colab_icon.svg"></a>

- [日本国憲法第 9 条をリカレントニューラルネットワークで理解する <img src="https://komazawa-deep-learning.github.io./assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619SRN_simulator.ipynb){:target="_blank"}
- [書画のデモ <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619sketch_RNN.ipynb){:target="_blank"}
- [word2vecのデモ <img src="https://komazawa-deep-learning.github.io./assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"}
-->

<!--
- [書画のデモ<img src="../assets/colab_icon.svg">](https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_sketch_RNN_demo.ipynb){target="_blank"}
<a target="_blank" href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619SRN_simulator.ipynb">日本国憲法第 9 条をリカレントニューラルネットワークで理解する <img src="../assets/colab_icon.svg"></a>
- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb">足し算をするリカレントニューラルネットワーク<img src="../assets/colab_icon.svg"></a>
- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_sketch_RNN_demo.ipynb">書画のデモ<img src="../assets/colab_icon.svg"></a>
-->

<!-- 
#### Want more? Check out

# 用語集

## 非線形性
- ReLU
- Sigmoid
- Tanh
- GRU
- LSTM

## 最適化
- SGD
- Momentum
- RMSProp
- Adagrad
- Adam
- KFac

## 結合パターン
- 完全結合
- 畳込み
- Dilated
- 再帰結合
- スキップコネクト，残渣

## 損失関数
- 交差エントロピー
- 敵対学習
- 変分原理
- 最尤法
- L2

## ハイパーパラメータ
- 学習率
- 層数
- バッチサイズ
- ドロップアウト率
- 初期化
- データ拡張
- 勾配クリップ
- モーメント

-->



  </div>

      </div>
    </main><div class="footer">
  <div class="wrap">
<!--
Thanks to <a href="http://www.imdb.com/">IMDB</a> for all the serie informations!
-->
駒澤大学
  </div>
</div>
<!---
<script src="https://datocms-middleman-example.netlify.com/javascripts/all.js"></script>
-->

</body>

</html>
