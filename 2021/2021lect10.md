---
title: "第10回"
author: 浅川 伸一
layout: home
---

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 18/Jun/2021<br/>
Appache 2.0 license<br/>
</div>

- [本日の課題 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626homework.ipynb){:target="_blank"}

# 実習
- [word2vecの先週の再録 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"}
- [BERT の注意の視覚化 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0626BERT_head_view.ipynb){:target="_blank"}
- [日本語 BERT の実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0624BERTja_test.ipynb){:target="_blank"}


## 雑談，余談

鳥の翼や羽の構造と空を飛ぶための仕組みの解明と飛行機との関係について。
鳥類や昆虫の翅と飛行機との対比は，人間の脳に宿る知性と，人工知能，あるいはニューラルネットワークモデルとの対比がなされます。
調べる限り サイエンティフィック・アメリカン に掲載された Ford と Hayes の記事が出典のようです。
この記事によれば，鳥の羽の構造の研究だけからは，飛行機は生まれなかった。飛行機を実用化するために必要な実験は「人工翼」の風洞実験でした。
飛行機の実現がもたらしたものは，空力学の理解，鳥の羽と飛行機と飛行することについての深い理解でした。
鳥の羽の解剖学は，つぎはぎ，付け足しから成る進化の産物である羽は，かえって飛行の本質を捉えにくかったと考えられます。
「鳥の羽」と「人工の翼」との関係を，「人間の脳」あるいは脳に宿る知性と「ニューラルネットワーク」に置き換えて考えれば，
人間の知性を，微に入り際に入り調べること，そこから一旦離れて，別の材料を用いた実験を行うことで，
人間と動物と機械の全てに共通する知性について深い理解が得られるだろう，と前述の Ford と Hayes は書いています。

Ford, K. and Hayes, P. (1998) On Computational Wings: Rethinking the Goals of Artificial Intelligence, Scientific American, 9(4), 79-83.

## 復習


## 0.1 デモ
- 前回できなかった [GAN のデモ TL-GAN (transparent latent-space GAN)<img src="https://komazawa-deep-learning.github.io/assets/kaggle-site-logo.png" style="width:09%">](https://www.kaggle.com/summitkwan/tl-gan-demo){:target="_blank"}
- [漱石「こころ」冒頭部分を文字ベースリカレントニューラルネットワークで言語モデル javascript 版](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
<!--- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_SRN_simulator.ipynb">2019cnps_SRN_simulator<img src="../assets/colab_icon.svg"></a>
-->
- [日本国憲法第 9 条をリカレントニューラルネットワークで理解する <img src="https://komazawa-deep-learning.github.io./assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619SRN_simulator.ipynb){:target="_blank"}
- [書画のデモ <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619sketch_RNN.ipynb){:target="_blank"}
- [word2vecのデモ <img src="https://komazawa-deep-learning.github.io./assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb){:target="_blank"}

<!--
- [書画のデモ<img src="../assets/colab_icon.svg">](https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_sketch_RNN_demo.ipynb){target="_blank"}
<a target="_blank" href="https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619SRN_simulator.ipynb">日本国憲法第 9 条をリカレントニューラルネットワークで理解する <img src="../assets/colab_icon.svg"></a>
- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_addtion_rnn.ipynb">足し算をするリカレントニューラルネットワーク<img src="../assets/colab_icon.svg"></a>
- <a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_sketch_RNN_demo.ipynb">書画のデモ<img src="../assets/colab_icon.svg"></a>
-->

#### Want more? Check out

# 用語集

## 非線形性
- ReLU
- Sigmoid
- Tanh
- GRU
- LSTM

## 最適化
- SGD
- Momentum
- RMSProp
- Adagrad
- Adam
- KFac

## 結合パターン
- 完全結合
- 畳込み
- Dilated
- 再帰結合
- スキップコネクト，残渣

## 損失関数
- 交差エントロピー
- 敵対学習
- 変分原理
- 最尤法
- L2

## ハイパーパラメータ
- 学習率
- 層数
- バッチサイズ
- ドロップアウト率
- 初期化
- データ拡張
- 勾配クリップ
- モーメント

---



---

<center>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Saddle_point.gif' style='width:74%'><br>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Beales_function.gif' style='width:74%'><br>
<img src='https://komazawa-deep-learning.github.io/assets/2014Imgur_Long_Valley.gif' style='width:74%'><br>
</center>


## 0.2 再帰型 (リカレント) ニューラルネットワークモデル
**リカレントニューラルネットワークモデル** (Recurrent Neural Networks, 以下 RNN) は **系列情報処理** (serial information processings) を扱うニューラルネットワークモデル
です。
観察された証拠から次に生じる事象を予想することは，生物の生存にとって意味ある情報処理であると考えます。
その適用範囲を思いつくままに考えてみると以下のような事柄が含まれるでしょう。

2. 生物の生存戦略
3. 制御，予測。天気予報，ロケットなどの弾道制御
1. データ処理
4. 未来予想，SF 的，心理学的，哲学的，歴史的意味あいも含めて。身近な例では占いや経済予測も含まれます

<!--
    - <a target="_blank" href="https://gauss-ai.jp/2019/04/05/siva%E9%81%8B%E5%96%B6%E5%85%83%E5%A4%89%E6%9B%B4%E3%81%AE%E3%81%8A%E7%9F%A5%E3%82%89%E3%81%9B/">競馬予想</a>[^gauss_supervisor], 
     - <a target="_blank" href="https://www.shikaku-square.com/yobishiken/miraimon">資格試験問題予測 "未来問"</a>

[^gauss_supervisor]: <a target="_blank" href="https://gauss-ai.jp/about/">この会社の技術顧問って $\ldots$ :)</a>
-->

<!--神経心理学モデルへの適用例では初期の読みのモデルから用いられて来ました。
1980年代のトライアングルモデル(Seidenberg and McClelland, 1989; Plaut et. al, 1996) や系列位置効果を検討する際
，用いられます。
-->

一方，機械学習，ディープラーニングの分野では，系列情報処理の中の **言語モデル** (Language models) として頻用されています。
昨今の **自然言語処理** (Natural Language Processings, 以下 NLP) では **機械翻訳** や種々の処理に採用されてきました。
2014 年以降の話題として **注意** (attention) を言語モデルに取り込んで精度向上を目指す動向が活発です。
**注意** と **言語** とはどちらも 心理学分野 で注目すべき話題でしょう。
RNN の応用可能性は神経心理学にとって一考の価値があるモデルと言えます。

2018 年，複数の言語課題で人間の成績を凌駕する自然言語処理モデルが提案されました。
このことから自然言語処理モデルを神経心理学に応用する機運は熟していると考えます。

## 0.3 リカレントニューラルネットワークの成果
- <a target="_blank" href="http://people.idsia.ch/~juergen/nips2009.pdf">手書き文字認識 Graves(2009)</a>
- <a target="_blank" href="https://arxiv.org/abs/1303.5778">音声認識 Graves (2013)</a>, <a target="_blank" hre
f="http://proceedings.mlr.press/v32/graves14.html">Grave and Jaitly (2014)</a>
- <a target="_blank" href="https://arxiv.org/abs/1308.0850" >手書き文字生成 Graves (2013)</a>
- <a target="_blank" href="https://arxiv.org/abs/1409.3215">系列学習 Sutskever (2014)</a>
- <a target="_blank" href="https://arxiv.org/abs/1409.0473">機械翻訳 Bahdanau (2014)</a>
- <a target="_blank" href="https://arxiv.org/abs/1508.04025">機械翻訳 Luong (2015)</a>
- <a target="_blank" href="https://arxiv.org/abs/1411.4555">画像脚注付け Vinyals et. al(2014)</a>
- <a target="_blank" href="https://arxiv.org/abs/1502.03044">注意つき画像脚注生成</a>
- <a target="_blank" href="https://arxiv.org/abs/1412.7449">構文解析 Vinayals et. al., (2014)</a>
- <a target="_blank" href="https://openreview.net/pdf?id=ByldLrqlx">プログラムコード生成 Zaremba (2015)</a>
- <a target="_blank" href="https://arxiv.org/abs/1506.05869">対話生成 Vinyals (2014)</a>
- <a target="_blank" href="https://arxiv.org/abs/1410.5401">ニューラルチューリングマシン NTM Graves et. al, (2
014)</a></a>
- <a target="_blank" href="https://worldmodels.github.io/">世界モデル Ha and Schmithuber (2018)</a>[^about_world_model]

<!--
- Machine generated [TED Talks](https://www.youtube.com/watch?v=-OodHtJ1saY)
-->

[^about_world_model]: 我々を取り巻く世界のイメージは脳内のメンタルモデルである。誰しも全ての世界，政府，国を想像できない。
我々は現実世界の表象するコンセプトを選んでその関係を使うだけだ(浅川拙訳)と<a target="_blank" href="https://en.wikipedia.org/wiki/Mental_model">フォレスター</a>
は言ったそうです。

## 0.4 参考文献

<!--- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
- <a target="_blank" href="../2019src2003final.pdf">浅川伸一 (2003) 単純再帰型ニューラルネットワークの心理学モデルとしての応用可能性, 心理学評論, 46(2), 274-287.</a>
- <a target="_blank" href="../6657.pdf">浅川伸一 (2016) リカレントニューラルネットワーク, 日本人工知能学会編，人工知能学事典新版，共立出版</a>
- <a target="_blank" href="../6658.pdf">浅川伸一 (2016) リカレントニューラルネットワークによる文法学習, 日本人工知能学会編，人工知能学事典新版，共立出版</a>

# 1. 自然言語処理 

## 1.1. 自然言語処理前史

1. 第一次ブーム 1960 年代
極度の楽観論: 辞書を丸写しすれば翻訳は可能だと思っていた，らしい...
2. 第二次ブーム 統計的自然言語処理
    -  [統計的言語モデル statistical language model](https://en.wikipedia.org/wiki/Language_model)
    - <a target="_blank" href="https://nlp.stanford.edu/manning/">Chris Manning (スタンフォード大学)</a>) and Schutze (1999) 著。定番の教科書 <a target="_blank" href="https://nlp.stanford.edu/fsnlp/">Fundations of Statistical Natural Language Processing</a>, あるいは
    <a target="_blank" href="https://nlp.stanford.edu/fsnlp/promo/">こちら</a>
    - もう一つ定評の教科書 <a target="_blank" href="https://web.stanford.edu/~jurafsky/">Jurafsky</a> 著) と Martin 著 <a target="_blank" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> は <a target="_blank" href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">改訂版</a> が出版されました。ニューラルネットワークによる言語モデルも載っています。

## 1.2. 用語解説
- <a target="_blank" href="https://en.wikipedia.org/wiki/Bag-of-words_model">BoW</a>: Bag of Words 単語の袋。ある文章を表現する場合に，各単語の表現を集めて袋詰めしたとの意味。従って語順が考慮されません。"犬が男を噛んだ" と "男が犬を噛んだ" では同じ表現になります。LSA, LDA, fastText なども同じような表現を与えます。
- TF-IDF: 単語頻度 (Term Frequency) と 逆(Inverse) 文書頻度 (Document Frequency) で文書のベクトル表現を定義する手法です。何度も出現する単語は重要なので単語頻度が高い文書には意味があります。一方，全ての文書に出現する単語は重要とは言えないので単語の出現る文書の個数の逆数の対数変換を用います。このようにしてできた文章表現を TF-IDF と言います。

##  言語モデル Language model
- 文献では言語モデルを **LM** と表記される。
- [統計的言語モデル statistical language model](https://en.wikipedia.org/wiki/Language_model)。言語系列に確率を与えるモデルの総称。良い言語モデル LM は，有意味文に高い確率を与え，曖昧な文には低い確率を与える。言語モデルは人工知能の問題。
1. n-gram 言語モデル
2. 指標: BELU, perplexity
3. 課題: NER, POS, COL, Summary, QA, Translation


<!--
## 関連分野
系列情報処理モデルには各分野で多くの試みがなされている。たとえば

1. 状態空間モデル (SSM), 隠れマルコフモデル (Hidden Markov models: HMM)
2. 自己回帰モデル (AR, ARMA, ARIMA, Box=Jenkins)
3. フィルタリング理論: カルマンフィルタ (Kalman filters), 粒子フィルタ(経済学部矢野浩一先生による[粒子フィルタの解説論文](https://www.terrapub.co.jp/journals/jjssj/pdf/4401/44010189.pdf))
3. ニューラルネットワーク
-->

## 1.3. N-グラム言語モデル

- 類似した言語履歴 $h$ について, n-gram 言語モデルは言語履歴 $h$ によって言語が定まることを言います。
- 実用的には n-gram 言語モデルは $n$ 語の単語系列パターンを表象するモデルです。
- n-gram 言語モデルでは $n$ の次数増大に従って，パラメータは指数関数的に増大します。
- すなわち高次 n グラム言語モデルのパラメータ推定に必要な言語情報のコーパスサイズは，次数増大に伴って，急激不足します
- Wikipedia からの引用では次式:
$$
p(w_1,\dots,w_m)=\prod_{i=1}^{m} P(w_i\vert w_1,\ldots,w_{i-1})\simeq \prod_{i=1}^{m}p(w_i\vert w_{i-(n-1)},\ldots,w_{i-1})
$$
- 上式では $m$ ですが，伝統的に $n$ グラムと呼びます。$n=1$ であれば直前の 1 つを考慮して
次語を予測することになります。

<!--
- n-グラム言語モデル: 文脈 $h$ の中で単語 $w$ が何回出現したかをカウント。観測した全ての文脈 $h$ で正化
- 伝統的解: n-グラム言語モデル: $P\left(w\vert h\right)=\displaystyle\frac{C\left(h,w\right)}{C\left(h\right)}$
- 確率 $p\left(w_n\vert w_{1},\ldots,w_{n-1}\right)$
-->

<!-- # from Manning (1999) page 191.

In such a stochastic problem, we use a classification of the previous
words, the _history_ to predict the next word. On the basis of having looked
at a lot of text, we know which words tend to follow other words.

For this task, we cannot possibly consider each textual history separately:
most of the time we will be listening to a sentence that we have
never heard before, and so there is no previous identical textual history
on which to base our predictions, and even if we had heard the beginning
of the sentence before, it might end differently this time. And so we
-->

余談[^gram] ですが

- $n=0$: ヌルグラム null-gram 
- $n=1$: ユニグラム uni-gram
- $n=2$: バイグラム bi-gram
- $n=3$: トリグラム tri-gram

などと呼ばれます。

[^gram]: 五月蝿いことを言えば Manning (1999, p.193) によると単語 _gram_ はギリシャ語由来の単語だそうです。従って _gram_ に付ける数接頭辞もギリシャ語である教養を持つべきです。そうすると $n=1$: mono-gram, $n=2$: di-gram, $n=4$: tetra-gram が教養です。$n=3$ はギリシャ，ローマ共通で tri-gram です。日常会話では $n=4$ をクワッドグラム(ラテン語由来)やフォーグラムと呼ぶことも多いです。

<!--
The cases of n-gram models that people usually use are for $n=2,3,4$ and
these alternatives are usually referred to as a bigram, a trigram
four-gram, model, respectively. Revealing this will surely be enough to
cause any Classicists who are reading this book to stop, and to leave the
field to uneducated engineering sorts: is a _gram_ is a Greek root and so
should be put together with Greek number prefixes.  Shannon actually did
use the term but with dtigram, with declining levels of education in recent
decades, this usage has not survived. As non-prescriptive linguists,
however, we think that the curious mixture of English, Greek, and Latin
that our collegues actuall use is quite fun.  So we will try to stamp it out.
Rather than _four-gram_, some people do make an attempt at appearing educated
by saying _quad-gram_, but this is not really correct use of a Latin number
prefix (which would give _quadgram_ cf. _quadilateral_), let alone correct
use of a Greek number prefix, which would give us "a _tetragram_ model.”
that we have to specify to determine a particular model within that model space.
-->


## 1.4. ニューラルネットワーク言語モデルあるいはミコロフ革命
<center>
<img src="../assets/Mikolov_portrait.jpg" style="width:24%">
<img src="../assets/2015Mikolov_NIPSportrait.png" style="width:33%"><br>
</center>

- スパースな言語履歴 $h$ は低次元空間へと射影される。類似した言語履歴は群化する
- 類似の言語履歴を共有することで，ニューラルネットワーク言語モデルは頑健
(訓練データから推定すべきパラメータが少ない)。

## 1.5 ニューラルネットワーク言語モデル NNLM フィードフォワード型 NNLM

<center>
<img src="../assets/2012Mikolov_Google_Slides8.svg" style="width:74%"></br>
図: フィードフォワード型ニューラルネットワーク言語モデル NNLM [@2003Bengio],[@2007Schwenk].
</center>

## リカレントニューラルネットワーク言語モデル RNNLM

<center>
<img src="../assets/2011Mikolov_Extention_Fig1.svg" style="width:74%"></br>
</center>

- 入力層 $w$ と出力層 $y$ は同一次元，総語彙数に一致。(約一万語から20万語)
- 中間層 $s$ は相対的に低次元 (50から1000ニューロン)
- 入力層から中間層への結合係数行列 $U$，中間層から出力層への結合係数行列 $V$，
- 再帰結合係数行列 $W$ がなければバイグラム(2-グラム)ニューラルネットワーク言語モデルと等しい

<!--
## Model description - recurrent NNLM

<center>
<img src="../assets/2011Mikolov_Extention_Fig1.svg" style="width:74%">
</center>

- Input layer $w$ and output layer $y$ have the same dimensionality as the vocabulary (10K - 200K)
- Hidden layer $s$ is orders of magnitude smaller (50 - 1000 neurons)
- $U$ is the matrix of weights between input and hidden layer, $V$ is the matrix of weights between hidden and output layer
- Without the recurrent weights $W$, this model would be a bigram neural network language model

### ニューラルネットワーク言語モデル(4) リカレントニューラルネットワーク言語モデル RNNLM(2)

- 中間層ニューロンの出力と出力層ニューロンの出力は，それぞれ以下のとおり：

$$
s(t) = f\left(\mathbf{Uw}(t) + \mathbf{W}\right)s\left(t-1\right)\\
y(t) = g\left(\mathbf{Vs}\left(t\right)\right),
$$

$f(z)$ はシグモイド関数，$g(z)$ はソフトマックス関数。
P
最近のほとんどのニューラルネットワークと同じく出力層にはソフトマックス関数を用いる。
出力を確率分布とみなすように，全ニューロンの出力確率を合わせると1となるように

$$
f(z)=\frac{1}{1+e^{-z}}, 
$$

$$
g\left(z_m\right)=\frac{e^{z_{m}}}{\sum_k e^{z_{k}}}
$$

## Model Description - Recurrent NNLM
The output values from neurons in the hidden and output layers
are computed as follows:
$$
s(t) = f\left(\mathbf{Uw}(t) + \mathbf{W}\right)s\left(t-1\right)
$$

$$
y(t) = g\left(\mathbf{Vs}\left(t\right)\right),
$$
where $f\left(z\right)$ and $g\left(z\right)$ are sigmoid and softmax activation
unctions (the softmax function in the output layer is used to
ensure that the outputs form a valid probability distribution, i.e.
all outputs are greater than 0 and their sum is 1):
$$
f\left(z\right) =\frac{1}{1+e^{-z}}, g\left(z_m\right)=\frac{e^{z_{m}}}{\sum_ke^{z_{k}}}
$$

## RNNLM の学習(1)
- 確率的勾配降下法 (SGD)
- 全訓練データを繰り返し学習，結合係数行列 $U$, $V$, $W$ をオンライン学習 (各単語ごとに逐次)
- 数エポック実施 (通常 5-10)

### Training of RNNLM
- The training is performed using Stochastic Gradient Descent (SGD)
- We go through all the training data iteratively, and update the weight
- matrices $U$, $V$ and $W$ online (after processing every word)
- Training is performed in several epochs (usually 5-10)

### RNNLM の学習(2)

時刻 $t$ における出力層の誤差ベクトル $\mathbf{e}_o\left(t\right)$ の勾配計算には

クロスエントロピー誤差を用いて：

$$
\mathbf{e}_o\left(t\right) = \mathbf{d}\left(t\right)-\mathbf{y}\left(t\right)
$$

$\mathbf{d}(t)$ は出力単語を表すターゲット単語であり
時刻 $t+1$ の入力単語 $\mathbf{w}\left(t+1\right)$ (ビショップは PRML~\citep{PRML}では
1-of-ｋ 表現と呼んだ。ベンジオはワンホットベクトルと呼ぶ)。

### Training of RNNLM
radient of the error vector in the output layer $\mathbf{e}_o\left(t\right)$ is
computed using a cross entropy criterion:
$$
\mathbf{e}_o\left(t\right) = \mathbf{d}\left(t\right)-\mathbf{y}\left(t\right)
$$
where $\mathbf{d}\left(t\right)$ is a target vector that represents the word
$\mathbf{w}\left(t+1\right)$ (encoded as 1-of-V vector(ワンホットベクター)).

### RNNLM の学習(3)

時刻 $t$ における中間層から出力層への結合係数行列 $V$ は，
中間層ベクトル $\mathbf{s}\left(t\right) と出力層ベクトル $\mathbf{y}\left(t\right)$ を用いて
次式のように計算する

$$
\mathbf{V}(t+1) = \mathbf{V}(t) + \alpha\mathbf{s}(t)\mathbf{e}_o(t)^{\top}
$$

$\alpha$ は学習係数

# Training of RNNLM
Weights $V$ between the hidden layer $\mathbf{s}\left(t\right)$ and the output layer
$\mathbf{y}\left(t\right)$ are updated as

$$
\mathbf{V}\left(t+1\right)=\mathbf{V}\left(t\right)+\alpha\mathbf{s}\left(t\right)\mathbf{e}_o\left(t\right)^{\top}
$$
where $\alpha$ is the learning rate.

# RNNLM の学習(4)

続いて，出力層からの誤差勾配ベクトルから中間層の誤差勾配ベクトルを計算すると，

$$
\mathbf{e}_h\left(t\right) = d_{h}\left(\mathbf{e}_o\left(t\right)^{\top}\mathbf{V},t\right),
$$

誤差ベクトルは関数 $d_h()$ をベクトルの各要素に対して適用して

$$
d_{hj}\left(x,t\right) = x s_j\left(t\right)\left(1-s_{j}\left(t\right)\right)
$$

Next, gradients of errors are propagated from the output layer to the hidden layer

$$
\mathbf{e}_h\left(t\right) = d_{h}\left(\mathbf{e}_o\left(t\right)^{\top}\mathbf{V},t\right),
$$

where the error vector is obtained using function $d_h()$ that is applied element-wise

$$
d_{hj}\left(x,t\right) = x s_j\left(t\right)\left(1-s_{j}\left(t\right)\right)
$$

### RNNLM の学習(5)

時刻 $t$ における入力層から中間層への結合係数行列 $\mathbf{U}$ は，ベクトル $\mathbf{s}\left(t\right)$ の更新を以下のようにする。

$$
\mathbf{U}(t+1) = \mathbf{U}(t) + \alpha\mathbf{w}(t)\mathbf{e}_{h}(t)^{\top}
$$

時刻 $t$ における入力層ベクトル $\mathbf{w}(t)$ は，一つのニューロンを除き全て $0$ である。
上式のように結合係数を更新するニューロンは入力単語に対応する
一つのニューロンのそれを除いて全て $0$ なので，計算は高速化できる。

Weights $\mathbf{U}$ between the input layer $\mathbf{w}\left(t\right)$ and
the hidden layer $\mathbf{s}\left(t\right)$ are then updated as

$$
\mathbf{U}\left(t+1\right) = \mathbf{U}\left(t\right) + \alpha\mathbf{w}\left(t\right)\mathbf{e}_{h}\left(t\right)^{\top}
$$

Note that only one neuron is active at a given time in the input vector
$\mathbf{w}\left(t\right)$. As can be seen from the equation (\ref{eq:8}),
the weight change for neurons with zero activation is none, thus the
computation can be speeded up by updating weights that correspond just to
the active input neuron.
-->

## 1.6. RNNLM の学習 時間貫通バックプロパゲーション BPTT

<center>
<img src="../assets/2011Mikolov_Extention_Fig3.svg" style="width:94%"><br>
2011 Mikolov Fig.3
</center>

- 再帰結合係数行列 $\mathbf{W}$ を時間展開し，多層ニューラルネットワークとみなして学習を行う
- 時間貫通バックプロパゲーションは Backpropagation Through Time (BPTT) といいます

<!--
### RNNLM の学習(8) 時間貫通バックプロパゲーション BPTT(3)

誤差伝播は再帰的に計算する。
通時バックプロパゲーションの計算方法では，
前の時刻の中間層の状態を保持しておく必要がある。

$$
{e}_h\left(t-\tau-1\right)d_{h}\left(e_h\left(t-\tau\right)^{\top}\mathbf{W},t-\tau-\right),
$$

時間展開したこの図で示すように各タイムステップで，繰り返し（再帰的に）で微分して
勾配ベクトルの計算が行われる。このとき各タイムステップの時々刻々の刻みを経るごとに
急速に勾配が小さくなってしまう
**勾配消失** が起きる。
活性化関数がロジスティック関数

$f(x)=\sigma(x)=\left(1+\exp(-x)\right)$

であれば，その微分は

$\sigma'(x)=x\left(1-x\right)$

であった。

ハイパータンジェント
$f(x)=\phi(x)=\left(e^{x}-e^{-x}\right)/\left(e^{x}+e^{-x}\right)$ であれば

$\phi'\left(x\right)=x\left(1-x^2\right)$ であるから，いずれの活性化関数を用いる場合でも
ニューロン$x$の値域（取りうる値）が $x=\left(x\vert 0\le x\le1\right)$
である限り，ロジスティック関数であれハイパータンジェント関数であれ，
元の値より $0$ に近い値となる。

本日は省略するが，これと反対の現象**勾配爆発** が起きる可能性がある。

**BPTT** で時刻に関する再帰が深いと深刻な問題となり
収束しない，学習がいつまで経っても終わらないことがある。

The unfolding can be applied for as many time steps as many training examples were already seen, however the error gradients quickly {\bf vanish} as they get backpropagated in time (in rare cases the errors can {\bf explode}), so several steps of unfolding are sufficient (this is sometimes referred to as __truncated BPTT__.

## RNNLM の学習(9) 時間貫通バックプロパゲーション BPTT(4)
再帰結合係数行列 $\mathbf{W}$ の更新には次の式を用いる：

$$
\mathbf{W}\left(t+1\right) = \mathbf{W}\left(t\right) + \alpha\sum_{z=0}^{T}\mathbf{s}\left(t-z-1\right)\mathbf{e}_{h}\left(t-z\right)^{\top}.
$$

行列 $\mathbf{W}$ の更新は誤差が逆伝播するたびに更新されるのではなく，一度だけ更
新する。そうしないと，どの時間を遡及している最中にどの時刻で計算した誤差によっ
て再帰結合行列を更新するのかという，更新の順番が影響する **タイムマシン問題** が発生する。
未来の子孫が過去の祖先を殺すと子孫は存在しえない。

計算効率の面からも，訓練事例をまとめて扱い，時間ステップニューラルネットワー
クの時刻 $T$ に関する時間展開に関する複雑さは抑えることが行われる。

The recurrent weights $\mathbf{W}$ are updated as

$$
\mathbf{W}\left(t+1\right) = \mathbf{W}\left(t\right) + \alpha\sum_{z=0}^{T}\mathbf{s}\left(t-z-1\right)\mathbf{e}_{h}\left(t-z\right)^{\top}.
$$

Note that the matrix $\mathbf{W}$ is changed in one update at once, and not
during backpropagation of errors.

%% It is more computationally efficient to unfold the network after processing
%% several training examples, so that the training complexity does not
%% increase linearly with the number of time steps $T$ for which the network
%% is unfolded in time.

-->

## 1.7. 時間貫通バックプロパゲーション BPTT

<center>
<img src="../assets/2012Mikolov_Google_Slides20.svg" style="width:74%"><br>
図: バッチ更新の例。赤い矢印は誤差勾配がリカレントニューラルネットワーク
</center>
の時間展開を遡っていく様子を示している。

## 1.8. 文字ベースか単語ベースか？
1. Pros/Cons
1. OOV problems。OOV: Out of Vocabulary 問題。ソーシャルメディアなどを活用する場合不可避の問題


---

# 2. 再帰型ニューラルネットワーク

## 2.1. NETtalk
系列情報処理を扱った初期のニューラルネットワーク例として NETTalk が挙げられます。
NETTalk[^NETTalk] は文字を音読するネットワークです。下図のような構成になっています。
下図のようにアルファベット 7 文字を入力して，空白はアンダーラインで表現されています，中央の文字の発音を学習する 3 層のニューラルネットワークです。NETTalk は 7 文字幅の窓を移動させながら
逐次中央の文字の発音を学習しました。たとえば /I ate the apple/ という文章では
"the" を "ザ" ではなく "ジ" と発音することになります。

印刷単語の読字過程のニューラルネットワークモデルである SM89[^SM89], PMSP96[^PMSP96] で用いられた発音表現は <a target="_blank" href="https://en.wikipedia.org/wiki/ARPABET">ARPABET</a> の亜種です。Python では `nltk` ライブラリを使うと ARPABET の発音を得ることができます(<a target="_blank" href="https://github.com/ShinAsakawa/2019cnps/blob/master/notebooks/2019cnps_arpabet_test.ipynb">ARPABET のデモ<img src="../assets/colab_icon.svg"></a>)。

[^NETTalk]: Sejnowski, T.J. and Rosenberg, C. R. (1987) Parallel Networks that Learn to Pronounce English Text, Complex Systems 1, 145-168.

[^SM89]: Seidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmetal model of word recognition and naming. Psychological Review, 96(4), 523–568.

[^PMSP96]: Plaut, D. C., McClelland, J. L., Seidenberg, M. S. & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56–115.

<center>

<img src="../assets/1986Sejnowski_NETtalkFig2.svg" style="width:74%"></br>
Sejnowski (1986) Fig. 2
</center>

## 2.2. 単純再帰型ニューラルネットワーク

NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案されました。
発案者の名前で **Jordan ネット**[^JordanNet]，**Elman ネット**[^ElmanNet] と呼ばれます。

[^JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.

[^ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211.

Jordan ネットも Elman ネットも上位層からの **帰還信号** を持ちます。これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われます。Jordan ネットでは一時刻前の出力層の情報が用いられます(下図)。

<center>

<img src="../assets/SRN_J.svg" style="width:74%"><br>
<p style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</p>
</center>

- 駄菓子菓子 <a target="_blank" href="../assets/MJ_air.jpg">彼（マイケル・ジェフェリー(エアー)・ジョーダン）</a> ではない :)
- <a target="_blank" href="../assets/c3-s4-jordan.jpg">マイケル・アーウィン・ジョーダン。ミスター機械学習[^jordan_ai_revolution_not_yet]</a>

[^jordan_ai_revolution_not_yet]: 彼は(も？)神様です。多くの機械学習アルゴリズムを提案し続けている影響力のある人です。長らく機械学習の国際雑誌の編集長でした。2018年 <a target="_blank" href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">AI 革命は未だ起こっていない</a> と言い出して議論を呼びました。


一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられます。

<center>

<img src="../assets/SRN_E.svg" style="width:74%"><br>
<p style="align:center; width:74%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</p>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられます。上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数です。Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かります。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきました。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味です。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられます。従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるでしょう。

## 2.3. リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができます。同時に時間発展を考慮すれば下図右のように描くことも可能です。

<center>

<img src="../assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてください。あるいは同義ですが上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してください。このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かります。

## 2.4. エルマンネットによる言語モデル

下図に <a target="_blank" href="../assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示しました。図中の数字はニューロンの数を表します。入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表します。

<center>

<img src="../assets/1991Elman_starting_small_Fig1.svg" style="width:74%"><br>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示しました。Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示されました(Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives 

これらの規則にはさらに 2 つの制約があります。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものです。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示しました。"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示しています。
同じ文型の文章は同じような状態遷移を辿ることが分かります。

<center>

<img src="../assets/1991Elman_Fig3.jpg" style="width:84%"></br>
<p align="left" style="width:74%">
<!--
Trajectories through state space for sentences boy chases boy, boy sees
boy, boy walks. Principal component 1 is plotted along the abscissa;
principal component 3 is plotted along the ordinate. These two PC’s
together encode differences in verb-argument expectations.
-->
</p>
</center>

<!--
<img src="../assets/1991Elman_Fig4a.jpg" style="width:84%"><br>
-->

下図は文 "boy chases boy who chases boy" を入力した場合の遷移図です。この文章には単語 "boy" が 3 度出てきます。それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかります。
同様に 'chases" が 2 度出てきますが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されています。</br> 

<center>

<img src="../assets/1991Elman_Fig4b.jpg" style="width:84%"><br>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示しました。</br>
<center>

<img src="../assets/1991Elman_Fig4c.jpg" style="width:84%"><br>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に島します。</br>
<center> 

<img src="../assets/1991Elman_Fig4d.jpg" style="width:84%"><br>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が
異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば，
同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが
示唆されます。このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，
後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上)


<!--
<p align="left" style="width:74%">
Movement through state space for sentences with relative clauses. Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. These two PC’s encode depth of embedding in relative clauses.
</p>
</center>
-->

## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語
であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への
入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層
の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係が
ないことです。逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいると
みなすことです。この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。

<!--
<center>

<img src="../assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>

<img src="../assets/2014Sutskever_S22_Fig1.svg" style="width:99%"><br>
From [@2014Sutskever_Sequence_to_Sequence]
</center>

$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$

## 2.6. 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログ[^karpathy] から下図に引用します。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示しています。

[^karpathy]: 去年までスタンフォード大学の大学院生。現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www

<center>

<img src="../assets/diags.jpeg" sytle="width:99%">><br>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきました。
ですがが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は
1024 程度まで用いられていることには注意してください。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現[^onehot]


[^onehot]: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。一つだけが "熱い" あるいは "辛い" ベクトルと呼びます。以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたのですが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多いです。ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じます。典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新します。従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになります。そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行いました。上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当します。単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展します。


<center>

<img src="../assets/charseq.jpeg" style="width:94%">></br>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

[@1991Siegelmann_RNN_universal] said Turing completeness of RNN.

## 2.7. 双方向 RNN BiRNN

RNN を改善するモデルとして 2 つ紹介します。一つは **双方向 RNN** bidirectional RNN (BiRNN) で
Shuster[^shuster]，別は LSTM です。ここでは BiRNN を扱います。下図に BiRNN の概念図を示しました。
BiRNN は RNN が 2 つ逆方向に走っていて互いに交わることはありません。
この意味では時間を逆向きに考えるだけなのでプログラム上の難しさは有りません。
時刻 $t$ での出力 $y(t)$ を得るためには，$[0\ldots,t-1]$ までの順方法 RNN と
$[T,\ldots,t+1]$ までの逆方法 RNN を用いて予測します。
逆方法 RNN は未来から過去を予測することを意味します。物理的因果律に違反することになるので
気持ち悪いとも言えます。ですが英単語 "the" の発音は後続する名詞を知っていれば発音を
予測することは容易です。同様にフランス語の定冠詞を "ラ" にするか "ル" にするかは
後続する名詞の性が分かっていれば容易です。このように自然言語処理では BiRNN を使うと
精度向上が期待される場合頻用されます。ここには神経心理学的な意味づけと工学的価値との齟齬，乖離が
あります。

[^shuter]: 提案当時 ATR と NICT 所属の博士課程研究員。現在は Google 所属。

<center>

<img src="../assets/1997Shuster_BiRNN.png" style="width:74%"></br>
</center>

下図に BiRNN の音声認識データセットを用いた性能比較を示しました。図中では "BiRNN" が "BRNN" と表記されています。
<center>

<img src="../assets/1997Shuster_BiRNN_Tab2.png" style="width:74%"></br>
<p align="left" style="width:74%">
Shuster (1997) Fig.1, Tab. 2
</p>
</center>

## 2.8. 長距離依存

上では RNN は時間方向でのディープラーニング(深層学習)であると説明しました。
ですが過去の情報を用いるために，一時刻前，すなわち直前の情報ではなく過去のある時点での情報を保持しておいて使いたい場合がありまs。英語の関係代名詞節を名詞の修飾に用いるような **中央埋め込み文** では，
主語と動詞との間で時制の一致が必要ですが，主語の後に関係代名詞節が埋め込まれると，主語の時制や数を
覚えておく必要が生じます。

文 "boy that girls chase plays the guitar" では関係代名詞節内の主語 "girls" が複数形です。
この複数形 "girls" に引きづられて動詞 "plays" を "play" としては正しい文法になりません。

このように過去の情報を覚えておく必要があります。これを **長距離依存** long term dependency と言います。SRN は長距離依存解消のために学習時間が長くなるという問題点があります。
これは中間層の内容が時々刻々変化し続けるため，特定の内容を保持することが困難になると考えられます。
この長距離依存解消が難しいという短所は，記憶内容を保持しておく別の場所，短期記憶バッファを用意するなどの解消方法も存在します。一方，短期記憶を保持する機構をリカレントニューラルネットワーク内に組み込むという考え方もあります。後者の考え方を実現する方法として次に紹介する長=短期記憶モデルがあります。

<center>

<img src="../assets/LTD.svg" style="width:49%"></br>
Schematic description of a long term dependency
</center>


## 2.9. 長=短期記憶 LSTM

**長=短期記憶** (Long Short-Term Memory: LSTM, henceforth) はシュミットフーバー (Shumithuber, J.) 一派により提案された長距離依存解消のためのニューラルネットワークモデルです。
長距離依存を解消するためには，ある内容を保持し続けて必要に応じてその内容を表出する必要があります。
このことを実現するために，ニューロンへの入力に門 (gate) を置くことが提案されました。
下図に長=短期記憶モデルの概念図を示しました。
<center>

<img src="../assets/2015Greff_LSTM_ja.svg" style="width:74%"><br>
LSTM from [@2016Asakawa_AIdict]
</center>

上図の LSTM は一つのニューロンに該当します。このニューロンには 3 つのゲート(gate, 門) が付いています。
3 つのゲートは以下の名前で呼ばれます。

1. 入力ゲート input gate
2. 出力ゲート output gate
3. 忘却ゲート forget gate

各ゲートの位置を上図で確認してください。入力ゲートと出力ゲートが閉じていれば，セルの内容(これまでは中間層の状態と呼んできました)が保持されることになります。
出力ゲートが開いている場合には，セル内容が出力されます。一方出力ゲートが閉じていればそのセル内容は出力されません。このように入力ゲートと出力ゲートはセル内容の入出力に関与します。
忘却ゲートはセル内容の保持に関与します。忘却ゲートが開いていれば一時刻前のセル内容が保持されることを意味します。反対に忘却ゲートが閉じていれば一時刻前のセル内容は破棄されます。全セルの忘却ゲートが全閉ならば通常の多層ニューラルネットワークであることと同義です。すなわち記憶内容を保持しないことを意味します。SRN でフィードバック信号が存在しない場合に相当します。セルへの入力は，

1. 下層からの信号，
2. 上層からの信号, すなわち Jordan ネットの帰還信号
3. 自分自身の内容，すなわち Elman ネットの帰還信号

が用いられます。これら入力信号が

1. 入力信号そのもの
2. 入力ゲートの開閉制御用信号
3. 出力ゲートの開閉制御用信号
4. 忘却ゲートの開閉制御用信号

という 4 種類に用いられます。従って LSTM のパラメータ数は SRN に比べて 4 倍になります。

LSTM に限らず一般のニューラルネットワークの出力には非線形関数が用いられます。代表的な非線形出力関数としては，以下のような関数が挙げられます。

1. シグモイド関数[^sigmoid]: $f(x)=\left[1+e^{-x}\right]^{-1}$
2. ハイパーボリックタンジェント関数:  $f(x)=\left(e^{x}-e^{-x}\right)/\left(e^{x}+e^{-x}\right)$
3. 整流線形ユニット関数: $f(x)=\max\left(0,x\right)$

[^sigmoid]: 1980 年代に用いられたシグモイド関数が用いられることはほとんどなくなりました。理由は収束が遅いからです[@1999LeCun]

この中で，セルの出力関数として 2. のハイパーボリックタンジェント関数が，ゲートの出力関数にはシグモイド関数が使われます。その理由はハイパーボリックタンジェント関数の方が収束が早いこと，シグモイド関数は値域が $[0,1]$ であるためゲートの開閉に直接対応しているからです。

- Le Cun, Y. Bottou, L., Orr, G. B, Muller K-R. (1988) Efficient BackProp, in Orr, G. and Muller, K. (Eds.) Neural Networks: tricks and trade, Springer.

<!--
The LSTM (left figure) can be described as the input signals $\mathbf{x}_t$ at
time $t$, the output signals $\mathbf{o}_t$, the forget gate $\mathbf{f}_t$, and
the output signal $\mathbf{y}_t$, the memory cell $\mathbf{c}_t$, then we can get
the following:
$i_{t}=\sigma\left(W_{xi}x_{t}+W_{hi}y_{t-1}+b_{i}\right)$, </br>
$f_{t}=\sigma\left(W_{xf}x_{t}+W_{hf}y_{t-1}+b_{f}\right)$, </br>
$o_{t}=\sigma\left(W_{xo}x_{t}+W_{ho}y_{t-1}+b_{o}\right)$, </br> 
$g_{t}=\phi\left(W_{xc}x_{t}+W_{hc}y_{t-1}+b_{c}\right)$,</br>
$c_{t}=f_{t}\odot c_{t-1} + i_{t}\odot g_{t}$,<br>
$h_{t}=o_{t}\odot\phi\left(c_{t}\right)$</br><!--\label{eq:LSTM}
where
$\sigma\left(x\right)=\displaystyle\frac{1}{1+\mbox{exp}\left(-x\right)}$ (logistic function)
%% =1/2\left(\phi\Brc{x}+1\right)$,
$\phi\left(x\right)=\displaystyle\frac{\mbox{exp}\left(x\right)-\mbox{exp}\left(-x\right)}{\mbox{exp}\left(x\right)+\mbox{exp}\left(-x\right)}$ (hyper tangent)
%% $=2\sigma\left(x\right)-1$
and $\odot$ menas Hadamard (element--wise) product.
-->

## 2.10. LSTM におけるゲートの生理学的対応物 <!--Physiological correlates of gates in LSTM-->

以下の画像は <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html>
よりの引用です。
ウミウシのエラ引っ込め反応時に，ニューロンへの入力信号ではなく，入力信号を修飾する結合
が存在します。下図参照。
<!--
<center>
<img src="../assets/2016McComas_presynaptic_inhibition.jpg" style="width:74%"></br>
</center>
-->

<center>
<!-- sea slug, ウミウシ。Mollush 軟体動物，-->
<img src="../assets/C87-fig2_24.jpg" style="width:37%">
<img src="../assets/shunting-inhibition.jpg" style="width:49%"></br>
<img src="../assets/C87-fig2_25.jpg" style="width:84%"></br>
アメフラシ (Aplysia) のエラ引っ込め反応(a.k.a. 防御反応)の模式図[^seaslang]
</center>

[^seaslang]: from <http://kybele.psych.cornell.edu/~edelman/Psych-2140/week-2-2.html> の 222ページより<br>
画像はそれぞれ http://kybele.psych.cornell.edu/~edelman/Psych-2140/shunting-inhibition.jpg<br>http://kybele.psych.cornell.edu/~edelman/Psych-2140/C87-fig2.25.jpg<br>http://kybele.psych.cornell.edu/~edelman/Psych-2140/C87-fig2.24.jpg<br>

また古くは PDP のバイブルにもシグマパイユニット ($\sigma\pi$ units) として既述が見られます。各ユニットを掛け算 ($\pi$) してから足し算 ($\sum$) するのでこのように命名されたのでしょう。

<center>

<img src="../assets/sigma-pi.jpg" style="width:64%"><br>
From [@PDPbook] chaper 7
</center>


## 2.11. 画像と言語との融合へ向けて

以上で今回の特別企画の目標である画像と言語とのマルチモーダル統合へ向けての準備がほぼ出揃いました。
2014 年に提案されたニューラル画像脚注付けのモデルを下図に示します。

<!--
<center>

<img src="../assets/2014KarpathyImageDescriptionsFig3.svg" style="width:84%"><br>
[@2015Karpathy_FeiFei_caption]
</center>
-->

<center>

<img src="../assets/2014Vinyals_Fig1.svg" style="width:84%"><br>
[@2014Vinyals_Bengio_Show_and_Tell]
</center>


<!--
<center>

<img src="../assets/2015Xu_ShowAttendTellFig1.svg" style="width:84%"></br>
</center>
-->

画像に対して注意を付加した脚注付けモデルの出力例を下図に示します。

<!--
<center>

<img src="../assets/2015Xu_ShowAttendTellFig2_upper.svg" style="width:84%"><br>
[@2015Xu_Bengio_NIC_attention]
</center>
-->
各画像対は右が入力画像であり，左はその入力画像の脚注付けである単語を出力している際にどこに注意しているのかを白色で表しています。
<center>

<img src="../assets/2015Xu_ShowAttendTellFig2_lower.svg" style="width:99%"></br>
[@2015Xu_Bengio_NIC_attention]
</center>

<!--
<center>

<img src="../assets/2014Mnih_attention.svg"></br>
</center>

Glimpse Sensor: Given the coordinates of the glimpse and an input image,
the sensor extracts a __retina-like__ representation
$\rho\left(x_t,l_{t-1}\right)$ centered at $l_{t-1}$ that contains multiple
resolution patches. 

- B) **Glimpse Network**: Given the location $\left(l_{t-1}\right)$ and
input image $\left(x_t\right)$, uses the glimpse sensor to extract retina
representation $\rho\left(x_t,l_{t-1}\right)$.  The retina representation
and glimpse location is then mapped into a hidden space using independent
linear layers parameterized by $\theta_g^{0}$ and $\theta_g^{1}$
respectively using rectified units followed by another linear layer
$\theta_2^{2}$ to combine the information from both components. The glimpse
network
$f_{g}\left(\dot;\left[\theta_g^0,\theta_g^1,\theta_g^2\right]\right)$
defines a trainable bandwidth limited sensor for the attention network
producing the glimpse representation $g_t$. 
- C) **Model Architecture**: Overall, the model is an RNN. The core network
of the model $f_h\left(\cdot;\theta_h\right)$ takes the glimpse
representation $g_t$ as input and combining with the internal
representation at previous time step $h_{t-1}$, produces the new internal
state of the model $h_t$. The location network
$f_l\left(\cdot;\theta_a\right)$ and the action network
$f_a\left(\cdot;\theta_a\right)$ use the internal state $h_t$ of the model
to produce the next location to attend to $l_t$ and the
action/classification at respectively. This basic RNN iteration is repeated
for a variable number of steps.[@2014Mnih_RNN_attention]
-->

<!--
#  World Models
<center>

<img src="../assets/2018Ha_WorldModel.svg" style="width:84%"></br>
[@2018Ha_WorldModels] Fig.1
</center>
<center>

<img src="../assets/2018HaWorldModelsFig1.svg"></br>
A World Model, from Scott McCloud’s Understanding Comics. (McCloud, 1993; E, 2012)
</center>

Jay Wright Forrester, the father of system dynamics,
described a mental model as:\\
    \begin{quote}
      The image of the world around us, which we carry in our
      head, is just a model. Nobody in his head imagines all
      the world, government or country. He has only selected
      concepts, and relationships between them, and uses those
      to represent the real system. \citep{1971Forrester}
    \end{quote}

<center>
-->

<!--
<img src="../assets/2015Greff_LSTM_ja.svg" style="width:74%"><br>
<p align="left" style="width:49%">
LSTM の概念 (Shumithuber ら 2015)を改変
</p>
</center>
-->


<!--
<center>

<img src="../assets/2010Mikolov_Fig1.svg" style="width:49%"></br>
\cite{2010Mikolov2010}
</center>

<center>

<img src="../assets/2011Mikolov_Extention_Fig1.jpg" style="width:49%"><br>
Mikolov Extension
</center>

<center>

<img src="../assets/2001Boden_Fig5.jpg" style="width:94%"></br>
Boden's BPTT
</center>
-->

<!--
- モチベーション
- ニューラルネットワーク言語モデル
- 訓練アルゴリズム
  - リカレントニューラルネットワーク
  - クラス
  - エントロピー最大化言語モデル

### モチベーション

### モチベーション (2) チューリングテスト
- チューリングテストは原理的に言語モデルの問題とみなすことが可能
- 会話の履歴が与えられた時，良い言語モデルは正しい応答に高い確率を与える

- 例:
  - $P\left(\mathbf{ox{月曜日}\vert \mathbf{ox{今日は何曜日ですか？}}} = ?$\\
  - $P\left(\mathbf{ox{赤}\vert \mathbf{ox{バラは何色？}}} = ?$\\

言語モデルの問題と考えれば以下の文のような問題と等価とみなせる:\\
$P\left(\mathbf{ox{赤}\vert {\mathbf{ox{バラの色は}}}=?$

### モチベーション(3) n-グラム言語モデル

- どうすれば「良い言語モデル」を創れるか？
- 伝統的解: n-グラム言語モデル: $P\left{w\vert h}=\displaystyle\frac{C\left{h,w\right}}{C\left(h\right)}$
-->

---

# 3. 意味論

ここでは意味論の研究史を心理学関連領域に絞ってまとめることを試みます。
<!-- 神経心理学症状との関連については
付録 <a target="_blank" href="https://github.com/ShinAsakawa/wbai_aphasia/blob/master/2019Primer_AphasiaDyslexia.pdf">失語，失読に関する神経心理学モデルの基礎</a> をご覧ください。
-->

意味についての言及は言語学者 Firth さらに遡れば Witgenstein まで辿ることが可能です。
ですがここでは直接関連する研究として以下をとりげます

- 第 1 世代 意味微分法 Osgood 
- 第 2 世代 潜在意味解析 Ladauer
- 第 3 世代 潜在ディレクリ配置，トピックモデル
- 第 4 世代 分散埋め込みモデル word2vec とその後継モデル
- 最近の展開


## 3.1. 1952 年 意味微分法 Semantics Differential (SD)
チャールズ・オズグッドによって提案された意味微分法は，被験者に対象を評価させる際に形容詞対を用います。
形容詞対は 5 件法あるいはその他の変種によって評価されます。
得られた結果を 評価対象 X 形容詞対の行列にします。
すなわち評価対象者の平均を求めて得た行列を **固有値分解**，正確には因子分析 FA を行います。
最大固有値から順に満足の行くまで求めます。
固有値行列への射影行列を因子負荷量と呼びます。得られた結果を下図に示しました。

<center>

<img src="../assets/1957Osgood_Tab1.svg" style="width:84%"><br>
From  Osgood (1952) Tab. 1
</center>
<!-- <a target="_blank" href="../assets/1957Osgood_Tab1.svg">Osgood (1952) Tab. 1</a>-->

上図では，50 対の形容詞対によって対象を評価した値が描かれています。

<a target="_blank" href="https://ja.wikipedia.org/wiki/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90">因子分析(FA)</a> 形容詞対による多段階評定

<center>

<img src="../assets/1957Osgood_Fig2.svg" style="width:64%"><br>
From  Osgood (1952) Fig.2
<!--- <a target="_blank" href="../assets/1957Osgood_Fig2.svg">Osgood (1952) Fig. 2</a>-->
</center>

意味微分法においては，研究者の用意した形容詞対の関係に依存して対象となる概念やモノ，コトが決まります。
従って研究者の想定していない概念空間については言及できないという点が問題点として指摘できます。

このことは評価対象がよくわかっている問題であれば精度良く測定できるという長所の裏返しです。

一般的な意味，対象者が持っている意味空間全体を考えるためには，50 個の形容詞対では捉えきれないことも意味します。従って以下のような分野に適用する場合には問題が発生する可能性があると言えます

- 神経心理学的な症状である **意味痴呆** semantic dimentia を扱う場合
- 入試問題などの一般知識を評価したい場合
- 一般言語モデルを作成する場合


## 3.2. 1997 年 潜在意味分析 Latent Semantic Analysis (LSA, LSI)
- **潜在意味分析**: <a target="_blank" href="https://ja.wikipedia.org/wiki/%E7%89%B9%E7%95%B0%E5%80%A4%E5%88%86%E8%A7%A3">特異値分解(SVD)</a> は，当時増大しつつあったコンピュータ計算資源を背景に一般意味論に踏み込む先鞭をつけたと考えることができます。

すなわち先代の意味微分法が持つ問題点である，評価方法が 50 対の形容詞であること，
50 をいくら増やしても，結局は研究者の恣意性が排除できないこと，評価者が人間であるため大量の評価対象を評価させることは，
心理実験参加者の拘束時間を長くするため現実的には不可能であることを解消するために，辞書そのものをコンピュータで解析するという手法を採用しました。

1. 辞書の項目とその項目の記述内容とを考えます
2. 特定の辞書項項目にはどの単語が使われているいるのかという共起行列 内容 $\times$ 単語 を
考え，この行列について **特異値分解** を行います。

Osgood の意味微分法で用いられた行列のサイズと比較すると，単語数が数万，項目数は数万から数十万に増加しています。
数の増加は網羅する範囲の拡大を意味します。
下図は持ちられたデータセット例を示したものです。

<center>

<img src="../assets/1997Landauer_Dumais_FigA2.svg" style="width:84%"><br>
From Landauer and Duman (1997) Fig. A2
</center>

LSA (LSI) の問題点としては以下図を見てください

<center>

<img src="../assets/1997Landauer_Fig3.svg" style="width:64%"><br>
From Landauer and Dumas (1997) Fig.3 
</center>

上図は，得られた結果を元に類義語テストを問いた場合に特異値分解で得られる次元数を横軸に，正解率を縦軸にプロットした図です。
次元を上げると成績の向上が認められます。
ですが，ある程度 300 以上の次元を抽出しても返って成績が低下することが示されています。

次元数を増やすことで本来の類義語検査に必要な知識以外の情報が含まれてしまうため推察されます。


<!--- <a target="_blank" href="../assets/1997Landauer_Fig3.svg">Landauer (1997) Fig. 3</a>-->

## 3.3. 2003 年 潜在ディレクリ配置 Latent Direchlet Allocation (LDA)

潜在ディレクリ配置 Latent Direchlet Allocation: LDA[^LDA] は LSA (LSI) を確率的に拡張したモデルであると考えることができます。すなわち LDA では単語と項目との関係に確率的な生成モデルを仮定します。

[^LDA]: 伝統的な統計学においては Fischer の線形判別分析を LDA と表記します。ですがデータサイエンス，すなわち統計学の一分野では近年の潜在ディレクリ配置の成功により LDA と未定義で表記された場合には潜在ディクレクリ配置を指すことが多くなっています。

その理由としては，対象となる項目，しばしば **トピック** と言い表すと，項目の説明に用いられる単語との間に，決定論的な関係を仮定しないと考えることによります。確率的な関係を仮定することにより柔軟な関係をモデル化が可能であるからです。

例えば，ある概念，話題(トピック) "神経" を説明する場合を考えます。
"神経" を説明するには多様な表現や説明が可能です。
"神経" を説明する文章を数多く集めてると，単語 "脳" は高頻度で出現すると予想できます。
同様にして "細胞" や "脳" も高頻度で観察できるでしょう。ところが単語 "犬" は低頻度でしょう。
単語 "アメフラシ" や "イカ" は場合によりけりでしょう。どちらも神経生理学の発展に貢献した実験動物ですから単語 "アメフラシ" や "イカ" が出現する文章もあれば，単語 "脳梗塞" や単語 "失語" と同時に出現する確率もありえます。
このように考えると確率的に考えた方が良い場合があることが分かります。

### ディレクリ分布

もう一点，ノンパラメトリックモデルについて説明します。
parametric model はパラーメータを用いたモデルほどの意味です。
心理統計学の古典的な教科書では，ノンパラメトリック検定とは母集団分布のパラメータに依存 **しない** 統計的検定という意味で用いられます。一方 LDA の場合には推定すべき分布のパラメータ(の数)を **事前に定めない** という意味で **ノンパラメトリック** なモデルであると言います。
すなわちある話題(トピック)とそれを説明する単語の出現確率について，取り扱う現象の複雑さに応じてモデルを記述するパラメータ数を適応的に増やして行くことを考えます。

数学的既述は省略しますが，<a target="_blank" href="https://en.wikipedia.org/wiki/Beta_distribution">ベータ分布</a> を用いると区間 $[a,b]$ の間をとる分布でパラメータにより分布が柔軟に記述できます。ベータ分布の多次元拡張を <a target="_blank" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">ディククリ分布</a> と言います。

確率空間に対して一定の成約を付した表現をシンプレックスと言ったりします。<!--例えばコインの裏表は
2 値ですからベータ分布を用いても表す事ができます。-->
たとえばじゃんけんで対戦相手が，グー，チョキ，パー のいずれかを出す確率は，2 つが分かれば 3 つ目の手は自ずと分かってきます。このような関係は 3 つの手の確率分布でディククリ分布として扱うことが可能です。
下図はウィキペディアから持ってきました。この図はそのようなじゃんけんの手の出現確率をディレクリ分布として表現した例だと思ってください。

<!--## ディククリ分布 (多次元ベルヌーイ分布)-->
<!-- 多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定-->

<center>

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Dirichlet-3d-panel.png" style="width:64%"><br>
<!--<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" style="width:49%"></br>-->
<p align="left" style="width:74%">
多次元ディレクリ分布(多次元ベータ分布)</a> によるノンパラメトリック推定<br>
図は <https://en.wikipedia.org/wiki/Dirichlet_distribution> より
</p>
</center>

トピック毎の単語の出現確率も上図と同じ枠組みで記述することが可能です。かつ，上図ではとりうる値が 3 つの場合ですが，話題が複雑になれば適応的に選択肢の数，すなわちディレクリ分布の次元数が増加することになります。

### プレート表記

あらかじめ定められた数のパラメータを用いて分布を記述するのではなく，
解くべき問題の複雑さに応じて適応的にパラメータ数を定めることに対応して，
LDA あるいはトピックモデルの図示方法として **プレート表記** plate notation があります。
下図にプレート表記の例を示しました。

<center>

<img src="../assets/2009Blei_Topic_Models_02.svg" style="width:74%"></br>
プレート表記: ノンパラメトリックモデルの表現に用いられる
</center>

- 丸は確率変数 
- 矢印は確率的依存関係を表現 
- 観測変数は影付き(文献によっては二重丸) 
- プレートは繰り返しを表す


Y からパラメータ X が生成される場合，矢印を使ってその依存関係を表現します。ノンパラメトリックモデルの場合，矢印の数を予め定めません。そのため矢印を多数描くのが煩雑なので，一つの箱代用して表現します。
これがプレート表記になります。

観測可能な変数をグレー，または二重丸で表し，観測不能な，類推すべきパラメータを白丸で表記します。
実際には観測不可能な潜在パラメータを観測データから類推することになります。

大まかなルールとして，潜在変数をギリシャアルファベット表記，観測される変数はローマアルファベット表記の場合が多いですが，一般則ですので例外もあります。

下図に潜在ディレクリ配置  LDA のプレート表記を示しました。
<!--### 潜在ディレクリ配置のプレート表記-->

<center>

<img src="../assets/2009Blei_Topic_Models_03.svg" style="width:74%"></br>
<!--<img src="../assets/2009Blei_Topic_Models_04.svg" style="width:94%"></br>-->
</center>

### トピックと単語の関係

トピックモデルの要点をまとめた下図はこれまでの説明をすべて含んでいます。

<center>

<img src="../assets/2009Blei_Topic_Models_01.svg" style="width:74%"></br>
<p align="left" style="width:74%">
 出典: ブライのスライド(2009)より，文章は話題(トピック)の混合</br>
 各文章はその話題から文章が生成されたと考える
 </p>
</center>

興味深い応用例として Mochihashi ら(2009) の示した教師なし学習による日本語分かち書き例を示します。
下図は源氏物語をトピックモデルにより分かち書きさせた例です。どこに空白を挿入すると文字間の隣接関係を表現できるかをトピックモデルで解くことを考えた場合，空白の挿入位置が確率的に定まると仮定して居ます。


<center>

<img src="../assets/2009Mochihashi_Fig10.svg" style="width:84%"></br>
</center>

Mochihashi らは，ルイス・キャロルの小説 "不思議の国のアリス" 原文から空白を取り除き，
文字間の隣接関係から文字の区切り，すなわち空白を推定することを試みました。結果を下図に示しました。

<center>

<img src="../assets/2009Mochihashi_Fig12.svg" style="width:84%"></br>
</center>

### 原著論文

- <a target="_blank" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei, Jordan (2003) Latent Dirichlet Allocation 原著論文</a>
- 発展モデル: <a target="_blank" href="http://arxiv.org/abs/0710.0845" >中華レストラン過程 CRP</a>
- <a target="_blank" href="http://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf">インド食堂過程 IBP</a> 
<!-- - パチンコ過程 -->

### R による実装 

- https://cran.r-project.org/web/packages/lda/index.html
- https://cran.r-project.org/web/packages/topicmodels/index.html


## 3.4. 2013 年 word2vec, 単語埋め込み, ベクトル埋め込みモデル

<center>

<a href="_blank" href="../assets/Mikolov_portrait.jpg"> ミコロフ</a>
<a href="_blank" href="../assets/2015Mikolov_NIPSportrait.png"> おなじくミコロフ</a>
</center>

- ミコロフは **word2vec** によりニューラルネットワークによる意味実装を示しました。
ワードツーベックと発音します。
Word2vec は実装に 2 種類あリます。それぞれ **CBOW** と **skip-gram** と命名されています。
"シーボウ" または "シーバウ" と日本人は言ったりすることが多いようです。

有名な "king" - "man" + "woman" = "queen" のアナロジーを解くことができると喧伝されました。

下図左は意味的なアナロジーがベクトルの向きとして表現されていることに注目してください。
ベクトルは方向と大きさを持っている矢印で表現されます。矢印の原点を移動する
ことを考えます。たとえば "MAN" から "WOMAN" へ向かう矢印を平行移動して "KING" まで
持ってくると，その矢印は "QUEEN" を重なることが予想できます。
これがアナロジー問題の解放の直感的説明になります。

<center>

<img src="../assets/2013Mikolov_KingQueenFig.svg" style="width:94%">
</center>

上図右は同じ word2vec でできた空間に対して，統語関係 syntax を解かせた場合を示しています。
"KING" から "KINGS" へ向かう矢印を "QUEEN" まで持ってくると "QUEENS" に重なる
ことが見て取れます。

このことから上図右の赤矢印で示されたベクトルは **複数形** への変換という統語情報，
文法情報を表現しているとみなすことが可能です。

伝統的な言語学の知識では，統語構造と意味構造は別個に取り組む課題であると考えられてきました。
ところが word2vec が示す意味空間はそのような区別を考える必要があるのか否かについて
問題を提起しているように思われます。

逆に一つのモジュールで処理することができるのであれば，分割して扱う意味があるのかどうかを考える切っ掛けになると考えます。

もう一つ面白い結果を下図に示します。下図は word2vec によって世界の国とその首都との関係を主成分分析 PCA で 2 次元に描画した図です。

<center>

<img src="../assets/2013Mikolov_FigCountries.svg" style="width:94%">
</center>

横軸は国と首都との関係を表現しているとみなすことができます。縦軸は下から上に向かって
おおまかにユーラシア大陸を西から東へ横断しているように配置されています。
意味を表現するということは，解釈によって，この場合 PCA によって 2 次元に図示してみると
大まかに我々の知識を表現できることを示唆していると考えます。

word2vec の実装には 2 種類あります。どちらを使っても同じような結果を得ることができます。

- CBOW: Continous Bog of Words 連続単語袋
- skip-gram: スキップグラム

両者は反対の関係になります。下図を参照してください。

<center>

<img src="../assets/2013Mikolov_Fig1.svg" style="width:94%"><br>
From Mikolov (2013) Fig. 1
</center>

CBOW も skip-gram も 3 層にニューラルネットワークです。その中間層に現れた表現を **ベクトル埋め込みモデル** あるいは **単語埋め込みモデル** と言ったりします。

- CBOW モデルは周辺の単語の単語袋詰め表現から中央の単語を予測するモデルです。
- skip-gram は中心の単語から周辺の単語袋詰表現を予測するモデルです。

たとえば，次の文章を考えます。

```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

表記を簡潔にするため各単語に ID をふることにします。
 
```python
{"彼":0, "は":1, "意味論":2, "を":3, "論じ":4, "た":5}
```

すると上記例文は

```python
[0, 1, 2, 3, 4, 5]
```

と表現されます。
ウィンドウ幅がプラスマイナス 2 である CBOW モデルでは 3 層の多層パーセプトロン
の入出力関係は，入力が 4 次元ベクトル，出力も 4 次元ベクトルとなります。
文の境界を無視すれば，以下のような入出力関係とみなせます。


```bash
[0,1,1,0,0,0] -> [1,0,0,0,0,0] # In:"は","意味論" Out:"彼"
[1,0,1,1,0,0] -> [0,1,0,0,0,0] # In:"彼","意味論","を" Out:"は"
[1,1,0,1,1,0] -> [0,0,1,0,0,0] # In:"彼","は","を","論じ" Out:"意味論"
[0,1,1,0,1,1] -> [0,0,0,1,0,0] # In:"は","意味論","論じ","た" Out:"を"
[0,0,1,1,0,1] -> [0,0,0,0,1,0] # In:"意味論","を","た" Out:"論じ"
[0,0,0,1,1,0] -> [0,0,0,0,0,1] # In:"を","論じ" 出力:"た"
```

を学習することとなります。

- CBOW にせよ skip-gram にせよ大規模コーパス，例えばウィキペディア全文を用いて訓練を行います。周辺の単語をどの程度取るかは勝手に決めます。
- Mikolov が類推に用いたデータ例を下図に示しました。国名と対応する首都名，国名とその通貨名，などは意味的関係です。一方罫線下方は文法関係です。
形容詞から副詞形を類推したり，反意語を類推したり，比較級，過去分詞，国名と国民，過去形，複数形，動詞の 3 人称単数現在形などです。

<center>

<img src="../assets/2013Mikolov_Tab1.svg" style="width:94%"><br>
From Milolov (2013) Tab. 1
</center>

- しばしば，神経心理学や認知心理学では，それぞれの品詞別の処理を仮定したり，意味的な脱落を考えたりする場合に，異なるモジュールを想定することが行われます。
- それらの仮定したモジュールが脳内に対応関係が存在するのであれば神経心理学的には説明として十分でしょう。
- ところが word2vec で示した表現では一つの意味と統語との表現を与える中間層に味方を変える (PCA など)で描画してみれば，異なる複数の言語知識を一つの表象で表現できることが示唆されます。
- word2vec による表現が脳内に分散していると考えるとカテゴリー特異性の問題や基本概念優位性の問題をどう捉えれば良いのかについて示唆に富むと考えます。

<!--
<img src="../assets/2013Mikolov_skip-gram_cbow.svg" style="width:74%">
<img src="../assets/skip-gram.svg" style="width:74%">
<img src="../assets/skip-gram_cbow.svg" style="width:74%">
</center>
-->

日本語のウィキペディアを用いた word2vec と NTT 日本語の語彙特性との関連に関心のある方は
<a target="_blank" href="../2017jpa_word2vec_NTTdict.pdf">日本語 Wikipedia の word2vec 表現と語彙特性との関係, 近藤・浅川 (2017) </a> をご覧ください

## さらなる蘊蓄 負例サンプリング

Word2vec を使って大規模コーパスを学習させる際に，学習させるデータ以外に全く関係のない組み合わせをペナルティーとして与えることで精度が向上します。


## 発展 文章埋め込みモデルへ

単語の word2vec による表現は 3 層パーセプトロンの中間層の活性値として表現されます。

単語より大きなまとまりの意味表現，たとえば，文，段落，などの表現をどのように得るのかが問題になります。
ここで詳細には触れませんが，文表現ベクトルは各単語表現の総和であると考えるのがもっとも簡単な表現になります。
すなわち次文:


```python
["彼", "は", "意味論", "を", "論じ", "た"]
```

の文表現を得るためには，各単語の word2vec 表現を足し合わせることが行われます。
ただし，単純に足し合わせたのでは BOW 単語袋表現と同じことですので，単語の順序情報が失われていることになります。
この辺りをどう改善すれば良いのかが議論されてきました。


### 文献

* <a target="_blank" href="https://papers.nips.cc/paper/5021-distributed-representations-of--words-and-phrases-and-their-compositionality.pdf">word2vec オリジナル論文</a> 2013年 Mikolov
* <a target="_blank" href="https://fasttext.cc/">fastText</a> 高速文埋め込みモデル
* その発展 <a target="_blank" href="../2018jsai.pdf">浅川, 岡, 楠見 (2018)</a>
<!-- - <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論概説</a> -->
<!-- [リカレントニューラルネットワーク](./lect08_RNN.pdf)-->
<!--- [word2vec のやや詳しい解説](/2016word_embbed_slides_tmp.pdf)-->

<!--


<center>
<img src="https://www.tensorflow.org/images/linear-relationships.png" style="width:84%"><br>
<p algin="left" style="width:74%">
Source: <https://www.tensorflow.org/tutorials/representation/word2vec>
</p>
</center>

- <a target="_blank" href="../lect08_semantics.pdf">計算論的意味論の蘊蓄</a>

-->

<!--<img src="https://upload.wikimedia.org/wikipedia/en/7/74/Elmo_from_Sesame_Street.gif" style="width:39%">-->

<!--
<img src="https://www.specialdaysofthemonth.com/wp-content/uploads/2018/01/elmo.jpg" style="width:39%">
-->
<!--](https://upload.wikimedia.org/wikipedia/en/7/74/Elmo_from_Sesame_Street.gif)-->
<!--
<img src="https://vignette.wikia.nocookie.net/muppet/images/e/e1/Bert_smile.png" style="width:29%"><br>
<p align="left" style="width:84%">
左: <https://www.specialdaysofthemonth.com/wp-content/uploads/2018/01/elmo.jpg>, 
右: <https://vignette.wikia.nocookie.net/muppet/images/e/e1/Bert_smile.png>
</center>
-->

<!--
<center>

|Task|Previous SOTA|Our Baseline|ELMo + Baseline|Increase (Absolute/Relative)|
|:---:|:---:|:---:|:---:|:---:|
|SQuAD |Liu et al. (2017) $84.4$            | $81.1$  | $85.8$         | $4.70/24.9\%$ |
|SNLI  |Chen et al. (2017) $88.6$           | $88.0$  | $88.7\pm0.17$  | $0.70/05.8\%$ |
|SRL   |He et al. (2017) $81.7$             | $81.4$  | $84.6$         | $3.20/17.2\%$ |
|Coref |Lee et al. (2017) $67.2$            | $67.2$  | $70.4$         | $3.20/09.8\%$  |
|NER   |Peters et al. (2017) $91.93\pm0.19$ | $90.15$ | $92.22\pm0.10$ | $2.06/21.0\%$  |
|SST-5 |McCann et al. (2017) $53.7$         | $51.4$  | $54.7\pm0.5$   | $3.30/06.8\%$  |

</center>

Table 1: Test set comparison of **ELMo** enhanced neural models with
state-of-the-art single model baselines across six benchmark NLP tasks. The
performance metric varies across tasks – accuracy for _SNLI_ and _SST-5_;
F1 for _SQuAD_, _SRL_ and _NER_; average **F1** for Coref. Due to the small
test sizes for __NER__ and __SST-5__, we report the mean and standard
deviation across five runs with different random seeds. The ``increase``
column lists both the absolute and relative improvements over our baseline.

- **Question answering.** The Stanford Question Answering Dataset
(**SQuAD**) (Rajpurkar et al., 2016) contains 100K+ crowd sourced
questionanswer pairs where the answer is a span in a given Wikipedia
paragraph. Our baseline model (Clark and Gardner, 2017) is an improved
version of the Bidirectional Attention Flow model in Seo et al.  (BiDAF;2017)
- **Textual entailment.** Textual entailment is the task of determining
whether a “__hypothesis__” is true, given a “__premise__”.  The
Stanford Natural Language Inference (**SNLI**) corpus (Bowman et al., 2015)
provides approximately 550K hypothesis/premise pairs.  Our baseline, the
ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the
premise and hypothesis, followed by a matrix attention layer, a local
inference layer, another biLSTM inference composition layer, and finally a
pooling operation before the output layer.
- **Semantic role labeling.** A semantic role labeling (**SRL**) system
models the predicate-argument structure of a sentence, and is often
described as answering “Who did what to whom".
- **Coreference resolution.** Coreference resolution is the task of
clustering mentions in text that refer to the same underlying real world
entities.  Our baseline model is the end-to-end span-based neural model of
Lee et al.(2017).
- **Named entity extraction.** The CoNLL 2003 NER task (Sang and
Meulder,2003) consists of newswire from the Reuters RCV1 corpus tagged with
four different entity types (PER, LOC, ORG, MISC). Following recent
state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the
baseline model uses pre-trained word embeddings, a character-based CNN
representation, two biLSTM layers and a conditional random field (CRF) loss
(Lafferty et al., 2001), similar to Collobert et al.  (2011).
- **Sentiment analysis.** The fine-grained sentiment classification task in
the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves
selecting one of five labels (from very negative to very positive) to
describe a sentence from a movie review.

<center>

![BERT Fig1](./assets/2018Devlin_BERT_Fig1.svg)
</center>

---

# BERT
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v1/)

<center>

<img src="../assets/2018Devlin_BERT_Fig1.svg" style="widht:94%"></br>
</center>

# GLUE: General Language Understanding Evaluation 

- **MNLI**: Multi-Genre Natural Language Inference is a large-scale,
crowdsourced entailment classification task (Williams et al., 2018). Given
a pair of sentences, the goal is to predict whether the second sentence is
an entailment, contradiction, or neutral with respect to the first one.
- **QQP**: Quora Question Pairs is a binary classification task where the
goal is to determine if two questions asked on Quora are semantically
equivalent (Chen et al., 2018).
- **QNLI**: Question Natural Language Inference is a version of the
Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been
converted to a binary classification task (Wang et al., 2018). The positive
examples are (question, sentence) pairs which do contain the correct
answer, and the negative examples are (question, sentence) from the same
paragraph which do not contain the answer.
- **SST-2**: The Stanford Sentiment Treebank is a binary single-sentence
classification task consisting of sentences extracted from movie reviews
with human annotations of their sentiment (Socher et al., 2013).
- **CoLA**: The Corpus of Linguistic Acceptability is a binary
single-sentence classification task, where the goal is to predict whether
an English sentence is linguistically “acceptable” or not (Warstadt et
al., 2018).
- **STS-B**: The Semantic Textual Similarity Benchmark is a collection of
sentence pairs drawn from news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1 to 5 denoting how similar
the two sentences are in terms of semantic meaning.
- **MRPC**: Microsoft Research Paraphrase Corpus consists of sentence pairs
automatically extracted from online news sources, with human annotations
for whether the sentences in the pair are semantically equivalent (Dolan
and Brockett, 2005).
- **RTE**: Recognizing Textual Entailment is a binary entailment task
similar to MNLI, but with much less training data (Bentivogli et al.,
2009).
- **WNLI**: Winograd NLI is a small natural language inference dataset
deriving from (Levesque et al., 2011). The GLUE webpage notes that there
are issues with the construction of this dataset, 7 and every trained
system that’s been submitted to GLUE has has performed worse than the 65.1
baseline accuracy of predicting the majority class.  We therefore exclude
this set out of fairness to OpenAI GPT. For our GLUE submission, we always
predicted the majority class

- **SQuAD v1.1**: The Standford Question Answering Dataset (SQuAD) is a
collection of 100k crowdsourced question/answer pairs (Rajpurkar et al.,
2016).  Given a question and a paragraph from Wikipedia

- **NER**: CoNLL 2003 Named Entity Recognition (NER) dataset.  This dataset
consists of 200k training words which have been annotated as __Person__,
__Organization__, __Location__, __Miscellaneous__, or __Other__ (non-named
entity).

- **SWAG**: The Situations With Adversarial Generations (SWAG) dataset
contains 113k sentence-pair completion examples that evaluate grounded
commonsense inference (Zellers et al., 2018).  Given a sentence from a
video captioning dataset, the task is to decide among four choices the most
plausible continuation.

<center>

<img src="../assets/2018Devlin_BERT_Tab2.svg" style="width:74%"></br>
2018Devlin_BERT_Tab. 2

<img src="../assets/2018Devlin_BERT_Tab4.svg" style="width:74%"></br>
2018Devlin_BERT_Tab. 2
</center>



- **Question answering.** The Stanford Question Answering Dataset
(**SQuAD**) (Rajpurkar et al., 2016) contains 100K+ crowd sourced
questionanswer pairs where the answer is a span in a given Wikipedia
paragraph. Our baseline model (Clark and Gardner, 2017) is an improved
version of the Bidirectional Attention Flow model in Seo et al.  (BiDAF;2017)
- **Textual entailment.** Textual entailment is the task of determining
whether a “__hypothesis__” is true, given a “__premise__”.  The
Stanford Natural Language Inference (**SNLI**) corpus (Bowman et al., 2015)
provides approximately 550K hypothesis/premise pairs.  Our baseline, the
ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the
premise and hypothesis, followed by a matrix attention layer, a local
inference layer, another biLSTM inference composition layer, and finally a
pooling operation before the output layer.
- **Semantic role labeling.** A semantic role labeling (**SRL**) system
models the predicate-argument structure of a sentence, and is often
described as answering “Who did what to whom".
- **Coreference resolution.** Coreference resolution is the task of
clustering mentions in text that refer to the same underlying real world
entities.  Our baseline model is the end-to-end span-based neural model of
Lee et al.(2017).
- **Named entity extraction.** The CoNLL 2003 NER task (Sang and
Meulder,2003) consists of newswire from the Reuters RCV1 corpus tagged with
four different entity types (PER, LOC, ORG, MISC). Following recent
state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the
baseline model uses pre-trained word embeddings, a character-based CNN
representation, two biLSTM layers and a conditional random field (CRF) loss
(Lafferty et al., 2001), similar to Collobert et al.  (2011).
- **Sentiment analysis.** The fine-grained sentiment classification task in
the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves
selecting one of five labels (from very negative to very positive) to
describe a sentence from a movie review.
-->



### Seq2sep 翻訳モデル

中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば **機械翻訳** や **対話** のモデルになる。
初期の翻訳モデルである "seq2seq" の概念図を示した。
"eos" は文末 end of sentence を表す。
中央の "eos" の前がソース言語であり，中央の "eos" の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，
それ以外の時刻ではソース言語とターゲット言語は関係がない。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
この点を改善することを目指すことが 2014 年以降盛んになった。
顕著な例が後述する **双方向 RNN**, **LSTM** を採用したり，**注意** 機構を導入することであった。

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src ="https://komazawa-deep-learning.github.io/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 1
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src ="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2left.svg" style="width:88%"><br/>
<img src ="https://komazawa-deep-learning.github.io/assets/2014Sutskever_Fig2right.svg" style="width:88%"><br/>
Sutskever et. al (2014) Sequence_to_Sequence, Fig. 2
</center>

<!-- 
# 自然言語系の注意

<center>

![](assets/2015Bahdanau_attention.jpg){style="width:30%"}
![](assets/2015Luong_Fig2.svg){style="width:33%"}
![](assets/2015Luong_Fig3.svg){style="width:33%"}><br />
左: [@2014Bahdanau_NMT], 中: [@2015Luong_attention] Fig. 2, 
右: [@2015Luong_attention] Fig. 3
</center>
-->


<!-- <center style="width:74% align:center">

![](assets/1957Osgood_fig19a.jpg){style="width:30%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19b.jpg){style="width:28%"} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/1957Osgood_fig19c.jpg){style="width:23%"}<br />
<div align="left">
Osgood (1957) 図. 19 より。ある女性患者（統合失調症）の心的イメージ。左が治療前，中央が治療中，右が治療後期
</div>
</center>
 -->
<!--
# 多頭=自己注意 Multi-Head Self-Attention: MHSA

- 自然言語処理 NLP **Transformer**[@2017Vaswani_transformer]; **BERT**[@2018BERT]; **RoBERTa**[@2019RoBERTa]; **distilBERT** [@2020Sanh_distilBERT]; and more...
- 画像処理 [@2019Ramachandran_attention_vision]; **A2-Net** [@2018Chen_A2-nets_double_attention]; **U-GAT-IT** [@2019Kim_U-GAT-IT]
- 強化学習，メタ学習 **SNAIL** [@2018Mishra_SNAIL]
- 敵対生成ネットワーク **SAGAN** [@2019Zhang_Goodfellow_SAGAN]


-->

# トランスフォーマー が提唱した 自己注意

専門用語としては，**多頭=自己注意** Multi-Head Self-Attention (以下 MHSA と表記)と呼びます。
多頭とは何か，なぜ 自己 がつく注意なのかを確認してください。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:24%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/2019Ramachandran_fig3.jpg" style="width:64%"><br/>
Left: [@2017Vaswani_transformer], Right: [@2019Ramachandran_attention_vision]
</center>

- 上図，クエリ，キー，バリュー に注目してください。
- 英単語の意味どおりに解釈すれば，問い合わせ，キー（鍵），値，となります。
- つまり，ある問い合わせに対して，キーを与えて，その答えとなる値を得ること。
- この操作を入力情報から作り出して答えを出力する仕組みに，ワンホット表現を使うことがポイント


下図左は上図右と同じものです。この下図右を複数個束ねると下図中央になります。

- 下図中央の Scaled Dot-Product Attention と書かれた右脇に小さく h と書かれています。
- この h とは ヘッド の意味です。
- 下図中央を 1 つの単位として，次に来る情報と連結させます。これが下図右です。
- 先週のリカレントニューラルネットワークでは，中間層の状態が次の時刻の処理に継続して用いられていました。
- ところが 多頭=自己注意 MHSA では一つ前の入力情報を，現在の時刻の情報に対するクエリとキーのように扱って情報を処理します。
- 下図右の下から入力される情報は，input と output と書かれています。さらに output の下には (Shifted right) と書かれています。すなわち，時系列情報を一時刻分だけ右にずらし（シフト）させて逐次情報を処理することを意味しています。
- 下図右の下から入力される情報は，embedding つまり埋め込み表現 と 位置符号化 position embedding が足し合わされたものです。埋め込み表現とは先週 word2vec で触れたベクトルで表現された，単語（あるいはそれぞれの項目）の 意味表現 に対応します。


<center>
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:15%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-20.png" style="width:23%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-21.png" style="width:29%">
</center>
<!--
![](assets/ModalNet-19.png){style="width:15%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-20.jpg){style="width:23%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-21.png){style="width:29%"}
</center>
-->

<!-- 
<center>

![](assets/2019Zhang_Goodfellow_SAGAN_fig2.jpg){style="width:88%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1upper.jpg){style="width:74%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1lower.jpg){style="width:74%"}<br/>
From [@2019Zhang_Goodfellow_SAGAN] Fig. 1, and 3.
画像生成において，近傍画素から情報だけでなく，関連する遠距離の特徴を利用して生成することにより一貫性のある対象やシナリオを生成可能。
各行の左の元画像上のカラー点は 5 つ の 代表的なクエリの場所を示す。
右側の 5 画像は 各クエリ位置における注意地図。最も注目されている領域が，色分けされた矢印で示されている。
</center>
-->

<!-- 
<center>

![](assets/2017Gupta_Non-local_fig2.svg){style="width:29%"}
![](assets/2017Gupta_Non-local_example_230_0_eps_18_9.svg){style="width:59%"}<br/>
時空の非局所ネットワークの概念図。特徴地図はテンソルとして示されている。
例えば 1024 チャンネルの場合は $T\times H\times W\times1024$ である。
$\otimes$ は行列積を，$\oplus$ は要素和を示す。
ソフトマックス演算は各行に対して実行される。
青いボックスは $1\times1\times1\times1$ の畳み込みを表す。
$512$ チャンネルのボトルネックを持つ埋め込みガウシアン版が示されている。
バニラガウス版は $\theta$ と $\phi$ とを除去することで ドット積版は $1/N$ のスケーリングでソフトマックスを置き換えることで行うことができる。
From [@2018Wang_Girshick_Non-local] 
</center>
-->

<!-- <center>

![](assets/2018snail_fig2b.svg){style="width:49%"}<br/>
From [@2018Mishra_SNAIL] Fig. 2
</center>

トランスフォーマーはリカレント構造や畳み込み構造を持たず埋め込みベクトルに位置符号化器を加えることで系列情報を処理する。
しかし、逐次的な順序情報が貧弱であるとの批判がある。
とりわけ強化学習のような位置依存性に敏感な課題では問題。
トランスフォーマーモデルにおける 位置問題を解決するため，自己注意機構 と 時間的な畳み込み temporal convolution を組み合わせたモデルが 
Simple Neural Attention Meta-Learner (SNAIL)[@2018Mishra_SNAIL]。
SNAIL は，メタ学習，強化学習の両方の課題に優れていることが実証された。
-->

少しだけまとめると:

- 自然言語処理，画像処理，強化学習，メタ学習の 4 分野でほほ同様の 多頭自己注意 MHSA が取り入れられている。
- クエリ，キー，バリュー の重みを学習することが MHSA の学習である。
- 従来手法である 畳み込み や LSTM を MHSA で置き換える動きがある。

# BERT の特徴

- 上記のトランスフォーマーに基づいて BERT が提案されました。
- BERT は **B**idirectional **E**ncoder **R**epresentations from **T**ransformers から命名したと原著論文には書いてあります。
- ですが，この原著論文の直前に提案されたモデルに ELMo があったため，こじつけた，ふざけた命名でしょう。
- もちろん ELMo (こちらは **E**mbeddings from **L**anguage **Mo**dels から命名されました)も BERT もセサミストリートに出てくるキャラクタです。

<!-- From singularitysalon2019/nlp.tex -->

<!--BERT の影響が大きいので，本稿でも BERT を中心に取り上げる。-->BERT の特徴を 3 つにまとめると以下の通り

1. トランスフォーマー Transformer に基づく 多頭自己注意 (MHSA) を使った多層ニューラルネットワークモデル
2. 2 つの事前訓練: **マスク化言語モデル** と **次文予測課題** を用いる
3. 事前訓練済のモデルを用いて，解くべき課題のそれぞれについて **ファインチューニング** Fine tuning を施す
4. 個別の課題は下流課題 down stream tasks と呼ばれます。上流 と 下流 との区別は，最初に行う事前訓練のことを時間的に先行するので上流，その後のファインチューニングするそれぞれの課題のことを下流課題と呼んでいます。
5. 複数の課題に対して個別にファインチューニングを行うことにより，複数の下流課題で性能向上が認められました。 [GLUE スコアボード](https://gluebenchmark.com/leaderboard), [SuperGLUE](https://super.gluebenchmark.com/leaderboard/) を参照してください。


# BERT の入力表現

- 上の図にもあったとおり BERT では入力情報が埋め込み表現だけでなく，位置符号化器の情報が加算されます。
- BERT では，埋め込み表現と位置符号化器の情報に加えて，セグメント埋め込み segment embeddings も加えた情報が入力情報となります。下図参照

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2018Devlin_BERT_Fig2.svg" style="width:84%"><br/>
<!-- ![](assets/2018Devlin_BERT_Fig2.svg){style="width:84%"}<br /> -->
埋め込みトークンの総和，位置符号器，分離埋め込みの 3 者 From [@2018BERT] Fig. 2
</center>

- 上図では，下 3 行が入力情報を構成する 3 つの要素になっています。上（ピンク色）が合算した入力情報になります。
- 3 つの入力情報とはそれぞれ，下から 位置符号化器 （薄灰色），セグメント埋め込み (淡緑)，トークン埋め込み (淡黄) です。

# 位置符号器 Position encoders

- 上述のようにトランスフォーマーの入力には，単語埋め込み表現に加えて，位置符号器の信号も加算されます。

<!-- 位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\begin{align}
\text{PE}_{(\text{pos},2i)} &= \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} &= \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{align}
$$
-->

- 位置符号器による位置表現は，i 番目の位置情報をワンホット表現するのではなく，周波数領域に変換することで周期情報を表現する試みと見なすことができます。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/PE_example.svg" style="width:74%"><br/>
位置符号化に用いられる符号化。位置情報を周波数情報へ変換して用いています。
<!-- ![](assets/PE_example.svg){style="width:74%"}<br/> -->
</center>

- 位置情報を周波数情報へ変換することが良いことなのか，どうなのか，は議論されている最中です。
一つの研究テーマでもあります。

- 数学的な説明は **フーリエ変換** を調べてください。任意の関数 y=f(x) では x は位置情報を表しているとみなすことができます。
従って，位置 x を与えると対応する値 y が得られることを表している式が y=f(x) です。
これに対して，任意の情報は周波数，すなわち，波の重ね合わせとして表現できます。
すべての周波数を重ね合わせると元の関数になります。
反対に，ある周波数の値は，関数 f(x) を周波数へ変換したときの特定の周波数成分として表現できます。

BERT における位置符号化器は位置情報を波の成分として表現したことになります。

このようにしてできた値を入力側と出力側で下図のように連結させたものが以下のトランスフォーマーです。

<center>

<img src="https://komazawa-deep-learning.github.io/assets/2017Vaswani_Fig1.svg" style="width:37%"><br/>
From [@2017Vaswani_transformer] Fig. 1
</center>

これまで見てきたように，トランスフォーマーでは入力信号に基づいて情報の変換が行なわれる。
この意味ではトランスフォーマーにおける 多頭 自己注意 MHSA とはボトムアップ注意の変形であるとみなしうる。
逆言すれば，RNN のように過去の履歴をすべて保持しているわけではないので，系列情報については，position encoders に頼っている側面が指摘できる。
<!-- %\input{ELMoBERTGPT_Gao2018.tex}
へーこれでインプットか？
-->
# BERT の事前訓練: マスク化言語モデル

全入力系列のうち 15% をランダムに [MASK] トークンで置き換える

- 入力はオリジナル系列を [MASK] トークンで置き換えた系列
- ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測 
- 80%: オリジナル入力系列を [MASK] で置換 
- 10%: [MASK] の位置の単語をランダムな無関連語で置き換える
- 10%: オリジナル系列

# BERT の事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力

- 入力: the man went to the store [SEP] he bought a gallon of milk.
- ラベル:  IsNext
- 入力:  the man went to the store [SEP] penguins are flightless birds.
- ラベル:  NotNext

# BERT: ファインチューニング

(a), (b) は文レベル課題，
(c),(d)はトークンレベル課題, E: 入力埋め込み表現, $T_i$: トークン $i$ の文脈表象。

<!-- 
- [CLS]: 分類出力記号,
- [SEP]: 文分離記号 
-->

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2018Devlin_BERT_Fig3.svg" style="width:88%"><br/>
From [@2018BERT] Fig.3
</center>

# GLUE: General Language Understanding Evaluation
- **CoLA**: 入力文が英語として正しいか否かを判定
- **SST-2**: スタンフォード大による映画レビューの極性判断
- **MRPC**: マイクロソフトの言い換えコーパス。2文 が等しいか否かを判定
- **STS-B**: ニュースの見出し文の類似度を5段階で評定
- **QQP**: 2 つの質問文の意味が等価かを判定
- **MNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **QNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **RTE**: MNLI に似た2つの入力文の含意を判定
- **WNI**: ウィノグラッド会話チャレンジ

その他

- **SQuAD**: スタンフォード大による Q and A ウィキペディアから抽出した文
- **RACE**: 中学入試，高校入試に相当するテスト多肢選択回答

# BERT モデルの詳細
- データ: Wikipedia (2.5B words) + BookCorpus (800M words)
- バッチサイズ: 131,072 words (1024 sequences * 128 length or 256 sequences * 512 length)
- 訓練時間: 1M steps (~40 epochs)
- 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
- BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
- BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
- 4x4 / 8x8 TPU で 4 日間

### CoLA サンプル

1 は正しい英文，0 は非文

- 1 They drank the pub dry.
- 0 __They drank the pub__.
- 1 The professor talked us into a stupor.
- 0 __The professor talked us__.
- 1 We yelled ourselves hoarse.
- 0 __We yelled ourselves__.

### SST-2 サンプル

0 は低評価，1 は高評価

- hide new secretions from the parental units     0
- contains no wit , only labored gags     0
- that loves its characters and communicates something rather beautiful about human nature        1
- remains utterly satisfied to remain the same throughout         0
- on the worst revenge-of-the-nerds clichés the filmmakers could dredge up        0
- that's far too tragic to merit such superficial treatment      0

<!-- - demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , pe
- rsonal film with an emotional wallop .  1
- of saucy        1
- a depressed fifteen-year-old 's suicidal poetry         0
- are more deeply thought through than in most ` right-thinking ' films   1
- goes to absurd lengths  0
- for those moviegoers who complain that ` they do n't make movies like they used to anymore      0
- the part where nothing 's happening ,   0
- saw how bad this movie was      0
- lend some dignity to a dumb story       0
 -->

### MRPC サンプル

- 1
	- 文1: "Please, keep doing your homework," said Bavelier, the mother of three.   
	- 文2: "Please, keep doing your homework," said Bavelier, the mother of 6-year-old twins and a 2-year old.
- 1
	- 文1: While Mr. Qurei is widely respected and has a long history of negotiating with the Israelis, he cannot expect such a warm welcome.  
	- 文2: While Qureia is respected and has a history of negotiating with the Israelis, a warm welcome is not expected.
- 1
	- 文1: "Nobody wants to go to war with anybody about anything ... it 's always very much a last resort thing and one to be avoided," Mr Howard told Sydney radio.
	- 文2: "We don't want to go to war with anybody . . . it's always very much a last resort, and one to be avoided.
- 0
	- 文1: GMT, Tab shares were up 19 cents, or 4.4% , at A $4.56, having earlier set a record high of A $4.57.       
	- 文2: Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A $4.57.
- 0
	- 文1: Martin, 58, will be freed today after serving two thirds of his five-year sentence for the manslaughter of 16-year-old Fred Barras.
	- 文2: Martin served two thirds of a five-year sentence for the manslaughter of Barras and for wounding Fearon.

<!-- - 1
	- 文1: The stock rose $2.11, or about 11 percent, to close Friday at $ 21.51 on the New York Stock Exchange.     
	- 文2: PG & E Corp. shares jumped $1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday.
- 1
	- 文1: Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.     
	- 文2: With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.
- 0
	- 文1: The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday.      
	- 文2: The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15.
- 1
	- 文1: The DVD-CCA then appealed to the state Supreme Court.  
	- 文2: The DVD CCA appealed that decision to the U.S. Supreme Court.

 -->
<!-- # BERT ファインチューニング手続き
<center>
<img src="./assets/2019Devlin_mask_method21.jpg" style="width:74%"><br/>
</center>
 -->

### SST-B サンプル

最後の数値が評価値

- A plane is taking off.  An air plane is taking off.   5.000
- A man is playing a large flute. A man is playing a flute.     3.800
- A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800
- Three men are playing chess.    Two men are playing chess.    2.600
- A man is playing the cello.     A man seated is playing the cello.    4.250
- Some men are fighting.  Two men are fighting. 4.250
- A man is smoking.   A man is skating 0.5000

### QQP サンプル

0 は異なると判断， 1 は同じと判断すべき文

- 0
	- How is the life of a math student? Could you describe your own experiences?     
	- Which level of prepration is enough for the exam jlpt5?
- 1
	- How do I control my horny emotions?     
	- How do you control your horniness?
- 0 
	- What causes stool color to change to yellow?    
	- What can cause stool to come out as little balls?     0
- 1
	- What can one do after MBBS?     
	- What do i do after my MBBS?
- 0
	- Where can I find a power outlet for my laptop at Melbourne Airport?     
	- Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?
- 0
	- How not to feel guilty since I am Muslim and I'm conscious we won't have sex together? 
	- I don't beleive I am bulimic, but I force throw up at least once a day after I eat something and feel guilty.  Should I tell somebody, and if so who?

### MNLI サンプル

- 矛盾
	- Met my first girlfriend that way. 
	- I didn’t meet my first girlfriend until later.
- 中立
	- 8 million in relief in the form of emergency housing. 
	- The 8 million dollars for emergency housing was still not enough to solve the problem.
- 中立
	- Now, as children tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.
	- All of the children love working in their gardens.
- 含意
	- At 8:34, the Boston Center controller received a third transmission from American 11
	- The Boston Center controller got a third transmission from American 11.
- 中立
	- I am a lacto-vegetarian. 
	- I enjoy eating cheese too much to abstain from dairy.
- 矛盾
	- someone else noticed it and i said well i guess that’s true and it was somewhat melodious in other words it wasn’t just you know it was really funny
	- No one noticed and it wasn’t funny at all.

# 事前訓練とマルチ課題学習

<center>
<img src="https://komazawa-deep-learning.github.io/assets/mt-dnn.png" style="width:66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

<!-- 
# Transformer: Attention is all you need

$$\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V$$

<center>

![](assets/2017Vaswani_Fig2_1.svg){style="width:17%"}
![](assets/2017Vaswani_Fig2_2.svg){style="width:23%"}<br />
From [@2017Vaswani_transformer] Fig. 2
</center>
-->

<!-- 
# Transformer(2): Attention is all you need

$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_1,\ldots,\mathop{head}_h\right)W^O
$$

where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$, 
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2$$

$$\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$

$$\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$
-->

<!-- 
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->

# 多言語対応
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Lample_Fig1.svg" style="width:88%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center>

# BERT の発展
<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Rajasekharan_conver.png" style="width:54%"><br/>
From <https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58>
</center>


# BERT: ファインチューニング手続きによる性能比較

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_mask_method21.jpg" style="width:66%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
 -->

# BERT: モデルサイズ比較

<center>
<img src="https://komazawa-deep-learning.githbub.io/assets/2019Devlin_model_size20.jpg" style="width:59%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。

<!-- # BERT モデル単方向，双方向モデル比較
<center>
<img src="./assets/2019Devlin_directionality19.jpg" style="width:66%"><br/>
</center>

 -->
# BERT: モデル単方向，双方向モデル比較

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_directionality19.jpg" style="width:59%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。


<!-- # BERT 事前訓練比較
<center>
<img src="./assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

# BERT: 事前訓練比較

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:59%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。

<!--![](../2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

# 各モデルの特徴

- RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
- XLNet: 順列言語モデル。2 ストリーム注意
- MT-DNN: BERT ベース の転移学習に重きをおいたモデル
- GPT-2: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
- BERT: Transformerに基づく言語モデル。**マスク化言語モデル** と **次文予測** に基づく 事前訓練，各下流課題をファインチューニング。事前訓練されたモデルは一般公開済。
- DistillBERT: BERT の蒸留版
- ELMo: 双方向 RNN による文埋め込み表現
- Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<!-- # 埋め込みモデルによる構文解析
<center>
<img src="assets/2019hewitt-header.jpg" style="width:79%"><br/>
From https://github.com/john-hewitt/structural-probes
</center>

 -->
<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクトル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。

<center>
![](assets/2019Devlin_BERT01upper.svg){style="width:74%"}
[@2019Devlin_BERT]  単語のベクトル表現
</center>

単語埋め込み (word2vec[@2013Mikolov_VectorSpace];[@2013Mikolov_VectorSpace]) 
単語は周辺単語の共起情報 [点相互情報量 PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) に基づく[@2014LevyGoldberg:nips],[@2014Levy:3cosadd]。
すなわち周辺単語との共起情報を用いて単語の意味を定義している。

<center>
![](assets/2019Devlin_BERT01lower.svg){style="width:74%"}
</center>

形式的には，skip-gram であれ CBOW であれ同じである。

# 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<center>
![](assets/2019Devlin_BERT02upper.svg){style="widht:74%"}<br/>
単語の意味を一意に定めることができない場合

![](assets/2019Devlin_BERT02lower.svg){style="width:74%"}<br/>
</center>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，
句，節，文のベクトル表現を得る努力がなされてきた。
適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<center>
![](assets/2019Devlin_BERT03.svg){style="width:74%"}<br/>
[@2014Sutskever_Sequence_to_Sequence] より
</center>

BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。

# BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。
事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類
の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。
事前訓練語，各課題毎にファインチューニングが施される。

<center>
![](assets/mt-dnn.png){style="width:89%"}<br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>
 -->

# BERT: 埋め込みモデルによる構文解析

BERT の構文解析能力を下図示した。
各単語の共通空間に射影し，
単語間の距離を計算することにより構文解析木と同等の表現を得ることができることが報告されている[@2019HewittManning_structural]。

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2019hewitt-header.jpg" style="width:39%">
&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/2019HewittManning_blogFig1.jpg" style="width:19%">
<img src="https://komazawa-deep-learning.github.io/assets/2019HewittManning_blogFig2.jpg" style="width:19%">
<!-- ![](assets/2019HewittManning_blogFig1.jpg){style="width:19%"}
![](assets/2019HewittManning_blogFig2.jpg){style="width:19%"}<br/>-->
BERT による構文解析木を再現する射影空間
</center>
From <https://github.com/john-hewitt/structural-probes>

- word2vec において単語間の距離は内積で定義されていました。
- このことから，文章を構成する単語で張られる線形内積空間内の距離が構文解析木を与えると見なすことは不自然ではないと予想できます。

<!-- 
% > **The syntax distance hypothesis**: There exists a linear transformation
% > $\mathbf{B}$ of the word representation space under which vector distance
% > encodes parse trees.  Equivalently, there exists an inner product on the
% > word representation space such that distance under the inner product
% > encodes parse trees. This (indefinite) inner product is specified by
% > $\mathbf{B}^{\top}\mathbf{B}$.

% We'll take a particular instance of this hypothesis for our probes; 
% we'll use the L2 distance, and let the squared vector distances equal the tree distances, but more on this later.  
-->

- そこで構文解析木を再現するような射影変換を見つけることができれば BERT を用いて構文解析が可能となるでしょう。
- 例えば上図における chef と store と was の距離を解析木を反映するような空間を見つけ出すことに相当します

<!-- % The distances we pointed out earlier between \_chef\_, \_store\_ and \_was\_, can be visualized in a vector space as follows, where $\mathbf{B}\in\mathbb{R}^{2\times3}$, mapping 3-dimensional word representations to a 2-dimensional space encoding syntax:
-->
<!--% Note in the image above that the distances between words before
% transformation by $\mathbf{B}$ aren't indicative of the tree. After the
% linear transformation, however, taking a minimum spanning tree on the
% distances recovers the tree, as shown in the following image:

% <center>
% % ![](assets/0.332019HewittManning_blogFig2.jpg}
% </center>

% Finding a parse tree-encoding distance metric Our potentially tree-encoding distances are parametrized by the linear transformation $\mathbf{B}\in\mathbb{R}^{k\times n}$, 

% \begin{equation}
% \left\|h_i-h_j\right\|_B^2=\left(B\left(h_i-h_j\right)\right)^{\top}\left(B\left(h_i-h_j\right)\right)
% \end{equation}

% where $\mathbf{B}_h$ is the linear transformation of the word representation; equivalently, it is the parse tree node representation. 
% This is equivalent to finding an L2 distance on the original vector space, parametrized by the positive semi-definite matrix $A=B^{\top}B$:

% \begin{equation}
% \left\|h_i-h_j\right\|_A^2=\left(h_i-h_j\right)^{\top}A\left(h_i-h_j\right)
% \end{equation}
% The set of linear transformations, $\mathbb{R}^{k\times n}$ for a given $k$ is the hypothesis class for our probing family.  
% We choose $B$ to minimize the difference between true parse tree distances from a human-parsed corpus and the predicted distances from the fixed word representations transformed
% by $B$: 
-->

<!-- 2 つの単語 $w_i$, $w_j$ とし単語間の距離を $d\left(w_i,w_j\right)$ とする。
適当な変換を施した後の座標を $h_i$, $h_j$ とすれば，求める変換 $B$ は次式のような変換を行なうことに相当する:
$$
\min_{B}\sum_l\frac{1}{\left|s_\ell\right|^2}\sum_{i,j}\left(d\left(w_i,w_j\right)-\left\|B\left(h_i-h_j
\right)\right\|^2\right)
$$
ここで $\ell$ は文 s の訓練文のインデックスであり，各文の長さで規格化することを意味している。
 -->

具体的には，以下のような操作をしています:

1. 文章に現れる全トークンを表すベクトルを BERT より求める。
2. すなわち BERT 全中間層ユニット活性値から構成される全ての値から構成されるベクトル群
3. 2 のベクトルが張る部分空間に全トークンを射影する。
4. 3 の部分空間内でトークン間の距離を求める。
5. 各トークンを短い順にグラフで結ぶ

<!--% where $\ell$ indexes the sentences $s_{\ell}$ in the corpus, and $\frac{1}{\left|s_\ell\right|^2}$ normalizes for the number of pairs of words in each sentence. 
% Note that we do actually attempt to minimize the difference between the squared distance $\left\|h_i-h_j\right\|_B^2$ and the tree distance. 
% This means that the actual vector distance $\left\|h_i-h_j\right\|_B$ will always be off from the true parse tree distances, but the tree information encoded is identical, and we found that optimizing with the squared distance performs considerably better in practice.

% Finding a parse depth-encoding norm As a second application of our method, we note that the directions of the edges in a parse tree is determined by the depth of words in the parse tree; the deeper node in the governance relationship is the governed word. The depth in the parse tree is like a norm, or length, defining a total order on the nodes in the tree. We denote this tree depth norm $\left\|w_i\right\|$.

% Likewise, vector spaces have natural norms; our hypothesis for norms is that there exists a linear transformation under which tree depth norm is encoded by the squared L2 vector norm $\left\|Bh_i\right\|_2^2$. 
% Just like  for the distance hypothesis, we can find the linear transformation under which the depth norm hypothesis is best-approximated:

% \begin{equation}
% \min_B\sum_\ell\frac{1}{\left|s_\ell\right|}\sum_i\left(\left\|w_i\right\|-\left\|Bh_i\right\|^2\right)
% \end{equation}

% To be effective, the manual should follow three key principles:
% \begin{enumerate}
% -  It should be simple and write on a single page, e.g. as a bulleted list of operating procedures.
% -  It should be prioritised in a strategic order that you can start executing tomorrow.
% -  It should be reviewed, evaluated, and understood by everyone crucial to the mission.
% \end{enumerate}
-->

# BERT 実装
- BERT 実装のパラメータを以下に示した。
- 現在配布されている BERT-base あるいは性能が良い BERT-large は各層のニューロン数と全体の層数である。
- ソースコードの配布先は https://github.com/google-research/bert 
- オリジナルの論文は https://arxiv.org/abs/1810.04805

* データ: Wikipedia (2.5B words) + BookCorpus (800M words)
* バッチサイズ: 131,072 words (1024 sequences $\times$ 128 length or 256 sequences $\times$ 512 length)
* 訓練ステップ: 1M steps (40 epochs)
* 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
* BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
* BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
* 訓練時間: 4x4 / 8x8 の TPU で 4 日間

# LSTM との異同

<center>
<img src="https://komazawa-deep-learning.github.io/assets/2015Greff_LSTM_ja.svg" style="width:54%">
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:26%"><br/>
左: LSTM (浅川, 2015) より，右: トランスフォーマー[@2017Vaswani_transformer]<br/>
入力ゲートと入力 は Q, K と同一視，出力ゲートと V とは同一視可能？
</center>

<!-- 
# Residual attention
<center>

![](assets/2017residual_attention.svg){style="width:33%"}
![](assets/2017residual_attention_motivation.svg){style="width:65%"}<br/>
![](assets/2017residual_attention_whole_net.svg){style="width:94%"}<br/>
[@2017Wang_residual_attention] Fig. 1, 2, 3
</center>
-->

<!-- 
# A2 net

<center>
![](assets/2018Chen_A2-Nets_fig1ja_a.svg){style="width:39%"}
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
![](assets/2018Chen_A2-Nets_fig1ja_b.svg){style="width:55%"}<br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>
 -->

# Relationship between self-attention and convolution

<center>
<img src="http://komazawa-deep-learning.github.io/assets/2019cordonnier_self_attention_convol.svg" style="width:88%"><br/>
<img src="http://komazawa-deep-learning.github.io/assets/2020Cordonnier_tab3.svg" style="width:88%"><br/>
From [@2020cordonnier_attention_and_convolution]
</center>

# まとめ

- MHSA は 畳み込み と同等の能力がありそうである。
- Reformer に見られるように position encodings を工夫する余地は残されているように思われる。
