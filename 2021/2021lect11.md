---
title: 第10回
author: 浅川 伸一
layout: home
---

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 25/Jun/2021<br/>
Appache 2.0 license<br/>
</div>

# 1. キーワード

- 敵対的生成ネットワーク (GAN: Generative Adversarial Networks)
- 画風変換 (Style Transfer)
- EM アルゴリズム (Estimation Maximization Algorithm)
- 変分自動符号化器モデル (VAE: Variational Auto-Encoders)
- 変分下限 (ELBO: Evidence Lower BOund) [ビデオ ](https://www.youtube.com/watch?v=jugUBL4rEIM){:target="_blank"}
- [幻視 パレイドリア](2018Nishio_LBDtmp.pdf)

# 2. 実習
- [DeepDream 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg"> ](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021deep_dream_corrected.ipynb){:target="_blank"}
- [CartoonGAN 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021_0628CartoonGAN_demo.ipynb)
- [CycleGAN 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0625_CycleGAN_demo.ipynb){:target="_blank"}
- [VAE の実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_vae_demo.ipynb#scrollTo=GC8fpkk6cFbw){:target="_blank"}

#  3. ビデオ供覧
- [GAN ビデオ教材](https://drive.google.com/file/d/18uxufT2rf49ItV1PjP2J1-QgmseVoJgT/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1QE5X5enbEXKzHXj82veUC3dMGO3yrRrN/view?usp=sharing){:target="_blank"}
- [VAE の予備知識ビデオ](https://drive.google.com/file/d/1AKvOa-EeGnpfNznpfBGZ4xqZJR7Z1ro8/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1hIenuObaY-DX244pjvKh5zjOORlSbeT3/view?usp=sharing){:target="_blank"}
- [VAE ビデオ](https://drive.google.com/file/d/10jxvAeJzfRv2RkJcv-BKX--EerpwMWu1/view?usp=sharing){:target="_blank"}
[資料](https://drive.google.com/file/d/1hIenuObaY-DX244pjvKh5zjOORlSbeT3/view?usp=sharing){:target="_blank"}


# 4. 生成モデル Generative models

<center>
<img src="https://komazawa-deep-learning.github.io/assets/Tenn2.jpg" width="48%">
<img src="https://komazawa-deep-learning.github.io/assets/Tenn_deepdream.jpg" width="48%"><br/>
天安門前広場の夢 (撮影は自民解放軍の兵士に依頼した)
</center>

畳み込みニューラルネットワークの精度は向上しました。ですが，2014 年まで画像の合成はできませんでした。

<!-- 
Throughout most of this book, we have talked about how to make predictions. 
In some form or another, we used deep neural networks learned mappings from data examples to labels. 
This kind of learning is called discriminative learning, as in, weʼd like to be able to discriminate between photos cats and photos of dogs. 
Classifiers and regressors are both examples of discriminative learning. 
And neural networks trained by backpropagation have upended everything we thought we knew about discriminative learning on large complicated datasets. 
Classification accuracies on high-res images has gone from useless to human-level (with some caveats) in just 5-6 years. 
We will spare you another spiel about all the other discriminative tasks where deep neural networks do astoundingly well.

But there is more to machine learning than just solving discriminative tasks. 
For example, given a large dataset, without any labels, we might want to learn a model that concisely captures the characteristics of this data. 
Given such a model, we could sample synthetic data examples that resemble the distribution of the training data. 
For example, given a large corpus of photographs of faces, we might want to be able to generate a new photorealistic image that looks like it might plausibly have come from the same dataset. This kind of learning is called generative modeling.

Until recently, we had no method that could synthesize novel photorealistic images. 
But the success of deep neural networks for discriminative learning opened up new possibilities. 
One big trend over the last three years has been the application of discriminative deep nets to overcome challenges in problems that we do not generally think of as supervised learning problems. 
The recurrent neural network language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model.

In 2014, a breakthrough paper introduced Generative adversarial networks (GANs) (Goodfellow et al., 2014), a clever new way to leverage the power of discriminative models to get good generative models. 
At their heart, GANs rely on the idea that a data generator is good if we cannot tell fake data apart from real data. 
In statistics, this is called a two-sample test - a test to answer the question whether datasets $X = \{x_1,\ldots, x_n\}$ and $X' = \{x'_1,\ldots,x'_n\}$ were drawn from the same distribution.
The main difference between most statistics papers and GANs is that the latter use this idea in a constructive way. 
In other words, rather than just training a model to say “hey, these two datasets do not look like they came from the same distribution”, they use the two-sample test to provide training signals to a generative model. 
This allows us to improve the data generator until it generates something that resembles the real data. 
At the very least, it needs to fool the classifier. 
Even if our classifier is a state of the art deep neural network.
-->
<!-- 
2014 年 画期的な論文により Generative adversarial networks (GAN) (Goodfellow et al., 2014) が紹介されました。
これは 識別モデルの力を活用して 優れた生成モデルを得るための巧妙な新しい方法です。
GAN の核心は 「偽のデータと本物のデータを見分けることができなければ データ生成器は優れている」という考え方にあります。
統計学では  「2 標本検定」といい、データセット $X=\left\\{x_1,\ldots, x_n\right\\}$ と $X'=\left\\{x'_1,\ldots,x'_n\right\\}$ が同じ分布から引き出されているかどうかを調べる検定です。
統計学の論文と GAN の大きな違いは GAN がこの考え方を建設的に利用していることです。
つまり 単に「おい、この 2 つのデータセットは同じ分布から来たようには見えないぞ」 とモデルを訓練するのではなく 生成モデルに訓練信号を与えるために 2 標本検定 を使うのです。
これにより 実際のデータに似たものを生成するまで、データ生成器 を改良することができます。
少なくとも、分類器 を騙す必要があります。
たとえ 分類器 が最先端のディープニューラルネットワークであったとしてもです。


本物そっくりのデータを生成できる可能性のあるネットワークが必要です。
画像を扱うのであれば、画像を生成する必要があります。画像を扱う場合は画像を、音声を扱う場合は音声シーケンスを生成する必要があります。
これを「生成器ネットワーク」と呼びます。
2 つ目のネットワークは 識別器ネットワークです。
このネットワークは、偽物と本物のデータを区別しようとします。
この2つのネットワークはお互いに競争しています。
生成器ネットワークは、識別器ネットワークを欺こうとします。
この時、識別器ネットワークは、新しい偽のデータに適応します。
この情報は 今度は生成器ネットワークを改善するために使用され という具合です。
 -->

<!-- The GAN architecture is illustrated in Fig. 17.1.1.  -->

<!-- As you can see, there are two pieces in GAN architecture - first off, we need a device (say, a deep network but it really could be anything, such as a game rendering engine) that might potentially be able to generate data that looks just like the real thing. 
If we are dealing with images, this needs to generate images. If we are dealing with speech, it needs to generate audio sequences, and so on. We call this the generator network. 
The second component is the discriminator network. 
It attempts to distinguish fake and real data from each other. 
Both networks are in competition with each other. The generator network attempts to fool the discriminator network. 
At that point, the discriminator network adapts to the new fake data. 
This information, in turn is used to improve the generator network, and so on.

識別器は 入力 $x$ が（実データからの）本物か（生成器からの）偽物かを区別するための二値分類器です。
一般に識別器は 隠れたサイズが 1 の密な層を使用するなどして、入力 $masbf{x}$ に対してスカラー予測 $o\in\mathcal{R}$ を出力し シグモイド関数を適用して予測確率 $D(x) = 1/(1 + e^o)$ を得ることができます。
真のデータのラベル $y$ は 1 偽のデータは 0 だとします。
交差エントロピー損失を最小化するように識別器を学習:

$$
\min_D\left\{y\log(D(x))-(1-y)\log(1-D(x))\right\},
$$
-->

<!-- The discriminator is a binary classifier to distinguish if the input $x$ is real (from real data) or fake (from the generator). 
Typically, the discriminator outputs a scalar prediction $o\in\mathcal{R}$ for input $\mathbf{x}$, such as using a dense layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(x) = 1/(1 + e^o)$. 
Assume the label $y$ for the true data is 1 and 0 for the fake data. 
We train the discriminator to minimize the cross-entropy loss, i.e.,
-->


<!-- - 生成器は ランダムなソース 正規分布 $z\sim\mathcal{N}(0, 1)$ から パラメータ $z$ をサンプリング
- $z$ を潜在変数として扱う
- 生成器ネットワークを適用して $x'=G(z)$ を生成
-->

<!-- For the generator, it first draws some parameter $z\in\mathbb{R}^d$ from a source of randomness, e.g., a normal distribution $z\sim\mathcal{N}(0, 1)$. 
We often call $z$ as the latent variable. 
It then applies a function to generate $x'= G(z)$. 
The goal of the generator is to fool the discriminator to classify $x'=G(z)$ as true data, i.e., we want $D(G(z))\sim 1$. 
In other words, for a given discriminator $D$, we update the parameters of the generator $G$ to maximize the cross-entropy loss when $y=0$, i.e.,
 -->
$$
\min_G\left\{-(1-y)\log(D(G(\mathbf{z})))\right\} = \min_G\left\{-\log(D(G(\mathbf{z})))\right\}
$$

- 認識の反対の操作が 生成器。**生成敵対ネットワーク** Generative Adversarial Networks: GAN 
- GAN では 2 つのニューラルネットワークが用いられ，**識別器** descriminator と **生成器** generator と呼びます(Goodfellow,2014)。
- 識別器も生成器も多層ニューラルネットワーク

- 通常の画像分類課題では，最上位層において推論，すなわち入力画像が何であるかを計算するためにソフトマックス関数などが用いられる。
- これに対して GAN の識別器は，0 か 1 かの出力
- 入力画像が通常の画像であれば 1 を，生成器によって生成 された画像であれあば 0 を出力
- 生成器は，識別器の最終直下層で得られたような画像表現に雑音を加えた値から画像を生成
- 生成器は，識別器が入力データから画像を推論するのと逆方法に推論から画像を生成
- すなわち GAN は入力が実在するか，偽造品，すなわちフェイクかを見破る訓練がなされる
- 生成器の目的は 識別器を騙すこと。 $x'=G(z)$ を真のデータとして分類させること。すなわち $D(G(z))$ を $\sim 1$ に近づけること。
- 識別器 $D$ に対して $y=0$ のときの交差エントロピー損失 が最大になるように 生成器 $G$ のパラメータを更新する
- 生成器 は 識別器 の学習成果であるデータの内部表現を模倣し，生成器を欺こうとする
- 識別器 と 生成器 との間で  **ゲーム理論** でいう **ナッシュ均衡** Nash's equilibrium が成り立つ (Heusel, 2017)。

<!-- GAN の模式的な流れを下図 に示しました。 -->
<div align="center" style="width:88%">
<img src="/assets/2016Goodfellow_fig12ja.svg" style="width:99%"><br/>
</div>

## 4.1. 画像変換

<center>
<img src='/assets/2017Taigman_fig.svg' style='width:94%'><br>
</center>

<center>
<img src='/assets/2017Reed_tmp_5.svg' style='width:94%'>
</center>

<center>
<img src='/assets/2017Reed_tmp_1.svg' style='width:94%'>
</center>


<center>
<img src='/assets/2017Reed_tmp_6.svg' style='width:94%'>
</center>

## 4.2.  サイクル GAN
<center>
<img src='/assets/2017Reed_tmp_7.svg' style='width:94%'>
</center>

### 4.2.1. サイクル GAN による領域変換
<center>
<img src='/assets/2017Reed_tmp8.svg' style='width:94%'>
</center>

<center>
<img src='/assets/2017Zhu_cycleGAN1.svg' style='width:94%'><br>
<img src='/assets/2017Zhu_cycleGAN2.svg' style='width:94%'><br>
<img src='/assets/2017Zhu_cycleGAN3.svg' style='width:94%'><br>
</center>

---

## 4.3. まんがの画風変換

<center>
<img src='/assets/2018Chen_CartoonGAN.svg' style="width:94%"></br>
``CartoonGAN: Generative Adversarial Networks for Photo Cartoonization'' CVPR 2018 (Conference on Computer Vision and Pattern Recognition)
</center>

<center>
<img src='https://cdn-images-1.medium.com/max/1800/1*GS5_bEgpy00cotNRFvAPyA.png' style='width:46%'>
<img src="https://d2l930y2yx77uc.cloudfront.net/production/uploads/images/7291694/rectangle_large_type_2_86d2e2a52336624f98ed8aa3163a1865.jpg" style='width:49%'><br>
>
左: 君の名は。右: 風の谷のナウシカ，より
</center>

