---
title: 第14回
author: 浅川 伸一
layout: home
---

# ディープラーニングの心理学的解釈 (心理学特講IIIA)

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 17/Sep/2021<br/>
Appache 2.0 license<br/>
</div>

## 課題

以下の Mitchell (2008) 論文 [名詞の意味に関連した人間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"} 
を読み，要約しなさい。


## 資料と参考文献

- [感情とはそもそも何なのか](https://www.amazon.co.jp/dp/4623083721){:target="_blamk"}，乾 敏郎, ミネルヴァ書房，2018
- [計算論的精神医学](https://www.amazon.co.jp/dp/432625131X){:target="_blank"}, 国里，片平，沖村，山下, 勁草書房, 2019
- [真のAIへの鍵を握る天才神経科学者 Friston の紹介](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/){:target="_blank"}
- [Friston インタビュー YouTube 動画](https://youtu.be/RXTizOtvsE8){:target="_blank"}


## 後期のねらい

大きく分けて 2 つのテーマ，細かく分けると 4 つのテーマを取り扱います。

1. リカレントニューラルネットワークの発展
2. 注意
3. 強化学習, ゲーム AI
4. 精神医学

このうち，1 と 2 とが関係が深く，3 と 4 とも同様に関連があります。
1 のりカレントニューラルネットワークは，前期も取り上げました。
本日は，前期の復習と発展を取り上げます。

* RNN 復習
* LSTM, bidirectional RNN, BPTT, 
* char based, word based Language model
* multimodal integration, NIC, beta-VAE, Helmholtz machines, 
* LSTM gate, forget bias, vanishing graddient, explording gradient, 
* winner-take-all circuit
* DeepGaze, Itti Koch 
* CAM, Grad CAM
* Transformer, 
* Attention universality
* Reinforcement Learning, Policy, Value, Reward, Q learning, SALSA, dueling, 


## 前期のまとめ

- ディープラーニングによる認識精度の向上がもららした革新
- 認識精度の向上が，多くの分野に変革を促した。産業，軍事，医療，教育，社会
- 機械学習，ニューラルネットワーク，データサイエンスの変遷データ，現象，$\Leftrightarrow$ 仮説，モデル


# 1. Glaser (2019) の 教師あり機械学習の 4 つのレベル

<center>
<img src="../assets/2019Glaser_fig2.jpg" width="49%"><br/>
</center>

1. 工学的な問題の解決 
機械学習は， 医療診断， ブレインコンピュータインターフェース， 研究ツールなど， 神経科学者が使用する手法の予測性能を向上させることができる。
2. 予測可能な変数の特定 
機械学習により， 脳や外界に関連する変数がお互いを予測しているかどうかをより正確に判断することができる。
3. 単純なモデルのベンチマーク。
解釈可能な簡易モデルと精度の高い ML モデルの性能を比較することで， 簡易モデルの良し悪しを判断するのに役立つ。
4. 脳のモデルとしての役割。
脳が機械学習システム， 例えばディープニューラルネットワークと同様の方法で問題を解決しているかどうかを論じることができる。


# 2. 機械学習と脳画像研究および心理モデル

### 2.1 言語と機能的脳画像研究を結びつけるために，単語の分散表現を機械学習的手法で表現する

- [名詞の意味に関連した人間の脳活動の予測, Mitchell, 2018, Predicting Human Brain Activity Associated with the  Meanings of Nouns](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with
_the_Meanings_of_Nounsscience.pdf){:target="_blank"}

<center>
<img src="../assets/2019mitchell-54_20.png" style="width:49%"><br/>
</center>

### 2.2 下図 左のように，「セロリ」から右の脳画像を予測するために，中間表現として，兆 単位の言語コーパス (言語研究では訓練や検証に用いる言語データをコーパスと呼ぶ) から得られた **意味特徴** を用いる

<center>
<img src="../assets/2008Mitchell_fig1.svg" style="width:49%"><br/>
<p style="text-align: left;width: 66%; background-color: cornsilk;">
Mitchell (2008) 図 1. 任意の名詞刺激に対するfMRI活性化を予測するモデルの形式。
fMRI の活性化は、2段階 プロセスで予測される。
第 1 段階では，入力刺激語の意味を，典型的な単語使用を示す大規模なテキストコーパスから値を抽出した中間的な意味的特徴の観点から符号化する。
第 2 段階では，これらの中間的な意味的特徴のそれぞれに関連する fMRIシグネチャ の線形結合として，fMRI 画像を予測する。
<!-- 
Form of the model for predicting fMRI activation for arbitrary noun stimuli. 
fMRI activation is predicted in a two-step process. 
The first step encodes the meaning of the input stimulus word in terms of intermediate semantic features whose values are extracted from a large corpus of text exhibiting typical word use. 
The second step predicts the fMRI image as a linear combination of the fMRI signatures associated with each of these intermediate semantic features. -->
</p>
</center>

### 2.3 他の単語 (下図左) eat, taset, fill などの単語から セロリ を予測する回帰モデルを使って予測する
<center>
<img src="../assets/2008Mitchell_fig2.svg" style="width:66%"><br/>
<p style="text-align: left;width: 66%;background-color: cornsilk;">
Mitchell (2008) 図 2. 与えられた刺激語に対する fMRI 画像の予測。
(A) 参加者 P1 が 「セロリ」刺激語に対して、他の 58 の単語で学習した後に予測を行う。
25 個の意味的特徴のうち 3 つの特徴量のベクトルを単位長にスケーリングすることである。
(食べる, 味わう, 満たす) について学習した $c_{vi}$ 係数は， パネル上部の 3 つの画像のボクセルの色で示されている。
刺激語「セロリ」に対する各特徴量の共起値は， それぞれの画像の左側に表示されている (例えば 「食べる（セロリ）」の 共起値は 0.84)。
刺激語の活性化予測値 ((A）の下部に表示) は 25個 の意味的 fMRI シグネチャを線形結合し， その共起値で重み付けしたものである。
この図は 予測された三次元画像の1つの水平方向のスライス [z=-12 mm in Montreal Neurological Institute (MNI) space] を示している。
(B) 「セロリ」と「飛行機」について， 他の 58 個の単語を使った訓練後に予測された fMRI 画像と観察された fMRI 画像。
予測画像と観測画像の上部（後方領域）付近にある赤と青の 2本 の長い縦筋は、左右の楔状回である。
<!-- Predicting fMRI images for given stimulus words. 
(A) Forming a prediction for participant P1 for the stimulus word “celery” after training on 58 other words. 
Learned $c_{vi}$ coefficients for 3 of the 25 semantic features (“eat,” “taste,” and “fill”) are depicted by the voxel colors in the three images at the top of the panel. 
The cooccurrence value for each of these features for the stimulus word “celery” is shown to the left of their respective images [e.g., the value for “eat (celery)” is 0.84]. 
The predicted activation for the stimulus word [shown at the bottom of (A)] is a linear combination of the 25 semantic fMRI signatures, weighted by their co-occurrence values. 
This figure shows just one horizontal slice [z = –12 mm in Montreal Neurological Institute (MNI) space] of the predicted three-dimensional image. 
(B) Predicted and observed fMRI images for “celery” and “airplane” after training that uses 58 other words. 
The two long red and blue vertical streaks near the top (posterior region) of the predicted and observed images are the left and right fusiform gyri. -->}
</p>
</center>


<center>
<img src="../assets/2008Mitchell_fig3.svg" style="width:49%"><br/>
<p style="text-align: left;width:66%;background-color:cornsilk;">
Mitchell (2008) 図 3. 最も正確に予測されたボクセルの位置。
参加者 P5 の訓練セット以外の単語について、予測されたボクセルの活性化と実際のボクセルの活性化の相関を表面（A）とグラスブレイン（B）で表したもの。
これらのパネルは、少なくとも 10個 の連続したボクセルを含むクラスタを示しており、それぞれのボクセルの予測-実際の相関は少なくとも 0.28 である。
これらのボクセル・クラスターは、大脳皮質全体に分布しており、左右の後頭葉と頭頂葉、左右の豆状部、中央後葉、中央前葉に位置しています。
左右の後頭葉、頭頂葉、中前頭葉、左下前頭回、内側前頭回、前帯状回に分布している。
(C) 9人の参加者全員で平均化した予測-実測相関の表面表現。
このパネルは、平均相関が 0.14 以上の連続した10 個以上のボクセルを含むクラスターを示している。
<!-- Locations of most accurately predicted voxels. 
Surface (A) and glass brain (B) rendering of the correlation between predicted and actual voxel activations for words outside the training set for participant P5. 
These panels show clusters containing at least 10 contiguous voxels, each of whose predicted-actual correlation is at least 0.28. 
These voxel clusters are distributed throughout the cortex and located in the left and right occipital and parietal lobes; left and right fusiform,
postcentral, and middle frontal gyri; left inferior frontal gyrus; medial frontal gyrus; and anterior cingulate. 
(C) Surface rendering of the predicted-actual correlation averaged over all nine participants. 
This panel represents clusters containing at least 10 contiguous voxels, each with average correlation of at least 0.14. -->
</p>
</center>

# 3. 注意の近似表現としてのソフトマックス関数

$$
\text{Softmax}\left(x\right) = \frac{\exp(x)}{\sum_{y\in\mathcal{D}}\exp(y)}\tag{1}
$$

以降，ソフトマックス関数を $\sigma(x)$ と表記する。
ソフトマックス関数の本質は，以下のとおり

1. 全データを指数の肩に乗せる $\exp(x)=e^x$ 
2. 指数の肩に乗せることで，実数の範囲では，$\exp(x) \ge0$ が成り立つ。すなわち全ての値が 0 または 正 になる。
3. すべての値が 0 または 正であるので，全データ $\mathcal{D}$ に渡って全て足し合わた (総和 $\sum$) 値で割る (分母にする) ことで確率として表現できる。
確率であるためには，全ての要素が 0 または 正 であり，かつ，全て足し合わせると 1 となれば良い。
4. 各要素を分子に，全要素の総和を分母にすることで，どの要素がもっともらしいかの確率を得ることができる

以上がもっとも簡単なソフトマックス関数の説明である。

例えば，要素が 2 つしかない，単純な例を考える。
$x$ と $y$ という 2 つの値が得られたとき，この確率をソフトマックス関数を用いて考えてみる。
$e^x$ と $e^y$ とから，$x$ の確率 $p(x)$ は $\displaystyle P(x)=\frac{e^x}{e^x+e^y}$  となる。
要素が $x$ と $y$ と 2 つしかないのだから，2 つの要素を足し合わせると 1 となる，すなわち $P(x) + P(y) = 1$ が成り立つ。

$$
P(x) + P(y) = \frac{e^x}{e^x+e^t} + \frac{e^y}{e^x+e^y} = \frac{e^x+e^y}{e^x+e^y} = 1\tag{2}
$$

上式の 2 つしか要素がない場合のソフトマックス関数のことを シグモイド関数 sigmoid function，あるいはロジスティックシグモイド関数 logistic sigmoid function と呼ぶ。
2 しか要素がない場合には，x の確率が決まれば，y の確率は $1-p(x)$ で定まる。
従って，$x$ と $y$ と 2 つの要素を考える必要はなく，一つだけ考えれば良い。
実際に以下のような計算が可能である:

$$
\begin{aligned}
\frac{e^x}{e^x+e^y} &= \frac{e^x/e^x}{e^x/e^x+e^y/e^v}\\
&= \frac{1}{1+e^{y-x}}\\
&= \frac{1}{1+e^{-(x-y)}}
\end{aligned}
$$

ここで $x-y$ を $z$ とすると，$\sigma(z)=[1+\exp(-z)]^{-1}$ となる。
$z$ は $x$ と $ｙ$ との差である。

$z$ が $0$ であれば $x=y$ を意味するので $e^{-z=0} = e^{-(x-y=0)} = z^{0} = 1 $ である。
従って $\sigma(z) = 1/(1+1) = 1/2 $ となる。

$z>0$ と $z$ が 正であれば，$z>0\rightarrow (x-y)>0 \rightarrow x>y$ であるので，$e^{-z} < 1$ となるから，$\sigma(z) > 1/2$ となる。

すなわち $x$ の方が $y$ より大きければ，$x$ である確率は 0.5 より大きくなる。
逆に $z<0$ すなわち $x$ の方が $y$ より小さければ $\sigma(z) < 0.5$ となる。
このことは $x$ と $y$ との大小関係により，その確率が 0.5 より大きくなるか小さくなるかが定まることを意味するので，我々の直感にも合致している。


$\sigma(x)$ の様子を描画する最小限の python プログラムを以下に示す


```python
import numpy as numpy
import matplotlib.pyplot as plt
%matplotlib inline

x = np.linspace(-5,5)
y = 1/(1+np.exp(-x))
plt.plot(x,y)
```


### 物理学メタファー

上記のソフトマックス関数は，熱力学，統計力学の分野との関係が指摘できる。
あるシステムには $N$ 個の粒子が存在するとする。
各粒子のエネルギー準位は $e^{\beta x_i}$ で表されるとする。
このシステムの全エネルギーは各粒子の総和であるので，$\sum_{i\in\mathcal{D}} e^{\beta x_i}$ である。
これは，上述のソフトマックス関数の分母に現れる式と同一である。
物理学では，この分母 $\sum_{i\in\mathcal{D}}e^{x_i}$ を **分配関数 partition function** と呼ぶ。

任意の粒子 $x_i$ のエネルギー分布は，この粒子のエネルギーを分子とし，直上の分配関数を分母として次式で表される。

$$
P(x_i) = \frac{e^{-\beta x_i}}{\sum_j e^{-\beta x_j}}\tag{3}
$$

$\beta$ は実際の温度の逆数であるので，逆温度 または 温度パラメータとも呼ばれる量である。
温度が高ければ，システム全体の挙動が不安定になる。
すなわち，各粒子のエネルギー準位をとる確率が不安定になる。

一方，温度が低いと，各粒子のエネルギー準位は決定論的に振る舞うことになる。



---

# デモと実習

- [リカレントニューラルネットワークによる文処理デモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [SRN のデモ <img src="../assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}
- [足し算のデモ <img src="../assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-l
earning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
- [百人一首データ取得](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0917get_hyakunin_isshu.ipynb){:target="_blank"}
- [BERT の超簡単な使い方 <img src="https://ShinAsakawa.github.io./assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"}
