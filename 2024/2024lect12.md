---
title: 第12回 2024年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+1" color="navy"><strong>心理学特講IIIA ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 05/Jul/2024<br/>
Appache 2.0 license<br/>
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

## デモと実習

* [百人一首の上の句とエンコーダによって符号化し，下の句をデコーダで生成するモデル比較 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2024_0705Chihaya_Transformer_RNN_comparison.ipynb){:target="_blank"}
* [word2vec 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0619word2vec.ipynb)
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ](https://komazawa-deep-learning.github.io/character_demo.html){:target="_blank"}
- [リカレントニューラルネットワークによる文字ベース言語モデルデモ みんなの日本語](https://komazawa-deep-learning.github.io/minnichi_char_rnn.html){:target="_blank"}
- [SRN のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702rnn_demo.ipynb){:target="_blank"}
- [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
<!-- - [百人一首データ取得](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0917get_hyakunin_isshu.ipynb){:target="_blank"} -->
<!-- - [BERT の超簡単な使い方 <img src="/assets/colab_icon.svg">](https://colab.research.google.c
om/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb){:target="_blank"} -->
<!-- - [最小限の MeCab](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2022_0916mecab_test.ipynb) -->

<!-- ## 課題

以下の Mitchell (2008) 論文 [名詞の意味に関連した人間の脳活動の予測](https://shinasakawa.github.io/2008Mitchell_Predicting_Human_Brain_Activity_Associated_with_the_Meanings_of_Nounsscience.pdf){:target="_blank"}
を読み，要約しなさい。 -->

# 1. リカレントニューラルネットワーク

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<div class="figcenter">
<img src="/assets/RNN_fold.svg" style="width:49%"><br>
Time unfoldings of recurrent neural networks
</div>

上図右を頭部を 90 度右に傾けて眺めて見て欲しい。
あるいは同義だが上図右を反時計回りに 90 度回転させたメンタルローテーションを想像しても良い。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

### エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示した。
図中の数字はニューロンの数を表す。
入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表す。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:33%"><br>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示し。
Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示された (Elman, 1990, 1991, 1993)。

<!--
- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | live
s

これらの規則にはさらに 2 つの制約がある。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもと
らなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この
文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というも
のです。-->

### LSTM

* LSTM (Long Short-Term Memory) は Schmithuber らによって開発されたリカレントネットワークモデルの改良版。
* 内部に 3 つのゲートを持つ。
* ゲートの出力関数は，シグモイド関数である。このため，信号の開閉をソフトに制御する機能を有する。
* 信号の取捨選択を，データから学習させることで，**長距離依存** に対応することを意図している。

<center>
<img src="/assets/2015Greff_LSTM_ja.svg" style="width:34%">
<!-- <img src="https://komazawa-deep-learning.github.io/assets/2015Greff_LSTM_ja.svg" style="width:54%"> -->
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-19.png" style="width:16%"><br/>
<!-- <img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:26%"><br/> -->
左: LSTM (浅川, 2015) より，右: トランスフォーマー[@2017Vaswani_transformer]<br/>
入力ゲートと入力 は Q, K と同一視，出力ゲートと V とは同一視可能？
</center>

<!--
# Residual attention
<center>

![](assets/2017residual_attention.svg){style="width:33%"}
![](assets/2017residual_attention_motivation.svg){style="width:65%"}<br/>
![](assets/2017residual_attention_whole_net.svg){style="width:94%"}<br/>
[@2017Wang_residual_attention] Fig. 1, 2, 3
</center>
-->

<!--
# A2 net

<center>
![](assets/2018Chen_A2-Nets_fig1ja_a.svg){style="width:39%"}
&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;
![](assets/2018Chen_A2-Nets_fig1ja_b.svg){style="width:55%"}<br/>
From [@2018Chen_A2-nets_double_attention] Fig. 1
</center>

# Relationship between self-attention and convolution

<center>
<img src="/assets/2019cordonnier_self_attention_convol.svg" style="width:66%"><br/>
<img src="/assets/2020Cordonnier_tab3.svg" style="width:66%"><br/>
From [@2020cordonnier_attention_and_convolution]
</center>

# まとめ

- MHSA は 畳み込み と同等の能力がありそうである。
- Reformer に見られるように position encodings を工夫する余地は残されているように思われる。
-->

# 2. 符号化器・復号化器モデル (encoder-decoder model), seq2seq model

* 中間層の最終時刻の状態に文表現が埋め込まれているとすると，これを応用するば **機械翻訳** や **対話** のモデルになる。
* 初期の翻訳モデルである "seq2seq" の概念図を示した。
* "eos" は文末 end of sentence を表す。
* 中央の "eos" の前がソース言語であり，中央の "eos" の後はターゲット言語の言語モデルである単純再帰型ニューラルネットワークの中間層への入力として用いられる。

* 注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がない。
* 逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことが可能である。
* この点を改善することを目指すことが 2014 年以降盛んになった。
* 顕著な例が後述する **双方向 RNN**, **LSTM** を採用したり，**注意** 機構を導入することであった。

<!--
![Time unfoldings of recurrent neural networks](./assets/RNN_fold.svg){width="74%"}
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" width="49%"><br/>
From [2014Sutskever_Sequence_to_Sequence]
rom [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$\mbox{argmax}_{\theta} \left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)$$
-->

<center>
<img src="/assets/2014Sutskever_Fig2left.svg" width="44%">
<img src="/assets/2014Sutskever_Fig2right.svg" style="width:44%"><br />
From [@2014Sutskever_Sequence_to_Sequence] Fig. 2, 3
</center>

# 3. 自然言語系の注意

<div class="figure figcenter">
<img src="/assets/2015Bahdanau_attention.jpg" width="30%">
<img src="/assets/2015Luong_Fig2.svg" width="30%">
<img src="/assets/2015Luong_Fig3.svg" width="30%">
<div class="figcaption">

左: Bahdanau+2014,
中: Luong+2015, Fig. 2,
右: Luong+2015, Fig. 3
</div></div>

# 4. トランスフォーマー

* 注意を用いて，RNN を置き換える [Devlin+2017,Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* 専門用語としては，**多頭=自己注意** Multi-Head Self-Attention (以下 MHSA と表記)と呼ぶ。
* 多頭とは何か，なぜ **自己** がつく注意なのかを確認してほしい。

<center>
<img src="/assets/ModalNet-19.png" style="width:15%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-20.jpg" style="width:23%">
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/ModalNet-21.png" style="width:29%">
</center>
<!--
![](assets/ModalNet-19.png){style="width:15%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-20.jpg){style="width:23%"}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
![](assets/ModalNet-21.png){style="width:29%"}
</center>
-->


* 上図，クエリ，キー，バリュー に注目してください。英単語の意味どおりに解釈すれば，問い合わせ，キー（鍵），値，となる。
* つまり，ある問い合わせに対して，キーを与えて，その答えとなる値を得ること。
* この操作を入力情報から作り出して答えを出力する仕組みに，ワンホット表現を使うことがポイント

<!-- 下図左は上図右と同じものです。この下図右を複数個束ねると下図中央になります。 -->

- 図中央の Scaled Dot-Product Attention と書かれた右脇に小さく h と書かれている。この h とは ヘッド の意味。
- 図中央を 1 つの単位として，次に来る情報と連結させる。図右。
- リカレントニューラルネットワークでは，中間層の状態が次の時刻の処理に継続して用いられていた。
- ところが 多頭=自己注意 MHSA では一つ前の入力情報を，現在の時刻の情報に対するクエリとキーのように扱って情報を処理する。
- 図右の下から入力される情報は，input と output と書かれている。
さらに output の下には (Shifted right) と書かれています。
すなわち，時系列情報を一時刻分だけ右にずらし（シフト）させて逐次情報を処理することを意味している。
- 図右の下から入力される情報は，embedding つまり埋め込み表現 と 位置符号化 position embedding が足し合わされたもの。
埋め込み表現とは先週 word2vec で触れたベクトルで表現された，単語（あるいはそれぞれの項目）の 意味表現 に対応。
* さらに，下図右は，視覚用に開発れたトランスフォーマーである。

<center>
<img src="/assets/ModalNet-19.png" style="width:14%">
<!-- <img src="https://komazawa-deep-learning.github.io/assets/ModalNet-19.png" style="width:24%"> -->
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
<img src="/assets/2019Ramachandran_fig3.jpg" style="width:44%"><br/>
<!-- <img src="https://komazawa-deep-learning.github.io/assets/2019Ramachandran_fig3.jpg" style="width:64%"><br/> -->
Left: [@2017Vaswani_transformer], Right: [@2019Ramachandran_attention_vision]
</center>

<!--
<center>

![](assets/2019Zhang_Goodfellow_SAGAN_fig2.jpg){style="width:88%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1upper.jpg){style="width:74%"}<br/>
![](assets/2019Zhang_Goodfellow_SAGAN_fig1lower.jpg){style="width:74%"}<br/>
From [@2019Zhang_Goodfellow_SAGAN] Fig. 1, and 3.
画像生成において，近傍画素から情報だけでなく，関連する遠距離の特徴を利用して生成することにより一貫性のある対象やシナリオを生成可能。
各行の左の元画像上のカラー点は 5 つ の 代表的なクエリの場所を示す。
右側の 5 画像は 各クエリ位置における注意地図。最も注目されている領域が，色分けされた矢印で示されている。
</center>
-->

<!--
<center>

![](assets/2017Gupta_Non-local_fig2.svg){style="width:29%"}
![](assets/2017Gupta_Non-local_example_230_0_eps_18_9.svg){style="width:59%"}<br/>
時空の非局所ネットワークの概念図。特徴地図はテンソルとして示されている。
例えば 1024 チャンネルの場合は $T\times H\times W\times1024$ である。
$\otimes$ は行列積を，$\oplus$ は要素和を示す。
ソフトマックス演算は各行に対して実行される。
青いボックスは $1\times1\times1\times1$ の畳み込みを表す。
$512$ チャンネルのボトルネックを持つ埋め込みガウシアン版が示されている。
バニラガウス版は $\theta$ と $\phi$ とを除去することで ドット積版は $1/N$ のスケーリングでソフトマックスを置き換えることで行うことができる。
From [@2018Wang_Girshick_Non-local]
</center>
-->

<!-- <center>

![](assets/2018snail_fig2b.svg){style="width:49%"}<br/>
From [@2018Mishra_SNAIL] Fig. 2
</center>

トランスフォーマーはリカレント構造や畳み込み構造を持たず埋め込みベクトルに位置符号化器を加えることで系列情報を処理する。
しかし、逐次的な順序情報が貧弱であるとの批判がある。
とりわけ強化学習のような位置依存性に敏感な課題では問題。
トランスフォーマーモデルにおける 位置問題を解決するため，自己注意機構 と 時間的な畳み込み temporal convolution を組み合わせたモデルが
Simple Neural Attention Meta-Learner (SNAIL)[@2018Mishra_SNAIL]。
SNAIL は，メタ学習，強化学習の両方の課題に優れていることが実証された。
-->

少しだけまとめると:

- 自然言語処理，画像処理，強化学習，メタ学習の 4 分野でほほ同様の 多頭自己注意 MHSA が取り入れられている。
- クエリ，キー，バリュー の重みを学習することが MHSA の学習である。
- 従来手法である 畳み込み や LSTM を MHSA で置き換える動きがある。

# 5. BERT

- 上記のトランスフォーマーに基づいて BERT が提案された [Devlin2018](https://arxiv.org/abs/1810.04805)。
- BERT は **B**idirectional **E**ncoder **R**epresentations from **T**ransformers から命名したと原著論文には書いてあります。
- ですが，この原著論文の直前に提案されたモデルに ELMo があったため，こじつけた，ふざけた命名でしょう。
- もちろん ELMo (こちらは **E**mbeddings from **L**anguage **Mo**dels から命名されました)も BERT もセサミストリートに出てくるキャラクタです。

<!-- From singularitysalon2019/nlp.tex -->

<!--BERT の影響が大きいので，本稿でも BERT を中心に取り上げる。-->BERT の特徴を 3 つにまとめると以下の通り

1. トランスフォーマー Transformer に基づく 多頭自己注意 (MHSA) を使った多層ニューラルネットワークモデル
2. 2 つの事前訓練: **マスク化言語モデル** と **次文予測課題** を用いる
3. 事前訓練済のモデルを用いて，解くべき課題のそれぞれについて **ファインチューニング** Fine tuning を施す
4. 個別の課題は下流課題 down stream tasks と呼ばれます。上流 と 下流 との区別は，最初に行う事前訓練のことを時間的に先行するので上流，その後のファインチューニングするそれぞれの課題のことを下流課題と呼んでいます。
5. 複数の課題に対して個別にファインチューニングを行うことにより，複数の下流課題で性能向上が認められました。 [GLUE スコアボード](https://gluebenchmark.com/leaderboard), [SuperGLUE](https://super.gluebenchmark.com/leaderboard/) を参照してください。


## BERT の入力表現

- 上の図にもあったとおり BERT では入力情報が埋め込み表現だけでなく，位置符号化器の情報が加算されます。
- BERT では，埋め込み表現と位置符号化器の情報に加えて，セグメント埋め込み segment embeddings も加えた情報が入力情報となります。下図参照

<center>
<img src="/assets/2018Devlin_BERT_Fig2.svg" style="width:66%"><br/>
<!-- ![](assets/2018Devlin_BERT_Fig2.svg){style="width:84%"}<br /> -->
埋め込みトークンの総和，位置符号器，分離埋め込みの 3 者 From [@2018BERT] Fig. 2
</center>

- 上図では，下 3 行が入力情報を構成する 3 つの要素になっています。上（ピンク色）が合算した入力情報になります。
- 3 つの入力情報とはそれぞれ，下から 位置符号化器 （薄灰色），セグメント埋め込み (淡緑)，トークン埋め込み (淡黄) です。

## 位置符号器 Position encoders

- 上述のようにトランスフォーマーの入力には，単語埋め込み表現に加えて，位置符号器の信号も加算されます。

<!-- 位置 $i$ の信号は次式で周波数領域へと変換される:

$$
\begin{align}
\text{PE}_{(\text{pos},2i)} &= \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\\
\text{PE}_{(\text{pos},2i+1)} &= \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{align}
$$
-->

- 位置符号器による位置表現は，i 番目の位置情報をワンホット表現するのではなく，周波数領域に変換することで周期情報を表現する試みと見なすことができます。

<center>
<img src="/assets/PE_example.svg" style="width:59%"><br/>
位置符号化に用いられる符号化。位置情報を周波数情報へ変換して用いています。
<!-- ![](assets/PE_example.svg){style="width:74%"}<br/> -->
</center>

- 位置情報を周波数情報へ変換することが良いことなのか，どうなのか，は議論されている最中です。
一つの研究テーマでもあります。

- 数学的な説明は **フーリエ変換** を調べてください。任意の関数 y=f(x) では x は位置情報を表しているとみなすことができます。
従って，位置 x を与えると対応する値 y が得られることを表している式が y=f(x) です。
これに対して，任意の情報は周波数，すなわち，波の重ね合わせとして表現できます。
すべての周波数を重ね合わせると元の関数になります。
反対に，ある周波数の値は，関数 f(x) を周波数へ変換したときの特定の周波数成分として表現できます。

BERT における位置符号化器は位置情報を波の成分として表現したことになります。

このようにしてできた値を入力側と出力側で下図のように連結させたものが以下のトランスフォーマーです。

<center>
<img src="/assets/2017Vaswani_Fig1.svg" style="width:33%"><br/>
From [@2017Vaswani_transformer] Fig. 1
</center>

これまで見てきたように，トランスフォーマーでは入力信号に基づいて情報の変換が行なわれる。
この意味ではトランスフォーマーにおける 多頭 自己注意 MHSA とはボトムアップ注意の変形であるとみなしうる。
逆言すれば，RNN のように過去の履歴をすべて保持しているわけではないので，系列情報については，position encoders に頼っている側面が指摘できる。

<!-- %\input{ELMoBERTGPT_Gao2018.tex}
へーこれでインプットか？-->

## BERT の事前訓練: マスク化言語モデル

全入力系列のうち 15% をランダムに [MASK] トークンで置き換える

- 入力はオリジナル系列を [MASK] トークンで置き換えた系列
- ラベル: オリジナル系列の [MASK] 部分にの正しいラベルを予測
- 80%: オリジナル入力系列を [MASK] で置換
- 10%: [MASK] の位置の単語をランダムな無関連語で置き換える
- 10%: オリジナル系列

## BERT の事前訓練: 次文予測課題

言語モデルの欠点を補完する目的，次の文を予測

[SEP] トークンで区切られた 2 文入力

- 入力: the man went to the store [SEP] he bought a gallon of milk.
- ラベル:  IsNext
- 入力:  the man went to the store [SEP] penguins are flightless birds.
- ラベル:  NotNext

## BERT: ファインチューニング (微調整)

(a), (b) は文レベル課題，
(c),(d)はトークンレベル課題, E: 入力埋め込み表現, $T_i$: トークン $i$ の文脈表象。

<!--
- [CLS]: 分類出力記号,
- [SEP]: 文分離記号
-->

<center>
<img src="/assets/2018Devlin_BERT_Fig3.svg" style="width:66%"><br/>
From [@2018BERT] Fig.3
</center>

## GLUE 課題 (General Language Understanding Evaluation)
- **CoLA**: 入力文が英語として正しいか否かを判定
- **SST-2**: スタンフォード大による映画レビューの極性判断
- **MRPC**: マイクロソフトの言い換えコーパス。2文 が等しいか否かを判定
- **STS-B**: ニュースの見出し文の類似度を5段階で評定
- **QQP**: 2 つの質問文の意味が等価かを判定
- **MNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **QNLI**: 2 入力文が意味的に含意，矛盾，中立を判定
- **RTE**: MNLI に似た2つの入力文の含意を判定
- **WNI**: ウィノグラッド会話チャレンジ

その他

- **SQuAD**: スタンフォード大による Q and A ウィキペディアから抽出した文
- **RACE**: 中学入試，高校入試に相当するテスト多肢選択回答

### BERT モデルのパラメータ詳細
- データ: Wikipedia (2.5B words) + BookCorpus (800M words)
- バッチサイズ: 131,072 words (1024 sequences * 128 length or 256 sequences * 512 length)
- 訓練時間: 1M steps (~40 epochs)
- 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
- BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
- BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
- 4x4 / 8x8 TPU で 4 日間

#### CoLA サンプル

1 は正しい英文，0 は非文

- 1 They drank the pub dry.
- 0 __They drank the pub__.
- 1 The professor talked us into a stupor.
- 0 __The professor talked us__.
- 1 We yelled ourselves hoarse.
- 0 __We yelled ourselves__.

#### SST-2 サンプル

0 は低評価，1 は高評価

- hide new secretions from the parental units     0
- contains no wit , only labored gags     0
- that loves its characters and communicates something rather beautiful about human nature        1
- remains utterly satisfied to remain the same throughout         0
- on the worst revenge-of-the-nerds clichés the filmmakers could dredge up        0
- that's far too tragic to merit such superficial treatment      0

<!-- - demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , pe
- rsonal film with an emotional wallop .  1
- of saucy        1
- a depressed fifteen-year-old 's suicidal poetry         0
- are more deeply thought through than in most ` right-thinking ' films   1
- goes to absurd lengths  0
- for those moviegoers who complain that ` they do n't make movies like they used to anymore      0
- the part where nothing 's happening ,   0
- saw how bad this movie was      0
- lend some dignity to a dumb story       0
 -->

#### MRPC サンプル

- 1
    - 文1: "Please, keep doing your homework," said Bavelier, the mother of three.
    - 文2: "Please, keep doing your homework," said Bavelier, the mother of 6-year-old twins and a 2-year old.
- 1
    - 文1: While Mr. Qurei is widely respected and has a long history of negotiating with the Israelis, he cannot expect such a warm welcome.
    - 文2: While Qureia is respected and has a history of negotiating with the Israelis, a warm welcome is not expected.
- 1
    - 文1: "Nobody wants to go to war with anybody about anything ... it 's always very much a last resort thing and one to be avoided," Mr Howard told Sydney radio.
    - 文2: "We don't want to go to war with anybody . . . it's always very much a last resort, and one to be avoided.
- 0
    - 文1: GMT, Tab shares were up 19 cents, or 4.4% , at A $4.56, having earlier set a record high of A $4.57.
    - 文2: Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A $4.57.
- 0
    - 文1: Martin, 58, will be freed today after serving two thirds of his five-year sentence for the manslaughter of 16-year-old Fred Barras.
    - 文2: Martin served two thirds of a five-year sentence for the manslaughter of Barras and for wounding Fearon.

<!-- - 1
    - 文1: The stock rose $2.11, or about 11 percent, to close Friday at $ 21.51 on the New York Stock Exchange.
    - 文2: PG & E Corp. shares jumped $1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday.
- 1
    - 文1: Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.
    - 文2: With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.
- 0
    - 文1: The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday.
    - 文2: The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15.
- 1
    - 文1: The DVD-CCA then appealed to the state Supreme Court.
    - 文2: The DVD CCA appealed that decision to the U.S. Supreme Court.

 -->
<!-- # BERT ファインチューニング手続き
<center>
<img src="./assets/2019Devlin_mask_method21.jpg" style="width:74%"><br/>
</center>
 -->

#### SST-B サンプル

最後の数値が評価値

- A plane is taking off.  An air plane is taking off.   5.000
- A man is playing a large flute. A man is playing a flute.     3.800
- A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800
- Three men are playing chess.    Two men are playing chess.    2.600
- A man is playing the cello.     A man seated is playing the cello.    4.250
- Some men are fighting.  Two men are fighting. 4.250
- A man is smoking.   A man is skating 0.5000

### QQP サンプル

0 は異なると判断， 1 は同じと判断すべき文

- 0
    - How is the life of a math student? Could you describe your own experiences?
    - Which level of prepration is enough for the exam jlpt5?
- 1
    - How do I control my horny emotions?
    - How do you control your horniness?
- 0
    - What causes stool color to change to yellow?
    - What can cause stool to come out as little balls?     0
- 1
    - What can one do after MBBS?
    - What do i do after my MBBS?
- 0
    - Where can I find a power outlet for my laptop at Melbourne Airport?
    - Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?
- 0
    - How not to feel guilty since I am Muslim and I'm conscious we won't have sex together?
    - I don't beleive I am bulimic, but I force throw up at least once a day after I eat something and feel guilty.  Should I tell somebody, and if so who?

#### MNLI サンプル

- 矛盾
    - Met my first girlfriend that way.
    - I didn’t meet my first girlfriend until later.
- 中立
    - 8 million in relief in the form of emergency housing.
    - The 8 million dollars for emergency housing was still not enough to solve the problem.
- 中立
    - Now, as children tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.
    - All of the children love working in their gardens.
- 含意
    - At 8:34, the Boston Center controller received a third transmission from American 11
    - The Boston Center controller got a third transmission from American 11.
- 中立
    - I am a lacto-vegetarian.
    - I enjoy eating cheese too much to abstain from dairy.
- 矛盾
    - someone else noticed it and i said well i guess that’s true and it was somewhat melodious in other words it wasn’t just you know it was really funny
    - No one noticed and it wasn’t funny at all.


### BERT 多言語対応
<center>
<img src="/assets/2019Lample_Fig1.svg" style="width:66%"><br/>
From [@2019Lample_Cross-lingual] Fig. 1
</center>

### BERT の発展

* BERTlogy バートロジーとして，BERT を弄り倒す研究が量産されるようになった。
* キーワードとしては，[プロンプト](https://arxiv.org/abs/2201.04337)，[センテンス BERT](https://arxiv.org/abs/1908.10084) 等がある。
* [プロンプトエンジニアリング](https://arxiv.org/abs/2107.13586) として，数多くの研究がなさている。

<center>
<img src="/assets/2019Rajasekharan_conver.png" style="width:54%"><br/>
From <https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58>
</center>

### BERT: ファインチューニング手続きによる性能比較

<center>
<img src="/assets/2019Devlin_mask_method21.jpg" style="width:33%"><br/>
マスク化言語モデルのマスク化割合の違いによる性能比較
</center>

マスク化言語モデルのマスク化割合は マスクトークン:ランダム置換:オリジナル=80:10:10 だけでなく，
他の割合で訓練した場合の 2 種類下流課題，
MNLI と NER で変化するかを下図 \ref{fig:2019devlin_mask_method21} に示した。
80:10:10 の性能が最も高いが大きな違いがあるわけではないようである。

<!-- # BERT モデルサイズ比較
<center>
<img src="./assets/2019Devlin_model_size20.jpg" style="width:69%"><br/>
</center>
-->

### BERT: モデルサイズ比較

<center>
<img src="/assets/2019Devlin_model_size20.jpg" style="width:33%"><br/>
モデルのパラメータ数による性能比較
</center>

パラメータ数を増加させて大きなモデルにすれば精度向上が期待できる。
下図では，横軸にパラメータ数で MNLI は青と MRPC は赤 で描かれている。
パラメータ数増加に伴い精度向上が認められる。
図に描かれた範囲では精度が天井に達している訳ではない。パラメータ数が増加すれば精度は向上していると認められる。

### BERT: モデル単方向，双方向モデル比較

<center>
<img src="/assets/2019Devlin_directionality19.jpg" style="width:33%"><br/>
言語モデルの相違による性能比較
</center>

言語モデルをマスク化言語モデルか次単語予測の従来型の言語モデルによるかの相違による性能比較を
下図 \ref{fig:2019devlin_directionality19} に示した。
横軸には訓練ステップである。訓練が進むことでマスク化言語モデルとの差は 2 パーセントではあるが認められるようである。


<!-- # BERT 事前訓練比較
<center>
<img src="./assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:66%"><br/>
</center>
-->

### BERT: 事前訓練比較

<center>
<img src="/assets/2019Devlin_Effect_of_Pretraining18.jpg" style="width:33%"><br/>
事前訓練の効果比較
</center>

図には事前訓練の比較を示しされている。
全ての事前訓練を用いた場合が青，次文訓練を除いた場合が赤，従来型言語モデルで次文予測課題をした場合を黄，
従来型言語モデルで次文予測課題なしを緑で描かれている。4 種類の下流課題は MNLI, QNLI, MRPC, SQuAD である。
下流のファインチューニング課題ごとに精度が分かれるようである。

<!--![](../2019document/2019Devlin_BERT_slides.pdf)-->
<!--8. [DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)-->

### BERT: 各モデルの特徴

- RoBERTa: BERT の訓練コーパスを巨大 (173GB) にし，ミニバッチサイズを大きした
- XLNet: 順列言語モデル。2 ストリーム注意
- MT-DNN: BERT ベース の転移学習に重きをおいたモデル
- GPT-2: BERT に基づく。人間超えして 2019 年 2 月時点で炎上騒ぎ
- BERT: Transformerに基づく言語モデル。**マスク化言語モデル** と **次文予測** に基づく 事前訓練，各下流課題をファインチューニング。事前訓練されたモデルは一般公開済。
- DistillBERT: BERT の蒸留版
- ELMo: 双方向 RNN による文埋め込み表現
- Transformer: 自己注意に基づく言語モデル。多頭注意，位置符号器.

<!-- # 埋め込みモデルによる構文解析
<center>
<img src="assets/2019hewitt-header.jpg" style="width:79%"><br/>
From https://github.com/john-hewitt/structural-probes
</center>

 -->
<!-- # under construction 従来モデルの問題点

BERT の意味，文法表現を知るために，從來モデルである word2vec の単語表現概説しておく。
各単語はワンホット onehot 表現からベクトル表現に変換するモデルを単語埋め込みモデル word embedding models あるいはベクトル表現モデル vector representation models と呼ぶ。
下図のように各単語を多次元ベクトルとして表現する。

<center>
![](assets/2019Devlin_BERT01upper.svg){style="width:74%"}
[@2019Devlin_BERT]  単語のベクトル表現
</center>

単語埋め込み (word2vec[@2013Mikolov_VectorSpace];[@2013Mikolov_VectorSpace])
単語は周辺単語の共起情報 [点相互情報量 PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) に基づく[@2014LevyGoldberg:nips],[@2014Levy:3cosadd]。
すなわち周辺単語との共起情報を用いて単語の意味を定義している。

<center>
![](assets/2019Devlin_BERT01lower.svg){style="width:74%"}
</center>

形式的には，skip-gram であれ CBOW であれ同じである。

# 単語埋め込みモデルの問題点

単語の意味が一意に定まらない場合，ベクトル表現モデルでは対処が難しい。
とりわけ多義語の意味を定めることは困難である。

下図の単語「アップル」は果物であるか，IT 企業であるかは，その単語を単独で取り出した場合一意に定める事ができない。

<center>
![](assets/2019Devlin_BERT02upper.svg){style="widht:74%"}<br/>
単語の意味を一意に定めることができない場合

![](assets/2019Devlin_BERT02lower.svg){style="width:74%"}<br/>
</center>

単語の多義性解消のために，あるいは単語のベクトル表現を超えて，より大きな意味単位である，
句，節，文のベクトル表現を得る努力がなされてきた。
適切な普遍文表現ベクトルを得ることができれば，翻訳を含む多くの下流課題にとって有効だと考えられる。
seq2seq モデルは RNN の中間層に文情報が表現されることを利用した翻訳モデルであった

<center>
![](assets/2019Devlin_BERT03.svg){style="width:74%"}<br/>
[@2014Sutskever_Sequence_to_Sequence] より
</center>

BERT は上述の從來モデルを凌駕する性能を示した。以下では BERT の詳細を見ていくこととする。

# BERT: 事前訓練とマルチ課題学習

図は事前訓練と GLUE の各課題に対応するためファインチューニングを示している。
事前訓練として図中レキシコンエンコーダと表記されている部分は，単語表現，位置符号器，文情報の 3 種類
の信号の合成である。合成された入力信号がトランスフォーマーへ入力され事前訓練が行なわれる。
事前訓練語，各課題毎にファインチューニングが施される。

<center>
![](assets/mt-dnn.png){style="width:89%"}<br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>
 -->

### BERT: 埋め込みモデルによる構文解析

BERT の構文解析能力を下図示した。
各単語の共通空間に射影し，
単語間の距離を計算することにより構文解析木と同等の表現を得ることができることが報告されている[@2019HewittManning_structural]。

<center>
<img src="/assets/2019hewitt-header.jpg" style="width:39%">
&nbsp;&nbsp;
<img src="/assets/2019HewittManning_blogFig1.jpg" style="width:19%">
<img src="/assets/2019HewittManning_blogFig2.jpg" style="width:19%"><br/>
<!-- ![](assets/2019HewittManning_blogFig1.jpg){style="width:19%"}
![](assets/2019HewittManning_blogFig2.jpg){style="width:19%"}<br/>-->
BERT による構文解析木を再現する射影空間
From <https://github.com/john-hewitt/structural-probes>
</center>

- word2vec において単語間の距離は内積で定義されていました。
- このことから，文章を構成する単語で張られる線形内積空間内の距離が構文解析木を与えると見なすことは不自然ではないと予想できます。

<!--
% > **The syntax distance hypothesis**: There exists a linear transformation
% > $\mathbf{B}$ of the word representation space under which vector distance
% > encodes parse trees.  Equivalently, there exists an inner product on the
% > word representation space such that distance under the inner product
% > encodes parse trees. This (indefinite) inner product is specified by
% > $\mathbf{B}^{\top}\mathbf{B}$.

% We'll take a particular instance of this hypothesis for our probes;
% we'll use the L2 distance, and let the squared vector distances equal the tree distances, but more on this later.
-->

- そこで構文解析木を再現するような射影変換を見つけることができれば BERT を用いて構文解析が可能となるでしょう。
- 例えば上図における chef と store と was の距離を解析木を反映するような空間を見つけ出すことに相当します

<!-- % The distances we pointed out earlier between \_chef\_, \_store\_ and \_was\_, can be visualized in a vector space as follows, where $\mathbf{B}\in\mathbb{R}^{2\times3}$, mapping 3-dimensional word representations to a 2-dimensional space encoding syntax:
-->
<!--% Note in the image above that the distances between words before
% transformation by $\mathbf{B}$ aren't indicative of the tree. After the
% linear transformation, however, taking a minimum spanning tree on the
% distances recovers the tree, as shown in the following image:

% <center>
% % ![](assets/0.332019HewittManning_blogFig2.jpg}
% </center>

% Finding a parse tree-encoding distance metric Our potentially tree-encoding distances are parametrized by the linear transformation $\mathbf{B}\in\mathbb{R}^{k\times n}$,

% \begin{equation}
% \left\|h_i-h_j\right\|_B^2=\left(B\left(h_i-h_j\right)\right)^{\top}\left(B\left(h_i-h_j\right)\right)
% \end{equation}

% where $\mathbf{B}_h$ is the linear transformation of the word representation; equivalently, it is the parse tree node representation.
% This is equivalent to finding an L2 distance on the original vector space, parametrized by the positive semi-definite matrix $A=B^{\top}B$:

% \begin{equation}
% \left\|h_i-h_j\right\|_A^2=\left(h_i-h_j\right)^{\top}A\left(h_i-h_j\right)
% \end{equation}
% The set of linear transformations, $\mathbb{R}^{k\times n}$ for a given $k$ is the hypothesis class for our probing family.
% We choose $B$ to minimize the difference between true parse tree distances from a human-parsed corpus and the predicted distances from the fixed word representations transformed
% by $B$:
-->

<!-- 2 つの単語 $w_i$, $w_j$ とし単語間の距離を $d\left(w_i,w_j\right)$ とする。
適当な変換を施した後の座標を $h_i$, $h_j$ とすれば，求める変換 $B$ は次式のような変換を行なうことに相当する:
$$
\min_{B}\sum_l\frac{1}{\left|s_\ell\right|^2}\sum_{i,j}\left(d\left(w_i,w_j\right)-\left\|B\left(h_i-h_j
\right)\right\|^2\right)
$$
ここで $\ell$ は文 s の訓練文のインデックスであり，各文の長さで規格化することを意味している。
 -->

具体的には，以下のような操作をしている:

1. 文章に現れる全トークンを表すベクトルを BERT より求める。
2. すなわち BERT 全中間層ユニット活性値から構成される全ての値から構成されるベクトル群
3. 2 のベクトルが張る部分空間に全トークンを射影する。
4. 3 の部分空間内でトークン間の距離を求める。
5. 各トークンを短い順にグラフで結ぶ

<!--% where $\ell$ indexes the sentences $s_{\ell}$ in the corpus, and $\frac{1}{\left|s_\ell\right|^2}$ normalizes for the number of pairs of words in each sentence.
% Note that we do actually attempt to minimize the difference between the squared distance $\left\|h_i-h_j\right\|_B^2$ and the tree distance.
% This means that the actual vector distance $\left\|h_i-h_j\right\|_B$ will always be off from the true parse tree distances, but the tree information encoded is identical, and we found that optimizing with the squared distance performs considerably better in practice.

% Finding a parse depth-encoding norm As a second application of our method, we note that the directions of the edges in a parse tree is determined by the depth of words in the parse tree; the deeper node in the governance relationship is the governed word. The depth in the parse tree is like a norm, or length, defining a total order on the nodes in the tree. We denote this tree depth norm $\left\|w_i\right\|$.

% Likewise, vector spaces have natural norms; our hypothesis for norms is that there exists a linear transformation under which tree depth norm is encoded by the squared L2 vector norm $\left\|Bh_i\right\|_2^2$.
% Just like  for the distance hypothesis, we can find the linear transformation under which the depth norm hypothesis is best-approximated:

% \begin{equation}
% \min_B\sum_\ell\frac{1}{\left|s_\ell\right|}\sum_i\left(\left\|w_i\right\|-\left\|Bh_i\right\|^2\right)
% \end{equation}

% To be effective, the manual should follow three key principles:
% \begin{enumerate}
% -  It should be simple and write on a single page, e.g. as a bulleted list of operating procedures.
% -  It should be prioritised in a strategic order that you can start executing tomorrow.
% -  It should be reviewed, evaluated, and understood by everyone crucial to the mission.
% \end{enumerate}
-->

### BERT 実装
- BERT 実装のパラメータを以下に示した。
- 現在配布されている BERT-base あるいは性能が良い BERT-large は各層のニューロン数と全体の層数である。
- ソースコードの配布先は https://github.com/google-research/bert
- オリジナルの論文は https://arxiv.org/abs/1810.04805

* データ: Wikipedia (2.5B words) + BookCorpus (800M words)
* バッチサイズ: 131,072 words (1024 sequences $\times$ 128 length or 256 sequences $\times$ 512 length)
* 訓練ステップ: 1M steps (40 epochs)
* 最適化アルゴリズム: AdamW, 1e-4 learning rate, linear decay
* BERT-Base: 12 層, 各層 768 ニューロン, 12 多頭注意
* BERT-Large: 24 層, 各層 1024 ニューロン, 16 多頭注意
* 訓練時間: 4x4 / 8x8 の TPU で 4 日間


### 事前訓練とマルチ課題学習

<center>
<img src="/assets/mt-dnn.png" style="width:66%"><br/>
From [@2019Liu_mt-dnn] Fig. 1
</center>

<!--
# Transformer: Attention is all you need

$$\mathop{attention}\left(Q,K,V\right)=\mathop{dropout}\left(\mathop{softmax}\left(\frac{QK^\top}{\sqrt{d}
}\right)\right)V$$

<center>

![](assets/2017Vaswani_Fig2_1.svg){style="width:17%"}
![](assets/2017Vaswani_Fig2_2.svg){style="width:23%"}<br />
From [@2017Vaswani_transformer] Fig. 2
</center>
-->

<!--
# Transformer(2): Attention is all you need

$$
\text{MultiHead}\left(Q,K,V\right)=\text{Concat}\left(\mathop{head}_1,\ldots,\mathop{head}_h\right)W^O
$$

where, $\text{head}_i =\text{Attention}\left(QW_i^Q,KW_i^K,VW_i^V\right)$

The projections are parameter matrices

- $W_i^Q\in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^K \in\mathbb{R}^{d_{\mathop{model}}\times d_k}$,
- $W_i^V\in\mathbb{R}^{d_{\mathop{model}}\times d_v}$,
- $W^O\in\mathbb{R}^{hd_v\times d_{\mathop{model}}}$. $h=8$
- $d_k=d_v=\frac{d_{\mathop{model}}}{h}=64$

$$\text{FFN}(x)=\max\left(0,xW_1+b_1\right)W_2+b_2$$

$$\text{PE}_{(\mathop{pos},2i)} = \sin\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$

$$\text{PE}_{(\mathop{pos},2i+1)} = \cos\left(\frac{\mathop{pos}}{10000^{\frac{2i}{d_{\mathop{model}}}}}\right)$$
-->

<!--
# BERT, GPT, ELMo 事前訓練の違い

- BERT:   トランスフォーマー，マスク化言語モデル，次文予測課題
- GPT:   順方向トランスフォーマー
- ELMo:  双方向 RNN による中間層の連結
-->


## カテゴリー特異性 (範疇特異性)

カテゴリー特異性 (category specificity) の意味障害とは，ある一つの意味カテゴリーだけが損傷され，他のカテゴリーは正常レベルに保たれる患者に対する記述に用いられる。
おそらくは Warrington 1975 以来。
<!-- \cite{WarringtonShallice1984}が報告して以来 100 以上の論文が刊行され，総説論文も多い
\citep{HIT2001,2001TylerMoss_concepts_categories,Capitani2003}。 -->

<!--図\ref{fig:1984WarringtonShalliceTab2}は\cite{WarringtonShallice1984}--> 下図は Warrington&Shallice(1984) の報告した 2 症例(J.B.R と S.B.Y.) の結果を示している。
視覚提示されたカラー写真，動物 48 枚，非動物 48 枚の呼称(identified)，口述説明(Named)，いずれのカテゴリーに属するかの 2 者択一 (Superordinate) のそれぞれの成績を示している。
続いて聴覚提示された語の写真を5つの刺激の中から選択する(Identified)ことが行われた。
<!-- %Secondly, they attempted to define each picture name, presented in the same order, auditorially -->
2 名の患者の成績に差はあるものの生物概念の成績が悪いことが示されている。

<center>
<img src="/2022assets/1984WarringtonShallice_Tab2.svg" width="66%">
<div class="figcaption">

From [@WarringtonShallice1984] Tab. 2
</div>
</center>


<!-- 二人の患者の口述説明を図\ref{fig:1984WarringtonShallicePage838}に示した。
\begin{figure}[H]
\centering
\resizebox{0.74\textwidth}{!}{\includegraphics{1984WarringtonShallice_page838.pdf}}
\caption{From \cite{WarringtonShallice1984} page 838}\label{fig:1984WarringtonShallicePage838}
\end{figure} -->

上の生物知識の選択的障害とは対照的に，[@1987WarringtonMcCarthy] は非生物に選択的な障害を示した患者 Y.O.T. を報告した
<!-- (図\ref{fig:1987WarringtonShalliceTab1})。 -->
手続きは同一カテゴリーに属するカラー写真を 5 枚提示し，その中から口頭で指示された刺激語の写真を選ぶというものであった。
<!--\begin{figure}[H]
  \centering
  \resizebox{0.84\textwidth}{!}{\includegraphics{1987WarringtonMcCarthy_Tab1.pdf}}
  \caption{From \cite{1987WarringtonMcCarthy} Tab. 1}\label{fig:1987WarringtonShalliceTab1}
\end{figure} -->
用いられた刺激は以下のとおりである。

* Foods: Mushrooms, Sprouts, Lemon, Buns, Jam, Onions, Beans, Lolly, Steak, Strawberries, Milk, Soup, Ice cream, Lettuce, Pear
* Animals: Lion, Duck, Chicken, Butterflies, Elephant, Dog, Horse, Sheep, Bird, Fish, Cat, Rabbit, Cow, Frog, Swan
* Objects: Present, Crayons, Cotton, Bicycle, Camera, Rollerskates, Umbrella, Clothes pegs, Wheelbarrow, Mirror, Pram, Buttons, Balloons, Swing, Paints

図 \ref{fig:2003CrutchWarrington_Fig2} は [@2003CrutchWarrington_fruits_vegetables] の提案したモデルを表している。
この図の中で，少なくとも単一乖離できる意味記憶のカテゴリーに以下の想定が可能なことを示している。
以下のリストは図\ref{fig:2003CrutchWarrington_Fig2}の最下段を書かれているカテゴリーである。

* 果物と野菜<!-- %\citep{1985Hart_fruitsVegetables} -->
* 動物 <!-- %\citep[][X:aninmals,plants]{WarringtonShallice1984}, -->
* 非動物の食べ物 <!-- %\citep[X:objects]{WarringtonMcCarthy1983,1987WarringtonMcCarthy} -->
* 材料
* 道具/手で扱えるモノ
* 手で扱えないモノ
* 乗り物
* 体の一部

他にも [@1985Hart_fruitsVegetables] は果物と野菜に選択的な意味障害を報告している。
彼らの患者 MD は 34 才右利き，は他の食べ物については障害が無かった。
図\ref{fig:1985HartCaramazza_Tab1}参照。

<!-- \begin{figure}[H]
  \centering
  \resizebox{0.94\textwidth}{!}{\includegraphics{2003CrutchWarrington_Fig2.pdf}}
  \caption{From \cite{2003CrutchWarrington_fruits_vegetables} Fig.2}\label{fig:2003CrutchWarrington_Fig2}
\end{figure} -->


図\ref{fig:2003CrutchWarrington_Tab1}に\cite{2003CrutchWarrington_fruits_vegetables} の報告を示した。
人工物，動物，植物 の各 40 刺激。semantic probe テストでは，三者択一式で，
属性に関する質問(apple:{\underline{pips}}, stone, segment)と
連想についての質問(ostrich: {\underline{walk}}, swim, fly)とが設けられた。
命名課題ではカラー写真が提示され刺激の名称を呼称することが求められた。
結果を図\ref{fig:2003CrutchWarrington_Tab1}に示した。
<!-- %% Methods: The stimuli consisted of 120 items: 40 man-made artefacts (including manipulable and nonmanipulable objects), 40 items of flora
%% (including fruit, vegetables, flowers), and 40 items of fauna (including
%% mammals, birds, and invertebrates). The three sets were matched for word
%% frequency using Thorndike–Lorge ratings (Thorndike \& Lorge, 1942; G
%% count). These ratings were used in preference to the Ku\,{c}era and
%% Francis set, which were only able to provide frequency data for 73\% of
%% the chosen pictureable items. Age of acquisition and visual complexity
%% ratings were not available for this corpus. The experiment was conducted
%% in two distinct parts. -->

<!-- %% Comprehension task. A series of written multiple-choice questions
%% probing conceptual knowledge of each item was devised. FAV was asked to
%% choose between the target and two semantically related distractor
%% items. The questions probed either attribute (e.g., apple: pips, stone,
%% segments) or associative knowledge (e.g., ostrich: walk, swim, fly).
%% The proportion of attribute questions to association questions for flora
%% and fauna items was identical (15 vs. 25 for both). All bar three of the
%% questions about man-made artefacts probed associative knowledge.  An
%% effort was also made to match the probe words themselves for word
%% frequency (Kuçera & Francis, 1967). In particular, the probe words for
%% fauna and flora items were well matched (fauna: mean = 56.3, SD = 104.1,
%% 88\% data available; flora: mean = 49.3, SD = 75.0, 87\% data available;
%% man-made artefacts: mean = 97.2, SD = 109.7, 96\% data available). -->

<!-- Naming task. The stimuli consisted of detailed colour photographs displayed on a high resolution PC monitor.

The stimuli were divided into two matched halves, each containing 20 artefacts, 20 flora items, and 20 fauna items. The experiment was %% conducted over two sessions, 1 week apart. In Session 1, only semantic probe questions were administered.
Half the items were presented in the pictorial form and the other half in the auditory verbal form.
In Session 2, all items were presented in pictorial form for naming to confrontation.
Subsequently, the semantic probe questions were readministered with stimuli presented either in auditory verbal or pictorial form, in a pattern complimentary to that of Session 1.
Thus over the two sessions, all items were probed in both auditory verbal and pictorial form, in a split ABBA design. -->

<!-- \begin{figure}[H]
  \centering
  \resizebox{0.74\textwidth}{!}{\includegraphics{2003CrutchWarrington_Tab1.pdf}}
  \caption{From \cite{2003CrutchWarrington_fruits_vegetables} Tab. 1}\label{fig:2003CrutchWarrington_Tab1}
\end{figure} -->

図\ref{fig:1985HartCaramazza_Tab1}に\cite{1985Hart_fruitsVegetables}の結果を示した。
視覚提示刺激の質が線画，色付き線画，写真，実物と異なっていたが，果物と野菜の概念が
一貫して悪いことが分かる。
<!-- %% M.D. showed considerable difficulty in naming individual fruits and
%% vegetables, but was able to name easily a large range of other pictures and
%% objects. His seven naming errors outside the fruits/vegetables category
%% involved two household items, four geometric shapes (for example, diamond)
%% and one tree.  He named correctly a total of 13 food products outside the
%% categories of fruits and vegetables. The ``other'' items given to M.D. for
%% naming were chosen to cover a wide range of semantic categories. Some of
%% these categories (for example, ``body parts'') have members with a very
%% high frequency of usage in the language, hence the mean frequency of
%% occurrence for the fruit/vegetable items (11.8 per million) was
%% considerably lower than the mean for the other items (32.8 per
%% million). This disparity does not explain M.D.'s selective impairment,
%% however, as 73\% of items in the other group were in the same frequency
%% range as the fruit/vegetable items and were named without difficulty. -->

<!-- \begin{figure}[H]
  \centering
  \resizebox{0.74\textwidth}{!}{\includegraphics{1985HartCaramazza_Tab1.png}}
  \caption{From \cite{1985Hart_fruitsVegetables} Tab. 1}\label{fig:1985HartCaramazza_Tab1}
\end{figure} -->


<!-- %% \cite{Warrington1975}:
%% \begin{quotation}
%% It was found that knowledge of pictorial representations of objects, and of words,
%% was impaired or impoverished, and in both instances knowledge of subordinate
%% categories was more vulnerable than superordinate categories. Evidence is
%% presented that this impairment of semantic memory cannot be accounted for by
%% intellectual impairment, sensory or perceptual deficits, or expressive language
%% disorder. The implications of damage to the semantic memory system for the
%% operation of other cognitive systems, in particular short and long-term memory
%% functions, are considered. Some tentative evidence for the structural basis for a
%% hierarchically organized modality-specific semantic memory system is discussed
%% \end{quotation} -->


上記の結果からは特定のカテゴリーに属する対象の呼称成績に障害があるとの考えがあるが，これに対しては反証も存在する。
図\ref{fig:2007Patterson_Fig3}左が原画像であり，右上が模写，右下は 10 秒後の遅延再生画である。
このようにある動物の特徴と考えられる視覚特徴を落としてしまうことから，視覚イメージ自体に損傷の影響が及んでいるとも考えられる。

<!-- \begin{figure}[H]
    \centering
    \begin{minipage}{0.44\textwidth}
        \centering
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig3a.png}}
    \end{minipage}
    \begin{minipage}{0.44\textwidth}
        \centering
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig3b.jpg}}
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig3c.jpg}}
    \end{minipage}
\caption{From \cite{2007Patterson_typicality} Fig. 3. 左が提示画。右上:模写。右下:10秒後の再生画}\label{fig:2007Patterson_Fig3}
\end{figure} -->

同様の例を\cite{2007RogersPatterson_Category} Fig. 2(D) から示した(図\ref{fig:2007RogersPatterson_Category_Fig2D})。左が元画像であり，10 秒後の遅延複写画が右である。各動物の特徴である考えられる部位が消失していることが見て取れる。
<!-- \begin{figure}[H]
    \centering
    \begin{minipage}{0.44\textwidth}
        \centering
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig2D1.pdf}}
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig2D2.pdf}}
    \end{minipage}
    \begin{minipage}{0.44\textwidth}
        \centering
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig2D3.pdf}}
        \resizebox{0.99\textwidth}{!}{\includegraphics{2007Patterson_Fig2D4.pdf}}
    \end{minipage}
\caption{From \cite{2007RogersPatterson_Category} Fig. 2. 意味痴呆患者の遅延コピー例}\label{fig:2007Patterson_Cateogry_Fig2D}
\end{figure} -->

