---
title: 第02回 2024 年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 19/Apr/2023<br/>
Appache 2.0 license<br/>
</div>


# 最近の動向

## 1. ChatGPT と心理学 <!-- source: https://www.assemblyai.com/blog/how-chatgpt-actually-works/ -->

各所で ChatGPT で遊ぶ人が現れ，話題となっています。
では，ChatGPT は実際にどのように機能するのでしょうか？
あるいは，この授業での学習内容と ChatGPT とは，どのように関係するのでしょうか。
ここでは，chatGPT と心理学との関係を探ってみます。

まず，chatGPT という言葉の意味を確認しましょう。
chat は，英単語そのもで，`おしゃべり` という意味です。
すなわち，オンライン上で対話が可能なプログラムであることを意味するため Chat と名付けられました。

次の GPT とは，何でしょうか？


<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_1.png" width="77%">
<!-- <img src="figures/2023Ruby_fig0.jpg" width="55%"> -->
</div>

GPT とは，以下のような単語の頭文字です。

* **G**: Generative 生成的
* **P**: Pre-trained 訓練済
* **T**: Transformer トランスフォーマー

それぞれが，キーワードとしてこの授業でも取り上げる予定です。
**生成的** とは，対になる単語，**認識** に対する用語です。
心理学に限らず，機械学習，マシンビジョン，AI などの関連諸分野で，認識モデルと生成モデルとは，しばしば対比して扱われます。

画像認識，音声認識，など，入力刺激を受け取って，その入力が何であるかを識別するモデルが認識モデルです。
一方，モデル内部にある情報から，出力情報を作り出すモデルを生成モデルと呼びます。
すなわち，認識モデルと生成モデルとは，情報の流れが反対方向のモデルを指します。

* 認識モデル: インプット $\rightarrow$ 内部表現
* 生成モデル: アウトプット $\leftarrow$ 内部表現


その内部構造の詳細は公表されていません。
ですが，最近の研究からその機能原理を整理することができます。
ここでは，推察可能な機能原理と心理学との関係を考えてみます。

もちろん，対話に特化した言語モデルですが，対話に関した人工知能の話題として **チューリングテスト** があります。
チューリングテストは，人工知能の話題でもありますが，近年では，心理学者が協力して，改訂版が提案されています。
ニューヨーク大学のマーカス・ガリー (本当に賢い AI を見分ける新チューリングテスト，日経サイエンス 2017年7月号)

まず，GPT は **大規模言語モデル (LLM: Large Language Model)** です。
ちなみに，**GPT** すなわち openAI が開発したモデルは，東海岸発であり，
西海岸発のモデルは **BERT** と呼びます。すなわち BERT は Google 発のモデルです。
いずれのモデルも，トランスフォーマー transformer に基づいています。
ちなみに，BERT は，セサミストリートのキャラクタの一つです。
[原著論文](https://arxiv.org/abs/1810.04805/) によれば，Bidirectional Encoder Representations from Transformers の頭文字をとって命名されたことになっています。

[トランスフォーマー](https://arXiv.org/abs/1706.03762) は，**自己注意** に基づく言語モデルです。

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_2.png" width="77%">
<!-- <img src="figures/2023Ruby_fig3.jpg" width="55%"> -->
</div>

注意は，心理学のテーマの一つでもあります。
そうすると，心理学で使われている注意と，トランスフォーマーで使わている注意とは，同じか，それとも，異なるのかという疑問が湧きます。


加えて chatGPT では，**事前学習** と **微調整 (fine tuning)** の組み合わせで，話題となっているような性能を達成しました。
chatGPT のファインチューニングが 2 段階に渡って行われたようです。

<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" style="width:144%;">
<!-- <img src="/2023assets/ChatGPT_Diagram.svg" style="width:114%"> -->
<!-- <img src="https://openaicom.imgix.net/cf717bdb-0c8c-428a-b82b-3c3add87a600/ChatGPT_Diagram.svg" width="77%"> -->

この中で，chatGPT の特徴としては，**強化学習** を用いたことが挙げられます。
この強化学習は **RLHF (Reinforcement Learning with Human Feedback)** と呼ばれます。
なぜ，RLHF が必要だったのかというと，**ミスアラインメント問題の解消** だと言われています。
ミスアラインメントとは，対話の中で話題がずれることを指します。

例えば，chatGPT の前身である GPT-3 では以下のような出力をすることがあることが知られていました:

* ユーザの明示的な指示に従わない，**親切さに欠ける helpfulness** 出力が得られる
* 存在しない，あるいは誤った事実を反映したた **幻想  hallucinations** が含まれる
* モデルがどのように特定の決定や予測に至ったかを人間が **理解することが困難  Lack interpretability** な解釈可能性を欠く
* 有害または攻撃的で，誤った情報を広める **有害または偏った  harmful or offensive** コンテンツを含む

ChatGPT では，標準的な LLM のこれらの固有の問題を解決するために，RLHF を含む 3 段階が導入されました。

### ステップ 1：教師あり微調整 (SFT: Supervised Fine Tuning) モデル <!-- ### Step 1: Supervised Fine Tuning (SFT) Model-->

最初の開発では，GPT-3 モデルの微調整を行うため，40 人の契約社員を雇い，モデルが学習するための既知の出力を持つ入力の教師付き学習データセットを作成した。
入力 (プロンプト) は，実際に Open API に入力されたユーザから収集されたものである。
ラベラはプロンプトに対して適切な応答を記述することで，各入力に対して既知の出力を作成する。
この新しい教師ありデータセットを用いて GPT-3 モデルを微調整し，GPT-3.5 (SFT モデルとも呼ばれる) を作成した。
<!-- The first development involved fine-tuning the GPT-3 model by hiring 40 contractors to create a supervised training dataset, in which the input has a known output for the model to learn from.
Inputs, or prompts, were collected from actual user entries into the Open API.
The labelers then wrote an appropriate response to the prompt thus creating a known output for each input.
The GPT-3 model was then fine-tuned using this new,supervised dataset, to create GPT-3.5, also called the SFT model. -->

プロンプトデータセットの多様性を最大化するため，任意のユーザー ID から得られるプロンプトは 200 件までとし，共通の長い接頭辞 shared long common prefixes を持つプロンプトは削除された。
最後に，特定可能な個人情報 (PII) を含むプロンプトはすべて削除された。
<!-- In order to maximize diversity in the prompts dataset, only 200 prompts could come from any given user ID and any prompts that shared long common prefixes were removed.
Finally, all prompts containing personally identifiable information (PII) were removed. -->

OpenAI API からプロンプトを集計した後，ラベラーには，実際のサンプルデータが少ないカテゴリーを埋めるためのサンプルプロンプトの作成も依頼した。
対象となったカテゴリは以下の通り：
<!-- After aggregating prompts from OpenAI API, labelers were also asked to create sample prompts to fill-out categories in which there was only minimal real sample data.
The categories of interest included:-->

* 平易なプロンプト: 任意の質問
* 少数撃プロンプト: 複数のクエリとレスポンスのペアを含む指示
* ユーザーベースのプロンプト: OpenAI API に要求された特定のユースケースに対応

<!-- * Plain prompts: any arbitrary ask.
* Few-shot prompts: instructions that contain multiple query/response pairs.
* User-based prompts: correspond to a specific use-case that was requested for the OpenAI API. -->

応答を生成する際，ラベラはユーザからの指示が何であるかを推察することに全力を尽くすよう求められた。

<!-- <div class="figure figcenter">
<img src="/2023assets/ChatGPT_Diagram.svg" width="77%">
<img src="figures/2023Ruby_fig4.jpg">
</div>-->

### ステップ 2: 報酬モデル <!-- ### Step 2: Reward Model-->

ステップ 1 で SFT モデルを訓練した後，このモデルはユーザのプロンプトに対してより適切な応答を生成する。
このモデルの入力は一連のプロンプトと応答であり，出力は報酬と呼ばれるスカラ値である。
報酬モデルは，モデルが報酬を最大化するように出力を生成することを学習する強化学習 (ステップ 3 参照) を活用するために必要なものである。
報酬モデルを訓練するために，ラベラには 1 つの入力プロンプトに対して 4～9 個の SFT モデル出力が提示される。
そして，これらの出力をベストからワーストにランク付けするよう求められ，以下のような出力ランクの組み合わせが作成される。
<!-- After the SFT model is trained in step 1, the model generates better aligned responses to user prompts.
The next refinement comes in the form of training a reward model in which a model input is a series of prompts and responses, and the output is a scaler value, called a reward.
The reward model is required in order to leverageReinforcement Learning in which a model learns to produce outputs to maximize its reward (see step 3).
To train the reward model, labelers are presented with 4 to 9 SFT model outputs for a single input prompt.
They are asked to rank these outputs from best to worst, creating combinations of output ranking as follows. -->

各組み合わせを個別のデータ点としてモデルに含めると，過学習 (見たデータ以上の外挿ができない) が発生した。
そこで，各順位群を 1 つのデータ点として活用し，モデルを構築した。
<!-- Including each combination in the model as a separate data point led to overfitting (failure to extrapolate beyondseen data).
To solve, the model was built leveraging each group of rankings as a single batch data point. -->

### ステップ 3: 強化学習モデル <!-- ### Step 3: Reinforcement Learning Model-->

最終段階では，モデルにランダムなプロンプトを提示し，応答を返す。
応答は，モデルがステップ 2 で学習した「方針」を用いて生成される。
方針は，機械が目標を達成するために学習した戦略であり，この場合，報酬を最大化することである。
ステップ 2 で開発された報酬モデルに基づいて，プロンプトと応答の対に対してスカラ報酬値が決定される。
この報酬は，方針を進化させるためにモデルにフィードバックされる。
<!-- In the final stage, the model is presented with a random prompt and returns a response.
The response is generated using the ‘policy’ that the model has learned in step 2. The policy represents a strategy that the machine has learned to use to achieve its goal; in this case, maximizing its reward.
Based on the reward model developed instep 2, a scaler reward value is then determined for the prompt and response pair.
The reward then feeds back into the model to evolve the policy. -->

2017 年 Schulman+ は，各応答が生成されるたびにモデルの方針を更新する際に使用される手法である Proximal Policy Optimization (PPO) を発表した。
PPO では，SFT モデルからトークンごとの KL (Kullback-Leibler) ペナルティを組み込んでいる。
KL ダイバージェンスは，2 つの分布関数の類似性を測定し，極端な距離にはペナルティを与える。
この場合，KL ペナルティを使用することで，ステップ 1 で学習した SFT モデル出力から応答が離れる距離を減らし，報酬モデルを最適化しすぎて人間の意図データセットから大きく逸脱するのを防ぐ。
<!-- In 2017, Schulman+ introduced Proximal Policy Optimization (PPO), the methodology that is used in updating the model’s policy as each response is generated.
PPO incorporates a per-token Kullback–Leibler (KL) penalty from the SFT model.
The KL divergence measures the similarity of two distribution functions and penalizes extreme distances.
In this case, using a KL penalty reduces the distance that the responses can be from the SFTmodel outputs trained in step 1 to avoid over-optimizing the reward model and deviating too drastically from the human intention dataset. -->

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_3.png" width="77%">
<!-- <img src="figures/2023Ruby_fig7.jpg" width="55%"> -->
</div>

## モデルの評価 <!-- ## Evaluation of the Model-->

モデルの評価は，訓練中に，モデルが見たことのない検証セットを用意することで行われる。
検証セットでは，モデルが前身である GPT-3 よりも優れているかどうかを判断するために，一連の評価が行われる。
<!-- Evaluation of the model is performed by setting aside a test set during training that the model has not seen.
On thetest set, a series of evaluations are conducted to determine if the model is better aligned than its predecessor, GPT-3. -->

* **有益性 helpfulness**: モデルの出力は，ラベラにとって好ましいものであった。
ラベラは GPT-3 よりも InstructGPT の出力を 85±3 %の確率で好んだ。
* **真実性 truthfulness**: モデルの偽りの傾向。PPO モデルは TruthfulQA データセットを用いて評価した場合，真実性と情報性がわずかに増加する出力を生成した。
* **無害性 harmlessness**: 不適切な内容、軽蔑的な内容、否定的な内容を回避する能力。無害性は，RealToxicityPrompts データセットを用いて検証された。テストは 3 つの条件下で行われた。

<!-- * **Helpfulness**: the model’s ability to infer and follow user instructions. Labelers preferred outputs from InstructGPTover GPT-3 85 ± 3% of the time.
* **Truthfulness**: the model’s tendency for hallucinations. The PPO model produced outputs that showed minor increases in truthfulness and informativeness when assessed using the TruthfulQA dataset.
* **Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset. The test was performed under three conditions. -->

1. 尊敬に値する回答をするよう指示された場合: 有害な回答が有意に減少した。
2. 敬意を設定せずに応答を行うように指示した場合: 毒性に大きな変化はない。
3. 有害な反応をするように指示した場合: GPT-3 モデルよりも有意に有害な反応をするようになった。


### 2. Diffusion model

画像系の生成モデルとして，**拡散モデル diffusion modeling** が注目されています。
今年に入って，数々のサイトが公開され，プロの絵師顔負けの画像を生成することが可能です。
おそらく，知っている方も多いと思いますが，私が適当に作ってみた画像を下に載せます。

<img src="/2023assets/DreamShaper_32_masterpiece_realistic_portrait_of_a_girl_beauti_2.jpg" width="29%">

* 次のリンクから絵を作成することができます: [Leonardoai<img src="https://app.leonardo.ai/img/leonardo-logo.png" width="2%">](https://app.leonardo.ai/ai-generations)

