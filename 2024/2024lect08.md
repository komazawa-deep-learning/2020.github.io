---
title: 第08回 2024 年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---

<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 07/Jun/2024<br/>
Appache 2.0 license<br/>
</div>

# 実習ファイル

* [DETR と Detectron2 とによる領域切り出しデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_0625DETR_demo.ipynb){:target="_blank"}
* [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"}
* [CAM 実習 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}
* [画像認識 PyTorch の基礎編  <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb){:target="_blank"}
* [3 層パーセプトロンと確率的勾配降下法のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
* [ニューラルネットワークモデルの定義 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1028komazawa_neural_networks_primer.ipynb){:target="_blank"}

* [ステップ・バイ・ステップで画像認識の基礎 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/notebooks/2020_0515komazawa_step_by_step_CNN_Pytorch.ipynb){:target="_blank"}
* [3 層パーセプトロンと確率的勾配降下法のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb){:target="_blank"}
* [ニューラルネットワークモデルの定義 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2022notebooks/2022_1028komazawa_neural_networks_primer.ipynb)

<!-- - [足し算のデモ <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0702RNN_binary_addtion_demo.ipynb){:target="_blank"}
* [CNN 畳み込み層の可視化 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb){:target="_blank"}
* [各画像の画面表示時に日本語キャプションを付与する準備 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/project-ccap/ccap/blob/master/notebooks/2020importing_ccap_from_GitHub.ipynb){:target="_blank"}
* [EfficientNet のパラメータ実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/drive/1QpKBHsBR5yvEOz2M-pKCUpliDh1XXplS){:target="_blank"}
* [データ拡張 <img src="https://komazawa-deep-learning.github.io/assets/colab_icon.svg">](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617plot_transforms_demo.ipynb) -->


## 画像切り出し

1. 物体位置
3. 物体認識 object recognition
2. 意味的切り出し semantic segmentation
4. 対象切り出し instance segmentation
5. 特徴点抽出 keypoint
6. パノプティック切り出し

<div align="center">
<img src="/assets/2017DangHa_History_Of_Object_Recognition_ja.svg" style="width:66%"><br/>
Dang and Ha (2017) より
</div>

<!-- # 転移学習

<div align="center" style="width:29%">
<img src="/assets/2017Li_Deeper_Broader_fig1ja.svg" style="width:84%"><br/>
</div> -->

### 残差ネット (ResNet, He et. al, 2015)

<center>
<img src='/assets/ResNet_Fig2.svg' style='width:19%'>
<img src='/assets/2015ResNet30.svg' style='width:66%'><br>
He (2015) より
</center>

### Fast R-CNN と Faster R-CNN (2014)

- R-CNN によって，位置 where 情報と 物体 what 情報 とを多層畳み込みニューラルネットワークで表現する試みが，発展。実時間で物体の切り出しと認識とが行えるようになった。[Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf){:target="_blank"}, [YOLO](https://arxiv.org/pdf/1506.02640.pdf){:target="_blank"}, [SSD](https://arxiv.org/pdf/1512.02325.pdf){:target="_blank"},

<center>
<img src="/assets/2015Fast_R-CNN_Fig1.svg" width="49%">
<img src="/assets/2015Faster_RCNN_RPN.svg" width="44%"><br/>
左: Fast R-CNN の模式図。
右: Faster RCNN の領域提案ネットワーク
</center>

### 意味的切り分け (セマンティックセグメンテーション) と 実体切り分け (インスタンスセグメンテーション)

- 完全畳み込みネットワーク (Fully Convolutional Network:FCN) と呼ばれるセマンティックセグメンテーションを実現するネットワーク
- FCN とは文字通り全ての層が畳込み層であるモデル

<center>
<img src='/assets/2015Long_FCN.svg' style='width:49%'><br/>
Long (2017) FCN
</center>

- 通常のCNN は，出力層のユニット数が識別すべきカテゴリー数であった。一方 FCN では入力画像の画素数だけ出力層が必要になる。
- すなわち各画素がそれぞれどのカテゴリーに属するのかを出力する必要があるため出力層には，縦画素数 $\times$ 横画素数 $\times$ カテゴリー数の出力ニューロンが用意される。
- 図 では，識別すべきカテゴリー数 が 20 であったたま，どのカテゴリーにも属さない，すなわち背景を指示するもう1 つのカテゴリーを加えた計 21 カテゴリーの分類を行うことになる。

- CNN では畳込演算によって畳込みのカーネル幅(受容野) だけ近傍の入力刺激を加えて計算することになるため，上位層では下位層に比べて受容野が大きくなることの影響で画像サイズは小さく(あるいは粗く) なってしまう
- このため，最終出力層に入力層と同じ解像度の画素数を得るためには，畳込みと反対方向の解像度を細かくする工夫が必要となる。
- これを解決する一つの方法がアンサンプリング(unsampling) と呼ばれる方法

<!-- ### 意味的切り分け (セマンティックセグメンテーション)

* 意味的切り分け (セマンティックセグメンテーション) とは画像中の各画素をあるクラスに分類する画像解析課題のこと。
* 我々人間が常に行っていることと同じで，見ているものを画像と見なすと，画像の各画素がどのクラスに属しているかがわかる。
* 意味的切り出し (セマンティック・セグメンテーション semantic segmentation) はコンピュータでこれを実現するための技術である。 -->

### 姿勢検出 key point detection

<center>
<div style="width:77%">
<video controls src="https://learnopencv.com/wp-content/uploads/2022/10/yolov7-vs-mediapipe-dance.mp4" muted="false" style="width:77%"></video><br/>
<br/><br/>
<video controls src="https://learnopencv.com/wp-content/uploads/2022/10/yolov7-gpu-vs-mediapipe-difficult-pose.mp4" muted="true" style="width:77%"></video>
<!-- <video controls src="https://learnopencv.com/wp-content/uploads/2022/10/yolov7-256-vs-960p-yoga-1.mp4" muted="false"></video> -->
<div class="figcaption">
from `https://learnopencv.com/yolov7-pose-vs-mediapipe-in-human-pose-estimation/`
</div></div>
</center>


# 注意


* 認知心理学，生理学，などではトップダウンとボトムアップの 2 種類の注意が区別されてきた。計算論的には Crick (1984) のサーチライト仮説などにより提唱された **勝者占有回路 winner-take-all 回路** である。
* 自然言語処理 (Vaswani+2017)，画像処理 (Ramachandran2019) や眼球運動の DeepGaze (Kummerer2019) などがある。

### 文献

- [⼼理学，神経科学，機械学習における注意, Lindsay, 2019](https://project-ccap.github.io/2020Lindsay_Attention_inPsychology_Neuroscience_and_Machine_Learning.pdf){:target="_blank"}


## 心理学的注意

<img src="/assets/1988Treisman_Fig1.svg" width="24%">
<img src="/assets/1994Wolfe_GS2Fig2.jpg" width="39%">
<img src="/assets/1998IttiKoch_fig1.jpg" width="29%"><br/>

<p style="text-align: left;width:88%;background-color: cornsilk;">
左: Treisman1988 Fig.1，特徴統合理論の概念図。ボトムアップ注意
中: Wolfe1994 Fig.2 ガイド付き探索 バージョン 2 モデル。トップダウン注意
右: Itti and Koch (1998) Fig. 1 計算モデルの実装例
</p>

## ニューラルネットワーク的注意

### CAM

<center>
<img src="/assets/2016Zhou_CAM_fig2ja.svg" width="66%"><br/>
CAM の概念図 Zhou (2016) Fig. 2 を改変
</center>

<br/><br/><br/>
<center>
<img src="/assets/2016Grad-CAM_boxer_tigercat.png" style="width:20%">
<img src="/assets/2016Grad-CAM_boxer.png" style="width:20%">
<img src="/assets/2016Grad-CAM_tigercat.png" style="width:20%"><br/>
Grad-CAM の結果。Selvaraj (2016) Fig. 5 より。左: 元画像。央: ボクサー犬と判断した場合のヒートマップ。右:トラ猫と判断した場合のヒートマップ
</center>


* [ccap 資料初心者のためのニューラルネットワーク](https://colab.research.google.com/github/project-ccap/project-ccap.github.io/blob/master/2022notebooks/2022_0418ccap_neural_networks_for_primer.ipynb){:target="_blank"}
* [ソフトマックス関数解題](https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1107softmax.ipynb){:target="_blank"}
また，ソフトマックス関数は，エネルギー関数とみなすことも可能である。

1. 交差エントロピー: $t\log(p) + (1-t)\log(1-p)$
損失関数として，最小自乗誤差を用いるよりも，分類課題での離散変数については，交差エントロピーを用いる方が収束が早く便利である。

# 注意とソフトマックス関数，CAM

前回は，自然画像分類のために訓練した畳み込みニューラルネットワークの結合係数を視覚化すると，特徴地図が現れることを示した。

今回は，この特徴地図を顕著性地図と考えて，ボトムアップによる注意を考える。


#### 付録 読み物

<!-- - [ジェフェリー・ヒントンのマクセル賞受賞記念講演(2016)](/Hinton_Maxwell_award/){:target="_blank"}
- [データサイエンス小史](/data_science/){:target="_blank"}
- [数式の読み方](/how_to_read_math/){:target="_blank"} -->
- [Sutton のブログ，苦い教訓](/2019sutton_bitter_Lesson.pdf){:target="_blank"}

<!-- - [Sutton のブログ，苦い教訓](/2019Sutton_Bitter_Lesson/){:target="_blank"} (../2019sutton_bitter_Lesson.pdf)
- オリジナル Sutton's blog [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html){:target="_blank"} -->
<!--[その和訳](2019sutton_bitter_lesson.pdf)-->

コンピュータビジョンにおいても、同様のパターンがある。
初期の手法では，視覚をエッジや一般化された円柱の探索，あるいは SIFT 特徴の観点から考えていた。
しかし今日では，このような考え方はすべて捨て去られている。
現代のディープラーニングニューラルネットワークは，畳み込みとある種の不変性という概念のみを使用し，はるかに優れた性能を発揮する。
<!--In computer vision, there has been a similar pattern.
Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features.
But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.-->

これは大きな教訓である。
我々は同じような過ちを犯し続けている。
このことを理解し，それに効果的に抵抗するためには，我々はこうした間違いの魅力を理解しなければならない。
我々は，我々の考え方を構築しても長期的にはうまくいかないという苦い教訓を学ばなければならない。
この苦い教訓は，次のような歴史的観察に基づいている。
1) AI 研究者は、しばしばエージェントに知識を組み込もうとしてきた，
2) これは短期的には常に役に立ち，研究者を個人的に満足させた，
3) 長期的には，それは停滞し，さらなる進歩を阻害した，
4) 探索と学習による計算のスケーリングに基づく反対のアプローチによって，画期的な進歩がもたらされる，

最終的な成功は苦渋を帯びたものであり，しばしば不完全に消化される、
なぜなら，それは人間中心のアプローチに対する成功だからである。

<!--This is a big lesson.
As a field, we still have not thoroughly learned it, as we are continuing to make the s ame kind of mistakes.
To see this, and to effectively resist it, we have to understand the appeal of these mistakes.
We have to learn the bitter lesson that building in how we think we think does not work in the long run.
The bitter lesson is based on the historical observations that
1) AI researchers have often tried to build knowledge into their agents,
2) this always helps in the short term, and is personally satisfying to the researcher, but
3) in the long run it plateaus and even inhibits further progress, and
4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.
The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.-->
