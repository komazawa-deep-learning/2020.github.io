---
title: 第13回 2024年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+1" color="navy"><strong>心理学特講IIIA ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 12/Jul/2024<br/>
Appache 2.0 license<br/>
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

# GPT

OpenAI は **GPT**（**Generative Pre-training Transformer**の略）([Radford+2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) として，教師なし言語モデルを膨大なフリーテキストコーパスで学習することでより大きな規模に拡張している。

## 言語モデルとしての Transformer 復号化器<!-- ## 5.1 Transformer Decoder as Language Model-->

[transformer 復号化器](https://arxiv.org/abs/1801.10198) は，[オリジナル Transformer](https://arxiv.org/abs/1706.03762) と比較して，符号化器部分を削除したモデルであり，入力文はソースとターゲットの 2 つではなく，1 つのみとなる。
<!-- Compared to the [original transformer](https://arxiv.org/abs/1706.03762) architecture, the [transformer decoder](https://arxiv.org/abs/1801.10198) model discards the encoder part, so there is only one single input sentence rather than two separate source and target sequences. -->

このモデルは入力配列の埋め込みに対して複数の Transformer ブロックを適用する。
各ブロックはマスク化 **多頭自己注意** 層と **点毎フィードフォワード** 層を含む。
最終出力は，ソフトマックス正規化後の目標トークンに対する分布を生成する。
<!-- This model applies multiple transformer blocks over the embeddings of input sequences.
Each block contains a masked *multi-headed self-attention* layer and a *pointwise feed-forward* layer.
The final output produces a distribution over target tokens after softmax normalization. -->

<div class="figure figcenter">
<img src="/2024assets/OpenAI-GPT-transformer-decoder.png" width="66%">
<div class="figcaption">

OpenAI の GPT における transformer 復号化器モデルアーキテクチャ<!-- Fig. 7. The transformer decoder model architecture in OpenAI GPT. -->
</div></div>

損失は負の対数尤度であり，[ELMo](#elmo) と同じであるが，後方計算は行わない。
例えば，サイズ $k$ の文脈窓が対象単語の前にあるとすると，損失は次のようになる:
<!-- The loss is the negative log-likelihood, same as [ELMo](#elmo), but without backward computation.
Let's say, the context window of the size $k$ is located before the target word and the loss would look like: -->

$$ {L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) $$

## バイト対符号化 Byte pair encoding<!-- ## 5.2 Byte Pair Encoding-->

入力配列を符号化するために，**Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) が使用される。
BPE はもともと 1990 年代にデータ圧縮アルゴリズムとして提案され，その後，機械翻訳におけるオープンボキャブラリーの問題を解決するために採用されたもので，新しい言語に翻訳する際に稀な単語や未知の単語に遭遇しやすくするためである。
BPE は，未知の単語はしばしば複数の部分単語に分解できるという直観に基づき，頻出する文字対を反復的かつ貪欲に結合することで最適な単語分割を求める。
<!-- **Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) is used to encode the input sequences. BPE was originally proposed as a data compression algorithm in 1990s and then was adopted to solve the open-vocabulary issue in machine translation, as we can easily run into rare and unknown words when translating into a new language.
Motivated by the intuition that rare and unknown words can often be decomposed into multiple subwords, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters. -->

## 教師付き微調整
<!-- ## 5.3 Supervised Fine-Tuning-->

OpenAI GPT が提案した最も実質的なアップグレードは，課題固有のモデルを取り除き，事前に学習された言語モデルを直接使用することである！
この方法は，課題特化モデルから，課題特化モデルへの移行を可能にする。
<!-- The most substantial upgrade that OpenAI GPT proposed is to get rid of the task-specific model and use the pre-trained language model directly! -->

分類を例にとって考えてみよう。
ラベル付きデータセットにおいて，各入力は $n$ 個のトークン，$\mathbf{x}=(x_1, \dots, x_n)$，1 つのラベル $y$ を持っているとする。
GPT はまず，入力系列 $\mathbf{x}$ を事前に学習した変換復号化器で処理し，最後のトークン$x_n$ に対する最終層出力は $\mathbf{h}_L^{(n)}$ となる。
そして，1 つの新しい学習可能な重み行列 $\mathbf{W}_y$ のみで，クラスラベル上の分布を予測することができる。
<!-- Let's take classification as an example.
Say, in the labeled dataset, each input has $n$ tokens, $\mathbf{x} = (x_1, \dots, x_n)$, and one label $y$.
GPT first processes the input sequence $\mathbf{x}$ through the pre-trained transformer decoder and the last layer output for the last token $x_n$ is $\mathbf{h}_L^{(n)}$.
Then with only one new trainable weight matrix $\mathbf{W}_y$, it can predict a distribution over class labels. -->

<div class="figure figcenter">
<img src="/2024assets/GPT-classification.png" width="49%">
</div>

$$ P(y|x_1, \dots, x_n) = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y) $$

この損失は，真のラベルに対する負の対数尤度を最小化するものである。
また，LM 損失を補助的な損失として追加することは，以下の理由で有益であることがわかった。
<!-- The loss is to minimize the negative log-likelihood for true labels.
In addition, adding the LM loss as an auxiliary loss is found to be beneficial, because:-->

1. 学習時の収束を早めることができる。
2. 教師付きモデルの汎化性を向上させることが期待される。

<!-- 1. it helps accelerate convergence during training and
2. it is expected to improve the generalization of the supervised  model. -->

$$ \begin{aligned}
{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in {D}} \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in {D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y) \\
{L}_\text{LM}  &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1}) \\
{L}            &= {L}_\text{cls} + \lambda {L}_\text{LM}
\end{aligned}$$

同様の設計で，他の終端課題のためにカスタマイズされたモデル構造は必要ない (図 7 参照)。
課題の入力が複数の文を含む場合，特別なデリミタトークン (`$`) が文の各対の間に追加される。
このデリミタトークンの埋め込みは新たに学習する必要のあるパラメータであるが，かなり最小限のものであるはずである。
<!-- With similar designs, no customized model structure is needed for other end tasks (see Fig. 7).
If the task input contains multiple sentences, a special delimiter token (`$`) is added between each pair of sentences.
The embedding for this delimiter token is a new parameter we need to learn, but it should be pretty minimal.-->

文の類似性課題では，順序は重要ではないので，両方の順序が含まれる。
多肢選択課題では，文脈は全ての回答候補と対になる。
<!-- For the sentence similarity task, because the ordering does not matter, both orderings are included.
For the multiple choice task, the context is paired with every answer candidate. -->

<div class="figure figcenter">
<img src="/2024assets/GPT-downstream-tasks.png" width="66%">
<div class="figcaption">

##### 図 8. 下流課題のために少し変更された GPT トランスフォーマーモデルで訓練対象 (画像出典: [原著論文](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
<!-- ##### Fig. 8. Training objects in slightly modified GPT transformer models for downstream tasks. -->
<!-- (Image source: [original paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)) -->
</div></div>

**まとめ**: このような一般的な枠組みが，当時 (2018 年 6 月) のほとんどの言語課題で SOTA に勝るというのは，超すてきで心強いことである。
第一段階では，言語モデルの生成的な事前学習は，可能な限り多くのフリーテキストを吸収することができる。
その後，第 2 段階では，少ないラベル付きデータセットと学習する新しいパラメータの最小セットで，特定の課題でモデルの微調整を行う。
<!-- **Summary**: It is super neat and encouraging to see that such a general framework is capable to beat SOTA on most language tasks at that time (June 2018).
At the first stage, generative pre-training of a language model can absorb as much free text as possible.
Then at the second stage, the model is fine-tuned on specific tasks with a small labeled dataset and a minimal set of new parameters to learn.-->

GPT の限界の一つは，その一方向的な性質であり，モデルは将来の左から右への文脈を予測するためにのみ学習される。
<!-- One limitation of GPT is its uni-directional nature --- the model is only trained to predict the future left-to-right context. -->

# BERT

**BERT** は，**Bidirectional Encoder Representations from Transformers** ([Devlin+2019](https://arxiv.org/abs/1810.04805)) の略で，自由テキストで大規模言語モデルを学習し，ネットワークアーキテクチャをカスタマイズせずに特定の課題で微調整するという，[GPT](#gpt) の直系に当たるものである。
<!-- **BERT**, short for **Bidirectional Encoder Representations from Transformers** ([Devlin+2019](https://arxiv.org/abs/1810.04805)) is a direct descendant to [GPT](#gpt): train a large language model on free text and then fine-tune on specific tasks without customized network architectures.-->

GPT と比較して，BERT の最大の違いと改良点は，学習を **双方向** にすることである。
このモデルは，左右両方の文脈を予測するように学習する。
アブレーション研究による論文では，次のように主張されている。
<!-- Compared to GPT, the largest difference and improvement of BERT is to make training **bi-directional**.
The model learns to predict both context on the left and right.
The paper according to the ablation study claimed that: -->

> "bidirectional nature of our model is the single most important new contribution"

## 事前学習課題
<!-- ## 6.1 Pre-training Tasks-->

BERT のモデル・アーキテクチャは，多層双方向トランスフォーマー符号化器である。
<!-- The model architecture of BERT is a multi-layer bidirectional Transformer encoder. -->

<div class="figure figcenter">
<img src="/2024assets/transformer-encoder-2.png" width="19%">
<div class="figcaption">

図 トランスフォーマー符号化器のモデル・アーキテクチャ
<!-- Fig. 9. Recap of Transformer Encoder model architecture. (Image source: [Transformer paper](https://arxiv.org/abs/1706.03762)) -->
</div></div>

双方向の予測と文レベルの理解を促すために，BERT は基本的な言語課題 (文脈が与えられた次トークンを予測する) の代わりに，2 つの課題で訓練される。
<!-- To encourage the bi-directional prediction and sentence-level understanding, BERT is trained with two tasks instead of the basic language task (that is, to predict the next token given context). -->

**課題 1: マスク化言語モデル (MLM)**<!-- **Task 1: Mask language model (MLM)** -->

> [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test)より。`クローズテスト (クローズ削除テストとも) とは，特定の項目，単語，記号が削除された言語の一部 (クローズテキスト) からなる訓練，検査，評価であり，参加者は失われた言語項目を置き換えるよう求められるものである。...
この訓練は，1953 年に W.L.テイラー によって初めて説明された。`
<!-- > From [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test): "A cloze test (also cloze deletion test) is an exercise, test, or assessment consisting of a portion of language with certain items, words, or signs removed (cloze text), where the participant is asked to replace the missing language item. ...
The exercise was first described by W.L. Taylor in 1953." -->

単語の直後ではなく，単語の周りの文脈を学習する表現が，構文的にも意味的にも，その意味をよりよく捉えることができると考えるのは当然である。
BERT は **言語モデルをマスク化課題** で訓練することにより，モデルがそうなるように促している。
<!-- It is unsurprising to believe that a representation that learns the context around a word rather than just after the word is able to better capture its meaning, both syntactically and semantically.
BERT encourages the model to do so by training on the *"mask language model" task*: -->

1. 各系列に含まれるトークンの 15％ をランダムにマスクする。
なぜなら，マスクされたトークンを特別なプレースホルダ `[MASK]` に置き換えるだけでは，微調整中に特別なトークンに遭遇することはないからである。
そこで，BERT はいくつかの発見的なトリックを採用した。
    - (a) 80 % の確率で，選択された単語を `[MASK]` に置き換える。
    - (b) 10 % の確率で，ランダムな単語で置き換える。
    - (c) 10 % の確率で，そのままの状態を維持する。
2. このモデルは欠落した単語を予測するだけで，どの単語が置換されたのか，どの単語を予測すべきなのかの情報は持っていない。
出力サイズは入力サイズの 15% に過ぎない。

<!-- 1. Randomly mask 15% of tokens in each sequence.
Because if we only replace masked tokens with a special placeholder `[MASK]`, the special token would never be encountered during fine-tuning.
Hence, BERT employed several heuristic tricks:
    - (a) with 80% probability, replace the chosen words with `[MASK]`;
    - (b) with 10% probability, replace with a random word;
    - (c) with 10% probability, keep it the same.
2. The model only predicts the missing words, but it has no information on which words have been replaced or which words should be predicted.
The output size is only 15% of the input size. -->

**課題 2: 次文予測**<!-- **Task 2: Next sentence prediction** -->

下流課題の多くが文の関係を理解すること (例 [QA](#qa), [NLI](#nli)) に動機づけられ，BERT は，ある文が他の文の次の文かどうかを判別する **2値分類器** を学習する補助課題を追加した。
<!-- Motivated by the fact that many downstream tasks involve the understanding of relationships between sentences (i.e., [QA](#qa), [NLI](#nli)), BERT added another auxiliary task on training a *binary classifier* for telling whether one sentence is the next sentence of the other: -->

1. 文の組 (A, B) を以下のようにサンプルする。
    - (a) 50％の確率で，B は A の次の文
    - (b) 50％の確率で，B は A の次の文ではない
2. モデルは両方の文を処理し，B が A の次の文であるかどうかを示す 2 値ラベルを出力する。

<!-- 1. Sample sentence pairs (A, B) so that:
    - (a) 50% of the time, B follows A;
    - (b) 50% of the time, B does not follow A.
2. The model processes both sentences and output a binary label indicating whether B is the next sentence of A.-->

上記 2 つの補助課題の学習データは，任意の単一言語コーパスから些細に生成することができる。
したがって，訓練の規模は無限大である。
訓練損失は，平均マスク化 LM 尤度と平均次文予測尤度の和である。
<!-- The training data for both auxiliary tasks above can be trivially generated from any monolingual corpus.
Hence the scale of training is unbounded.
The training loss is the sum of the mean masked LM likelihood and mean next sentence prediction likelihood. -->

<div class="figure figcenter">
<img src="/2024assets/language-model-comparison.png" width="66%">
<div class="figcaption">

##### 図 10. BERT, OpenAI GPT, ELMo アーキテクチャの比較 (画像出典: [原著論文](https://arxiv.org/abs/1810.04805))
<!-- Fig. 10. Comparison of BERT, OpenAI GPT and ELMo model architectures.
(Image source: [original paper](https://arxiv.org/abs/1810.04805)) -->
</div></div>

## 6.2 入力埋め込み
<!-- ## 6.2 Input Embedding-->

入力埋め込みは，3 つの部分の合計: <!-- The input embedding is the sum of three parts: -->

1.  **WordPiece トークン埋込み**:
[WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [model](https://arxiv.org/pdf/1609.08144.pdf) は，もともと日本語や韓国語の文節問題のために提案された。
英単語を自然に分割するのではなく，さらに小さな部分単語単位に分割することで，希少語や未知の単語を扱うのに効果的である。
最適な単語分割方法については [リンク](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [論文](https://arxiv.org/pdf/1609.08144.pdf) を参照。
2.  **セグメント埋め込み Segment Embeddings**: 入力が 2 つの文を含む場合，それぞれ文 A 埋め込み，文 B 埋め込みとし，特殊文字 `[SEP]` で区切られる。
3.  **位置埋め込み**。位置埋め込みは，ハードコーディングではなく，学習される。

<!-- 1.  *WordPiece tokenization embeddings*:
The [WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [model](https://arxiv.org/pdf/1609.08144.pdf) was originally proposed for Japanese or Korean segmentation problem.
Instead of using naturally split English word, they can be further divided into smaller sub-word units so that it is more effective to handle rare or unknown words.
Please read [linked](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) [papers](https://arxiv.org/pdf/1609.08144.pdf) for the optimal way to split words if interested.
2.  *Segment embeddings*: If the input contains two sentences, they have sentence A embeddings and sentence B embeddings respectively and they are separated by a special character `[SEP]`; Only sentence A embeddings are used if the input only contains one sentence.
3.  *Position embeddings*: Positional embeddings are learned rather than hard-coded. -->

<div class="figure figcenter">
<img src="/2024assets/BERT-input-embedding.png" width="55%">
<div class="figcaption">

##### 図 11. BERT の入力表現 (画像出典: [原著論文](https://arxiv.org/abs/1810.04805))
<!-- Fig. 11. BERT input representation. (Image source: [original paper](https://arxiv.org/abs/1810.04805)) -->
</div></div>

最初のトークンは常に `[CLS]` --- 下流課題で予測するために後で使われるプレースホルダーであることを強制されることに注意
<!-- Note that the first token is always forced to be `[CLS]` --- a placeholder that will be used later for prediction in downstream tasks. -->

## 下流課題における BERT の使用<!-- ## 6.3 Use BERT in Downstream Tasks-->

BERT の微調整は，OpenAI の GPT と同様，いくつかの新しいパラメータを追加するだけでよい。
<!-- BERT fine-tuning requires only a few new parameters added, just like OpenAI GPT. -->

分類課題では，特別な最初のトークン `[CLS]` の最終的な隠れ状態 $\mathbf{h}^\text{[CLS]}_L$ を取り出し，小さな重み行列 $\text{softmax}(\mathbf{h}^\text{[CLS]}_L\mathbf{W}_\text{cls})$ と乗算して予測を得ることができる。
<!-- For classification tasks, we get the prediction by taking the final hidden state of the special first token `[CLS]`, $\mathbf{h}^\text{[CLS]}_L$, and multiplying it with a small weight matrix, $\text{softmax}(\mathbf{h}^\text{[CLS]}_L\mathbf{W}_\text{cls})$. -->

SQuAD のような [QA](#qa) 課題では，与えられた問題に対して与えられたパラグラフのテキストスパンを予測する必要がある。
BERT は，すべてのトークンについて，テキストスパンの開始と終了という 2 つの確率分布を予測する。
新しい小さな行列は $\mathbf{W}_\text{s}$ と $\mathbf{W}_\text{e}$ の 2 つ，は微調整の際に新たに学習したもので，$\text{softmax}(\mathbf{h}^\text{(i)}_L\mathbf{W}_\text{s})$ と $\text{softmax}(\mathbf{h}^\text{(i)}_L\mathbf{W}_\text{e})$ は二つの確率分布である。
<!-- For [QA](#qa) tasks like SQuAD, we need to predict the text span in the given paragraph for an given question.
BERT predicts two probability distributions of every token, being the start and the end of the text span.
Only two new small matrices, $\mathbf{W}_\text{s}$ and $\mathbf{W}_\text{e}$, are newly learned during fine-tuning and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$ and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$ define two probability distributions. -->

全体として，エンド課題の微調整のためのアドオン部分は非常に少なく，トランスフォームの隠れ状態を解釈可能な形式に変換するための 1 つか 2 つの重み行列である。
他のケースでの実装の詳細については，論文を参照。
<!-- Overall the add-on part for end task fine-tuning is very minimal --- one or two weight matrices to convert the Transform hidden states to an interpretable format.
Check the paper for implementation details for other cases. -->

<div class="figure figcenter">
<img src="/2024assets/BERT-downstream-tasks.png" width="55%">
<div class="figcaption">

図 下流課題のためにわずかに修正された BERT モデルの訓練目的関数 <!-- ##### Fig. 12. Training objects in slightly modified BERT models for downstream tasks. -->(Image source: [original paper](https://arxiv.org/abs/1810.04805))
</div></div>

以下は [OpenAI GPT](#gpt) と BERT のファインチューニングの違いを比較したまとめ表である。
<!-- A summary table compares differences between fine-tuning of [OpenAI GPT](#gpt) and BERT. -->

|| **OpenAI GPT** | **BERT**|
|:---|:------------|:---------|
|特殊文字| `[SEP]` と `[CLS]` は微調整の段階でのみ導入される。| `[SEP]` と `[CLS]` と 文 A/B の埋め込みは，事前学習の段階で学習する。|
|学習処理| 1M ステップ，バッチサイズ 32 k 語 |  1M ステップ，バッチサイズ 128 k 単語|
|微調整  | すべての微調整課題に対して lr=5e-5 | 微調整は課題固有の lr を使用|


<!-- | | **OpenAI GPT** | **BERT** |
|:---|:------------|:---------|
| Special char| `[SEP]` and `[CLS]` are only introduced at fine-tuning stage.| `[SEP]` and `[CLS]` and sentence A/B embeddings are learned at the pre-training stage. |
|Training process | 1M steps, batch size 32k words. | 1M steps, batch size 128k words.|
| Fine-tuning | lr = 5e-5 for all fine-tuning tasks. | Use task-specific lr for fine-tuning. | -->

# ALBERT

**ALBERT** ([Lan+2019](https://arxiv.org/abs/1909.11942)) は **A Lite BERT** の略で  [BERT](#6-bert) モデルの軽量版である。
ALBERT モデルは，同様の構成の BERT モデルと比較して，l18 倍少ないパラメータで1.7 倍速く学習することができる。
ALBERT は，以下の 3 つの変更点を取り入れている。
最初の 2 つは，パラメータとメモリ消費量を削減することで学習速度を向上させるもので，3 つ目は，次文予測 (NSP) 目的関数に代わるより困難な学習課題を提案するものである。
<!-- **ALBERT** ([Lan+2019](https://arxiv.org/abs/1909.11942)), short for **A Lite BERT**, is a light-weighted version of [BERT](#BERT) model.
An ALBERT model can be trained 1.7x faster with 18x fewer parameters, compared to a BERT model of similar configuration.
ALBERT incorporates three changes as follows: the first two help reduce parameters and memory consumption and hence speed up the training speed, while the third one proposes a more challenging training task to replace the next sentence prediction (NSP) objective. -->

## 因子分解埋め込みパラメタリゼーション <!--Factorized Embedding Parameterization-->

BERT では，WordPiece トークン化埋め込みサイズ $E$ は，隠れ状態サイズ $H$ と同じになるように設定されている。
すなわち，モデルサイズを大きくする ($H$ を大きくする) には，トークン化埋め込みも大きく学習する必要があり，語彙サイズ ($V$) に依存するためコストがかかる。
<!-- In BERT, the WordPiece tokenization embedding size $E$ is configured to be the same as the hidden state size $H$.
That is saying, if we want to increase the model size (larger $H$), we need to learn a larger tokenization embedding too, which is expensive because it depends on the vocabulary size ($V$). -->

概念的には，トークン化埋め込みは **文脈に依存しない** 表現を学習し，隠れ状態は **文脈に依存する** ので，隠れ層の大きさと語彙埋め込みの大きさを分けるのが理にかなっている。
因子分解埋込パラメタリゼーションにより，サイズ $V\times H$ の大きな語彙埋込行列は，サイズ $V\times E$ と $E\times H$ の二つの小さな行列に分解される。
$H\gt E$ あるいは $H \gg E$ が与えられた場合，因子分解によってパラメータを大幅に削減することができる。
<!--Conceptually, because the tokenization embedding is expected to learn *context-independent* representation and the hidden states are *context-dependent*, it makes sense to separate the size of the hidden layers from the size of vocabulary embedding.
Using factorized embedding parameterization, the large vocabulary embedding matrix of size $V\\times H$ is decomposed into two small matrices of size $V \times E$ and $E \times H$.
Given $H \gt E$ or even $H \gg E$, factorization can result in significant parameter reduction. -->

## 交差層パラメータ共有 <!-- Cross-layer Parameter Sharing-->

(a) フィードフォワード部分のみを共有する，(b) 注目パラメータのみを共有する，(c) すべてのパラメータを共有する，など，層間のパラメータ共有は様々な方法で行われる。
この手法では，パラメータの数を 1 トン減らすことができ，性能に大きなダメージを与えることは無い。
<!-- Parameter sharing across layers can happen in many ways: (a) only share feed-forward part; (b) only share attention parameters; or (c) share all the parameters.
This technique reduces the number of parameters by a ton and does not damage the performance too much.-->

## 文順序予測 (SOP) <!-- ## 7.3 Sentence-Order Prediction (SOP)-->

興味深いことに BERT の [次文予測 (NSP)](#NSP) 課題は，簡単すぎることが判明した。
ALBERT は，代わりに文順予測 (SOP)[自己教師あり学習 self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/) の損失を採用した:
<!-- Interestingly, the [next sentence prediction (NSP)](#NSP) task of BERT turned out to be too easy.
ALBERT instead adopted a sentence-order prediction (SOP) [self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/) loss, -->

- 正事例：同じ文書から連続する 2 つのセグメント。
- 負事例：上記と同じだが，セグメントの順序が入れ替わる。

<!-- - Positive sample: two consecutive segments from the same document.
- Negative sample: same as above, but the segment order is switched. -->

NSP 課題では，A と B とが異なる文脈のものである場合にトピックを検出することができれば，モデルは妥当な予測を行うことができる。
それに比べて，SOP は，セグメント間の一貫性と順序を完全に理解することがモデルに求められるため，より困難である。
<!-- For the NSP task, the model can make reasonable predictions if it is able to detect topics when A and B are from different contexts.
In comparison, SOP is harder as it requires the model to fully understand the coherence and ordering between segments. -->

# GPT-2

[OpenAI](https://blog.openai.com/better-language-models/) の [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 言語モデルは，[GPT](#gpt) の直継モデルである。
GPT-2 は，オリジナルの GPT の 10倍 にあたる 1.5 B のパラメータを持ち，課題固有の微調整を必要としない **零撃転移学習設定** で，検証した 8 つの言語モデルデータセットのうち 7 つで SOTA を達成した。
事前学習データセットには [Reddit](https://www.reddit.com/) からの適格なアウトバウンドリンクをクロールして収集した 800 万件の Web ページが含まれている。
OpenAI GPT-2 による大きな改善は，小規模なデータセットや，**長距離依存** を測定するためのデータセットで特に顕著に見られる。
<!-- The [OpenAI](https://blog.openai.com/better-language-models/) [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) language model is a direct successor to [GPT](#gpt).
GPT-2 has 1.5B parameters, 10x more than the original GPT, and it achieves SOTA results on 7 out of 8 tested language modeling datasets in a *zero-shot transfer setting* without any task-specific fine-tuning.
The pre-training dataset contains 8 million Web pages collected by crawling qualified outbound links from [Reddit](https://www.reddit.com/).
Large improvements by OpenAI GPT-2 are specially noticeable on small datasets and datasets used for measuring *long-term dependency*. -->

## 零撃転移学習 Zero-Shot Transfer

GPT-2 の事前学習課題は，あくまで言語モデルである。
下流の言語課題はすべて条件付き確率の予測という枠組みになっており，課題に特化した微調整はない。
<!-- The pre-training task for GPT-2 is solely language modeling.
All the downstream language tasks are framed as predicting conditional probabilities and there is no task-specific fine-tuning. -->

* テキスト生成は LM を使って簡単にできる。
* 機械翻訳課題，例えば英語から中国語への翻訳は，「英文＝中国語文」と「対象の英文＝」の対を最後に LM で条件付けすることで誘導する。
    * 例えば，予測する条件付き確率は次のようなものになるだろう： `P(?|I like green apples.=我喜欢绿苹果。A cat meows at him.=一只猫对他喵。It is raining cats and dogs. =")`
* QA 課題は，文脈中の質問と回答の対を用いて，翻訳と同様のフォーマットで行われる。
* 要約課題は，文脈中の記事の後に `TL;DR:` を追加することで誘導される。

<!--
- Text generation is straightforward using LM.
- Machine translation task, for example, English to Chinese, is induced by conditioning LM on pairs of "English sentence = Chinese sentence" and "the target English sentence =" at the end.
    - For example, the conditional probability to predict might look like:
        `P(? | I like green apples. = 我喜欢绿苹果。 A cat meows at him. = 一只猫对他喵。It is raining cats and dogs. =")`
- QA task is formatted similar to translation with pairs of questions and answers in the context.
- Summarization task is induced by adding `TL;DR:` after the articles in the context.-->

## バイト系列上の BPE<!--BPE on Byte Sequences-->

GPT-2 は，オリジナルの GPT と同様，[UTF-8](https://en.wikipedia.org/wiki/UTF-8) のバイト列に対して [BPE](#byte-pair-encoding) を使用している。
UTF-8 は 1 文字に 4 バイトまで使用でき，合計で $2^{31}$ 文字までサポートする。
したがって，バイト列表現では，256 サイズの語彙があればよく，前処理やトークン化などを気にする必要はない。
このような利点があるにもかかわらず，現在のバイトレベル LM は，SOTA の単語レベル LM と無視できない性能差を持っている。
<!-- Same as the original GPT, GPT-2 uses [BPE](#byte-pair-encoding) but on [UTF-8](https://en.wikipedia.org/wiki/UTF-8) byte sequences.
Each byte can represent 256 different values in 8 bits, while UTF-8 can use up to 4 bytes for one character, supporting up to $2^{31}$ characters in total.
Therefore, with byte sequence representation we only need a vocabulary of size 256 and do not need to worry about pre-processing, tokenization, etc.
Despite of the benefit, current byte-level LMs still have non-negligible performance gap with the SOTA word-level LMs. -->

BPE は，頻繁に出現するバイト対を貪欲にマージする。
GPT-2 は，BPE が一般的な単語の複数のバージョンを生成するのを防ぐために (例えば，単語 dog に対して dog.，dog!，dog?)，カテゴリ間で文字をマージするのを防ぐ (したがって dog は`.`, `!`, `?` などの句読点とマージされないだろう)。
このトリックは，最終的なバイトセグメンテーションの品質を高めるのに役立つ。
<!-- BPE merges frequently co-occurred byte pairs in a greedy manner.
To prevent it from generating multiple versions of common words (i.e. `dog.`, `dog!` and `dog?` for the word `dog`), GPT-2 prevents BPE from merging characters across categories (thus `dog` would not be merged with punctuations like `.`, `!` and `?`). This tricks help increase the quality of the final byte segmentation.-->

バイト列表現を使って，GPT-2 は，前処理に関係なく，あらゆる Unicode 文字列に確率を割り当てることができる。
<!-- Using the byte sequence representation, GPT-2 is able to assign a probability to any Unicode string, regardless of any pre-processing steps. -->

## モデルの変更点 <!--Model Modifications-->

GPT-2 は GPT と比較して，Transformer 層とパラメー タの数が多いだけでなく，アーキテクチャの変更もわずかである。
<!-- Compared to GPT, other than having many more transformer layers and parameters, GPT-2 incorporates only a few architecture modifications: -->

* [層正規化](https://arxiv.org/abs/1607.06450) を各部分ブロックの入力に移し，[ビルディングブロック](https://arxiv.org/abs/1603.05027) の残差ユニットと同様にした (本来の [ボトルネック](https://arxiv.org/abs/1512.03385) とは異なり，重み層の前にバッチ正規化を適用)。
* 最終的な自己注意ブロックの後に，さらに層正規化を追加
* 初期化をモデルの深さの関数として構築
* 残差層の重みを $1/\sqrt{N}$ (N は残差層数) の係数で初期化
* より大きな語彙サイズと文脈サイズを使用

<!-- * [Layer normalization](https://arxiv.org/abs/1607.06450) was moved to the input of each sub-block, similar to a residual unit of type ["building block"](https://arxiv.org/abs/1603.05027) (differently     from the original type ["bottleneck"](https://arxiv.org/abs/1512.03385), it has batch normalization applied before weight layers).
* An additional layer normalization was added after the final self-attention block.
* A modified initialization was constructed as a function of the model depth.
* The weights of residual layers were initially scaled by a factor of $1/\sqrt{N}$ where N is the number of residual layers.
* Use larger vocabulary size and context size. -->

# RoBERTa

**RoBERTa** (**R**obustly **o**ptimized **BERT** **a**pproach の略；[Liu+2019](https://arxiv.org/abs/1907.11692)) は，元の BERT モデルが大幅に訓練不足であることを発見し，より良い結果を得るために BERT を訓練する新しいレセプトを指す。
このレセプトには，以下の学習が含まれている:
<!-- **RoBERTa** (short for **R**obustly **o**ptimized **BERT** **a**pproach; [Liu+2019](https://arxiv.org/abs/1907.11692)) refers to a new receipt for training BERT to achieve better results, as they found that the original BERT model is significantly undertrained.
The receipt contains the following learnings: -->

1. より長い時間，より大きなバッチサイズで訓練する。
2. 次文予測(NSP) 課題を削除する。
3. 学習データ形式として，より長い系列を使用する。
この論文では，個々の文章を入力として使用すると，下流の性能が低下することを発見した。
その代わりに，複数の文章を連続的にサンプリングして，より長いセグメントを形成する必要がある。
4. マスキングパターンを動的に変更する。
オリジナルの BERT は，データの前処理段階でマスキングを一度だけ適用し，その結果，訓練エポック全体で静的なマスクとなる。
RoBERTa は，40 エポックにわたって 10 種類の異なる方法でマスクを適用する。

<!-- 1. Train for longer with bigger batch size.
2. Remove the [next sentence prediction (NSP)](#nsp) task.
3. Use longer sequences in training data format.
The paper found that using individual sentences as inputs hurts downstream performance.
Instead we should use multiple sentences sampled contiguously to form longer segments.
4. Change the masking pattern dynamically.
The original BERT applies masking once during the data preprocessing stage, resulting in a static mask across training epochs. RoBERTa applies masks in 10 different ways across 40 epochs.-->

RoBERTa はまた，新しいデータセット [CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/) を追加し，**より多くのデータによる事前訓練が下流課題の性能向上に役立つ** ことをさらに確認した。
[GPT-2](#gpt-2) と同じ，[BPE on byte sequences](#bpe-on-byte-sequences) で学習している。
また，ハイパーパラメータの選択がモデルの性能に大きな影響を与えることがわかった。
<!-- RoBERTa also added a new dataset [CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/) and further confirmed that pretraining with *more data helps* improve the performance on downstream tasks.
It was trained with the [BPE on byte sequences](#bpe-on-byte-sequences), same as in [GPT-2](#gpt-2).
They also found that choices of hyperparameters have a big impact on the model performance. -->

# T5

言語モデル **T5** は，**Text-to-Text Transfer Transformer** ([Raffel+2020](https://arxiv.org/abs/1910.10683)) の略である。
符号化器−復号化器の実装は，[オリジナルの Transformer](https://arxiv.org/abs/1706.03762) のアーキテクチャに従っている: トークン→埋め込み→符号化→復号化→出力。
T5 は，フレームワーク「Natural Language Decathlon」([McCann+2018](https://arxiv.org/abs/1806.08730)) を採用しており，多くの一般的な NLP 課題が文脈上の質問応答へと変換されている。
T5 は，明示的な QA フォーマットの代わりに，短い課題接頭辞 prefix を使って課題の意図を区別し，個々の課題ごとに個別にモデルを微調整している。
また，Text-to-Text フレームワークにより，多様な課題に対して同じモデルで容易に転移学習評価を行うことができる。
<!-- The language model **T5** is short for **"Text-to-Text Transfer Transformer"** ([Raffel+2020](https://arxiv.org/abs/1910.10683)).
The encoder-decoder implementation follows the [original Transformer](https://arxiv.org/abs/1706.03762) architecture: tokens → embedding → encoder → decoder → output.
T5 adopts the framework "Natural Language Decathlon" ([McCann+2018](https://arxiv.org/abs/1806.08730)), where many common NLP tasks are translated into question-answering over a context.
Instead of an explicit QA format, T5 uses short task prefixes to distinguish task intentions and separately fine-tunes the model on every individual task.
The text-to-text framework enables easier transfer learning evaluation with the same model on a diverse set of tasks. -->

<div class="figure figcenter">
<img src="/2024assets/2019Raffel_T5_fig1ja.svg" width="66%">
<img src="/2024assets/T5.png" width="66%">
<div class="figcaption">

図 T5 課題評価の概念図 <!-- Fig. 13. A diagram of T5 task evaluation. -->
テキスト2テキストの枠組みは，あらゆる課題を一般的な形に落とし込む：入力テキストを与えて，あるターゲットテキストを予測する。
<!-- The text-to-text framework casts every task into a generic form: feeding input text to predict some target text. -->
(Image source: [Raffel+2020](https://arxiv.org/abs/1910.10683))
</div></div>

モデルは，2019 年 4 月に抽出された Web コーパスに様々なフィルタを適用して学習された。
モデルは 「適応総」(訓練のために追加層を追加する) または「漸進的凍結解除」([ULMFiT](#ulmfit) 参照) によって，各下流課題に対して個別に微調整される。
どちらの微調整アプローチも，モデルパラメータの大部分を変更せずに，部分的なパラメータのみを更新するものである。
T5-11 B は，多くの NLP 課題で SOTA の結果を達成した。
<!-- The model is trained on Web corpus extracted from Apr 2019 with various filters applied.
The model is fine-tuned for each downstream task separately via "adapter layers" (add an extra layer for training) or "gradual unfreezing" (see [ULMFiT](#ulmfit)).
Both fine-tuning approaches only update partial parameters while keeping the majority of the model parameters unchanged. T5-11B achieved SOTA results on many NLP tasks.-->

著者らが論文で「...我々の目標は新しい手法を提案することではなく，この分野の立ち位置について包括的な視点を提供することである」と述べているように，T5 の長文論文には多くの訓練設定と評価処理過程が詳細に記述されており，ゼロからの LM 訓練に興味がある人にとっては良い一論文である。
<!-- As the authors mentioned in the paper "...our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands", the T5 long paper described a lot of training setup and evaluation processes in detail, a good read for people who are interested in training a LM from scratch. -->

# GPT-3

**GPT-3** ([Brown+2020](https://arxiv.org/abs/2005.14165)) は [GPT-2](#gpt-2) と同じアーキテクチャだが，パラメータは  175 B と GPT-2  (1.5B) の 10 倍の大きさを持つ。
また，GPT-3 では [sparse transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#sparse-attention-matrix-factorization-sparse-transformers) と同じように，密と局所的に帯状の疎の注目パターンを交互に使用している。
このような巨大なモデルを複数の GPU で適合させるために，GPT-3 は幅と奥行きの両方の次元に沿ったパーティションで訓練される。
訓練データは，Common Crawl のフィルタリング版と，他のいくつかの高品質なキュレーションデータセットを混合したものである。
下流課題が学習データに現れる可能性を避けるため，著者らは学習データセットから，研究されたすべてのベンチマークデータセットとの重複をすべて削除することを試みた。
しかし，このフィルタリング処理はバグにより完全ではなかった。
<!-- **GPT-3** ([Brown+2020](https://arxiv.org/abs/2005.14165)) has the same architecture as [GPT-2](#gpt-2) but contains 175B parameters, 10x larger than GPT-2 (1.5B).
In addition, GPT-3 uses alternating dense and locally banded sparse attention patterns, same as in [sparse transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#sparse-attention-matrix-factorization-sparse-transformers).
In order to fit such a huge model across multiple GPUs, GPT-3 is trained with partitions along both width and depth dimension.
The training data is a filtered version of Common Crawl mixed with a few other high-quality curated datasets.
To avoid the contamination that downstream tasks might appear in the training data, the authors attempted to remove all the overlaps with all the studied benchmark dataset from the training dataset.
Unfortunately the filtering process is not perfect due to a bug. -->

<div class="figure figcenter">
<img src="/2024assets/GPT3-train-data.png" width="49%">
<div class="figcaption">

##### Fig. 14. GPT-3 の訓練用データセット。なお 訓練中の各データセットの出現率は，データセットサイズに比例しない。 <!-- Training datasets for GPT-3. Note that the occurrence of each dataset during training is not proportional to the dataset size.--> (Table source: [Brown+2020](https://arxiv.org/abs/2005.14165))
</div></div>

GPT-3 は，下流の評価では，勾配に基づく微調整を行わず，数発の設定でテストしている。
ここでは，プロンプトの一部として，少数撃の例が提供されている。
GPT-3 は，多くの NLP データセットにおいて，微調整された BERT モデルに匹敵する強力な性能を達成した。
<!-- For all the downstream evaluation, GPT-3 is tested in the few-shot setting without any gradient-based fine-tuning.
Here the few-shot examples are provided as part of the prompt. GPT-3 achieves strong performance on many NLP datasets, comparable with fine-tuned BERT models. -->

<div class="figure figcenter">
<img src="/2024assets/GPT3-eval.png" width="77%">
<div class="figcaption">

評価性能はモデルサイズと事例数によって上昇する <!--The evaluation performance increases with the model size and the number of examples.--> (Image source: [Brown+2020](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html))
</div></div>

