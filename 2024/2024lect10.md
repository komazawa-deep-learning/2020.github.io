---
title: 第10回 2024年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<!--
Stroop 効果 2，転移学習の調整
-->


<link href="/css/asamarkdown.css" rel="stylesheet">

<div align="center">
<font size="+1" color="navy"><strong>心理学特講IIIA ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 21/Jun/2024<br/>
Appache 2.0 license<br/>
</div>

$$
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\Brc}[1]{\left(#1\right)}
\newcommand{\Rank}{\text{rank}\;}
\newcommand{\Hat}[1]{\widehat{#1}}
\newcommand{\Prj}[1]{\mb{#1}\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}}
\newcommand{\RegP}[2]{\Brc{\mb{#1}^{\top}\mb{#1}}^{-1}\mb{#1}^{\top}\mb{#2}}
\newcommand{\NSQ}[1]{\left|\mb{#1}\right|^2}
\newcommand{\Norm}[1]{\left|#1\right|}
\newcommand{\IP}[2]{\left({#1}\cdot{#2}\right)}
\newcommand{\Bar}[1]{\overline{\;#1\;}}
$$

## 告知

* [WCCI 2024 AIガバナンス公開フォーラム](https://groups.oist.jp/ja/ncu/event/wcci-forum){:target="_blank"}


## 実習ファイル

* [転移学習による Stroop 効果2 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2024notebooks/2023_1114Stroop_model.ipynb){:target="_blank"}
* [フィラデルフィア絵画命名検査課題 PNT を転移学習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618pnt_transfer_learning.ipynb){:target="_blank"}
* [CAM 実習 <img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb){:target="_blank"}

* [1990 年代の Stroop 効果のシミュレーション<img src="/assets/colab_icon.svg">](https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_1110Stroop_1990Cohen_model.ipynb){:target="_blank"}

* [ニューラルネットワークで遊んでみよう](/tensorflow-playground/){:target="_blank"}
<!-- * [ニューラルネットワークで遊んでみよう](https://project-ccap.github.io/tensorflow-playground/){:target="_blank"} -->
* [RNN 文字ベースの言語モデル 漱石「こころ」](/character_demo.html){:target="_blank"}

## Stroop 課題または Stroop 効果

<div class="figcenter">

<img src="/2023assets/1990Cohen_McClelland_stroop_fig3.svg" style="width:33%;">
<img src="/2023assets/2023_1110task_demand_conflict_ja.svg" style="width:39%;">
<!-- <div style="align:center;width:88%">
<img src="/2023assets/1990Cohen_McClelland_stroop_fig3.svg">
<img src="/2023assets/2023_1110task_demand_conflict_ja.svg" width="49%"> -->

<div class="figcaption" style="width:77%;">

左: 図 3. 単語読解と色名学習後の接続強度を示すネットワーク図。
(強度は接続の横に示され，中間ユニットのバイアスはユニットの内側に示されている。
課題要求ユニットから中間ユニットへの注意強度は固定され，中間ユニットのバイアスも固定された。
課題要求ユニットがオンのとき，対応する経路のユニットの基本入力が 0.0 になり，もう一方の経路のユニットの基本入力が，実験によって -4.0 から -4.9 の範囲になるように選ばれた)。
<!-- Figure 3. Diagram of the network showing the connection strengths after training on the word-reading and color-naming tasks.
(Strengths are shown next to connections; biases on the intermediate units are shown inside the units.
Attention strengths-from task demand units to intermediate units-were fixed, as were biases for the intermediate units.
The values were chosen so that when the task demand unit was on, the base input for units in the corresponding  pathway was 0.0, whereas the base input to units in the other pathway was in the range of -4.0 to -4.9, depending on the experiment.) -->
出典: Cohen, Dunbar, and McClelland (1990) __On the Control of Automatic Processes: A Parallel Distributed Processing Account of the Stroop Effect__, Psychological Review, Vol. 97, No. 3, 332-361.

右: 転移学習，微調整を用いた Stroop 課題の枠組み
</div></div>

<div class="figcenter">

<img src="/2023assets/2003Roelofs_stroop_fig7.svg" width="33%">　　　　　　　　　
<img src="/2023assets/2003Roelofs_stroop_fig9.jpg" width="44%">
<div class="figcaption">

図 7. WEAVER++ における処理レベル。
単語の読解はレンマ選択を伴う場合もあれば (経路 a)，伴わない場合もある (経路 b)。
<!-- Figure 7. Levels of processing in WEAVER++.
Word reading may involve lemma selection (Route a) or may not (Route b). -->
<!-- Adapted from Cognition, 42, A. Roelofs, “A Spreading-Activation Theory of Lemma Retrieval in Speaking,” p
. 114, Figure 1, Copyright 1992, with permission from Elsevier Science. -->

図 9. Stroop 課題における，単語計画と実行制御。
ヒトの左半球の側面図 (上) と 中央 (下)。
単語計画系は，色知覚 (cp)，概念同定 (ci)，レンマ検索 (lr)，単語携帯符号化 (wfe)，構音処理 (art) を介して色名呼
称へと至る。
単語形態知覚 (wfp) は，語彙と形態と並列的に至る。
単語音読は，最小限 wfp, wfe, art を含む。
実行系は前帯状回にあり，目標と入力制御に関与する。
<!-- Figure 9. Word planning and executive control in the Stroop task.
Lateral view (top panel) and medial view (bottom panel) of the left hemisphere of the human brain.


Lateral view (top panel) and medial view (bottom panel) of the left hemisphere of the human brain.
The word-planning system achieves color naming through color perception (cp), conceptual identification (ci),
lemma retrieval (lr), word-form encoding (wfe), and articulatory processing (art);
word-form perception (wfp) activates lemmas and word forms in parallel.
Word reading minimally involves wfp, wfe, and art.
The executive system centered on the anterior cingulate achieves goal and input control. -->
出典: Roelofs (2003) __Goal-Referenced Selection of Verbal Action: Modeling Attentional Control in the Stroop Task__, Psychological Review, 2003, Vol. 110, No. 1, 88–125.
</div></div>

#### 文献

* ストループ効果 J. Ridley Stroop (1935) __Studies of Interference in Serial Verbal Reactions__, Journal of Experimental Psychology, 18, 643-662.
* サイモン効果 J. Richard Simon (1969) __Reactions Toward the Source of Stimulation__, Journal of Experimental Psychology, 81(1), 174-176.
* 絵画‐単語干渉効果 W. Glaser & F-J. Dungelhoff, (1984) __The Time Course of Picture-Word Interference__, Journal of Experimental Psychology: Human Perception and Performance, 10(5), 640-654.
* 絵画‐単語課題の意味促進 M-T. Bajo (1988) __Semantic Facilitation With Pictures and Words__, Journal of Experimental Psychology: Learning, Memory, and Cognition, 14(4), 579-589.
* 絵画命名課題における累積意味抑制 D. Howard+ (2006) __Cumulative semantic inhibition in picture naming: experimental and computational studies__, Cognition 100, 464–482.
* 絵画命名課題における言語間の干渉と促進 A. Roelofs+ (2016) __Electrophysiology of cross-language interference and facilitation in picture naming__. http://dx.doi.org/10.1016/j.cortex.2015.12.003
* 累積意味，意味ブロック，意味妨害効果 A. Roelofs+ (2018) __A unified computational account of cumulative semantic, semantic blocking, and semantic distractor effects in picture naming__. https://doi.org/10.1016/j.cognition.2017.12.007

#### 関連研究

##### 絵画‐単語干渉課題

<div class="figcenter">
<img src="/2023assets/1984Glaser_picture-word_fig1.svg" style="width:33%;">
<img src="/2023assets/1984Glaser_picture-word_fig2.svg" style="width:49%;">
<div class="figcaption">

刺激成分の特徴例: (a) 非絵画，(b) 空単語付き絵画刺激, (c) 非単語，(d) 不調和刺激

図 課題 $\times$ SOA $\times$ 一致性における平均促進・抑制得点
<!-- Figure 2. Mean facilitation and inhibition scores in the Task X SOA X Congruency cells of Experiment I. (SOA = stimulus onset asynchrony.) -->
</div></div>

###### カテゴリー累積効果

<div class="figcenter">
<img src="/2023assets/2006Howard_fig1.svg" width="44%">
<img src="/2023assets/2006Howard_fig3.svg" width="44%">
<div class="figcaption">

左 カテゴリー内の順序位置が命名反応時間 (補正なし RT) に及ぼす影響。<br/>
右  (A) 健常群の反応時間 (上) と (B) モデル出力
</div></div>



# 単純再帰型ニューラルネットワーク

<!-- NETTalk を先がけとして **単純再帰型ニューラルネットワーク** Simple Recurrent Neural networks (SRN) が提案された。 -->
発案者の名前で **Jordan ネット**，**Elman ネット** と呼ばれる。
<!-- [JordanNet]: Joradn, M.I. (1986) Serial Order: A Parallel Distributed Processing Approach, UCSD tech report.
[ElmanNet]: Elman, J. L. (1990)Finding structure in time, Cognitive Science, 14, 179-211. -->
Jordan ネットも Elman ネットも上位層からの **帰還信号** を持つ。
これを **フィードバック結合** と呼び，位置時刻前の状態が次の時刻に使われる。
Jordan ネットでは一時刻前の出力層の情報が用いられる (下図)。
一方，Elman ネットでは一時刻前の中間層の状態がフィードバック信号として用いられる。

<center>
<img src="/assets/SRN_J.svg" style="width:47%"><br/>
<div style="width:74%" align="center">
図：マイケル・ジョーダン発案ジョーダンネット [@1986Jordan]
</div>
</center>

<!-- - 駄菓子菓子 <a target="_blank" href="/assets/MJ_air.jpg">彼（マイケル・ジェフェリー(エアー)・ジョーダン）</a> ではない :)
- <a target="_blank" href="/assets/c3-s4-jordan.jpg">マイケル・アーウィン・ジョーダン。ミスター機械学習[^jordan_ai_revolution_not_yet]</a> -->
<!-- [^jordan_ai_revolution_not_yet]: 彼は(も？)神様です。多くの機械学習アルゴリズムを提案し続けている影響力のある人です。長らく機械学習の国際雑誌の編集長でした。2018年 <a target="_blank" href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">AI 革命は未だ起こっていない</a> と言い出して議論を呼びました。
-->

<center>
<img src="/assets/SRN_E.svg" style="width:47%"><br/>
<div style="align:center; width:47%">
図：ジェフ・エルマン発案のエルマンネット[@lman1990],[@Elman1993]
</div>
</center>

どちらも一時刻前の状態を短期記憶として保持して利用するのですが，実際の学習では一時刻前の状態をコピーして保存しておくだけで，実際の学習では通常の **誤差逆伝播法** すなわちバックプロパゲーション法が用いられる。
上 2 つの図に示したとおり U と W とは共に中間層への結合係数であり，V は中間層から出力層への結合係数である。
Z=I と書き点線で描かれている矢印はコピーするだけですので学習は起こりません。このように考えれば SRN は 3 層のニューラルネットワークであることが分かる。

SRN はこのような単純な構造にも関わらず **チューリング完全** であろうと言われてきた。
すなわちコンピュータで計算可能な問題はすべて計算できるくらい強力な計算機だという意味である。

- Jordan ネットは出力層の情報を用いるため **運動制御** に
- Elan ネットは内部状態を利用するため **言語処理** に

それぞれ用いられる。
従って **失行** aparxia (no matter what kind of apraxia such as 'ideomotor' or 'conceptual')，**行為障害** のモデルを考える場合 Jordan ネットは考慮すべき選択肢の候補の一つとなるだろう。

## リカレントニューラルネットワークの時間展開

一時刻前の状態を保持して利用する SRN は下図左のように描くことができる。
同時に時間発展を考慮すれば下図右のように描くことも可能である。

<center>
<img src="/assets/RNN_fold.svg" style="width:49%"><br/>
Time unfoldings of recurrent neural networks
</center>

上図右を頭部を 90 度右に傾けて眺めてみよ。
あるいは同義だ上図右を反時計回りに 90 度回転させたメンタルローテーションを想像してほしい。
このことから **"SRN とは時間方向に展開したディープラーニングである"** ことが分かる。

<!-- ## 2.4. エルマンネットによる言語モデル

下図に <a target="_blank" href="/assets/Elman_portrait.jpg">エルマン</a> が用いたネットワークモデルを示しました。
図中の数字はニューロンの数を表します。入力層と出力層のニューロン数 26 とは，もちいた語彙数が 26 であったことを表します。

<center>
<img src="/assets/1991Elman_starting_small_Fig1.svg" style="width:47%"><br/>
from [@Elman1991startingsmall]
</center>

エルマンは，系列予測課題によって次の単語を予想することを繰り返し学習させた結果，文法構造がネットワークの結合係数として学習されることを示しました。Elman ネットによって，埋め込み文の処理，時制の一致，性や数の一致，長距離依存などを正しく予測できることが示されました(Elman, 1990, 1991, 1993)。

- S     $\rightarrow$  NP VP “.”
- NP    $\rightarrow$  PropN | N | N RC
- VP    $\rightarrow$  V (NP)
- RC    $\rightarrow$  who NP VP | who VP (NP)
- N     $\rightarrow$  boy | girl | cat | dog | boys | girls | cats | dogs
- PropN $\rightarrow$  John | Mary |
- V     $\rightarrow$  chase | feed | see | hear | walk | live | chases | feeds | seeds | hears | walks | lives

これらの規則にはさらに 2 つの制約があります。

1. N と V の数が一致していなければならない
2. 目的語を取る動詞に制限がある。例えばhit, feed は直接目的語が必ず必要であり，see とhear は目的語をとってもとらなくても良い。walk とlive では目的語は不要である。

文章は 23 個の項目から構成され，8 個の名詞と 12 個の動詞，関係代名詞 who，及び文の終端を表すピリオドです。この文法規則から生成される文 S は，名詞句 NP と動詞句 VP と最後にピリオドから成り立っている。
名詞句 NP は固有名詞 PropN か名詞 N か名詞に関係節 RC が付加したものの何れかとなります。
動詞句 VP は動詞 V と名詞句 NP から構成されるが名詞句が付加されるか否かは動詞の種類によって定まる。
関係節 RC は関係代名詞 who で始まり，名詞句 NP と動詞句 VP か，もしくは動詞句だけのどちらかかが続く，というものです。

下図に訓練後の中間層の状態を主成分分析にかけた結果を示しました。"boy chases boy", "boy sees boy", および "boy walks" という文を逐次入力した場合の遷移を示しています。
同じ文型の文章は同じような状態遷移を辿ることが分かります。 -->

<!-- <center>
<img src="/assets/1991Elman_Fig3.jpg" style="width:49%"><br/>
<p align="left" style="width:47%">
<!-- Trajectories through state space for sentences boy chases boy, boy sees boy, boy walks.
Principal component 1 is plotted along the abscissa; principal component 3 is plotted along the ordinate.
These two PC’s together encode differences in verb-argument expectations.
</p>
</center> -->

<!-- <img src="/assets/1991Elman_Fig4a.jpg" style="width:84%"><br> -->

<!-- 下図は文 "boy chases boy who chases boy" を入力した場合の遷移図です。この文章には単語 "boy" が 3 度出てきます。それぞれが異なるけれど，他の単語とは異なる位置に附置されていることがわかります。
同様に 'chases" が 2 度出てきますが，やはり同じような位置で，かつ，別の単語とは異なる位置に附置されています。<br/>

<center>
<img src="/assets/1991Elman_Fig4b.jpg" style="width:49%"><br/>
</center>

同様にして "boy who chases boy chases boy" (男の子を追いかける男の子が男の子を追いかける) の状態遷移図を下図に示しました。<br/>
<center>

<img src="/assets/1991Elman_Fig4c.jpg" style="width:48%"><br/>
</center>

さらに複雑な文章例 "boy chases boy who chases boy who chases boy" の状態遷移図を下図に島します。</br>
<center>

<img src="/assets/1991Elman_Fig4d.jpg" style="width:48%"><br/>
</center>

Elman ネットが構文，文法処理ができるということは上図のような中間層での状態遷移で同じ単語が異なる文位置で異なる文法的役割を担っている場合に，微妙に異なる表象を，図に即してで言えば， 同じ単語では，同じような場所を占めるが，その文法的役割によって異なる位置を占めることが示唆されます。
このことから中間層の状態は異なる文章の表現を異なる位置として表現していることが考えられ，後述する **単語の意味** や **自動翻訳** などに使われることに繋がります(浅川の主観半分以上) -->

<!--
<p align="left" style="width:74%">
Movement through state space for sentences with relative clauses. Principal component 1 is displayed along the abscissa; principal component 11 is displayed along the ordinate. These two PC’s encode depth of embedding in relative clauses.
</p>
</center>
-->

<!-- ## 2.5. Seq2sep 翻訳モデル

上記の中間層の状態を素直に応用すると **機械翻訳** や **対話** のモデルになります。
下図は初期の翻訳モデルである "seq2seq" の概念図を示しました。
"`<eos>`" は文末 end of sentence を表します。中央の "`<eos>`" の前がソース言語であり，中央の "`<eos>`" の後はターゲット言語の言語モデルである SRN の中間層への入力として用います。

注意すべきは，ソース言語の文終了時の中間層状態のみをターゲット言語の最初の中間層の入力に用いることであり，それ以外の時刻ではソース言語とターゲット言語は関係がないことです。
逆に言えば最終時刻の中間層状態がソース文の情報全てを含んでいるとみなすことです。
この点を改善することを目指すことが 2014 年以降盛んに行われてきました。
顕著な例が後述する **双方向 RNN**， **LSTM** を採用したり，**注意** 機構を導入することでした。 -->

<!--
<center>
<img src="/assets/RNN_fold.svg" style="width:94%"></br>
Time unfoldings of recurrent neural networks
</center>
-->

<center>
<img src="/assets/2014Sutskever_S22_Fig1.svg" style="width:88%"><br/>
From [@2014Sutskever_Sequence_to_Sequence]
</center>
<!--
$$
\mbox{argmax}_{\theta}
\left(-\log p\left(w_{t+1}\right)\right)=f\left(w_{t}\vert \theta\right)
$$ -->

## 多様な RNN とその万能性
双方向 RNN や LSTM を紹介する前に，カルパシーのブログから下図に引用する。
下の 2 つ図ではピンク色が入力層，緑が中間層，青が出力層を示している。

<!-- [^karpathy]: 去年までスタンフォード大学の大学院生。現在はステラ自動車，イーロン・マスクが社長，の AI 部長さんです。図は彼のブログから引用です。蛇足ですがブログのタイトルが unreasonable effectiveness of RNN です。過去の偉大な論文 Wiegner (1960), Hamming (1967), Halevy (2009) からの <del>パクリ</del> **敬意を表したオマージュ**です。
"unreasonable effectiveness of [science|mathematics|data]" $\ldots$ www -->

<center>
<img src="/assets/diags.jpeg" style="width:77%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

- 上図最左は通常の多層ニューラルネットワークで画像認識，分類，識別問題に用いられます。
- 上図左から 2 つ目は，画像からの文章生成
- 上図中央，左から 3 つ目は，極性分析，文章のレビュー，星の数推定
- 上図右から 2 つ目は翻訳や文章生成
- 上図最右はビデオ分析，ビデオ脚注付け

などに用いられます。これまで理解を促進する目的で中間層をただ一層として描いてきた。
だが中間層は多層化されていることの方が多いこと，中間層各層のニューロン数は 1024 程度まで用いられていることには注意。

数は各層のニューロン数が 4 つである場合の数値例を示しています。入力層では **ワンホット** 表現が用いられている。
<!-- [^onehot]: ベクトルの要素のうち一つだけが "1" であり他は全て "0” である疎なベクトルのこと。一つだけが "熱い" あるいは "辛い" ベクトルと呼びます。以前は one-of-$k$ 表現 (MacKay の PRML など) と呼ばれていたのですが ワンホット表現，あるいは ワンホットベクトル (おそらく命名者は Begnio 一派)と呼ばれることが多いです。ワンホットベクトルを学習させると時間がかかるという計算上の弱点が生じます。典型的な誤差逆伝播法による学習では，下位層の入力値に結合係数を掛けた値で結合係数を更新します。従って，下位層の値のほとんどが "0" であるワンホットベクトルは学習効率が落ちることになります。そこで Elman はワンホットベクトルを実数値を持つ多次元ベクトルに変換してから用いることを行いました。上のエルマンネットによる文法学習において,ニューロン数 10 の単語埋め込み層と書かれた層がこれに該当します。単語埋め込み層を用いることで学習効率が改善し，後に示す word2vec などの **分散ベクトルモデル** へと発展します。 -->

<!-- <center>
<img src="/assets/charseq.jpeg" style="width:66%"><br/>
RNN variations from <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
</center>

[@1991Siegelmann_RNN_universal] said Turing completeness of RNN.
 -->

