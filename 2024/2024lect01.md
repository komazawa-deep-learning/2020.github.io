---
title: 第01回 2024 年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 15/Apr/2023<br/>
Appache 2.0 license<br/>
</div>

<!-- ## 目次 -->

<!-- ### 本日のメニュー
1. [授業情報，自己紹介 およそ30分を予定](#自己紹介)
2. [chatGPT について](#1-chatgpt-と心理学)
3. [画像生成 diffusion モデル](#2-diffusion-model)
4. [企業紹介，就職情報 およそ30分](#関連企業団体)
5. [用語](#用語の理解と区別) -->

# 質問

1. 関心のある分野を教えて下さい
2. 関連授業の履修状況
2. プログラミングの経験を教えて下さい
4. 高等学校までの数学，情報科目の履修状況について教えて下さい
3. 統計学の知識について教えて下さい
推測統計学，ベイズ統計学




# 用語の整理

* 人工知能
* ニューラルネットワーク
* ディープラーニング (深層学習)
* データサイエンス: **データサイエンティストは 21 世紀で最もカッコいい (the sexist) 職業だ** というハーバードビジネスレビューの [ポジショントーク記事 (2012年)](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century) が話題になって久しい。
* ビッグデータ: こちらも[ポジショントークらしく学術論文は存在しない](https://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/)。
ただし [データが増え続けている](http://www.uvm.edu/pdodds/files/papers/others/2011/hilbert2011a.pdf) ことは事実なので社会的な傾向とも言える。

<div style="align:center;">

<img src='/assets/2017Goodfellow_Fig1_4ja.svg' width="66%"><br/>
Goodfellow et al. (2017) Fig.1 を改変
</div>

# 歴史的背景

心理学の歴史を考えると，

* 1950年代 の認知革命，から認知科学が生まれ，
* 1980年代 の脳画像技術の進歩に伴って，神経科学との接点が強調されるようになりました。
* 2000年代以降の データサイエンスの流行に伴って，データを持って語ることが多くなりました。
* 2014年 ディープラーニングによる画像認識技術が注目を浴びるようになりました。
このような背景から，心理学とAI，データサイエンスとの融合が数多く試みられるようになっています。
* 2019 年以降，ディープラーニングと神経科学と認知科学との三者関係を論じる風潮が増えてます。

* [ニューウェルの疑問](/2022/1973Newell_ja)

# 素朴な疑問

1. 機械学習とニューラルネットワーク違うの？
1. 機械学習と人工知能は違うの？
1. ニューラルネットワークと人工知能は違うの？
1. 機械学習とニューラルネットワークと人工知能は関係は？
2. ディープラーニングとニューラルネットワークは違うの？

<!-- # 人工知能 AI とは何か

- 「人工知能の基礎」（小林 一郎）
    - 人の知能，つまり，人が行なう知的作業は，推論，記憶，認識，理解，学習，創造といった現実世界に適応するため
の能力を指す．人工の「知能」とは，人の「知能」のある部分を機械に行わせることによって創られる．
- デジタル大辞泉 《artificial intelligence》コンピューターで，記憶・推論・判断・学習など，人間の知的機能を代行
できるようにモデル化されたソフトウエア・システム．AI．

シャピロ (Shapiro, Stuart C., 1992) は次の3つの分野だと書いています。

1. 計算論的心理学 Computational Psychology:  __人間の知的活動を理解するために人間のように振る舞うコンピュータプログラムを作ること__
1. 計算論的哲学 Computational Philosophy:  __人間レベルの知的活動を計算論的に理解すること。計算論的理解=コンピュータ上に実装可能なモデル__
1. 計算機科学 Advanced Computer Science:  __コンピュータ科学の拡張，発展。現在のコンピュータはプログラムされたことしか実行できないが，人間はプログラムされていなくても勝手に振る舞う。__

* Shapiro, Stuart C. (1992), "Artificial Intelligence", in Stuart C. Shapiro (ed.), Encyclopedia of Artificial Intelligence, 2nd edition (New York: John Wiley & Sons) -->


# 時代背景

- 18世紀 第 1 次産業革命: <span style="color:Blue">蒸気機関，都市部に大規模工場が出現</span>
- 20世紀初頭 第2次産業革命: <span style="color:Blue">電気，オートメーション化，自動車，飛行機，電車による移動
手段の変化</span>
- 20世紀後半 第3次産業革命: <span style="color:Blue">情報化，コンピュータ化，グローバル化</span>
- 21世紀から 第4次産業革命: <span style="color:Blue">AI 人間の能力を越える機械</span>

<!--from [http://bootcamp.lif.univ-mrs.fr:8080/mainpage](http://bootcamp.lif.univ-mrs.fr:8080/mainpage)-->

<center>

<img src='/assets/2009Gray_4th_paradigm.svg' style='width:66%'><br>
Gray (2009) The 4th paradigm より
</center>


<!-- 人工知能 は知的機能をコンピュータで作ろうとする **構成論的研究** です。
浅川にはむしろ人工知能研究者の方が，人間の心を真摯に向き合っているようにも見えます。-->

* **認知科学** や **神経科学** は 時代ごとに流行があり，その都度変わってきました。
<!--* 心のはたらき，人間の活動を理解するために必要な要素が異なっています（表1）。-->
* **認知心理学** は **行動主義** がブラックボックスとみなしていた心的機能を，情報処理の用語と理論を援用して理解しようと試みました。
    * $\rightarrow$ 明示的な計算モデルを提案できていません。
* **認知科学** は 情報処理理論をさらに進めることを提案しました。
    * $\rightarrow$ 神経生理学的なデータによる制約がないため、行動データと一致する複数の代替モデルを判断することが困難です
* **コネクショニズム** は 神経生物学的に妥当な計算の枠組みを提供しました
    * 第２世代までのニューラルネットワークの技術は 現実的な課題に取り組むには十分ではありませんでした。
    * その結果 AI システムとしての期待に応えることができず、認知科学ではモデル化は **おもちゃの問題** に限られていました。
* **認知神経科学** では 神経生理学的なデータを用いて 脳の情報処理を研究しました。
    * $\rightarrow$ 複雑な脳画像データの解析という新たな課題に手を焼いているうちに、理論的な洗練度は認知心理学の段階に後退し、**箱と矢印モデル** を脳領域にマッピングすることから（おそらく合理的に）始めました。
* **計算論的神経科学** では 生物学的妥当性を持つ **計算モデル** を用いて 神経生理学的データや行動学的データを記述，予測しようとします。
    * $\rightarrow$ 現実世界の複雑な計算課題や，より高度な脳の表現に取り組むことはできませんでした。
* 現在 **ディープニューラルネットワーク (ディープラーニング)** は 複雑な認知課題に取り組み 脳と行動の両方の反応を予測するための枠組みとなっています。

* 行動主義
* 認知心理学
* 認知科学
* 生理学
* 機能的脳画像
*

# ニューラルネットワークって何？

ニューラルネットワーク (神経回路網 neural networks) とは，神経細胞 (ニューロン) の結合 (ネットワーク) のことです。

クイズのようなダジャレのような話ですが，ANN, BNN, CNN, DNN という省略形で呼ばれています。

* ANN: 人工ニューラルネットワーク
* BNN: 生物学的ニューラルネットワーク
* CNN: 畳み込みニューラルネットワーク。アメリカ合衆国のケーブルテレビの名称でもある。
* DNN: ディープニューラルネットワーク

広義には ニューロンを基本計算単位とした情報処理モデルであると言えます。
**計算** という言葉は，算術演算を意味しません。脳の働き，さまざまな心の作用はすべて 計算である との立場です。
すなわち，我々の知的活動，心的状態は，ニューロンを基本構成単位とする ネットワーク働きとして説明されるという、心，あるいは 脳を理解するためのパラダイム一般をニューラルネットワーク，
あるいは，心 （精神） の計算理論と呼びます。

現在までのところ，ニューラルネットワーク研究では，脳の血流 （とその異常，障害），神経伝達物質の代謝 (とその異常，障害) ，と言った側面にまで
及んでいるわけではありません。

神経細胞は，人間を含む動物が持っていますが，人工ニューラルネットワーク (ANN) では，コンピュータ上で，ニューロンの働きを模倣することにより，複雑な課題を解くことを目指し，
場合によっては，人間以上の性能を示すまでになっています。

ANN は 生物学的ニューラルネットワーク (BNN) にヒントを得て作成されました。
ですが，現在の ANN は BNN に比べて極端な単純化を行った並列情報処理モデルです。

たとえば，スパイキングモデルは計算機科学の分野では重要な位置を占めています。
スパイキングモデルでは 樹状突起による計算や 各ニューロン内の他のプロセス（Gallistel & King, 2011) あるいは，異なるタイプのニューロンからの関与を考慮しているとは言えません。

通常 ANN では ニューロンの空間構造は プログラミング可能な形で抽象化されています。
ニューロンのスパイク出力はスパイク率のような実数としてモデル化されています。
<!--このレートは、静的な非線形性を介して入力される活性化の加重和としてモデル化されます。-->
このように単純化されているにもかかわらず，ニューラルネットワークは脳の情報処理を理解するための最も重要な方法の 1 つとなっています。

おそらく，この方法が主流になると予想しています。

実際のニューロンの活動を調べる 神経科学 と 人間の 知的活動を 材料とする 認知科学 との橋渡しをする 理論モデルとして 中心的な役割を果たすことになると予想します。

逆にこれ以外の方法で，心の理解がありえるのだろうか？

## 議論

ディープラーニング(という AI 技術) を心理学的に解釈するという科目名になっています。
確かにディープラーニングとは，世間一般的には AI 技術であるという認識が一般には広まっています。
ところが，内容を考えてみると，人間の行う知的活動をコンピュータで再現するという意味です。
そのようにしてできたモデルを応用してみると，工学的な応用が可能であることから，大いに注目されました。

* ドローン ([兵士が機械に殺される…現代の戦争で使われる「軍事ドローン」の驚くべき実態](https://gendai.ismedia.jp/articles/-/76635?page=5){:target="_blank"})
* 顔認証([ファーウェイ、ウイグル人を顔認識して警察に通報するテストへの関与を認める](https://japanese.engadget.com/huawei-tested-facialrecognition-uyghurs-065011752.html){:target="_blank"})
* グーグル翻訳([Does Google’s BERT Matter in Machine Translation?](https://slator.com/machine-translation/does-googles-bert-matter-in-machine-translation/){:target="_blank"})
* アルファ碁ゼロ([AlphaGo Zero: Starting from scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch){:target="_blank"})，

上記で使われている技法が心理学にどのように関連しているのかを探ります。

* 上の進歩を支えているのが 人工ニューラルネットワーク (ANN) です。
* この ANN は 神経細胞 (ニューロン) に似た ノード (ニューロンとか，シナプスとか，ユニットとか様々な呼び名がありますが取り敢えず同じものだと考えます) で構成された幾重もの層から構成されています。
* 入力層のノードは ニューロン間のシナプスのような動作を模した数学的演算を経て、中間層 (隠れ層) のノードへ信号が伝達されます。
* 中間層 (隠れ層) では 同様の演算操作を繰り返し，その結果を，出力層へと伝達します。
* 顔認識などの課題では，ｌ入力データは 顔画像の各画素の値を 白から黒までの 100 点満点で表した数字からなる行列で表現します。カラー画像であれば，色の三原色である，赤，緑，青，の画素値からなる行列で表現します。
* データが入力されると 各層のノード（ニューロン，あるいは，シナプス，あるいは，ユニット）は入力値とその入力に付随する重み係数 (結合係数) 掛け合わせて 答えを計算します。
* 正しい答えを出すようにシステムを訓練するには この出力を 入力に対して出力が完全に一致した場合の出力と比較して， その差分を用いてノード間の重みを調整します。
* この処理をより複雑にしたものが ディープニューラルネットワーク (深層学習 あるいは DNN) と呼ばれるモデルです。この場合のディープとは，中間層が複数個存在することを意味します。
* Google の親会社である Alphabet が所有するロンドンの AI 研究会社 ディープマインド テクノロジー社 (DeepMind Technologies) は この種のシステムを使って 2015 年に 囲碁のプロの人間を破ったコンピュータを開発しました。 これは機械知能の勝利として広く歓迎されています。
* ですが ANN は 脳がどのように機能するかの大まかなしか捉えていません。
* 例えば 上記 ディープマインド の アルファ碁では シナプス (神経細胞間の結合) を 行列としてモデル化しています。 ですが， 実際には 化学的および電気的活動を利用して信号を送信または終了し 隣接するシナプスとダイナミックなパターンで相互作用する複雑な生物学的機械の一部です。
* この意味で，現在のディープラーニングモデルは おおよそ現実の脳の行う計算とは異なっています。神経細胞のスパイク発火と 我々の認識とを結びつけるには，なお大きな溝があるという意味です。ですが，これは大きな一歩でもあり，心の働き，我々の活動を理解する上で大きな一里塚 (マイルストーン)  にはなっています。 <!--"あなたは シナプスとは何なのか、その真実からは遠く離れています。シナプスの正体は、行列の中のひとつの数字にすぎません。
スシローは言う。-->


# 第 1 次ニューロブーム

## 1950年代:
- ウォーレン・マッカロックとワイルダー・ピッツによる **形式ニューロン** の提案
(サイバネティクスの創始者ノーバート・ウィーナーの集めた研究者集団)

<center>
<img src='https://komazawa-deep-learning.github.io/assets//mcculloch.jpg' style="width:38%">
<img src='https://komazawa-deep-learning.github.io/assets//pitts.jpg' style='width:50%'><br>
ウォーレン・マッカロック と ワイルダー・ピッツ<br>
<!--img src='../assets/mcculloch.jpg' style="width:19%">
<img src='../assets/pitts.jpg' style='width:25%'><br>-->
</center>

形式ニューロンは，シナプス結合荷重ベクトルと出力を決定するための伝達関数とで構成される(次式)

$$
y_i=\phi\left(\sum_jw_{ij}x_j\right),\label{eq:formal_neuron}
$$

ここで $y_i$ は $i$ 番目のニューロンの出力，$x_j$ は $j$ 番目のニューロンの出力，$w_{ij}$ はニューロン $i$ と
$j$ との間の **シナプス結合荷重**。
$\phi$ は活性化関数。

<center>
<img src='https://komazawa-deep-learning.github.io/assets//Formal_r.svg' style="width:84%"><br>
形式ニューロン
</center>


# 最近の動向

## 1. ChatGPT と心理学 <!-- source: https://www.assemblyai.com/blog/how-chatgpt-actually-works/ -->

各所で ChatGPT で遊ぶ人が現れ，話題となっています。
では，ChatGPT は実際にどのように機能するのでしょうか？
あるいは，この授業での学習内容と ChatGPT とは，どのように関係するのでしょうか。
ここでは，chatGPT と心理学との関係を探ってみます。

まず，chatGPT という言葉の意味を確認しましょう。
chat は，英単語そのもで，`おしゃべり` という意味です。
すなわち，オンライン上で対話が可能なプログラムであることを意味するため Chat と名付けられました。

次の GPT とは，何でしょうか？


<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_1.png" width="77%">
<!-- <img src="figures/2023Ruby_fig0.jpg" width="55%"> -->
</div>

GPT とは，以下のような単語の頭文字です。

* **G**: Generative 生成的
* **P**: Pre-trained 訓練済
* **T**: Transformer トランスフォーマー

それぞれが，キーワードとしてこの授業でも取り上げる予定です。
**生成的** とは，対になる単語，**認識** に対する用語です。
心理学に限らず，機械学習，マシンビジョン，AI などの関連諸分野で，認識モデルと生成モデルとは，しばしば対比して扱われます。

画像認識，音声認識，など，入力刺激を受け取って，その入力が何であるかを識別するモデルが認識モデルです。
一方，モデル内部にある情報から，出力情報を作り出すモデルを生成モデルと呼びます。
すなわち，認識モデルと生成モデルとは，情報の流れが反対方向のモデルを指します。

* 認識モデル: インプット $\rightarrow$ 内部表現
* 生成モデル: アウトプット $\leftarrow$ 内部表現


その内部構造の詳細は公表されていません。
ですが，最近の研究からその機能原理を整理することができます。
ここでは，推察可能な機能原理と心理学との関係を考えてみます。

もちろん，対話に特化した言語モデルですが，対話に関した人工知能の話題として **チューリングテスト** があります。
チューリングテストは，人工知能の話題でもありますが，近年では，心理学者が協力して，改訂版が提案されています。
ニューヨーク大学のマーカス・ガリー (本当に賢い AI を見分ける新チューリングテスト，日経サイエンス 2017年7月号)

まず，GPT は **大規模言語モデル (LLM: Large Language Model)** です。
ちなみに，**GPT** すなわち openAI が開発したモデルは，東海岸発であり，
西海岸発のモデルは **BERT** と呼びます。すなわち BERT は Google 発のモデルです。
いずれのモデルも，トランスフォーマー transformer に基づいています。
ちなみに，BERT は，セサミストリートのキャラクタの一つです。
[原著論文](https://arxiv.org/abs/1810.04805/) によれば，Bidirectional Encoder Representations from Transformers の頭文字をとって命名されたことになっています。

[トランスフォーマー](https://arXiv.org/abs/1706.03762) は，**自己注意** に基づく言語モデルです。

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_2.png" width="77%">
<!-- <img src="figures/2023Ruby_fig3.jpg" width="55%"> -->
</div>

注意は，心理学のテーマの一つでもあります。
そうすると，心理学で使われている注意と，トランスフォーマーで使わている注意とは，同じか，それとも，異なるのかという疑問が湧きます。


加えて chatGPT では，**事前学習** と **微調整 (fine tuning)** の組み合わせで，話題となっているような性能を達成しました。
chatGPT のファインチューニングが 2 段階に渡って行われたようです。

<img src="/2023assets/2022Quyang_instructGPT_fig2ja.svg" style="width:144%;">
<!-- <img src="/2023assets/ChatGPT_Diagram.svg" style="width:114%"> -->
<!-- <img src="https://openaicom.imgix.net/cf717bdb-0c8c-428a-b82b-3c3add87a600/ChatGPT_Diagram.svg" width="77%"> -->

この中で，chatGPT の特徴としては，**強化学習** を用いたことが挙げられます。
この強化学習は **RLHF (Reinforcement Learning with Human Feedback)** と呼ばれます。
なぜ，RLHF が必要だったのかというと，**ミスアラインメント問題の解消** だと言われています。
ミスアラインメントとは，対話の中で話題がずれることを指します。

例えば，chatGPT の前身である GPT-3 では以下のような出力をすることがあることが知られていました:

* ユーザの明示的な指示に従わない，**親切さに欠ける helpfulness** 出力が得られる
* 存在しない，あるいは誤った事実を反映したた **幻想  hallucinations** が含まれる
* モデルがどのように特定の決定や予測に至ったかを人間が **理解することが困難  Lack interpretability** な解釈可能性を欠く
* 有害または攻撃的で，誤った情報を広める **有害または偏った  harmful or offensive** コンテンツを含む

ChatGPT では，標準的な LLM のこれらの固有の問題を解決するために，RLHF を含む 3 段階が導入されました。

### ステップ 1：教師あり微調整 (SFT: Supervised Fine Tuning) モデル <!-- ### Step 1: Supervised Fine Tuning (SFT) Model-->

最初の開発では，GPT-3 モデルの微調整を行うため，40 人の契約社員を雇い，モデルが学習するための既知の出力を持つ入力の教師付き学習データセットを作成した。
入力 (プロンプト) は，実際に Open API に入力されたユーザから収集されたものである。
ラベラはプロンプトに対して適切な応答を記述することで，各入力に対して既知の出力を作成する。
この新しい教師ありデータセットを用いて GPT-3 モデルを微調整し，GPT-3.5 (SFT モデルとも呼ばれる) を作成した。
<!-- The first development involved fine-tuning the GPT-3 model by hiring 40 contractors to create a supervised training dataset, in which the input has a known output for the model to learn from.
Inputs, or prompts, were collected from actual user entries into the Open API.
The labelers then wrote an appropriate response to the prompt thus creating a known output for each input.
The GPT-3 model was then fine-tuned using this new,supervised dataset, to create GPT-3.5, also called the SFT model. -->

プロンプトデータセットの多様性を最大化するため，任意のユーザー ID から得られるプロンプトは 200 件までとし，共通の長い接頭辞 shared long common prefixes を持つプロンプトは削除された。
最後に，特定可能な個人情報 (PII) を含むプロンプトはすべて削除された。
<!-- In order to maximize diversity in the prompts dataset, only 200 prompts could come from any given user ID and any prompts that shared long common prefixes were removed.
Finally, all prompts containing personally identifiable information (PII) were removed. -->

OpenAI API からプロンプトを集計した後，ラベラーには，実際のサンプルデータが少ないカテゴリーを埋めるためのサンプルプロンプトの作成も依頼した。
対象となったカテゴリは以下の通り：
<!-- After aggregating prompts from OpenAI API, labelers were also asked to create sample prompts to fill-out categories in which there was only minimal real sample data.
The categories of interest included:-->

* 平易なプロンプト: 任意の質問
* 少数撃プロンプト: 複数のクエリとレスポンスのペアを含む指示
* ユーザーベースのプロンプト: OpenAI API に要求された特定のユースケースに対応

<!-- * Plain prompts: any arbitrary ask.
* Few-shot prompts: instructions that contain multiple query/response pairs.
* User-based prompts: correspond to a specific use-case that was requested for the OpenAI API. -->

応答を生成する際，ラベラはユーザからの指示が何であるかを推察することに全力を尽くすよう求められた。

<!-- <div class="figure figcenter">
<img src="/2023assets/ChatGPT_Diagram.svg" width="77%">
<img src="figures/2023Ruby_fig4.jpg">
</div>-->

### ステップ 2: 報酬モデル <!-- ### Step 2: Reward Model-->

ステップ 1 で SFT モデルを訓練した後，このモデルはユーザのプロンプトに対してより適切な応答を生成する。
このモデルの入力は一連のプロンプトと応答であり，出力は報酬と呼ばれるスカラ値である。
報酬モデルは，モデルが報酬を最大化するように出力を生成することを学習する強化学習 (ステップ 3 参照) を活用するために必要なものである。
報酬モデルを訓練するために，ラベラには 1 つの入力プロンプトに対して 4～9 個の SFT モデル出力が提示される。
そして，これらの出力をベストからワーストにランク付けするよう求められ，以下のような出力ランクの組み合わせが作成される。
<!-- After the SFT model is trained in step 1, the model generates better aligned responses to user prompts.
The next refinement comes in the form of training a reward model in which a model input is a series of prompts and responses, and the output is a scaler value, called a reward.
The reward model is required in order to leverageReinforcement Learning in which a model learns to produce outputs to maximize its reward (see step 3).
To train the reward model, labelers are presented with 4 to 9 SFT model outputs for a single input prompt.
They are asked to rank these outputs from best to worst, creating combinations of output ranking as follows. -->

各組み合わせを個別のデータ点としてモデルに含めると，過学習 (見たデータ以上の外挿ができない) が発生した。
そこで，各順位群を 1 つのデータ点として活用し，モデルを構築した。
<!-- Including each combination in the model as a separate data point led to overfitting (failure to extrapolate beyondseen data).
To solve, the model was built leveraging each group of rankings as a single batch data point. -->

### ステップ 3: 強化学習モデル <!-- ### Step 3: Reinforcement Learning Model-->

最終段階では，モデルにランダムなプロンプトを提示し，応答を返す。
応答は，モデルがステップ 2 で学習した「方針」を用いて生成される。
方針は，機械が目標を達成するために学習した戦略であり，この場合，報酬を最大化することである。
ステップ 2 で開発された報酬モデルに基づいて，プロンプトと応答の対に対してスカラ報酬値が決定される。
この報酬は，方針を進化させるためにモデルにフィードバックされる。
<!-- In the final stage, the model is presented with a random prompt and returns a response.
The response is generated using the ‘policy’ that the model has learned in step 2. The policy represents a strategy that the machine has learned to use to achieve its goal; in this case, maximizing its reward.
Based on the reward model developed instep 2, a scaler reward value is then determined for the prompt and response pair.
The reward then feeds back into the model to evolve the policy. -->

2017 年 Schulman+ は，各応答が生成されるたびにモデルの方針を更新する際に使用される手法である Proximal Policy Optimization (PPO) を発表した。
PPO では，SFT モデルからトークンごとの KL (Kullback-Leibler) ペナルティを組み込んでいる。
KL ダイバージェンスは，2 つの分布関数の類似性を測定し，極端な距離にはペナルティを与える。
この場合，KL ペナルティを使用することで，ステップ 1 で学習した SFT モデル出力から応答が離れる距離を減らし，報酬モデルを最適化しすぎて人間の意図データセットから大きく逸脱するのを防ぐ。
<!-- In 2017, Schulman+ introduced Proximal Policy Optimization (PPO), the methodology that is used in updating the model’s policy as each response is generated.
PPO incorporates a per-token Kullback–Leibler (KL) penalty from the SFT model.
The KL divergence measures the similarity of two distribution functions and penalizes extreme distances.
In this case, using a KL penalty reduces the distance that the responses can be from the SFTmodel outputs trained in step 1 to avoid over-optimizing the reward model and deviating too drastically from the human intention dataset. -->

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_3.png" width="77%">
<!-- <img src="figures/2023Ruby_fig7.jpg" width="55%"> -->
</div>

## モデルの評価 <!-- ## Evaluation of the Model-->

モデルの評価は，訓練中に，モデルが見たことのない検証セットを用意することで行われる。
検証セットでは，モデルが前身である GPT-3 よりも優れているかどうかを判断するために，一連の評価が行われる。
<!-- Evaluation of the model is performed by setting aside a test set during training that the model has not seen.
On thetest set, a series of evaluations are conducted to determine if the model is better aligned than its predecessor, GPT-3. -->

* **有益性 helpfulness**: モデルの出力は，ラベラにとって好ましいものであった。
ラベラは GPT-3 よりも InstructGPT の出力を 85±3 %の確率で好んだ。
* **真実性 truthfulness**: モデルの偽りの傾向。PPO モデルは TruthfulQA データセットを用いて評価した場合，真実性と情報性がわずかに増加する出力を生成した。
* **無害性 harmlessness**: 不適切な内容、軽蔑的な内容、否定的な内容を回避する能力。無害性は，RealToxicityPrompts データセットを用いて検証された。テストは 3 つの条件下で行われた。

<!-- * **Helpfulness**: the model’s ability to infer and follow user instructions. Labelers preferred outputs from InstructGPTover GPT-3 85 ± 3% of the time.
* **Truthfulness**: the model’s tendency for hallucinations. The PPO model produced outputs that showed minor increases in truthfulness and informativeness when assessed using the TruthfulQA dataset.
* **Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset. The test was performed under three conditions. -->

1. 尊敬に値する回答をするよう指示された場合: 有害な回答が有意に減少した。
2. 敬意を設定せずに応答を行うように指示した場合: 毒性に大きな変化はない。
3. 有害な反応をするように指示した場合: GPT-3 モデルよりも有意に有害な反応をするようになった。


### 2. Diffusion model

画像系の生成モデルとして，**拡散モデル diffusion modeling** が注目されています。
今年に入って，数々のサイトが公開され，プロの絵師顔負けの画像を生成することが可能です。
おそらく，知っている方も多いと思いますが，私が適当に作ってみた画像を下に載せます。

<img src="/2023assets/DreamShaper_32_masterpiece_realistic_portrait_of_a_girl_beauti_2.jpg" width="29%">

* 次のリンクから絵を作成することができます: [Leonardoai<img src="https://app.leonardo.ai/img/leonardo-logo.png" width="2%">](https://app.leonardo.ai/ai-generations)

