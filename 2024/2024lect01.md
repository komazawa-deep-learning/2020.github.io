---
title: 第01回 2024年度開講 駒澤大学 心理学特講 IIIA
author: 浅川 伸一
layout: home
---
<link href="/css/asamarkdown.css" rel="stylesheet">
<div align="center">
<font size="+1" color="navy"><strong>ディープラーニングの心理学的解釈</strong></font><br/><br/>
<!-- <img src="/assets/header_logo.png" sytle="width:09%"> -->
</div>

<div align='right'>
<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>
Date: 15/Apr/2023<br/>
Appache 2.0 license<br/>
</div>

# 本日のメニュー

1. [chatGPT について](#1-chatgpt-と心理学)
2. [画像生成 diffusion モデル](#2-diffusion-model)
3. [企業紹介，就職情報 :およそ30分](#関連企業団体)
4. [用語](#用語の理解と区別)


## 質問

1. [関心のある分野を教えて下さい](https://webclass.komazawa-u.ac.jp/webclass/show_frame.php?set_contents_id=6bfcb87ecc866898835cb3752a796f90){:target="_blank"}
2. [関連授業の履修状況](https://webclass.komazawa-u.ac.jp/webclass/qstn_frame.php?set_contents_id=023cb22aa6e60dcba9dbe78b3b661007){:target="_blank"}
2. [プログラミングの経験を教えて下さい](https://webclass.komazawa-u.ac.jp/webclass/qstn_frame.php?set_contents_id=60bc9dc1dea077b16ff4a5040c9f1cec){:target="_blank"}
4. [高等学校までの数学，情報科目の履修状況について教えて下さい](https://webclass.komazawa-u.ac.jp/webclass/show_frame.php?set_contents_id=221bd5d883547dce424a91baae17d0f1){:target="_blank"}
3. 統計学の知識について教えて下さい
推測統計学，ベイズ統計学，


## 情勢

2025 年度から大学入試科目に「情報」が加わります。
データサイエンス，人工知能系のエンジニアの育成が急務です。
すなわち，資格，就職にとって武器になり得るでしょう。
実際に，検索すれば，文化系だけどデータサイエンティストになった，という宣伝をする YouTuber が多数見つかります。

加えて，2022 年度から，駒澤大学ではデータサイエンス AI コースが始まったようです。
本授業は，これら関連授業とも関連します。
カリキュラムを観る限りでは，本授業は，関連授業よりも，やや高度な内容を含むようです。

* [政府、AI 人材年 25 万人育成へ　全大学生に初級教育](https://www.nikkei.com/article/DGXMZO42932250W9A320C1SHA000/)
* [AI人材25万人目標達成へ 政府、統合イノベ戦略を閣議決定](https://www.nikkei.com/article/DGXMZO46386930R20C19A6EAF000/)
* [東大など　学生が学ぶべき「AIリテラシー」を定義](https://www.nikkei.com/article/DGXMZO56107550X20C20A2000000/)
* [引く手あまたのデータサイエンティスト 学生は及び腰?](https://style.nikkei.com/article/DGXMZO47952800Q9A730C1000000)
* [高等学校情報科「情報Ⅱ」教員研修用教材(本編)](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/mext_00742.html)
* [文部科学省 高等学校情報科「情報Ⅰ」教員研修用教材（本編）](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/1416756.htm){:target="_blnak"}
    * [上サイト第 3 章コンピュータとプログラミング](https://www.mext.go.jp/component/a_menu/education/micro_detail/__icsFiles/afieldfile/2019/10/09/1416758_005.pdf){:target="_blank"} Python でサンプルコードが書かれている
* [高等学校情報科「情報Ⅱ」教員研修用教材(本編) 第3章 情報とデータサイエンス 後半](https://www.mext.go.jp/content/20200609-mxt_jogai01-000007843_007.pdf)  160 ページ以降にニューラルネットワークの記述あり。
    * [上サイト第3章前半 情報とデータサイエンス pdf ファイル](https://www.mext.go.jp/content/20200702-mxt_jogai01-000007843_004.pdf)
    * [上サイト第3章後半 pdf ファイル](https://www.mext.go.jp/content/20200609-mxt_jogai01-000007843_007.pdf)
    * [上サイト第 4 章情報通信ネットワークとデータの活用](https://www.mext.go.jp/content/20200722-mxt_jogai02-100013300_006.pdf){:target="_blank"} テキストマイニングで R with Mecab -->

## AI 人材不足という社会背景

- AI 人材の不足。経済産業省の[調査報告書](https://www.meti.go.jp/policy/it_policy/jinzai/houkokusyo.pdf) によれば，2030 年には最大で 79 万人の IT 人材不足との試算があります
(同報告書 26 ページ)。
- 自分の強みを活かして，よりよい人生を送るための選択肢として，AI エンジニアという，あるいは AI 関連業界に身を置くという選択肢があります。
− この授業の目的の一つは，そのような職業選択へ向けての道標を示すことも含まれます。


## 最近の動向

### 1. ChatGPT と心理学 <!-- source: https://www.assemblyai.com/blog/how-chatgpt-actually-works/ -->

各所で ChatGPT で遊ぶ人が現れ，話題となっています。
では，ChatGPT は実際にどのように機能するのでしょうか？
あるいは，この授業での学習内容と ChatGPT とは，どのように関係するのでしょうか。
ここでは，chatGPT と心理学との関係を探ってみます。

まず，chatGPT という言葉の意味を確認しましょう。
chat は，英単語そのもで，`おしゃべり` という意味です。
すなわち，オンライン上で対話が可能なプログラムであることを意味するため Chat と名付けられました。

次の GPT とは，何でしょうか？


<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_1.png" width="77%">
<!-- <img src="figures/2023Ruby_fig0.jpg" width="55%"> -->
</div>

GPT とは，以下のような単語の頭文字です。

* **G**: Generative 生成的
* **P**: Pre-trained 訓練済
* **T**: Transformer トランスフォーマー

それぞれが，キーワードとしてこの授業でも取り上げる予定です。
**生成的** とは，対になる単語，**認識** に対する用語です。
心理学に限らず，機械学習，マシンビジョン，AI などの関連諸分野で，認識モデルと生成モデルとは，しばしば対比して扱われます。

画像認識，音声認識，など，入力刺激を受け取って，その入力が何であるかを識別するモデルが認識モデルです。
一方，モデル内部にある情報から，出力情報を作り出すモデルを生成モデルと呼びます。
すなわち，認識モデルと生成モデルとは，情報の流れが反対方向のモデルを指します。

* 認識モデル: インプット $\rightarrow$ 内部表現
* 生成モデル: アウトプット $\leftarrow$ 内部表現


その内部構造の詳細は公表されていません。
ですが，最近の研究からその機能原理を整理することができます。
ここでは，推察可能な機能原理と心理学との関係を考えてみます。

もちろん，対話に特化した言語モデルですが，対話に関した人工知能の話題として **チューリングテスト** があります。
チューリングテストは，人工知能の話題でもありますが，近年では，心理学者が協力して，改訂版が提案されています。
ニューヨーク大学のマーカス・ガリー (本当に賢い AI を見分ける新チューリングテスト，日経サイエンス 2017年7月号)

まず，GPT は **大規模言語モデル (LLM: Large Language Model)** です。
ちなみに，**GPT** すなわち openAI が開発したモデルは，東海岸発であり，
西海岸発のモデルは **BERT** と呼びます。すなわち BERT は Google 発のモデルです。
いずれのモデルも，トランスフォーマー transformer に基づいています。
ちなみに，BERT は，セサミストリートのキャラクタの一つです。
[原著論文](https://arxiv.org/abs/1810.04805/) によれば，Bidirectional Encoder Representations from Transformers の頭文字をとって命名されたことになっています。

[トランスフォーマー](https://arXiv.org/abs/1706.03762) は，**自己注意** に基づく言語モデルです。

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_2.png" width="77%">
<!-- <img src="figures/2023Ruby_fig3.jpg" width="55%"> -->
</div>

注意は，心理学のテーマの一つでもあります。
そうすると，心理学で使われている注意と，トランスフォーマーで使わている注意とは，同じか，それとも，異なるのかという疑問が湧きます。


加えて chatGPT では，**事前学習** と **微調整 (fine tuning)** の組み合わせで，話題となっているような性能を達成しました。
chatGPT のファインチューニングが 2 段階に渡って行われたようです。

<img src="https://openaicom.imgix.net/cf717bdb-0c8c-428a-b82b-3c3add87a600/ChatGPT_Diagram.svg" width="77%">

この中で，chatGPT の特徴としては，**強化学習** を用いたことが挙げられます。
この強化学習は **RLHF (Reinforcement Learning with Human Feedback)** と呼ばれます。
なぜ，RLHF が必要だったのかというと，**ミスアラインメント問題の解消** だと言われています。
ミスアラインメントとは，対話の中で話題がずれることを指します。

例えば，chatGPT の前身である GPT-3 では以下のような出力をすることがあることが知られていました:

* ユーザの明示的な指示に従わない，**親切さに欠ける helpfulness** 出力が得られる
* 存在しない，あるいは誤った事実を反映したた **幻想  hallucinations** が含まれる
* モデルがどのように特定の決定や予測に至ったかを人間が **理解することが困難  Lack interpretability** な解釈可能性を欠く
* 有害または攻撃的で，誤った情報を広める **有害または偏った  harmful or offensive** コンテンツを含む

ChatGPT では，標準的な LLM のこれらの固有の問題を解決するために，RLHF を含む 3 段階が導入されました。

### ステップ 1：教師あり微調整 (SFT: Supervised Fine Tuning) モデル <!-- ### Step 1: Supervised Fine Tuning (SFT) Model-->

最初の開発では，GPT-3 モデルの微調整を行うため，40 人の契約社員を雇い，モデルが学習するための既知の出力を持つ入力の教師付き学習データセットを作成した。
入力 (プロンプト) は，実際に Open API に入力されたユーザから収集されたものである。
ラベラはプロンプトに対して適切な応答を記述することで，各入力に対して既知の出力を作成する。
この新しい教師ありデータセットを用いて GPT-3 モデルを微調整し，GPT-3.5 (SFT モデルとも呼ばれる) を作成した。
<!-- The first development involved fine-tuning the GPT-3 model by hiring 40 contractors to create a supervised training dataset, in which the input has a known output for the model to learn from.
Inputs, or prompts, were collected from actual user entries into the Open API.
The labelers then wrote an appropriate response to the prompt thus creating a known output for each input.
The GPT-3 model was then fine-tuned using this new,supervised dataset, to create GPT-3.5, also called the SFT model. -->

プロンプトデータセットの多様性を最大化するため，任意のユーザー ID から得られるプロンプトは 200 件までとし，共通の長い接頭辞 shared long common prefixes を持つプロンプトは削除された。
最後に，特定可能な個人情報 (PII) を含むプロンプトはすべて削除された。
<!-- In order to maximize diversity in the prompts dataset, only 200 prompts could come from any given user ID and any prompts that shared long common prefixes were removed.
Finally, all prompts containing personally identifiable information (PII) were removed. -->

OpenAI API からプロンプトを集計した後，ラベラーには，実際のサンプルデータが少ないカテゴリーを埋めるためのサンプルプロンプトの作成も依頼した。
対象となったカテゴリは以下の通り：
<!-- After aggregating prompts from OpenAI API, labelers were also asked to create sample prompts to fill-out categories in which there was only minimal real sample data.
The categories of interest included:-->

* 平易なプロンプト: 任意の質問
* 少数撃プロンプト: 複数のクエリとレスポンスのペアを含む指示
* ユーザーベースのプロンプト: OpenAI API に要求された特定のユースケースに対応

<!-- * Plain prompts: any arbitrary ask.
* Few-shot prompts: instructions that contain multiple query/response pairs.
* User-based prompts: correspond to a specific use-case that was requested for the OpenAI API. -->

応答を生成する際，ラベラはユーザからの指示が何であるかを推察することに全力を尽くすよう求められた。

<!-- <div class="figure figcenter">
<img src="/2023assets/ChatGPT_Diagram.svg" width="77%">
<img src="figures/2023Ruby_fig4.jpg">
</div>-->

### ステップ 2: 報酬モデル <!-- ### Step 2: Reward Model-->

ステップ 1 で SFT モデルを訓練した後，このモデルはユーザのプロンプトに対してより適切な応答を生成する。
このモデルの入力は一連のプロンプトと応答であり，出力は報酬と呼ばれるスカラ値である。
報酬モデルは，モデルが報酬を最大化するように出力を生成することを学習する強化学習 (ステップ 3 参照) を活用するために必要なものである。
報酬モデルを訓練するために，ラベラには 1 つの入力プロンプトに対して 4～9 個の SFT モデル出力が提示される。
そして，これらの出力をベストからワーストにランク付けするよう求められ，以下のような出力ランクの組み合わせが作成される。
<!-- After the SFT model is trained in step 1, the model generates better aligned responses to user prompts.
The next refinement comes in the form of training a reward model in which a model input is a series of prompts and responses, and the output is a scaler value, called a reward.
The reward model is required in order to leverageReinforcement Learning in which a model learns to produce outputs to maximize its reward (see step 3).
To train the reward model, labelers are presented with 4 to 9 SFT model outputs for a single input prompt.
They are asked to rank these outputs from best to worst, creating combinations of output ranking as follows. -->

各組み合わせを個別のデータ点としてモデルに含めると，過学習 (見たデータ以上の外挿ができない) が発生した。
そこで，各順位群を 1 つのデータ点として活用し，モデルを構築した。
<!-- Including each combination in the model as a separate data point led to overfitting (failure to extrapolate beyondseen data).
To solve, the model was built leveraging each group of rankings as a single batch data point. -->

### ステップ 3: 強化学習モデル <!-- ### Step 3: Reinforcement Learning Model-->

最終段階では，モデルにランダムなプロンプトを提示し，応答を返す。
応答は，モデルがステップ 2 で学習した「方針」を用いて生成される。
方針は，機械が目標を達成するために学習した戦略であり，この場合，報酬を最大化することである。
ステップ 2 で開発された報酬モデルに基づいて，プロンプトと応答の対に対してスカラ報酬値が決定される。
この報酬は，方針を進化させるためにモデルにフィードバックされる。
<!-- In the final stage, the model is presented with a random prompt and returns a response.
The response is generated using the ‘policy’ that the model has learned in step 2. The policy represents a strategy that the machine has learned to use to achieve its goal; in this case, maximizing its reward.
Based on the reward model developed instep 2, a scaler reward value is then determined for the prompt and response pair.
The reward then feeds back into the model to evolve the policy. -->

2017 年 Schulman+ は，各応答が生成されるたびにモデルの方針を更新する際に使用される手法である Proximal Policy Optimization (PPO) を発表した。
PPO では，SFT モデルからトークンごとの KL (Kullback-Leibler) ペナルティを組み込んでいる。
KL ダイバージェンスは，2 つの分布関数の類似性を測定し，極端な距離にはペナルティを与える。
この場合，KL ペナルティを使用することで，ステップ 1 で学習した SFT モデル出力から応答が離れる距離を減らし，報酬モデルを最適化しすぎて人間の意図データセットから大きく逸脱するのを防ぐ。
<!-- In 2017, Schulman+ introduced Proximal Policy Optimization (PPO), the methodology that is used in updating the model’s policy as each response is generated.
PPO incorporates a per-token Kullback–Leibler (KL) penalty from the SFT model.
The KL divergence measures the similarity of two distribution functions and penalizes extreme distances.
In this case, using a KL penalty reduces the distance that the responses can be from the SFTmodel outputs trained in step 1 to avoid over-optimizing the reward model and deviating too drastically from the human intention dataset. -->

<div class="figure figcenter">
<img src="/2023assets/2023_0406chatGPT_3.png" width="77%">
<!-- <img src="figures/2023Ruby_fig7.jpg" width="55%"> -->
</div>

## モデルの評価 <!-- ## Evaluation of the Model-->

モデルの評価は，訓練中に，モデルが見たことのない検証セットを用意することで行われる。
検証セットでは，モデルが前身である GPT-3 よりも優れているかどうかを判断するために，一連の評価が行われる。
<!-- Evaluation of the model is performed by setting aside a test set during training that the model has not seen.
On thetest set, a series of evaluations are conducted to determine if the model is better aligned than its predecessor, GPT-3. -->

* **有益性 helpfulness**: モデルの出力は，ラベラにとって好ましいものであった。
ラベラは GPT-3 よりも InstructGPT の出力を 85±3 %の確率で好んだ。
* **真実性 truthfulness**: モデルの偽りの傾向。PPO モデルは TruthfulQA データセットを用いて評価した場合，真実性と情報性がわずかに増加する出力を生成した。
* **無害性 harmlessness**: 不適切な内容、軽蔑的な内容、否定的な内容を回避する能力。無害性は，RealToxicityPrompts データセットを用いて検証された。テストは 3 つの条件下で行われた。

<!-- * **Helpfulness**: the model’s ability to infer and follow user instructions. Labelers preferred outputs from InstructGPTover GPT-3 85 ± 3% of the time.
* **Truthfulness**: the model’s tendency for hallucinations. The PPO model produced outputs that showed minor increases in truthfulness and informativeness when assessed using the TruthfulQA dataset.
* **Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset. The test was performed under three conditions. -->

1. 尊敬に値する回答をするよう指示された場合: 有害な回答が有意に減少した。
2. 敬意を設定せずに応答を行うように指示した場合: 毒性に大きな変化はない。
3. 有害な反応をするように指示した場合: GPT-3 モデルよりも有意に有害な反応をするようになった。


### 2. Diffusion model

画像系の生成モデルとして，**拡散モデル diffusion modeling** が注目されています。
今年に入って，数々のサイトが公開され，プロの絵師顔負けの画像を生成することが可能です。
おそらく，知っている方も多いと思いますが，私が適当に作ってみた画像を下に載せます。

<img src="/2023assets/DreamShaper_32_masterpiece_realistic_portrait_of_a_girl_beauti_2.jpg" width="29%">

* 次のリンクから絵を作成することができます: [Leonardoai<img src="https://app.leonardo.ai/img/leonardo-logo.png" width="2%">](https://app.leonardo.ai/ai-generations)


#### 関連企業，団体

* [エクサウィザーズ](https://exawizards.com/){:target="_blank"}
* [サイトビジット](https://sight-visit.com/){:target="_blank"}
* [Gauss](https://gauss-ai.jp/){:target="_blank"}
* [KUNO](https://kuno-corp.com/company){:target="_blank"}
* [AVILEN](https://avilen.co.jp/){:target="_blank"}
* [スタンダード](https://standard-dx.com/){:target="_blank"}
* [日本ディープラーニング協会](https://www.jdla.org/){:target="_blank"}

- [ディープラーニング，ビッグデータ，機械学習](https://www.shin-yo-sha.co.jp/book/b455586.html)
- [Pythonで体験する深層学習](https://www.coronasha.co.jp/np/isbn/9784339028515/)
- [深層学習教科書 ディープラーニング G検定(ジェネラリスト)公式テキスト](https://www.amazon.co.jp/-/en/%E7%8C%AA%E7%8B%A9-%E5%AE%87%E5%8F%B8/dp/4798165948/)
- [これ 1 冊で最短合格 ディープラーニングG検定ジェネラリスト 要点整理テキスト&問題集](https://www.shuwasystem.co.jp/book/9784798057309.html)


## 用語の理解と区別

* 人工知能
* ニューラルネットワーク
* ディープラーニング (深層学習)
* データサイエンス: **データサイエンティストは 21 世紀で最もカッコいい (the sexist) 職業だ** というハーバードビジネスレビューの [ポジショントーク記事 (2012年)](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century) が話題になって久しい。
* ビッグデータ: こちらも[ポジショントークらしく学術論文は存在しない](https://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/)。
ただし [データが増え続けている](http://www.uvm.edu/pdodds/files/papers/others/2011/hilbert2011a.pdf) ことは事実なので社会的な傾向とも言える。

<div align="center">
<img src='/assets/2017Goodfellow_Fig1_4ja.svg' width="33%"><br/>
Goodfellow et al. (2017) Fig.1 を改変
</div>

## 歴史的背景

心理学の歴史を考えると，

* 1950年代 の認知革命，から認知科学が生まれ，
* 1980年代 の脳画像技術の進歩に伴って，神経科学との接点が強調されるようになりました。
* 2000年代以降の データサイエンスの流行に伴って，データを持って語ることが多くなりました。
* 2014年 ディープラーニングによる画像認識技術が注目を浴びるようになりました。
このような背景から，心理学とAI，データサイエンスとの融合が数多く試みられるようになっています。
* 2019 年以降，ディープラーニングと神経科学と認知科学との三者関係を論じる風潮が増えてます。


# AI についての素朴な疑問？

1. 機械学習とニューラルネットワーク違うの？
1. 機械学習と人工知能は違うの？
1. ニューラルネットワークと人工知能は違うの？
1. 機械学習とニューラルネットワークと人工知能は関係は？
2. ディープラーニングとニューラルネットワークは違うの？

# 人工知能 AI とは何か

- 「人工知能の基礎」（小林 一郎）
    - 人の知能，つまり，人が行なう知的作業は，推論，記憶，認識，理解，学習，創造といった現実世界に適応するため
の能力を指す．人工の「知能」とは，人の「知能」のある部分を機械に行わせることによって創られる．
- デジタル大辞泉 《artificial intelligence》コンピューターで，記憶・推論・判断・学習など，人間の知的機能を代行
できるようにモデル化されたソフトウエア・システム．AI．

シャピロ (Shapiro, Stuart C., 1992) は次の3つの分野だと書いています。

1. 計算論的心理学 Computational Psychology:  __人間の知的活動を理解するために人間のように振る舞うコンピュータプログラムを作ること__
1. 計算論的哲学 Computational Philosophy:  __人間レベルの知的活動を計算論的に理解すること。計算論的理解=コンピュータ上に実装可能なモデル__
1. 計算機科学 Advanced Computer Science:  __コンピュータ科学の拡張，発展。現在のコンピュータはプログラムされたことしか実行できないが，人間はプログラムされていなくても勝手に振る舞う。__

* Shapiro, Stuart C. (1992), "Artificial Intelligence", in Stuart C. Shapiro (ed.), Encyclopedia of Artificial Intelligence, 2nd edition (New York: John Wiley & Sons)


# 時代背景

- 18世紀 第 1 次産業革命: <span style="color:Blue">蒸気機関，都市部に大規模工場が出現</span>
- 20世紀初頭 第2次産業革命: <span style="color:Blue">電気，オートメーション化，自動車，飛行機，電車による移動
手段の変化</span>
- 20世紀後半 第3次産業革命: <span style="color:Blue">情報化，コンピュータ化，グローバル化</span>
- 21世紀から 第4次産業革命: <span style="color:Blue">AI 人間の能力を越える機械</span>

<!--
from [http://bootcamp.lif.univ-mrs.fr:8080/mainpage](http://bootcamp.lif.univ-mrs.fr:8080/mainpage)-->

<center>
<img src='/assets/2009Gray_4th_paradigm.svg' style='width:66%'><br>
Gray (2009) The 4th paradigm より
</center>

