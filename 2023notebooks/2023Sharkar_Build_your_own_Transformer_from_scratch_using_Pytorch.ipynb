{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023Sharkar_Build_your_own_Transformer_from_scratch_using_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7765dc43-eccd-40f8-bddf-ac60af4084c6",
      "metadata": {
        "id": "7765dc43-eccd-40f8-bddf-ac60af4084c6"
      },
      "source": [
        "# [Build your own Transformer from scratch using Pytorch](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb)\n",
        "\n",
        "このチュートリアルでは PyTorch を使用して基本的な Transformer モデルをゼロから構築する。\n",
        "Transformer モデルは Vaswani+ が論文 Attention is All You Need で導入したもので，機械翻訳やテキスト要約などの seq2seq 課題のために設計された深層学習アーキテクチャである。\n",
        "自己注意機構に基づいており，GPT や BERT など，多くの最先端の自然言語処理モデルの基盤となっている。\n",
        "<!-- In this tutorial, we will build a basic Transformer model from scratch using PyTorch.\n",
        "The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization.\n",
        "It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT. -->\n",
        "\n",
        "Transformer の詳細は，以下の 2 記事を参照:\n",
        "<!-- To understand Transformer models in detail kindly visit these two articles: -->\n",
        "\n",
        "1. [All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)\n",
        "2. [All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)\n",
        "\n",
        "Transformer モデルの作成においては以下の段階を踏む:<!-- To build our Transformer model, we’ll follow these steps:-->\n",
        "\n",
        "1.  必要なライブラリやモジュールをインポートする\n",
        "2.  基本的な構成要素を定義する: マルチヘッド注意，位置ごとのフィードフォワードネットワーク，および，位置符号化器\n",
        "3. 符号化器と復号化器の層を構築\n",
        "4. 符号化器と復号化器の層を組み合わせて，完全な transformer モデルを作成する。\n",
        "5. サンプルデータの作成\n",
        "6. モデルの訓練\n",
        "\n",
        "<!--1. Import necessary libraries and modules\n",
        "2. Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
        "3. Build the Encoder and Decoder layers\n",
        "4. Combine Encoder and Decoder layers to create the complete Transformer model\n",
        "5. Prepare sample data\n",
        "6. Train the model -->\n",
        "\n",
        "まずは必要なライブラリやモジュールをインポートするところから始めよう。\n",
        "<!-- Let’s start by importing the necessary libraries and modules. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7442f857-e304-4c93-9c02-62afe668e737",
      "metadata": {
        "id": "7442f857-e304-4c93-9c02-62afe668e737"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f92e416-9c9f-48da-a3f5-b9983cd9cb9b",
      "metadata": {
        "id": "3f92e416-9c9f-48da-a3f5-b9983cd9cb9b"
      },
      "source": [
        "Transformer モデルの基本的な構成要素を定義する:\n",
        "<!-- Now, we’ll define the basic building blocks of the Transformer model. -->\n",
        "\n",
        "## 1. マルチヘッド注意 <!-- ## Multi-Head Attention-->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*--TCGWYxwASbv2ra.png\" width=\"33%\"><br/>\n",
        "図 1. マルチヘッド注意\n",
        "<!-- Figure 2. Multi-Head Attention (source: image created by author) -->\n",
        "</center>\n",
        "\n",
        "マルチヘッド注意機構は，系列内の各対の位置間の注意を計算する。\n",
        "これは，入力系列の異なる側面を捉える複数の `注意ヘッド` で構成される。\n",
        "<!-- The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence.\n",
        "It consists of multiple “attention heads” that capture different aspects of the input sequence. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a3cfacf-412b-4691-8aeb-899e1828a22f",
      "metadata": {
        "id": "2a3cfacf-412b-4691-8aeb-899e1828a22f"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model:int,    # 各層の素子数\n",
        "                 num_heads:int   # ヘッド数，マルチヘッド注意の定義に必要\n",
        "                ):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model は num_heads で割り切れる数である必要がある\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "        self.W_o = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self,\n",
        "                                     Q:torch.Tensor,\n",
        "                                     K:torch.Tensor,\n",
        "                                     V:torch.Tensor,\n",
        "                                     mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self,\n",
        "                    x:torch.Tensor):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self,\n",
        "                      x:torch.Tensor):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self,\n",
        "                Q:torch.Tensor,\n",
        "                K:torch.Tensor,\n",
        "                V:torch.Tensor, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d864370-728d-487c-bcb2-a0b70a183e92",
      "metadata": {
        "id": "6d864370-728d-487c-bcb2-a0b70a183e92"
      },
      "source": [
        "MultiHeadAttention のコードは，入力パラメータと線形変換層でモジュールを初期化する。\n",
        "注意得点を計算し，入力テンソルを複数のヘッドに再整形し，すべてのヘッドからの注意出力を結合する。\n",
        "`forward` メソッドはマルチヘッド自己注意を計算し，モデルが入力系列の別の面に注意を向けることを可能にする。\n",
        "<!-- The MultiHeadAttention code initializes the module with input parameters and linear transformation layers.\n",
        "It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads.\n",
        "The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence. -->\n",
        "\n",
        "## 位置ごとのフィードフォワードネットワーク Position-wise_FeedForward_Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7dfe58-1ca1-4de8-9e34-99f4de6d5a4a",
      "metadata": {
        "id": "7c7dfe58-1ca1-4de8-9e34-99f4de6d5a4a"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model:int,\n",
        "                 d_ff:int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=d_model, out_features=d_ff)\n",
        "        self.fc2 = nn.Linear(in_features=d_ff, out_features=d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        #return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab64b9b-d7af-4ae8-8d08-d7b1363ddd89",
      "metadata": {
        "id": "0ab64b9b-d7af-4ae8-8d08-d7b1363ddd89"
      },
      "source": [
        "`PositionWiseFeedForward` クラスは、PyTorchの `nn.Module` を拡張し，位置ごとのフィードフォワードネットワークの実装である。\n",
        "このクラスは，2 つの線形変換層と ReLU 活性化関数で初期化される。\n",
        "`forward` メソッドは，これらの変換と活性化関数を順次適用して出力を計算する。\n",
        "この処理により，モデルは入力要素の位置を考慮しながら予測を行うことができる。\n",
        "<!-- The PositionWiseFeedForward class extends PyTorch’s `nn.Module` and implements a position-wise feed-forward network.\n",
        "The class initializes with two linear transformation layers and a ReLU activation function.\n",
        "The forward method applies these transformations and activation function sequentially to compute the output.\n",
        "This process enables the model to consider the position of input elements while making predictions. -->\n",
        "\n",
        "## 位置符号化器 Positional Encoding\n",
        "\n",
        "位置符号化は，入力系列の各トークンの位置情報を挿入するために使用される。\n",
        "異なる周波数の正弦波関数と余弦波関数を使用して位置情報を生成する。\n",
        "<!--Positional Encoding is used to inject the position information of each token in the input sequence.\n",
        "It uses sine and cosine functions of different frequencies to generate the positional encoding. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad593e9e-a8ad-436f-9784-00a5f2c50275",
      "metadata": {
        "id": "ad593e9e-a8ad-436f-9784-00a5f2c50275"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model:int,\n",
        "                 max_seq_length:int):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c4c16d-b440-4ba3-8365-992003257671",
      "metadata": {
        "id": "59c4c16d-b440-4ba3-8365-992003257671"
      },
      "outputs": [],
      "source": [
        "# The positional encoding will add in a sine wave based on position.\n",
        "# The frequency and offset of the wave is different for each dimension.\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(d_model=20, max_seq_length=500)\n",
        "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([f\"次元 {p}\" for p in [4,5,6,7]])\n",
        "plt.title('位置符号化器の各位置に対応する出力\\n位置符号化の下では，位置に対応した正弦波が追加される。 この正弦波の周波数とオフセットは，各次元で異なる。')\n",
        "plt.xlabel('位置')\n",
        "plt.show()\n",
        "#None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079669ae-5a93-4728-bf25-e706fa3688e9",
      "metadata": {
        "id": "079669ae-5a93-4728-bf25-e706fa3688e9"
      },
      "source": [
        "`PositionalEncoding` クラスは，入力パラメータ `d_model` と `max_seq_length` で初期化し，位置符号化器の値を格納するテンソルを作成する。\n",
        "このクラスは，スケール因子 `div_term` に基づいて，偶数インデックスと奇数インデックスの正弦波と余弦波の値をそれぞれ計算する。\n",
        "forward メソッドは，格納された位置符号化値を入力テンソルに追加することで位置符号化を計算し，モデルが入力配列の位置情報を捕捉できるようにする。\n",
        "<!-- The PositionalEncoding class initializes with input parameters `d_model` and `max_seq_length`, creating a tensor to store positional encoding values.\n",
        "The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term.\n",
        "The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence. -->\n",
        "\n",
        "では，符号化器と復号化器の層を構築する。\n",
        "<!-- Now, we’ll build the Encoder and Decoder layers. -->\n",
        "\n",
        "## 符号化器層 Encoder Layer\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*bPKV4ekQr9ZjYkWJ.png\" width=\"18%\"><br/>\n",
        "図 3. transformer ネットワークの符号化器部分\n",
        "<!-- Figure 3. The Encoder part of the transformer network (Source: image from the original paper) -->\n",
        "</center>\n",
        "\n",
        "符号化器層は，マルチヘッド注意層，位置ごとのフィードフォワード層，2 つの層正規化層で構成される。\n",
        "<!-- An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11c510b-f0a9-4cc5-993e-bae43b5051a9",
      "metadata": {
        "id": "d11c510b-f0a9-4cc5-993e-bae43b5051a9"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99f6603-f839-480b-b686-3c8e77d3afc8",
      "metadata": {
        "id": "b99f6603-f839-480b-b686-3c8e77d3afc8"
      },
      "source": [
        "EncoderLayer クラスは，入力パラメータと，`MultiHeadAttention` モジュール，`PositionWiseFeedForward` モジュール，2 つの層正規化モジュール，ドロップアウト層などの成分で初期化する。\n",
        "forward メソッドは，自己注意を適用して符号化層の出力を計算し，注意出力を入力テンソルに加え，その結果を正規化する。\n",
        "次に，位置ごとのフィードフォワード出力を計算し，正規化された自己注意出力と結合し，最終結果を正規化してから処理されたテンソルを返す。\n",
        "<!-- The EncoderLayer class initializes with input parameters and components, including a `MultiHeadAttention` module, a `PositionWiseFeedForward` module, two layer normalization modules, and a dropout layer.\n",
        "The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result.\n",
        "Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.-->\n",
        "\n",
        "## 復号化器層 Decoder Layer\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*SPZgT4k8GQi37H__.png\" width=\"18%\"><br/>\n",
        "図 4. Transformer ネットワークの復号化器部分    \n",
        "<!-- Figure 4. The Decoder part of the Transformer network (Souce: Image from the original paper) -->\n",
        "</center>\n",
        "\n",
        "復号化器層は，2 つのマルチヘッド注意層，位置ごとのフィードフォワード層，3 つの層正規化層で構成される。\n",
        "<!-- A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b328fed3-19ce-4713-8a57-35c05d579f01",
      "metadata": {
        "id": "b328fed3-19ce-4713-8a57-35c05d579f01"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b302eb-47a8-4785-8c37-d4d493342c73",
      "metadata": {
        "id": "72b302eb-47a8-4785-8c37-d4d493342c73"
      },
      "source": [
        "復号化器層は，入力パラメータと，マスクされた自己注意と交差注意のためのマルチヘッド注意モジュール，位置ごとのフィードフォワードモジュール，3  層の正規化モジュール，およびドロップアウト層などの成分で初期化する。\n",
        "<!-- The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.-->\n",
        "\n",
        "forward メソッドは，以下のステップを実行することで，復号化器層の出力を計算する:\n",
        "<!-- The forward method computes the decoder layer output by performing the following steps: -->\n",
        "\n",
        "1. マスクされた自己注意出力を計算し，入力テンソルに加算した後，ドロップアウトと層正規化を行う。\n",
        "2. 復号化器出力と符号化器出力の間の交差注意出力を計算し，正規化されたマスクされた自己注意出力に加え，ドロップアウトと層正規化を行う。\n",
        "3. 位置ごとのフィードフォワード出力を計算し，正規化された交差注意出力に加え，ドロップアウトと層正規化を行う。\n",
        "4. 処理されたテンソルを返す。\n",
        "\n",
        "<!-- 1. Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.\n",
        "2. Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.\n",
        "3. Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.\n",
        "4. Return the processed tensor. -->\n",
        "\n",
        "これらの操作により，復号化は入力と符号化出力に基づいて標的系列を生成することができる。\n",
        "<!-- These operations enable the decoder to generate target sequences based on the input and the encoder output.-->\n",
        "\n",
        "さて，符号化器と復号化器の層を組み合わせて，完全な transformer モデルを作る。\n",
        "<!--Now, let’s combine the Encoder and Decoder layers to create the complete Transformer model.\n",
        " -->\n",
        "## Transformer Model\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1090/format:webp/0*ljYs7oOlKC71SzSr.png\" width=\"33%\"><br/>\n",
        "Figure 5. The Transformer Network (Source: Image from the original paper)\n",
        "</center>\n",
        "\n",
        "Merging it all together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f409dfb-3531-40c9-bcb1-13fcbc1ee405",
      "metadata": {
        "id": "7f409dfb-3531-40c9-bcb1-13fcbc1ee405"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 num_layers,\n",
        "                 d_ff,\n",
        "                 max_seq_length,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06a7ed6-b65b-40ca-9147-c41cb3ff1d6c",
      "metadata": {
        "id": "b06a7ed6-b65b-40ca-9147-c41cb3ff1d6c"
      },
      "source": [
        "Transformer クラスは，先に定義されたモジュールを組み合わせて，完全な Transformer モデルを作成する。\n",
        "初期化の際，Transformer モジュールは入力パラメータを設定し，ソースとターゲット系列用の埋め込み層，PositionalEncoding モジュール，スタック層を作成する EncoderLayer とDecoderLayer モジュール，復号化器出力を映し出すための線形層，ドロップアウト層など様々な成分を初期化する。\n",
        "<!-- The Transformer class combines the previously defined modules to create a complete Transformer model.\n",
        "During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer. -->\n",
        "\n",
        "generate_mask メソッドは，パディングトークンを無視し，復号化器が将来のトークンに注目しないように，ソースとターゲット系列に二値化マスクを作成する。\n",
        "forward メソッドは，以下のステップで Transformer モデルの出力を計算する：\n",
        "<!-- The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens.\n",
        "The forward method computes the Transformer model’s output through the following steps: -->\n",
        "\n",
        "1. generate_maskメソッドでソースマスクとターゲットマスクを生成する。\n",
        "2. ソースとターゲットの埋め込みを計算し，位置符号化とドロップアウトを適用する。\n",
        "3. ソース系列を符号化層で処理し，enc_output テンソルを更新する。\n",
        "4. 符号化出力とマスクを用いて，ターゲット系列を復号化器層で処理し，dec_output テンソルを更新する。\n",
        "5. 復号化器出力に線形射影層を適用し，出力ロジットを得る。\n",
        "\n",
        "<!-- 1. Generate source and target masks using the generate_mask method.\n",
        "2. Compute source and target embeddings, and apply positional encoding and dropout.\n",
        "3. Process the source sequence through encoder layers, updating the enc_output tensor.\n",
        "4. Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\n",
        "5. Apply the linear projection layer to the decoder output, obtaining output logits.-->\n",
        "\n",
        "これらのステップにより，Transformer モデルは，成分の組み合わせ機能に基づいて，入力系列を処理し，出力系列を生成することができる。\n",
        "<!--These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f6b1ba-d047-452e-a3e8-c51ec66168bf",
      "metadata": {
        "id": "f7f6b1ba-d047-452e-a3e8-c51ec66168bf"
      },
      "source": [
        "## サンプルデータの準備 Preparing Sample Data\n",
        "\n",
        "この例では，デモ用におもちゃのデータセットを作成する。\n",
        "実際には，より大きなデータセットを使用し，テキストを前処理し，ソース言語とターゲット言語の語彙写像を作成することになる。\n",
        "<!-- In this example, we will create a toy dataset for demonstration purposes.\n",
        "In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49edeeb9-d348-41b5-9418-5ede1e752699",
      "metadata": {
        "id": "49edeeb9-d348-41b5-9418-5ede1e752699"
      },
      "outputs": [],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8987cd-e182-48ff-8c6d-1c2ed6d6f877",
      "metadata": {
        "id": "6e8987cd-e182-48ff-8c6d-1c2ed6d6f877"
      },
      "source": [
        "## モデルの訓練 Training the Model\n",
        "\n",
        "では，サンプルデータを使ってモデルを訓練してみよう。\n",
        "実際には，もっと大きなデータセットを使って，訓練セットと検証セットに分けることになる。\n",
        "<!-- Now we’ll train the model using the sample data.\n",
        "In practice, you would use a larger dataset and split it into training and validation sets. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adea9b6-a8b1-4194-9b93-ee8bc9658e99",
      "metadata": {
        "id": "7adea9b6-a8b1-4194-9b93-ee8bc9658e99"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1:3d}, Loss: {loss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a0e472-0b04-45fe-9934-9908d3e5ae01",
      "metadata": {
        "id": "75a0e472-0b04-45fe-9934-9908d3e5ae01"
      },
      "source": [
        "この方法を使えば，Pytorch でゼロから簡単な Transformer を構築することができる。\n",
        "すべての大規模言語モデルは，これらの Transformer 符号器または復号化器ブロックを学習に使用する。\n",
        "したがって，すべてを開始したネットワークを理解することは非常に重要である。\n",
        "この記事が LLM に深入しようとする皆の役に立つことを願っている。\n",
        "<!-- We can use this way to build a simple Transformer from scratch in Pytorch.\n",
        "All Large Language Models use these Transformer encoder or decoder blocks for training.\n",
        "Hence understanding the network that started it all is extremely important.\n",
        "Hope this article helps all looking to deep dive into LLM’s. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7ff607-1f61-497c-a7dc-beb01c66df48",
      "metadata": {
        "id": "2f7ff607-1f61-497c-a7dc-beb01c66df48"
      },
      "source": [
        "## References\n",
        "\n",
        "* [Attention is all you need, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, {. Kaiser, and I. Polosukhin. Advances in Neural Information Processing Systems , page 5998–6008. (2017)](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da01980-42a2-4739-88bd-136866b11882",
      "metadata": {
        "id": "3da01980-42a2-4739-88bd-136866b11882"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}