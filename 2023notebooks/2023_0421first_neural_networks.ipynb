{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2023notebooks/2023_0421first_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CVH5_ypyigwV",
      "metadata": {
        "id": "CVH5_ypyigwV"
      },
      "source": [
        "# [HAD](https://norimune.net/had) のサンプルデータを使ってニューラルネットワーク\n",
        "\n",
        "\n",
        "* sepal (萼片 がくべん)： 花の外側の部分 (多くの場合緑色で葉のようなもの) で発達中の蕾を包む。\n",
        "* petal (花弁，花びら)：花の外側の部分で，しばしば目立つ色をしている。\n",
        "\n",
        "<img src=\"https://dictionary.goo.ne.jp/img/daijisen/ref/113205.jpg\"><br/>\n",
        "Goo 国語辞典より\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05208395-1ccb-4d3a-a3ae-456260a89f15",
      "metadata": {
        "id": "05208395-1ccb-4d3a-a3ae-456260a89f15"
      },
      "outputs": [],
      "source": [
        "# HAD データの取得\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade xlrd\n",
        "    !pip install --upgrade pandas\n",
        "\n",
        "    # HAD サンプルデータをダウンロード\n",
        "    !wget 'https://files.au-1.osf.io/v1/resources/32cyp/providers/osfstorage/5fb340145502ac018d8c86ab/?zip=' -O had.zip\n",
        "    !unzip had.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8de19d57-e873-44c6-9d87-a560f2baeafd",
      "metadata": {
        "id": "8de19d57-e873-44c6-9d87-a560f2baeafd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn.datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#X, y = sklearn.datasets.load_iris(return_X_y=True,as_frame=True)\n",
        "#_X, _y = X.to_numpy(), y.to_numpy()\n",
        "\n",
        "had_dir = '.'  if isColab else '/Users/_asakawa/study/2022HAD'\n",
        "had_samples = pd.read_excel(os.path.join(had_dir, 'HAD_sample_data.xls'),sheet_name='iris')\n",
        "X = had_samples[['Sepal.L','Sepal.W', 'Petal.L', 'Petal.W']].to_numpy()\n",
        "# sepal:萼片, petal:花弁\n",
        "# `花は内側から外に向かって、雌しべ、雄しべ、花弁、萼片、そして場合によっては苞という順番で器官が並んでいます。`\n",
        "# `ですので、雄しべのすぐ外にある器官が花弁で、そのさらに外にある器官が萼片ということになります。`\n",
        "# https://jspp.org/hiroba/q_and_a/detail.html?id=1219 より\n",
        "\n",
        "y = had_samples[['Species']].to_numpy()  # Species すなわち [`setosa`, 'verginica', 'vergicolor'] を y とする\n",
        "\n",
        "# y と同じ行数で，3 列のデータを作って _y とする。\n",
        "_y = np.zeros((y.shape[0], 3))\n",
        "\n",
        "# 上で作成した _y に値を代入する。\n",
        "for i, __y in enumerate(y):\n",
        "     _y[i,__y[0]-1] = 1\n",
        "\n",
        "# _y を y に置き換える\n",
        "y = np.copy(_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e448a228-dffa-4cfe-a7e1-f4d494349e0f",
      "metadata": {
        "id": "e448a228-dffa-4cfe-a7e1-f4d494349e0f"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリを輸入\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "X = torch.Tensor(X)  # X を PyTorch 用データへ変換\n",
        "y = torch.Tensor(y)  # y を PyTorch 用データへ変換\n",
        "lr = 0.01            # lr は学習率 `learning ratio` の頭文字\n",
        "\n",
        "\n",
        "# 多層パーセプトロンのクラス定義\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"多層パーセプトロンの定義\n",
        "    PyTorch のモデル定義。最低限，`__init__()` と `forward()` の定義が必要\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_inp, n_hid, n_out=1):\n",
        "        super().__init__()\n",
        "        self.n_inp = n_inp  # 入力層のニューロン数\n",
        "        self.n_hid = n_hid  # 中間層のニューロン数\n",
        "        self.n_out = n_out  # 出力層のニューロン数\n",
        "        \n",
        "        # 中間層への結合の定義: 入力信号を受け取って，中間層次元 (ニューロン数) へ変換\n",
        "        self.hid_layer = nn.Linear(in_features =self.n_inp, \n",
        "                                   out_features=self.n_hid,\n",
        "                                   bias=True)\n",
        "        # 出力層への結合の定義: 中間層の出力を受け取って，出力層次元 へ変換\n",
        "        self.out_layer = nn.Linear(self.n_hid, self.n_out)\n",
        "        \n",
        "        self.sigmoid = torch.nn.Sigmoid()  # シグモイド関数の定義\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 前向き (feed forward) 処理の定義\n",
        "        \n",
        "        x = self.hid_layer(x)  # 入力 x を中間層を通すことで変換\n",
        "        x = self.sigmoid(x)    # 変換した x をシグモイド関数へ通す非線形処理\n",
        "        x = self.out_layer(x)  # 非線形処理した x を出力層へ伝達\n",
        "        x = self.sigmoid(x)    # 出力層へ送られた x をシグモイド関数によって変換\n",
        "        return x               # 変換した値を返す\n",
        "\n",
        "# 多層パーセプトロンの実体化\n",
        "# 以下の例では，中間層数を 8 にしている。この値は勝手に決めて良い\n",
        "mlp = MLP(n_inp=X.shape[1], n_hid=8, n_out=y.shape[1])\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_f = nn.BCELoss()            # 2値化交差エントロピー Binary Cross Entropy loss の頭文字\n",
        "#loss_f = nn.MSELoss()           # 平均自乗誤差 Mean Squared Error の頭文字\n",
        "#loss_f = nn.CrossEntropyLoss()  # 交差エントロピー Cross Entropy \n",
        "\n",
        "# 最適化手法の定義\n",
        "optim_f = torch.optim.Adam(mlp.parameters(), lr=lr)  # Adam (https://arxiv.org/abs/1412.6980/) \n",
        "#optim_f = torch.optim.SGD(mlp.parameters(),lr=lr)   # 確率的勾配降下法 Stochastic Gradient Decent method, Bottou (2010)\n",
        "\n",
        "mlp.eval()                   # eval モードに設定。学習は行われない。\n",
        "_y = mlp(X)                  # データをモデルに通して出力 _y を得る\n",
        "_pre_loss = loss_f(_y, y)    # 出力 _y と教師信号 y に基づいて損失値 `_pre_loss` を計算\n",
        "print(f'訓練開始前の損失値:    {_pre_loss.item():.3f}')\n",
        "\n",
        "mlp.train()                  # train モードに設定。学習，すなわちパラメータの更新が行われる。\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    optim_f.zero_grad()\n",
        "    \n",
        "    _y = mlp(X)                  # モデルに処理させて出力を得る\n",
        "    loss = loss_f(_y, y)         # 損失値の計算\n",
        "\n",
        "    if epoch % (epochs>>2) == 0: # 途中結果の出力\n",
        "        print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n",
        "\n",
        "    loss.backward()              # 誤差逆伝播\n",
        "    optim_f.step()               # 重み更新。すなわち学習\n",
        "\n",
        "# 最終時刻の損失値を印字\n",
        "print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KeK1HVUsKv1h",
      "metadata": {
        "id": "KeK1HVUsKv1h"
      },
      "outputs": [],
      "source": [
        "mlp.eval()\n",
        "_y = mlp(X)\n",
        "for i,z in enumerate(_y):\n",
        "    # 結果の出力。一行に一データ。各データが 3 種類のアヤメのち，いずれに属するかを出力\n",
        "    print(f'{i:2d} {z.detach().numpy()}')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VfRw5kUeNk9C",
      "metadata": {
        "id": "VfRw5kUeNk9C"
      },
      "outputs": [],
      "source": [
        "# 上記の出力が見ずらいので若干の修正。\n",
        "# 各データごとに最大値 (np.argmax)を与える列名を印字\n",
        "np.argmax(_y.detach().numpy(),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0HqF__8tOGyJ",
      "metadata": {
        "id": "0HqF__8tOGyJ"
      },
      "outputs": [],
      "source": [
        "# 同様にして，正解データ，教師信号も出力\n",
        "np.argmax(y.detach().numpy(), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhw4YeAyOU3o",
      "metadata": {
        "id": "dhw4YeAyOU3o"
      },
      "outputs": [],
      "source": [
        "# 予測値 _y と教師信号 y とが同じであるかを判定し，同じである数を表示\n",
        "np.sum(np.argmax(_y.detach().numpy(),axis=1) == np.argmax(y.detach().numpy(), axis=1) * 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7466e75e-adfb-4083-ae75-678440bfda76",
      "metadata": {
        "id": "7466e75e-adfb-4083-ae75-678440bfda76"
      },
      "source": [
        "# 別解\n",
        "\n",
        "別解というか，今回はこっちが本質。最終層の出力にソフトマックス関数を使っている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfabdc5-1aee-4e05-97cd-e72cc290eb01",
      "metadata": {
        "id": "dcfabdc5-1aee-4e05-97cd-e72cc290eb01"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリを輸入\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "X = torch.Tensor(X)  # X を PyTorch 用データへ変換\n",
        "y = torch.Tensor(y)  # y を PyTorch 用データへ変換\n",
        "lr = 0.01            # lr は学習率 `learning ratio` の頭文字\n",
        "\n",
        "\n",
        "# 多層パーセプトロンのクラス定義\n",
        "class MLP2(nn.Module):\n",
        "    \"\"\"多層パーセプトロンの定義\n",
        "    PyTorch のモデル定義。最低限，`__init__()` と `forward()` の定義が必要\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_inp, n_hid, n_out=1):\n",
        "        super().__init__()\n",
        "        self.n_inp = n_inp  # 入力層のニューロン数\n",
        "        self.n_hid = n_hid  # 中間層のニューロン数\n",
        "        self.n_out = n_out  # 出力層のニューロン数\n",
        "        \n",
        "        # 中間層への結合の定義: 入力信号を受け取って，中間層次元 (ニューロン数) へ変換\n",
        "        self.hid_layer = nn.Linear(in_features =self.n_inp, \n",
        "                                   out_features=self.n_hid,\n",
        "                                   bias=True)\n",
        "        # 出力層への結合の定義: 中間層の出力を受け取って，出力層次元 へ変換\n",
        "        self.out_layer = nn.Linear(self.n_hid, self.n_out)\n",
        "        \n",
        "        #self.non_lin = torch.nn.ReLU()  # 整流線形化ユニット\n",
        "        self.non_lin = torch.nn.Tanh()  # 整流線形化ユニット\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 前向き (feed forward) 処理の定義\n",
        "        \n",
        "        x = self.hid_layer(x)  # 入力 x を中間層を通すことで変換\n",
        "        x = self.non_lin(x)    # 変換した x を非線形処理\n",
        "        x = self.out_layer(x)  # 非線形処理した x を出力層へ伝達\n",
        "        x = self.softmax(x)    # 出力層へ送られた x をソフトマックス変換\n",
        "        return x               # 変換した値を返す\n",
        "\n",
        "# 多層パーセプトロンの実体化\n",
        "# 以下の例では，中間層数を 8 にしている。この値は勝手に決めて良い\n",
        "mlp = MLP2(n_inp=X.shape[1], n_hid=8, n_out=y.shape[1])\n",
        "\n",
        "# 損失関数の定義\n",
        "loss_f = nn.BCELoss()            # 2値化交差エントロピー Binary Cross Entropy loss の頭文字\n",
        "#loss_f = nn.MSELoss()           # 平均自乗誤差 Mean Squared Error の頭文字\n",
        "#loss_f = nn.CrossEntropyLoss()  # 交差エントロピー Cross Entropy \n",
        "\n",
        "# 最適化手法の定義\n",
        "optim_f = torch.optim.Adam(mlp.parameters(), lr=lr)  # Adam (https://arxiv.org/abs/1412.6980/) \n",
        "#optim_f = torch.optim.SGD(mlp.parameters(),lr=lr)   # 確率的勾配降下法 Stochastic Gradient Decent method, Bottou (2010)\n",
        "\n",
        "mlp.eval()                   # eval モードに設定。学習は行われない。\n",
        "_y = mlp(X)                  # データをモデルに通して出力 _y を得る\n",
        "_pre_loss = loss_f(_y, y)    # 出力 _y と教師信号 y に基づいて損失値 `_pre_loss` を計算\n",
        "print(f'訓練開始前の損失値:    {_pre_loss.item():.3f}')\n",
        "\n",
        "mlp.train()                  # train モードに設定。学習，すなわちパラメータの更新が行われる。\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    optim_f.zero_grad()\n",
        "    \n",
        "    _y = mlp(X)                  # モデルに処理させて出力を得る\n",
        "    loss = loss_f(_y, y)         # 損失値の計算\n",
        "\n",
        "    if epoch % (epochs>>2) == 0: # 途中結果の出力\n",
        "        print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n",
        "\n",
        "    loss.backward()              # 誤差逆伝播\n",
        "    optim_f.step()               # 重み更新。すなわち学習\n",
        "\n",
        "# 最終時刻の損失値を印字\n",
        "print(f'エポック:{epoch:4d}: 損失値:{loss.item():.3f}')\n",
        "\n",
        "# 予測値 _y と教師信号 y とが同じであるかを判定し，同じである数を表示\n",
        "np.sum(np.argmax(_y.detach().numpy(),axis=1) == np.argmax(y.detach().numpy(), axis=1) * 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4985f920-f057-4561-b17b-40d175a2eba3",
      "metadata": {
        "id": "4985f920-f057-4561-b17b-40d175a2eba3"
      },
      "source": [
        "# 蛇足\n",
        "PyTorch を使わずに，すなわち，昔の流儀で MLP を実装すると以下のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b9eac7-3a61-49b2-a1ae-1b2892c27184",
      "metadata": {
        "id": "42b9eac7-3a61-49b2-a1ae-1b2892c27184"
      },
      "outputs": [],
      "source": [
        "import numpy as np                                # 必要となるライブラリの輸入\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "had_dir = '.'  if isColab else '/Users/_asakawa/study/2022HAD'\n",
        "had_samples = pd.read_excel(os.path.join(had_dir, 'HAD_sample_data.xls'),sheet_name='iris')\n",
        "X = had_samples[['Sepal.L','Sepal.W', 'Petal.L', 'Petal.W']].to_numpy()\n",
        "y = had_samples[['Species']].to_numpy()\n",
        "_y = np.zeros((y.shape[0],3))\n",
        "for i, y1 in enumerate(y):\n",
        "     _y[i,y1[0]-1] = 1\n",
        "\n",
        "y = np.copy(_y)        \n",
        "\n",
        "lr, N_hid = 0.002, 8                              # lr: 学習率, N_hid: 中間層数\n",
        "Wh = np.random.random((X.shape[1], N_hid)) - 1/2  # 入力層から中間層への結合係数の初期化\n",
        "Wo = np.random.random((N_hid, y.shape[1])) - 1/2  # 中間層から出力層への結合係数の初期化\n",
        "epochs = 10000\n",
        "#epochs = 5000\n",
        "for t in range(epochs):                           # 繰り返し\n",
        "    H  = np.tanh(np.dot(X, Wh))                   # 入力層から中間層への計算。ハイパータンジェント関数\n",
        "    _y = 1/(1. + np.exp(-(np.dot(H, Wo))))        # 中間層から出力層への計算。シグモイド関数\n",
        "    Dy = (y - _y) * (_y * (1. - _y))              # 誤差の微分\n",
        "    DH = Dy.dot(Wo.T) * (1. - H ** 2)             # 誤差逆伝播\n",
        "    Wo += lr * H.T @ Dy\n",
        "    Wh += lr * X.T @ DH                           # 中間層から入力層への重み更新\n",
        "    if t % (epochs>>2) == 0:\n",
        "        print(np.array((y-_y)**2).mean())\n",
        "\n",
        "for i,z in enumerate(_y):\n",
        "    print(f'{i:2d} {np.argmax(z)}')                          # 結果の出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-nvvcJMOrY4",
      "metadata": {
        "id": "7-nvvcJMOrY4",
        "outputId": "2e0ece36-1721-4744-e9b7-7a03d24a9285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(_y,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gSaa_2yqYbzr",
      "metadata": {
        "id": "gSaa_2yqYbzr",
        "outputId": "741e0906-c71c-4fca-a36c-7f7ad69d28f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(y,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vmwUuGrwYl7w",
      "metadata": {
        "id": "vmwUuGrwYl7w",
        "outputId": "98ab8285-e0bf-41fb-ec79-4fa8448aa8c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(np.argmax(_y,axis=1) == np.argmax(y, axis=1) * 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VyxbhNgRae7C",
      "metadata": {
        "id": "VyxbhNgRae7C"
      },
      "source": [
        "#### 積分発火モデル\n",
        "\n",
        "ニューロンが振動数符号化法のみを利用している --- すなわちニューラルネットワークにおけるすべての情報はニューロンの発火頻度によって伝達される --- ことを仮定する。\n",
        "すなわち，このニューロンに到達する信号を単位時間あたりで等しく貢献するものと考える。\n",
        "このような記述の仕方は integrate--and--fire (積分発火) モデルと呼ばれる。\n",
        "\n",
        "一つのニューロンの振る舞いは，n 個のニューロンから入力を受け取って出力を計算する多入力，1 出力の情報処理素子である (下図 1)。\n",
        "\n",
        "$n$ 個の入力信号を $x_1, x_2, \\cdots, x_n$ とし、$i$ 番目の軸索に信号が与えられたとき、この信号 1 単位によって変化する膜電位の量をシナプス荷重(または結合荷重, 結合強度とも呼ばれる) といい $w_i$ と表記する。\n",
        "抑制性のシナプス結合については $w_i < 0$, 興奮性の結合については $w_i > 0$ である。\n",
        "このニューロンのしきい値を $h$, 膜電位の変化を $u$, 出力信号を $z$ とする。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/formal_neuron.svg\"><br/>\n",
        "<!-- <img src=\"figures/formal_neuron.svg\"><br/> -->\n",
        "図 1. ニューロンの模式図\n",
        "</center>\n",
        "\n",
        "出力信号 $z$ は次式で表される:\n",
        "$$\n",
        "z=f(\\mu)=f\\left(\\sum_{i=1}^{n}w_{i}x_{i}-h\\right).\\tag{1}\n",
        "$$\n",
        "$f(\\mu)$ は出力関数であり、$0\\le f(\\mu)\\le1$ の連続量を許す場合や、0 または 1 の値しかとらない場合などがある。\n",
        "連想記憶などを扱うときなどは $-1\\le f(\\mu)\\le 1$ とする場合もある。\n",
        "\n",
        "- 上図で，$f$ がなければ，$z=\\sum wx - h$ となり，回帰と同じである。\n",
        "- すなわちニューラルネットワークのもっとも簡単な形は，統計学の用語では，**回帰 regression** である。\n",
        "- 統計学においては，回帰，この場合，線形回帰になる，にかかわる変数 $x_{i}$ \n",
        "- 回帰の中を複雑にしていくことで，データに適合させようとする努力が，データサイエンスであり，機械学習であり，ニューラルネットワークであると言える。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yW5FjWxvaqVT",
      "metadata": {
        "id": "yW5FjWxvaqVT"
      },
      "source": [
        "## マッカロック・ピッツの形式ニューロン\n",
        "\n",
        "信号入力の荷重和 \n",
        "$$\n",
        "\\mu = \\sum_{i=1}^{n}w_{i}x_{i},\\tag{2}\n",
        "$$\n",
        "に対して、出力 $z$ は $u$ がしきい値 $h$ を越えたときに $1$, そうでなければ 0 を出力するモデルのことをマッカロックとピッツの形式ニューロン (formal neuron) と呼ぶ。\n",
        "マッカロック・ピッツの形式ニューロンは神経細胞の振る舞いを記述するもっとも古く、単純な神経細胞のモデルであるが、現在でも用いられることがある。\n",
        "\n",
        "$$\n",
        "z = \\left\\{ \\begin{array}{ll}\n",
        " 1, & \\mbox{$u > 0$ のとき} \\\\\n",
        " 0, & \\mbox{それ以外}\n",
        " \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "とすればマッカロック・ピッツのモデルは次式で表すことができる\n",
        "\n",
        "$$\n",
        "z=\\mathbb{1}\\left(\\sum_{i=1}^nw_ix_i -h\\right)\\tag{3}\n",
        "$$\n",
        "\n",
        "式中の $\\mathbb{1}$ は数字ではなく 上式で表される関数の意味である。\n",
        "このモデルは、単一ニューロンのモデルとしてではなく、ひとまとまりのニューロン群の動作を示すモデルとしても用いることがある。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pkYkUZ1ta2CR",
      "metadata": {
        "id": "pkYkUZ1ta2CR"
      },
      "source": [
        "### 出力が連続関数の場合\n",
        "\n",
        "時間 $t$ における入力信号は $x_i(t)$ は $i$ 番目のシナプスの興奮伝達の時間 $t$ 付近での平均ととらえることができる。\n",
        "すると、最高頻度の出力を 1, 最低(興奮無し)を 0 と規格化できると考えて $0\\le f(\\mu) \\le1$ とする。\n",
        "入出力関係は $f$ を用いて\n",
        "\n",
        "$$\n",
        "z = f(\\mu) = f\\left(\\sum w_ix_i(t)-h\\right)\\tag{4}\n",
        "$$\n",
        "\n",
        "のように表現される。\n",
        "このモデルは、ニューロン集団の平均活動率ととらえることもできる。\n",
        "\n",
        "良く用いられる出力関数 $f$ の形としては、$\\mu = \\sum w_ix_i -h$ として、\n",
        "\n",
        "$$\n",
        "f(\\mu) = \\frac{1}{1+e^{-\\mu}},\\tag{5}\n",
        "$$\n",
        "\n",
        "や\n",
        "\n",
        "$$\n",
        "f(\\mu) = \\tanh(\\mu)\\tag{6}, \n",
        "$$\n",
        "\n",
        "ただし $\\mu = \\sum w_{i}x_{i} -h$ などが使われることが多い。\n",
        "(5) 式および (6) 式は、入力信号の重みつき荷重和 $\\mu$ としてニューロンの活動が定まることを示している。\n",
        "後述するバックプロパゲーション則で必要となるので、$\\mu$ の微小な変化がニューロンの活動どのような影響を与えるか調べるために (5) 式 および (6) 式 を $\\mu$ で微分することを考える。\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{1+e^{-x}},\n",
        "$$\n",
        "\n",
        "を $x$ について微分すると\n",
        "\n",
        "$$\n",
        "\\frac{df}{dx} = f(x)\\left(1- f(x)\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(x) = \\tanh{x} = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
        "$$\n",
        "\n",
        "を $x$ について微分すると\n",
        "\n",
        "$$\n",
        "\\frac{df}{dx} = \n",
        "1 - \\tanh^2x = 1 - \\left(f(x)\\right)^2\n",
        "$$\n",
        "\n",
        "$\\tanh$ は双曲線関数である。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n_yfcUpCa9cu",
      "metadata": {
        "id": "n_yfcUpCa9cu"
      },
      "source": [
        "以降では表記を簡単にするために線形数学の表記、すなわちベクトルと行列による\n",
        "表記方法を導入する。$n$ 個の入力信号の組 $\\left(x_1, x_2, \\cdots, x_n\\right)$ をまと\n",
        "めて $\\mathbf{x}$ のようにボールド体で表す。本章では一貫してベクトル表記には小\n",
        "文字のボールド体を、行列には大文字のボールド体を用いることにする。例えば \n",
        "$n$ 個の入力信号の組 $(x_1, x_2, \\cdots, x_n)=\\mathbf{x}$ に対して、同数の結合\n",
        "荷重 $\\left(w_1, w_2, \\cdots, w_n\\right)=\\mathbf{w}$ が存在するので、加算記号 $\\sum$ を使っ\n",
        "て表現していた任意のニューロンへの全入力 $\\mu=\\sum w_{i}x_{i}$ はベクトルの内\n",
        "積を用いて $\\mu=(\\mathbf{w}\\cdot\\mathbf{x})$ と表現される。\n",
        "\n",
        "なお、横一行のベクトルを行ベクトル、縦一列のベクトルを列ベクトルと呼ぶことがある。\n",
        "ここでは行ベクトルと混乱しないように、必要に応じて列ベクトルを表現する際には $\\{x_1, x_2, \\cdots, x_n\\}^{\\top}=\\mathbf{x}$ とベクトルの肩に $\\top$ を使って表現することもある。\n",
        "\n",
        "そして、これらをベクトル表現や行列表現で表せば、表記も簡単になり、行列の演算法則を活用することもできる。\n",
        "そのため、ニューラルネットワークに関する文献でも行列表現が用いられることが多い。\n",
        "\n",
        "図のような単純な 2  層の回路を例に説明する。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/matrix-notation.svg\"><br/>\n",
        "<!-- <img src=\"figures/matrix-notation.svg\"><br/> -->\n",
        "ネットワークの行列表現\n",
        "</center>\n",
        "\n",
        "わかりやすいように図と対応させながら、対応する行列表現とシグマ記号による表記を併記するので、よく理解した上で先に読み進んでいただきたい。\n",
        "なお、どうしても行列表現にはなじめないという読者は、行列表現の箇所だけをとばして読んでもある程度はわかるよう記述するつもりである。\n",
        "\n",
        "それでは、まず図 4 のような単純な 2 層のネットワークを例に説明しよう。\n",
        "図には、３つの入力素子 (ユニットと呼ばれることもある) \n",
        "と２つの出力素子の活性値（ニューロンの膜電位に相当する）\n",
        "\n",
        "$x_{1}, x_{2}, x_{3}$ と $y_{1}, y_{2}$，および入力素子と出力素子の結合強度を表す $w_{11}, w_{12}, \\cdots, w_{32}$ が示されている。\n",
        "これらの記号をベクトル $\\mathbf{x}$, $\\mathbf{y}$ と行列 $\\mathbf{w}$ を使って表すと $\\mathbf{y}=\\mathbf{Wx}$ となる。\n",
        "\n",
        "図\\ref{fig:matrix-notation.eps}の場合、ベクトルと行列の各要素を書き下せば、\n",
        "$$\n",
        "\\left(\\begin{array}{l}y_{1}\\\\\n",
        "y_2\\\\\n",
        "\\end{array}\\right)\n",
        "=\\left(\n",
        "\\begin{array}{lll}\n",
        "w_{11}&w_{12}&w_{13}\\\\\n",
        "w_{21}&w_{22}&w_{23}\n",
        "\\end{array}\n",
        "\\right)\n",
        "\\left(\n",
        "\\begin{array}{l}\n",
        "x_1\\\\\n",
        "x_2\\\\\n",
        "x_3\\\\\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "のようになる。　　\n",
        "\n",
        "---\n",
        "\n",
        "行列の積は、左側の行列の $i$ 行目の各要素と右側の行列（ベクトルは１列の行列でもある）の $i$ 列目の各要素とを掛け合わせて合計することなので、以下のような、加算記号を用いた表記と同じである。\n",
        "\n",
        "$$\n",
        "\\begin{array}{lllll}\n",
        "y_1 &=& w_{11}x_1 + w_{12}x_2 + w_{13}x_3 &=&\\sum_i w_{1i} x_i\\\\\n",
        "y_2 &=& w_{21}x_1 + w_{22}x_2 + w_{23}x_3 &=&\\sum_i w_{2i} x_i\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "これを、$m$ 個の入力ユニットと $n$ 個の出力ユニットの場合に一般化すれば、\n",
        "以下のようになる。\n",
        "\n",
        "\n",
        "単純な 2 層のネットワークを考える。\n",
        "$i$ 番目の出力層の各ニューロンの膜電位 $y_i,(i=1,2,\\cdots,n)$ をまとめて $\\mathbf{y}$ とベクトル表現し、同様に入力層も $\\mathbf{x}$ とする。\n",
        "ニューロン $x_{j}$ から ニューロン $y_{i}$ へのシナプス結合強度を $w_{ij}$ と表現すれば、入力層から出力層への関係は\n",
        "\n",
        "$$\n",
        "\\left(\\begin{array}{l}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right)  = \n",
        "\\left(\\begin{array}{llll}\n",
        "w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
        "w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
        "\\vdots &        & \\ddots & \\vdots \\\\\n",
        "w_{n1} & w_{n2} & \\cdots & w_{nm} \n",
        "\\end{array}\\right)\n",
        "\\left(\\begin{array}{l}\n",
        "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{array}\\right) \\\\\n",
        "\\mathbf{y} = \\mathbf{Wx}\n",
        "$$\n",
        "\n",
        "と表現できる。\n",
        "しきい値の扱いについては、常に 1 を出力する仮想的なニューロン$x_0=1$を\n",
        "考えて $W$ に組み込むことも可能である。\n",
        "\n",
        "\n",
        "実際の出力は $\\mathbf{y}$ の各要素に対して \n",
        "\n",
        "$$\n",
        "f(y) = \\frac{1}{1+e^{-y}},\n",
        "$$\n",
        "\n",
        "のような非線型変換を施すことがある。\n",
        "\n",
        "階層型のネットワークにとっては上式，非線型変換が本質的な役割を果たす。\n",
        "なぜならば、こうした非線形変換がなされない場合には、ネットワークの構造が何層になったとしても、この単純なシナプス結合強度を表す行列を $\\mathbf{W}_{i}$\n",
        "($i=1,\\cdots, p$) としたとき、$\\mathbf{W} = \\prod_{i=1}^p \\mathbf{W}_{i}$ と置くことによって本質的には 1 層のネットワークと等価になるからである。\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W}_{p}\\mathbf{W}_{p-1}\\cdots\\mathbf{W}_{1}\\mathbf{y}=\\left(\\prod_{i=1}^p\\mathbf{W}_{i}\\right)\\mathbf{y}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zbXCu8GtbByp",
      "metadata": {
        "id": "zbXCu8GtbByp"
      },
      "source": [
        "<img src=\"https://komazawa-deep-learning.github.io/assets/xor.svg\"><br/>\n",
        "\n",
        "<img src=\"https://komazawa-deep-learning.github.io/assets/xor-graph.svg\"><br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ao3qyb7a21M",
      "metadata": {
        "id": "9ao3qyb7a21M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}