{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VlE-_rY5rON"
   },
   "source": [
    "---\n",
    "* source: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "* date: 2021_1103\n",
    "* BERT の fine-turing コード\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFOTiqrtNvyy"
   },
   "source": [
    "# Transfomers ライブラリのインストール\n",
    "<!-- Install Transformers Library-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. はじめに\n",
    "<!-- # Introduction -->\n",
    "\n",
    "深層学習の進歩に伴い，リカレント・ニューラル・ネットワーク (RNN および LSTM) や畳み込みニューラル・ネットワーク (CNN) などのニューラル・ネットワーク・アーキテクチャは，テキスト分類，言語モデリング，機械翻訳など，いくつかの自然言語処理 (NLP) 課題を解決する上で，それなりの性能向上を示している。\n",
    "<!-- With the advancement in deep learning, neural network architectures like recurrent neural networks (RNN and LSTM) and convolutional neural networks (CNN) have shown a decent improvement in performance in solving several Natural Language Processing (NLP) tasks like text classification, language modeling, machine translation, etc. -->\n",
    "\n",
    "しかし，このような NLP における深層学習モデルの性能は，コンピュータビジョンにおける深層学習の性能に比べて見劣りします。\n",
    "<!-- However, this performance of deep learning models in NLP pales in comparison to the performance of deep learning in Computer Vision.  -->\n",
    "\n",
    "このように進歩が遅れている主な理由の一つは，大規模なラベル付きテキストデータセットがないことであると考えられる。\n",
    "ほとんどのラベル付きテキストデータセットは，ディープニューラルネットワークを学習するのに十分な大きさではありません。\n",
    "なぜなら，ディープニューラルネットワークは膨大な数のパラメータを持っており，小さなデータセットでそのようなネットワークを学習すると，過学習が発生するからです。\n",
    "<!-- One of the main reasons for this slow progress could be the lack of large labeled text datasets. \n",
    "Most of the labeled text datasets are not big enough to train deep neural networks because these networks have a huge number of parameters and training such networks on small datasets will cause overfitting.-->\n",
    "\n",
    "NLP がコンピュータビジョンに遅れをとっているもう一つの重要な理由は，NLP における転移学習の欠如です。\n",
    "コンピュータビジョンにおける深層学習の成功には，転移学習が大きく貢献しています。\n",
    "これは，Imagenet のような巨大なラベル付きデータセットが利用可能であり，その上で CNN ベースの深層学習モデルが学習され，その後，幅広いコンピュータビジョン課題のための事前学習済みモデルとして使用されたためです。\n",
    "<!-- Another quite important reason for NLP lagging behind computer vision was the lack of transfer learning in NLP. \n",
    "Transfer learning has been instrumental in the success of deep learning in computer vision. This happened due to the availability of huge labeled datasets like Imagenet on which deep CNN based models were trained and later they were used as pre-trained models for a wide range of computer vision tasks. -->\n",
    "\n",
    "それが NLP では，2018 年に Google 転移モデルを導入するまでは，そうではありませんでした。\n",
    "それ以来，NLP の転移学習は多くの課題を最先端の成績で解決するのに役立っています。\n",
    "<!-- That was not the case with NLP until 2018 when the transformer model was introduced by Google. \n",
    "Ever since the transfer learning in NLP is helping in solving many tasks with state of the art performance. -->\n",
    "\n",
    "この記事では，テキスト分類のために BERT をどのように微調整するかを説明します。\n",
    "<!-- In this article, I explain how do we fine-tune BERT for text classification. -->\n",
    "\n",
    "NLP を一から学びたい方は，[Python を使った自然言語処理 (NLP) コース](https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=fine_tune_BERT) をご覧ください。\n",
    "<!-- If you want to learn NLP from scratch, check out our course – Natural Language Processing (NLP) Using Python -->\n",
    "\n",
    "\n",
    "# 2. NLPにおける転移学習\n",
    "<!-- # Transfer Learning in NLP -->\n",
    "\n",
    "転移学習とは，大規模なデータセットで学習した深層学習モデルを用いて，別のデータセットで同様の課題を実行する手法です。\n",
    "このような深層学習モデルを事前訓練済みモデルと呼びます。\n",
    "事前訓練されたモデルの最も有名な例は，[ImageNet データセット](https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/?utm_source=blog&utm_medium=fine_tune_BERT) で学習されたコンピュータビジョンの深層学習モデルです。\n",
    "そのため，ゼロからモデルを構築するのではなく，問題を解決するための出発点として，事前学習済みモデルを使用するのが良いでしょう。\n",
    "<!-- Transfer learning is a technique where a deep learning model trained on a large dataset is used to perform similar tasks on another dataset. \n",
    "We call such a deep learning model a pre-trained model. \n",
    "The most renowned examples of pre-trained models are the computer vision deep learning models trained on the ImageNet dataset. \n",
    "So, it is better to use a pre-trained model as a starting point to solve a problem rather than building a model from scratch. -->\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/transfer-learning.jpeg\">\n",
    "</center>\n",
    "\n",
    "このコンピュータビジョンにおける転移学習のブレークスルーは，2012 年から 13 年にかけて起こったものです。\n",
    "しかし，最近の NLP の進歩により，NLP でも転移学習が有効な選択肢となってきました。\n",
    "<!-- This breakthrough of transfer learning in computer vision occurred in the year 2012-13. \n",
    "However, with recent advances in NLP, transfer learning has become a viable option in this NLP as well.-->\n",
    "\n",
    "テキスト分類，言語モデリング，機械翻訳など，NLP 課題のほとんどは系列モデリング課題です。\n",
    "従来の機械学習モデルやニューラルネットワークでは，テキストに存在する連続した情報を捉えることができません。\n",
    "そこで，リカレント・ニューラル・ネットワーク (RNN や LSTM) を使うようになりました。\n",
    "<!-- Most of the tasks in NLP such as text classification, language modeling, machine translation, etc. are sequence modeling tasks. \n",
    "The traditional machine learning models and neural networks cannot capture the sequential information present in the text. \n",
    "Therefore, people started using recurrent neural networks (RNN and LSTM) because these architectures can model sequential information present in the text. -->\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/RNN.png\"><br/>\n",
    "    典型的な RNN \n",
    "</center>\n",
    "\n",
    "しかし，このリカレント・ニューラル・ネットワークにも問題があります。\n",
    "大きな問題の 1 つは，RNN は一度に 1 つの入力を取るため，並列化できないことです。\n",
    "文字列の場合，RNN や LSTM は，一度に 1 つのトークンを入力として受け取ります。\n",
    "つまり，トークンごとに系列を通過することになります。\n",
    "そのため，このようなモデルを大規模なデータセットで学習するには，膨大な時間がかかります。\n",
    "<!-- However, these recurrent neural networks have their own set of problems. \n",
    "One major issue is that RNNs can not be parallelized because they take one input at a time. \n",
    "In the case of a text sequence, an RNN or LSTM would take one token at a time as input. \n",
    "So, it will pass through the sequence token by token. \n",
    "Hence, training such a model on a big dataset will take a lot of time.-->\n",
    "\n",
    "そのため，NLP における転移学習の必要性が高まっていたのです。\n",
    "2018 年，Google が Attention is All You Need という論文の中で転移学習を導入し，それがNLP の画期的な一里塚となりました。\n",
    "<!-- So, the need for transfer learning in NLP was at an all-time high. \n",
    "In 2018, the transformer was introduced by Google in the paper “Attention is All You Need” which turned out to be a groundbreaking milestone in NLP. -->\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/transformer.png\"><br/>\n",
    "トランスフォーマーモデル (Source: https://arxiv.org/abs/1706.03762)\n",
    "</center>\n",
    "\n",
    "ほどなく，さまざまな NLP 課題のために，さまざまなトランスフォーマーベースのモデルが登場し始めました。\n",
    "トランスフォーマーベースのモデルを使用することには複数の利点がありますが，最も重要なものは以下の通りです。\n",
    "<!-- Soon a wide range of transformer-based models started coming up for different NLP tasks. \n",
    "There are multiple advantages of using transformer-based models, but the most important ones are:-->\n",
    "\n",
    "* 第 1 の利点\n",
    "<!-- * First Benefit -->\n",
    "\n",
    "これらのモデルは，入力系列をトークンごとに処理するのではなく，系列全体を一度に入力とします。\n",
    "これは RNN ベースのモデルに比べて大幅に改善されています。\n",
    "<!-- These models do not process an input sequence token by token rather they take the entire sequence as input in one go which is a big improvement over RNN based models because now the model can be accelerated by the GPUs. -->\n",
    "\n",
    "* 第 2 の利点\n",
    "<!-- * 2nd Benefit -->\n",
    "\n",
    "これらのモデルを事前に学習させるためのラベル付きデータは必要ありません。\n",
    "つまり，膨大な量のラベルのないテキストデータを提供するだけで，トランスフォーマーベースのモデルを学習することができます。\n",
    "この学習されたモデルは、テキスト分類、名前付きエンティティ認識、テキスト生成など、他の NLP 課題に使用することができます。\n",
    "これが NLP における伝達学習の仕組みです。\n",
    "<!-- We don’t need labeled data to pre-train these models. \n",
    "It means that we have to just provide a huge amount of unlabeled text data to train a transformer-based model. \n",
    "We can use this trained model for other NLP tasks like text classification, named entity recognition, text generation, etc. \n",
    "This is how transfer learning works in NLP. -->\n",
    "\n",
    "BERT と GPT-2 は，最も一般的な変換器ベースのモデルであり，この記事では，BERT に焦点を当て，事前に訓練された BERT モデルを使用してテキスト分類を行う方法を学びます。\n",
    "<!-- BERT and GPT-2 are the most popular transformer-based models and in this article, we will focus on BERT and learn how we can use a pre-trained BERT model to perform text classification. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. モデルのファイン・チューニングとは何か？\n",
    "<!-- # 3. What is Model Fine-Tuning?-->\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) は，大規模なニューラルネットワークアーキテクチャであり，1 億から 3 億を超える膨大な数のパラメータを持っています。\n",
    "そのため，小さなデータセットで BERT モデルをゼロから訓練すると，過学習になってしまいます。\n",
    "<!--\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. \n",
    "So, training a BERT model from scratch on a small dataset would result in overfitting.-->\n",
    "\n",
    "そのため，巨大なデータセットで事前に訓練された BERT モデルを出発点として使用するのがよいでしょう。\n",
    "その後，比較的小さいデータセットでモデルをさらに訓練することができ，この過程はモデルのファインチューニングとして知られています。\n",
    "<!-- So, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. \n",
    "We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning. -->\n",
    " \n",
    "## 3.1 さまざまな微調整の手法\n",
    "<!-- ## 3.1 Different Fine-Tuning Techniques -->\n",
    "\n",
    "* アーキテクチャー全体の学習 - 事前に学習したモデル全体をデータセットでさらに学習し，その出力をソフトマックス層に与えることができます。\n",
    "この場合，エラーはアーキテクチャ全体にバックプロパゲーションされ，事前に学習したモデルの重みが新しいデータセットに基づいて更新されます。\n",
    "<!-- * Train the entire architecture – We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset. -->\n",
    "* 一部の層を学習し，他の層は凍結する - 学習済みのモデルを使用するもう一つの方法は，部分的に学習することです。\n",
    "モデルの初期層の重みを固定したまま，上位層のみを再学習することができます。\n",
    "数層の結合係数を凍結し，他の数層の結合係数を訓練するかを試してみましょう。\n",
    "<!-- * Train some layers while freezing others – Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained. -->\n",
    "* アーキテクチャ全体の凍結 - モデルのすべての層を凍結し，独自のニューラルネットワーク層をいくつか追加して，この新しいモデルを訓練することもできます。\n",
    "なお、モデルの訓練中は，接続された層の結合係数のみが更新されます。\n",
    "<!-- * Freeze the entire architecture – We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training. -->\n",
    "\n",
    "このチュートリアルでは，3 つ目の方法を使います。\n",
    "微調整中の BERT の全ての層を凍結し，アーキテクチャに密な層とソフトマックス層を追加します。\n",
    "<!-- In this tutorial, we will use the third approach. We will freeze all the layers of BERT during fine-tuning and append a dense layer and a softmax layer to the architecture. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERT の概要\n",
    "<!-- # 4. Overview of BERT -->\n",
    "\n",
    "BERT について聞いたことがあり，それがいかに驚異的であるか，そしてそれがいかに NLP の状況を変える可能性があるかについて読んだことがあるでしょう。\n",
    "しかし，そもそも BERT とは何でしょうか？\n",
    "<!-- You’ve heard about BERT, you’ve read about how incredible it is, and how it’s potentially changing the NLP landscape. \n",
    "But what is BERT in the first place? -->\n",
    "\n",
    "BERT を開発した研究チームは，NLP の枠組みについてこう説明しています。\n",
    "<!-- Here’s how the research team behind BERT describes the NLP framework: -->\n",
    "\n",
    "> BERTとは，Bidirectional Encoder Representations from Transformers の略です。\n",
    "これは，左右両方の文脈を共同で条件付けすることにより，ラベルのないテキストから深層双方向表現を事前に学習するように設計されています。\n",
    "その結果，事前学習された BERT モデルは，たった 1 つの出力層を追加するだけで微調整が可能となり，幅広い NLP 課題に対応した最先端のモデルを作成することができます。\n",
    "<!-- “BERT stands for Bidirectional Encoder Representations from Transformers. \n",
    "It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. \n",
    "As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.” -->\n",
    "\n",
    "出発点としてはあまりにも複雑に聞こえますね。\n",
    "しかし，これは BERT の機能をうまくまとめているので，それを分解してみましょう。\n",
    "<!-- That sounds way too complex as a starting point. \n",
    "But it does summarize what BERT does pretty well so let’s break it down. -->\n",
    "\n",
    "まず BERT とは Bidirectional Encoder Representations from Transformers の略です。\n",
    "ここでの各単語には意味があり，本記事ではそれを 1 つずつ紹介していきます。\n",
    "今のところ，この行から得られる重要なポイントは 「BERT は Transformer のアーキテクチャに基づいている」ということです。\n",
    "次に BERT は，Wikipedia全 体 (25億語！) と Book Corpus (8億語) を含む，ラベル付けされていないテキストの大規模なコーパスで事前学習されています。\n",
    "<!-- Firstly, BERT stands for Bidirectional Encoder Representations from Transformers. \n",
    "Each word here has a meaning to it and we will encounter that one by one in this article. \n",
    "For now, the key takeaway from this line is – BERT is based on the Transformer architecture. \n",
    "Secondly, BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia (that’s 2,500 million words!) and Book Corpus (800 million words). -->\n",
    "\n",
    "この事前学習ステップは BERT の成功を支える魔法のようなものです。\n",
    "なぜなら，大規模なテキストコーパスでモデルを訓練すると，モデルは，言語がどのように機能するかについて，より深く親密な理解を得られるようになるからです。\n",
    "この知識は，ほとんどすべての NLP 課題に役立つスイスアーミーナイフです。\n",
    "<!-- This pre-training step is half the magic behind BERT’s success. \n",
    "This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works. \n",
    "This knowledge is the swiss army knife that is useful for almost any NLP task. -->\n",
    "\n",
    "第 3 に，BERT は「深層双方向」モデルです。\n",
    "双方向とは、BERT が学習段階において、トークンの文脈の左側と右側の両方から情報を学習することを意味します。\n",
    "<!-- Third, BERT is a “deep bidirectional” model. \n",
    "Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase. -->\n",
    "\n",
    "BERTのアーキテクチャと学習前のタスクについて詳しく知りたい方は、以下の記事をご覧ください。\n",
    "<!-- To learn more about the BERT architecture and its pre-training tasks, then you may like to read the below article: -->\n",
    "\n",
    "[BERT の神秘化：画期的な NLP フレームワークへの包括的ガイド](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/?utm_source=blog&utm_medium=fine_tune_BERT)\n",
    "<!-- Demystifying BERT: A Comprehensive Guide to the Groundbreaking NLP Framework -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. スパム分類のための BERT の微調整\n",
    "<!-- # 5. Fine-Tune BERT for Spam Classification -->\n",
    "\n",
    "ここでは Transformer ライブラリを使用して，テキスト分類を行うために BERT モデルを微調整します。\n",
    "PyTorch でのニューラルネットワークモデルの定義，訓練，評価の基本的な理解が必要です。\n",
    "もし PyTorch について簡単に復習したい場合は，以下の記事をご覧ください。\n",
    "<!-- Now we will fine-tune a BERT model to perform text classification with the help of the Transformers library. \n",
    "You should have a basic understanding of defining, training, and evaluating neural network models in PyTorch. \n",
    "If you want a quick refresher on PyTorch then you can go through the article below: -->\n",
    "\n",
    "* [初心者のためのPyTorchガイドとスクラッチでの動作方法](https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/?utm_source=blog&utm_medium=fine_tune_BERT)\n",
    "<!-- * A Beginner-Friendly Guide to PyTorch and How it Works from Scratch -->\n",
    "\n",
    "[Colab Notebook](https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb) へのリンク\n",
    "<!-- Link to Colab Notebook -->\n",
    "\n",
    "## 5.1 問題提起\n",
    "<!-- ## 5.1 Problem Statement -->\n",
    "\n",
    "我々は SMS メッセージのコレクションを持っている。\n",
    "これらのメッセージの一部はスパムであり，残りは本物である。\n",
    "我々の課題は，あるメッセージがスパムかどうかを自動的に検出するシステムを構築することです。\n",
    "<!-- We have a collection of SMS messages. \n",
    "Some of these messages are spam and the rest are genuine. \n",
    "Our task is to build a system that would automatically detect whether a message is spam or not. -->\n",
    "\n",
    "このユースケースで使用するデータセットは，ここからダウンロードできます (右クリックして「Save link as...」をクリックしてください)。\n",
    "<!-- The dataset that we will be using for this use case can be downloaded from here (right-click and click on “Save link as…”). -->\n",
    "\n",
    "この課題を実行するには GPU を使用できるように Google Colab を使用することをお勧めします。\n",
    "まず Colab の Runtime → Change runtime type → Select GPU をクリックして，GPU ランタイムを有効にします。\n",
    "<!-- I suggest you use Google Colab to perform this task so that you can use the GPU. Firstly, activate the GPU runtime on Colab by clicking on Runtime -> Change runtime type -> Select GPU. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1hkhc10wNrGt"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "isMac = True if platform.system() == 'Darwin' else None\n",
    "if not isMac:\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Darwin'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dir(platform.system))\n",
    "platform.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x4giRzM7NtHJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "#device = torch.device(\"cuda\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPU が利用可能であれば GPU にセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKd-Tj3hOMsZ"
   },
   "source": [
    "# 6. データセットの読み込み\n",
    "<!--Load Dataset-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cwJrQFQgN_BE",
    "outputId": "d8e5649f-cdbe-464e-e2b5-a7075486bdec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サンプルデータのダウンロード\n",
    "#!wget https://raw.githubusercontent.com/prateekjoshi565/Fine-Tuning-BERT/master/spamdata_v2.csv\n",
    "#!ls /content/drive/MyDrive\n",
    "df = pd.read_csv(\"spamdata_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzPPOrVQWiW5",
    "outputId": "e1021b75-5872-4c53-d94f-75cf4d6c7702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "676DPU1BOPdp",
    "outputId": "13a227b2-fc30-4ff7-d362-afb91b251a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKfWnApvOoE7"
   },
   "source": [
    "# 7. データセットを分割して，訓練データとテストデータにする\n",
    "<!-- # Split train dataset into train, validation and test sets -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mfhSPF5jOWb7"
   },
   "outputs": [],
   "source": [
    "# 以下の `random_state` の設定は任意です。再現性を担保するために，乱数の種を指定します。\n",
    "# テストデータの割合を 0.3 にしていますが，これも任意です。\n",
    "# 別の指定方法としては `train_size=0.9` みたいに訓練データの方を指定する方法もあります\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify = df['label'])\n",
    "\n",
    "# temp_labels は妥当性検証のために使います。ここでは，temp_labels を 半分に分割して，\n",
    "# 検証データ val_ と test_ にしています\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7hsdLoCO7uB"
   },
   "source": [
    "# 8. BERT モデルとトークナイザ を輸入 import する\n",
    "<!-- # Import BERT Model and BERT Tokenizer -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248,
     "referenced_widgets": [
      "3c847c49b9aa4aafa954d8919291ee6e",
      "eb1add9176c3478c83cf1355c7b0fadc",
      "a4360f3a75d749f798295160b407ce87",
      "2d470316638f4b5faf38080fd8b5c791",
      "ff12ad35ec4a4a9e895826f600d8c0df",
      "cf21162c6a2f4eae9238224640aca167",
      "26b2d94f5610419ca10c7b0b4af08e4c",
      "269e009e552d4ed58e342b3eed8a4c5f",
      "a6359a5069d74b1292875cec3eb1f7c3",
      "7f8c7c251af34e53967dc7488fd4de1e",
      "79abe54a0cf7472b93629111e9eb795f",
      "760a4dfce494439bafe858996134d5eb",
      "4cdee0350b0941d1b8a3fffd6a503268",
      "a375f6ff0c9847f6bb0583d6b4916e4c",
      "cfe1d1a172e74f128800ba7d9a8a486b",
      "914e796273ba4271a432bad58d83d704",
      "0562499bb02b44528ff0d96fd6796b21",
      "1907f103279f4b4981a88e203c4628b5",
      "265d65b026ca4feb800076af9e8ac8dd",
      "50b0c9f39d5f4a12a693d05c39f939e8",
      "df088af36a7c4d57913dacf2d7d72e7f",
      "fb869bb893014b7891b5a2d5a3740466",
      "b11a5fd30a9b46aebb287ac5e0207c46",
      "4216662c2ccd412ea07ab395cab78c58",
      "3c10d70bbeef478b8f4a49e26ccb0b27",
      "2446ea2cca784cdaaf2b7128eb80b1b5",
      "f41fd9d8b60f44f0bce31ea24138deea",
      "3151bad03cda41fea4bfde428c26bfbe",
      "c9abd65be85c4c72bbc029d12447034a",
      "b52b7769f83f49aa8c8713133b4dad5b",
      "616c816725cb421fb6b357d7768b8e8c",
      "29f211f48f664b248b7095144de6c347",
      "79427b4d1f66431aa6e263e3fc56adf0",
      "1de3da106d9f4f2782ff8d33d17eefbb",
      "e95f596daad44f4e9a2e749f18b91678",
      "d3d390b4263a4aee92d90275474e42e3",
      "f25a0e4afe67423290a77c3f82d6b562",
      "58481ca7b8e849c68712d5fd83591e0d",
      "831d97cc0d1e4643bf2fadab6166e62c",
      "3b803824d85b4f999008e6ee52cb9e5e",
      "d2ac24ea6b144d5caa9c8f455b361f7e",
      "0fbb7378c9ef4a008772feb153a33b43",
      "0bb870b4519f48e6989d2d5c539a9af3",
      "05ad6beef594438d9b65c675cf3ad750",
      "837f0aeb5370497cb7ab555f5a982a8a",
      "eccd7632ec2a421eb4e1241d270c527d",
      "c8fcbcd01f9b4ee39771e352a343e753",
      "4b2f89f6f25e4f28b9687a01542fc7c0",
      "f36093d063914278b6b10c7de525924d",
      "a6ee7c06b3e54db182efd3835cc305f2",
      "d117e7a9adc9420fb942cf571b29d1e5",
      "c07299fa92ed470e983563c9e7405f4a",
      "0daf4c7b71ed456c936477e708cee3df",
      "995215a7e65e484387cdd681d34c48c9",
      "31cda07a26c7436f9691a4d77ea799e1"
     ]
    },
    "id": "S1kY3gZjO2RE",
    "outputId": "4cf532e6-879d-472a-9ebd-5e148b295051"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_zOKeOMeO-DT"
   },
   "outputs": [],
   "source": [
    "# サンプルデータの設定\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "# 直上のデータをトークン ID 化\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAH73n39PHLw",
    "outputId": "102510c2-4754-4355-8161-f6ab860e39bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 直上セルで得たトークンID を印字\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wIYaWI_Prg8"
   },
   "source": [
    "ご覧の通り，出力は 2 つの項目からなる辞書です。\n",
    "<!-- As you can see the output is a dictionary of two items. -->\n",
    "\n",
    "input_ids には，入力文の整数列が入っています。\n",
    "整数の 101 と 102 は特別なトークンです。\n",
    "これらは両方の配列に追加され 0 はパディングトークンを表しています。\n",
    "attention_mask は 1 と 0 を含みます。\n",
    "これは，マスク値 1 に対応するトークンに注意を払い，それ以外は無視するようにモデルに指示します。\n",
    "<!-- ‘input_ids’ contains the integer sequences of the input sentences. \n",
    "The integers 101 and 102 are special tokens. \n",
    "We add them to both the sequences, and 0 represents the padding token.\n",
    "‘attention_mask’ contains 1’s and 0’s. \n",
    "It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest. -->\n",
    "\n",
    "# トークン化\n",
    "<!-- # Tokenization -->\n",
    "\n",
    "\n",
    "## 文のトークン化\n",
    "<!-- ## Tokenize the Sentences -->\n",
    "\n",
    "データセットに含まれるメッセージ (テキスト) の長さはまちまちなので，すべてのメッセージが同じ長さになるようにパディングを行います。\n",
    "メッセージのパディングには，最大配列長を使うことができます。\n",
    "しかし，訓練セットの配列長の分布を見て，適切なパディングの長さを見つけることもできます。\n",
    "<!-- Since the messages (text) in the dataset are of varying length, therefore we will use padding to make all the messages have the same length. \n",
    "We can use the maximum sequence length to pad the messages. \n",
    "However, we can also have a look at the distribution of the sequence lengths in the train set to find the right padding length. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "yKwbpeN_PMiu",
    "outputId": "57838ec8-e616-454c-a048-d839bcc2b264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 全データの長さ (一文あたりのトークン数) のヒストグラムを作成\n",
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ほとんどのメッセージの長さが 25 語以下であることがはっきりとわかります。\n",
    "一方，最大の長さは 175 です。\n",
    "そこで，パディング長を 175 に設定すると，すべての入力配列の長さが 175 になり，それらの配列に含まれるトークンのほとんどがパディングトークンになります。\n",
    "<!-- We can clearly see that most of the messages have a length of 25 words or less. \n",
    "Whereas the maximum length is 175. \n",
    "So, if we select 175 as the padding length then all the input sequences will have length 175 and most of the tokens in those sequences will be padding tokens which are not going to help the model learn anything useful and on top of that, it will make the training slower.-->\n",
    "\n",
    "そこで，パディングの長さを 25 に設定します。\n",
    "<!-- Therefore, we will set 25 as the padding length. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OXcswEIRPvGe"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tk5S7DWaP2t6",
    "outputId": "bb830515-3247-4821-ad45-d6c83b0622bc"
   },
   "outputs": [],
   "source": [
    "# 訓練データのトークン化と系列の符号化\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    #pad_to_max_length=True,  # 最新バージョンの padding 指定方法が変更になったため，直下行へと書き換えた\n",
    "    padding = 'longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# 検証データのトークン化と系列の符号化\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    #pad_to_max_length=True,  # 最新バージョンの padding 指定方法が変更になったため，直下行へと書き換えた\n",
    "    padding = 'longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# テストデータのトークン化と系列の符号化\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    #pad_to_max_length=True,  # 最新バージョンの padding 指定方法が変更になったため，直下行へと書き換えた\n",
    "    padding = 'longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで 訓練，検証，テストの各データセットに含まれるメッセージが，それぞれ 25 トークンの長さの整数列に変換されたことになります。\n",
    "<!-- So, we have now converted the messages in train, validation, and test set to integer sequences of length 25 tokens each.-->\n",
    "\n",
    "次に，この整数列をテンソルに変換してみましょう。\n",
    "<!-- Next, we will convert the integer sequences to tensors. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsm8bkRZQTw9"
   },
   "source": [
    "# 整数系列ベクトルをテンソルへと変換\n",
    "<!-- # Convert Integer Sequences to Tensors -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QR-lXwmzQPd6"
   },
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# 検証データ\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# テストデータ\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，訓練セットと検証セットの両方のデータローダを作成します。\n",
    "これらのデータローダは，学習段階でモデルへの入力として訓練データと検証データのバッチを渡します。\n",
    "<!-- Now we will create dataloaders for both train and validation set. \n",
    "These dataloaders will pass batches of train data and validation data as input to the model during the training phase. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov1cOBlcRLuk"
   },
   "source": [
    "# データローダ Dataloader の作成\n",
    "<!-- # Create DataLoaders -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qUy9JKFYQYLp"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32  #ミニバッチサイズの定義 define a batch size\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)  # テンソルを包み込んでデータセット化\n",
    "train_sampler = RandomSampler(train_data)                   # 訓練データからランダムサンプリング\n",
    "\n",
    "# 訓練データセットの定義 dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)          # テンソルを包み込んでデータセット化 wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)                   # 訓練中に検証データから逐次サンプリング sampler for sampling the data during training\n",
    "\n",
    "# 検証データセットの定義 dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, \n",
    "                            sampler = val_sampler, \n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2HZc5ZYRV28"
   },
   "source": [
    "# モデルのアーキテクチャを定義する\n",
    "<!-- # Define Model Architecture -->\n",
    "\n",
    "思い出していただければ，以前，この記事の中で，モデルのすべてのレイヤーを凍結させてからファインチューニングを行うと書きました。\n",
    "では，まずそれをやってみましょう。\n",
    "<!-- If you can recall, earlier I mentioned in this article that I would freeze all the layers of the model before fine-tuning it. \n",
    "So, let’s do it first. -->\n",
    "\n",
    "## パラメータの凍結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wHZ0MC00RQA_"
   },
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これにより，微調整中にモデルの重みが更新されるのを防ぐことができる。\n",
    "BERT モデルの事前学習済みの重みを微調整したい場合は，上記のコードを実行しないでください。\n",
    "\n",
    "次は，モデルのアーキテクチャを定義しましょう。\n",
    "<!-- This will prevent updating of model weights during fine-tuning. \n",
    "If you wish to fine-tune even the pre-trained weights of the BERT model then you should not execute the code above.\n",
    "\n",
    "Moving on we will now let’s define our model architecture. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "b3iEtGyYRd0A"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        \n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        #define the forward pass\n",
    "        \n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        # このままでは動作しないので，下記を参照した\n",
    "        #source: https://stackoverflow.com/questions/66846030/typeerror-linear-argument-input-position-1-must-be-tensor-not-str\n",
    "        #I've been working on this repo too. \n",
    "        # Motivated by the answer provided on this link. \n",
    "        #There is a class probably named Bert_Arch that inherits the nn.Module and this class has a overriden method named forward. \n",
    "        #Inside forward method just add the parameter 'return_dict=False' to the self.bert() method call. Like so:\n",
    "        # ```_, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)```\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        #_, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cBAJJVuJRliv"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "taXS0IilRn9J"
   },
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9CDpoMQR_rK"
   },
   "source": [
    "# クラス重みを見つける\n",
    "<!-- # Find Class Weights -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izY5xH5eR7Ur",
    "outputId": "d6c1cfe4-436b-4d04-e699-c098b058bcc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57743559 3.72848948]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# クラス重みを計算 Compute the class weights\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "r1WvfY2vSGKi"
   },
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights = torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My4CA0qaShLq"
   },
   "source": [
    "# BERT のファインチューニング(微調整)\n",
    "<!-- # Fine-Tune BERT -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rskLk8R_SahS"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"function to train the model\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds=[]  # empty list to save model predictions\n",
    "  \n",
    "    for step,batch in enumerate(train_dataloader):  # iterate over batches\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        batch = [r.to(device) for r in batch]  # push the batch to gpu\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad() # clear previously calculated gradients\n",
    "        preds = model(sent_id, mask)  # get model predictions for the current batch\n",
    "        loss = cross_entropy(preds, labels)  # compute the loss between actual and predicted values\n",
    "        total_loss = total_loss + loss.item()  # add on to the total loss\n",
    "        loss.backward()  # backward pass to calculate the gradients\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()  # update parameters\n",
    "        preds=preds.detach().cpu().numpy()  # model predictions are stored on GPU. So, push it to CPU\n",
    "        total_preds.append(preds)  # append the model predictions\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)  # compute the training loss of the epoch\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds  #returns the loss and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yGXovFDlSxB5"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"function for evaluating the model\"\"\"\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()  # deactivate dropout layers\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []  # empty list to save the model predictions\n",
    "\n",
    "    for step,batch in enumerate(val_dataloader):  # iterate over batches\n",
    "        if step % 50 == 0 and not step == 0:  # Progress update every 50 batches.\n",
    "            elapsed = format_time(time.time() - t0)  # Calculate elapsed time in minutes.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))  # Report progress.\n",
    "\n",
    "        batch = [t.to(device) for t in batch]  # push the batch to gpu\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():  # deactivate autograd\n",
    "            preds = model(sent_id, mask)  # model predictions\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    \n",
    "    avg_loss = total_loss / len(val_dataloader)  # compute the validation loss of the epoch\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KZEgxRRTLXG"
   },
   "source": [
    "# 訓練の開始\n",
    "<!-- # Start Model Training -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1USGTntS3TS",
    "outputId": "b2b15c65-6600-474b-9c4c-13e624a23aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -277.982\n",
      "Validation Loss: -892.769\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -1979.929\n",
      "Validation Loss: -3749.645\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -5595.438\n",
      "Validation Loss: -8686.635\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -11159.002\n",
      "Validation Loss: -15790.685\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -18712.133\n",
      "Validation Loss: -25092.235\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -28628.342\n",
      "Validation Loss: -36397.635\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -39832.108\n",
      "Validation Loss: -49960.577\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -53742.699\n",
      "Validation Loss: -65962.138\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -69795.144\n",
      "Validation Loss: -84426.587\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: -87306.655\n",
      "Validation Loss: -105097.123\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')  # set initial loss to infinite\n",
    "train_losses, valid_losses = [], [] # empty lists to store training and validation loss of each epoch\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #for each epoch         \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    train_loss, _ = train()          #train model\n",
    "    valid_loss, _ = evaluate()       #evaluate model\n",
    "    if valid_loss < best_valid_loss:  #save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yrhUc9kTI5a"
   },
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OacxUyizS8d1",
    "outputId": "98849217-37d3-44e5-b79d-71283e8a0d3d"
   },
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4SVftkkTZXA"
   },
   "source": [
    "# Get Predictions for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "NZl0SZmFTRQA"
   },
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ms1ObHZxTYSI",
    "outputId": "a8682066-9380-4138-ff0b-d36e1a14afa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       724\n",
      "           1       0.00      0.00      0.00       112\n",
      "\n",
      "    accuracy                           0.87       836\n",
      "   macro avg       0.43      0.50      0.46       836\n",
      "weighted avg       0.75      0.87      0.80       836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asakawa/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/asakawa/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/asakawa/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "YqzLS7rHTp4T",
    "outputId": "cc347575-0560-4310-a98f-b3c911c1ecc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0\n",
       "row_0     \n",
       "0      724\n",
       "1      112"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpX1uTwjUPY6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2021_1103Fine-Tuning BERT for Spam Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
