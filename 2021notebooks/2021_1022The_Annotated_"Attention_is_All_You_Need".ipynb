{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Annotated \"Attention is All You Need\".ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022The_Annotated_%22Attention_is_All_You_Need%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wArePITKUgQG"
      },
      "source": [
        "# トランスフォーマー注釈\n",
        "\n",
        "- [オリジナルブログ The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
        "- [オリジナル Colab ソース](https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view)\n",
        "- このファイル <https://drive.google.com/open?id=1naZHpTLgfLd54RcY4yi0WyQYoKf1yBBo>\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_0_0.png\" width=\"66%\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWEk4ttUgQH"
      },
      "source": [
        "私は教える際，機械学習の最近の動向を理解する方法として，実装を重視しています。\n",
        "この投稿は， この目標に沿って自分を誠実に保つための試みです。\n",
        "NIPS 2017 で発表された最近の [Attention is All You Need](https://arxiv.org/abs/1706.03762) 論文は， 機械翻訳や 潜在的に自然言語処理 全般の新しい手法として， 瞬く間にインパクトのある論文となりました。\n",
        "この論文は非常にわかりやすく書かれていますが， これまでの常識では， 正しく実装するのはかなり難しいとされてきました。\n",
        "\n",
        "> この投稿では， 論文の最初から最後までを追い， 各構成要素をコードで実装してみました。\n",
        "(元の論文から若干の順序変更やスキップをしています)。\n",
        "この文書自体が作業用のノートであり， 完全に使用可能で効率的な実装であるべきです。\n",
        "[PyTorch](http://pytorch.org/) と [torchtext](https://github.com/pytorch/text) をインストールする必要があります。\n",
        "完全なコードは [github](https://github.com/harvardnlp/annotated-transformer) にあります。\n",
        "> - Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaYyfFUqUnGY"
      },
      "source": [
        "!pip install --upgrade pytorch torchtext numpy matplotlib spacy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LTc4HW7UgQI"
      },
      "source": [
        "# Standard PyTorch imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# For plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxhOQubUgQL"
      },
      "source": [
        "# 1. 背景"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-PiEMOUgQM"
      },
      "source": [
        "Extended Neural GPU [16], ByteNet [18],  ConvS2S [9] などのモデルでは， 基本的な構成要素として畳み込みニューラルネットワークが用いられており， すべての入出力位置について隠れた表現を並列に計算しています。\n",
        "これらのモデルでは， 任意の 2 つの入出力位置からの信号を関連付けるために必要な演算の数は， 位置間の距離に応じて ConvS2S では線形に， ByteNet では対数に増加します。\n",
        "このため， 離れた位置間の依存関係を学習することが難しくなります [12]。\n",
        "トランスフォーマーでは， 注目度の高い位置を平均化することで実効的な解像度が低下しますが， これは 3.2 節で説明する 多頭注意機構によって相殺され， 一定の演算数に抑えられます。\n",
        "\n",
        "自己注意 (イントラ注意と呼ばれることもある) は 1 つの配列の異なる位置を関連づけて， その配列の表現を計算するための注意機構です。\n",
        "自己注意は， 読解，抽象的な要約，テキストの含意，課題に依存しない文の表現の学習など，さまざまな課題で利用されています [4, 27, 28, 22]。\n",
        "エンド2エンドの メモリネットワークは， 配列に沿った再帰ではなく， 再帰的な注意機構に基づいており， 簡易な言語の質問応答や言語モデルの課題で優れた性能を発揮することが示されています [34]。\n",
        "\n",
        "しかし，私たちの知る限り，トランスフォーマーは， 配列を揃えた RNN や畳み込みを使用せずに， 自己注意に完全に依存して入力と出力の表現を計算する初めての伝達モデルです。\n",
        "以下の節では， トランスフォーマーについて説明し， 自己注意の動機付けを行い [17, 18] や[9] などのモデルと比較して，その利点を議論します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84vTAA5TUgQM"
      },
      "source": [
        "# 2. モデル構成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBOvyU9BUgQN"
      },
      "source": [
        "ほとんどの競合するニューラルネットワーク伝達モデルは， 符号化器-復号化器構造を持っています[(cite)](cho2014learning,bahdanau2014neural,sutskever14)。\n",
        "ここで，符号化器は， 入力された記号表現の列 $(x_1, ..., x_n)$ を連続表現の列 $mathbf{z} = (z_1, ..., z_n)$ にマッピングします。\n",
        "復号化器は $mathbf{z}$ が与えられると，1 要素ずつ記号の出力系列 $(y_1,...,y_m)$ を生成します。\n",
        "各ステップにおいて， モデルは自己回帰的に [(cite)](graves2013generating) 次の記号を生成する際に， 前に生成された記号を追加入力として消費します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AC8KeDJUgQO"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base model for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encoder(self.src_embed(src), src_mask)\n",
        "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0EXqvEUgQQ"
      },
      "source": [
        "トランスフォーマーは， この全体的なアーキテクチャに沿って， 図 1 の左半分と右半分にそれぞれ示されているように， 符号化器と復号化器の両方に， 積み重ねられた自己注意と点ごとの完全連結層を使用しています。\n",
        "\n",
        "<center>\n",
        "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png\" width=\"33%\"><br/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euyRXbaMUgQR"
      },
      "source": [
        "## 2.1 符号化器と復号化器の積み重ね\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKvRjPKF53JI"
      },
      "source": [
        "\n",
        "### 2.1.1 符号化器: \n",
        "\n",
        "符号化器は $N=6$ 枚の同じ層の積み重ねで構成されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yy7pY85UgQR"
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psiq5idJUgQT"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mie9sUeSUgQV"
      },
      "source": [
        "2 つの下位層それぞれの周りに残差接続 [(cite)](he2016deep) を採用し， 続いて層正規化 [(cite)](layernorm2016) を行います。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEz9kLClUgQV"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx4On5PCUgQY"
      },
      "source": [
        "つまり，各下位層の出力は  $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$ となり， $\\mathrm{Sublayer}(x)$ は下位層自身が実装する関数です。\n",
        " 各下位層の出力に ドロップアウト  [(cite)](srivastava2014dropout) を適用してから， 下位層の入力に加えて正規化します。 \n",
        "\n",
        "これらの残差接続を容易にするために， モデル内のすべての下位層と埋め込み層は，次元$d_{text{model}}=512$ の出力を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9JBwAcUgQY"
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity we apply the norm first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZduB6mIlUgQa"
      },
      "source": [
        "各層には 2 つの下位層があります。\n",
        "1 つ目は多頭自己注意機構で， 2 つ目は単純な位置ごとの完全連結フィードフォワードネットワークです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mEBw9tIUgQb"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHOZnpnBUgQd"
      },
      "source": [
        "### 2.1.2 復号化器:\n",
        "\n",
        "また，復号化器は $N=6$ 個の同一の階層の積み重ねで構成されています。 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o_ZB42sUgQd"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOi_W1qaUgQf"
      },
      "source": [
        "各符号化器層の 2 つの下位層に加えて， 復号化器は 3 つ目の下位層を挿入し， 符号化器の積層の出力に対して多頭注意を行います。\n",
        "符号化器と同様， 各下位層の周りに残差接続を採用し， その後，層正規化を行います。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMm6xHWVUgQg"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nen9h7wUgQi"
      },
      "source": [
        "また， 復号化器積層の自己注意下位層を変更し， 位置が後続の位置に注目しないようにしました。 \n",
        "このマスキングと， 出力埋め込みが 1 位置分の離されていることを組み合わせることで，位置 $i$ の予測は，$i$ よりも小さい位置での既知の出力にのみ依存することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQKI9AgUgQj"
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQMtvM-UgQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "b6c1ddc3-7b1c-42ef-fe51-93ce2ec7a25a"
      },
      "source": [
        "# 注意のマスクは，各目標単語 (行) が見ることを許される位置 (列) を示す。\n",
        "# 単語は訓練中に将来の単語を注目するためにブロックされる。\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedb279b8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3twSbimFUgQq"
      },
      "source": [
        "### 2.1. 3注意: \n",
        "\n",
        "注意関数は， クエリとキーと値の対のセットを出力にマッピングするもので， クエリ， キー，バリュー， 出力はすべてベクトルです。\n",
        "出力は，バリューの加重和として計算され， 各バリューに割り当てられた重みは， クエリと対応するキーの互換性関数によって計算されます。\n",
        "\n",
        "我々は， この特別な注意を「規格化した内積注意」と呼びます。\n",
        "入力は，$d_k$ 次元のクエリとキー，$d_v$ 次元のバリューからなっています。\n",
        "クエリとすべてのキーの内積を計算し， それぞれを $sqrt{d_k}$ で割り， ソフトマックス関数を適用して値の重みを求めます。\n",
        "\n",
        "<center>\n",
        "<img width=\"220px\" src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png\">\n",
        "</center>\n",
        "\n",
        "実際には， 行列 $Q$ にまとめられた一連のクエリに対して同時に注意関数を計算します。  キーとバリューも行列 $K$ と $V$ にまとめられます。\n",
        "出力の行列を次のように計算します。                     \n",
        "                                                                 \n",
        "$$\n",
        "\\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZ8zw9PUgQr"
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=0.0):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    # (Dropout described below)\n",
        "    p_attn = F.dropout(p_attn, p=dropout)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV7cIqbrUgQs"
      },
      "source": [
        "最も一般的に使用される 2 つの注意関数は， Bahdanau の加算型注意[(cite)](bahdanau2014neural)と，Luong の 内積型 (乗算型) 注意です。\n",
        "内積型注意は，$frac{1}{\\sqrt{d_k}}$ という規格化因子を除いて， 我々のアルゴリズムと同じです。\n",
        "加算型注意は，隠れ層が 1 つのフィードフォワードネットワークを使って互換性関数を計算します。\n",
        "この 2 つは理論的な複雑さは似ていますが， 内積型注意は， 高度に最適化された行列乗算コードを用いて実装できるため， 実際にはより高速で， よりスペース効率の高いものとなっています。\n",
        "\n",
        "$d_k$ の小さな値では 2 つの機構は同じような性能を発揮しますが，$d_k$ が大きな値では規格化なしで加算的注意が内積注意を上回ります [(引用)](DBLP:journals/corr/BritzGLL17)。\n",
        "$d_k$ の値が大きくなると， 内積の大きさが大きくなり， ソフトマックス関数の勾配が極端に小さくなる領域に押し込まれるのではないかと考えています (内積が大きくなる理由を説明するために， $q$ と $k$ の成分が平均 $0$,  分散 $1$ の独立した確率変数であると仮定します。\n",
        "そうすると， それらの内積である $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$ は 平均 $0$,  分散 $d_k$ となります)。\n",
        "この効果を打ち消すために， 内積を $\\frac{1}{\\sqrt{d_k}}$ で規格化します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiaCxaGGUgQt"
      },
      "source": [
        "### 2.1.4 多頭注意\n",
        "\n",
        "$d_{\\text{model}}$ 次元のキー，バリュー， クエリで単一の注意関数を実行する代わりに， クエリ， キー， バリューをそれぞれ $d_k$ 次元,  $d_k$ 次元, $d_v$ 次元に異なる，学習した線形射影で $h$ 回投影することが有益であることがわかりました。\n",
        "\n",
        "これらの射影されたバージョンのクエリ，キー， バリューのそれぞれに対して， 注意関数を並行して実行し， $d_v$ 次元の出力値を得ます。\n",
        "これらは連結され， 再び射影され，最終的な値が得られます。\n",
        "\n",
        "<center>\n",
        "<img width=\"33%\" src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\"><br/>\n",
        "</center>\n",
        "\n",
        "多頭注意では， モデルは異なる位置にある異なる表現下位空間からの情報を共同で注目することができます。\n",
        "単一注意では， 平均化がこれを阻害します。\n",
        "\n",
        "$$    \n",
        "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O\n",
        "$$\n",
        "\n",
        "ここで，$\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$\n",
        "\n",
        "ここで，射影されるのは，パラメータ行列 $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ と$W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$ です。\n",
        "\n",
        "ここでは，$h=8$ 個の並列注意層 (ヘッド) を採用しています。\n",
        "それぞれのヘッドには $d_k=d_v=d_{\\text{model}}/h=64$ を使用します。\n",
        "各ヘッドの次元が小さくなったことで， 全体の計算コストは， 全次元の単一注意の場合と同様になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ea0UrEgUgQt"
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.p = dropout\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) すべての線形投影を一括して行う d_model => h x d_k\n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) バッチ内のすべての投影されたベクトルに注意をかけます。\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
        "        \n",
        "        # 3) view  を使って “連結 concat” し 最終的に線形変換を適用する。 \n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVdpQ_KwUgQv"
      },
      "source": [
        "### 2.1.5 本モデルにおける注意の応用\n",
        "\n",
        "トランスフォーマーは， 多頭注意を 3 つの方法で使用しています。\n",
        "\n",
        "1. 「エンコーダ・デコーダ注意」層では，クエリは前のデコーダ層から来て， メモリのキーとバリューはエンコーダの出力から来ます。\n",
        "これにより， デコーダのすべての位置が， 入力系列のすべての位置に注目することができます。\n",
        "これは [(cite)](wu2016google, bahdanau2014neural, JonasFaceNet2017) などのsequence-to-sequence モデルにおける典型的な符号化器-復号化器間の注意機構を模倣しています。   \n",
        "\n",
        "2. 符号化器には自己注意層がある。\n",
        "自己注意層では， すべてのキー， バリュー， およびクエリが同じ場所 (この場合は符号化器の前の層の出力) から来る。\n",
        "符号化器の各位置は， 符号化器の前の層のすべての位置に注目することができます。\n",
        "\n",
        "3. 同様に，復号化器内の自己注意層は， 復号化器内の各位置が，その位置までの符号化器内のすべての位置に言及することを可能にします。\n",
        "自己回帰性を維持するためには， 復号化器内での左向きの情報の流れを防ぐ必要があります。\n",
        "ソフトマックスの入力に含まれる， 不正な接続に対応するすべての値をマスク化排除 ($-infty$ に設定) することで， スケーリングされた内積型注意の内部にこれを実装します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gERLhK-FUgQw"
      },
      "source": [
        "## 2.2 位置ごとのフィードフォワードネットワーク\n",
        "\n",
        "注意下位層に加えて， 符号化器と復号化器の各層には， 完全結合のフィード・フォワード・ネットワークが含まれており，これは各位置に個別かつ同一に適用されます。\n",
        "このネットワークは， ReLU 活性化を挟んだ 2 つの線形変換で構成されています。\n",
        "\n",
        "$$\n",
        "\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2\n",
        "$$                                                                                      \n",
        "\n",
        "線形変換は， 異なる位置では同じですが，層ごとに異なるパラメータを使用します。\n",
        "これを別の表現で表すと， カーネルサイズ 1 の 2 つの畳み込みとなります。\n",
        "入出力の次元は $d_{\\text{model}}=512$ であり，内部層の次元は $d_{ff}=2048$ です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuDPthO2UgQx"
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # Torch linears have a `b` by default. \n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68VLwifsUgQz"
      },
      "source": [
        "## 2.3 埋め込みとソフトマックス\n",
        "\n",
        "他の系列変換モデルと同様に，学習済み埋め込み層を用いて，入力トークンと出力トークンを次元 $d_{text{model}}$ のベクトルに変換します。\n",
        "また，復号化器の出力を次のトークンの予測確率に変換するために， 通常の学習した線形変換とソフトマックス関数を使用します。\n",
        "我々のモデルでは [(cite)](press2016using) と同様に，2 つの埋め込み層と事前ソフトマックス線形変換の間で同じ重み行列を共有しています。\n",
        "埋め込み層では， その重みに $\\sqrt{d_{text{model}}}$ を掛けています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl5JzPeGUgQz"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_hw5TyCUgQ1"
      },
      "source": [
        "## 2.4 位置の符号化\n",
        "\n",
        "我々のモデルには再帰も畳み込みもないので， モデルが系列の順序を利用するためには， 系列内のトークンの相対的または絶対的な位置に関する何らかの情報を注入する必要があります。\n",
        "そのために， 符号化器と復号化器の積層の底部にある入力埋め込みに「位置符号化」を追加します。\n",
        "位置符号化は埋め込みと同じ次元 $d_{\\text{model}}$ を持ち， 2 つの埋め込みの和が取れるようになっています。\n",
        "位置符号化には学習型と固定型の多くの選択肢があります [(cite)](JonasFaceNet2017)。\n",
        "\n",
        "ここでは，異なる周波数の正弦波(サイン)と 余弦波 (コサイン) 関数を使用しています。\n",
        "\n",
        "$$\n",
        "PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})\\\\\n",
        "PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})\n",
        "$$\n",
        "\n",
        "ここで、$pos$ は位置, $i$ は次元を表します。\n",
        "つまり， 位置情報の各次元は， 正弦波に対応しています。\n",
        "波長は， $2\\pi$ から $10000\\cdot 2\\pi$ へと幾何学的に変化していきます。\n",
        "この関数を選んだのは， 任意の固定ズレ $k$ に対して $PE_{pos+k}$ は $PE_{pos}$ の一次関数として表現できるため， モデルが相対的な位置による注意を容易に学習できるのではないかと考えたからです。\n",
        "\n",
        "さらに， 符号化積層と復号化積層の両方で， 埋め込みと位置符号化の合計にドロップアウトを適用します。\n",
        "ベースモデルでは $P_{drop}=0.1$ の割合を使用しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsjhp6uUgQ1"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMsBRCuLUgQ3"
      },
      "source": [
        "# The positional encoding will add in a sine wave based on position.\n",
        "# The frequency and offset of the wave is different for each dimension.\n",
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(20, 0)\n",
        "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjtmgH1UgQ5"
      },
      "source": [
        "また，学習した位置符号化 [(cite)](JonasFaceNet2017) を代わりに使う実験も行いましたが，2 つのバージョンはほぼ同じ結果を得ることができました。\n",
        "正弦波バージョンを選択したのは， 学習時に遭遇したものよりも長い系列長にモデルを外挿することができるかもしれないからです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtnFnHH9UgQ6"
      },
      "source": [
        "## 2.5 生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaKRIaZUgQ6"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Standard generation step. (Not described in the paper.)\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i97-Y7AUgQ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3lP9fTUgQ9"
      },
      "source": [
        "## 2.6 完全モデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-LDhRoaUgQ-"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Construct a model object based on hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP-g4KfhUgQ_"
      },
      "source": [
        "# Small example model.\n",
        "tmp_model = make_model(10, 10, 2)\n",
        "tmp_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuV1e8nEUgRB"
      },
      "source": [
        "# 3. 訓練\n",
        "\n",
        "この節では，我々のモデルの訓練体制について説明します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz6n3REAUgRH"
      },
      "source": [
        "## 3.1 学習データとバッチ処理\n",
        "\n",
        "約 450 万の文対からなる標準的な WMT 2014 英独データセットで学習を行いました。\n",
        "文は バイトペアエンコーディング cite{DBLP:journals/corr/BritzGLL17} を用いて符号化されており，約 37000 トークンのソース-ターゲット語彙が共有されています。\n",
        "英仏語については，3600 万文からなるかなり大規模な WMT2014 英仏語データセットを使用し，トークンを 32000 語ピースの語彙に分割しました [(cite)](wu2016google)。\n",
        "\n",
        "文の対は，おおよその配列の長さごとにまとめられました。\n",
        "各学習バッチには，約 25000 個のソーストークンと約 25000 個のターゲットトークンを含む文対のセットが含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52xwbbb4UgRK"
      },
      "source": [
        "## 3.2 ハードウェアとスケジュール\n",
        "\n",
        "8 台の NVIDIA P100 GPU を搭載した 1 台のマシンでモデルの学習を行いました。\n",
        "本稿で紹介したハイパーパラメータを用いた基本モデルでは，各訓練ステップに約 0.4 秒かかりました。\n",
        "ベースモデルの学習には，合計 100,000 ステップ，12 時間を要しました。\n",
        "ビッグモデルの場合，ステップタイムは 1.0 秒でした。\n",
        "大規模モデルは 300,000 ステップ (3.5 日) の訓練を行いました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPp__T_uUgRK"
      },
      "source": [
        "## 3.3 最適化\n",
        "\n",
        "Adam 最適化 [(cite)](kingma2014adam)を使用し  $\\beta_1=0.9$, $\\beta_2=0.98$, $\\epsilon=10^{-9}$ としました。\n",
        "学習の過程で学習率を式にしたがって変化させました。\n",
        "\n",
        "$$\n",
        "lrate = d_{\\text{model}}^{-0.5} \\cdot \\min({step\\_num}^{-0.5},\n",
        "{step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
        "$$\n",
        "\n",
        "これは、最初の $warmup\\_steps$ の学習ステップでは学習率を線形に増加させ， その後はステップ数の逆平方根に比例して学習率を減少させることに相当します。\n",
        "ここでは $warmup\\_steps=4000$ としました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yV0f2QUgRL"
      },
      "source": [
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup of the model is very unstable.\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC2wCVEFUgRO"
      },
      "source": [
        "# Three settings of the lrate hyperparameters.\n",
        "opts = [NoamOpt(512, 1, 4000, None), \n",
        "        NoamOpt(512, 1, 8000, None),\n",
        "        NoamOpt(256, 1, 4000, None)]\n",
        "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkzxQYKUgRQ"
      },
      "source": [
        "## 3.4 正則化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVbTo8qt9Ru0"
      },
      "source": [
        "\n",
        "### 3.4.1 ラベル平滑化\n",
        "\n",
        "学習時には，$\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15)という値のラベル平滑化を採用しました。\n",
        "これは， モデルがより不確かになるように学習するため，パープレキシティ (錯乱度) を悪化させますが，精度と BLEU スコアを向上させます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVWDJLXrUgRQ"
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXwt5HVUgRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "e13bd5d2-16f9-4930-943a-23bd133c4beb"
      },
      "source": [
        "#Example\n",
        "crit = LabelSmoothing(5, 0, 0.5)\n",
        "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
        "                             [0, 0.2, 0.7, 0.1, 0], \n",
        "                             [0, 0.2, 0.7, 0.1, 0]])\n",
        "v = crit(Variable(predict.log()), \n",
        "         Variable(torch.LongTensor([2, 1, 0])))\n",
        "\n",
        "# Show the target distributions expected by the system.\n",
        "plt.imshow(crit.true_dist)\n",
        "None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHBHTwmjUgRU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "33ed2615-05f2-4158-cad7-87ea7d5e4d1c"
      },
      "source": [
        "# 与えられた選択肢に非常に自信を持った場合\n",
        "# ラベル平滑化はモデルにペナルティを与え始める \n",
        "crit = LabelSmoothing(5, 0, 0.2)\n",
        "def loss(x):\n",
        "    d = x + 3 * 1\n",
        "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
        "                                 ])\n",
        "    print(predict.log())\n",
        "    return crit(Variable(predict.log()),\n",
        "                 Variable(torch.LongTensor([1]))).item() \n",
        "\n",
        "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae2be3aa57b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 与えられた選択肢に非常に自信を持った場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ラベル平滑化はモデルにペナルティを与え始める\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelSmoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LabelSmoothing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoyfFgLoUgRW"
      },
      "source": [
        "### 3.4.2 メモリ最適化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVKyONFsUgRW"
      },
      "source": [
        "def loss_backprop(generator, criterion, out, targets, normalize):\n",
        "    \"\"\"\n",
        "    Memory optmization. Compute each timestep separately and sum grads.\n",
        "    \"\"\"\n",
        "    assert out.size(1) == targets.size(1)\n",
        "    total = 0.0\n",
        "    out_grad = []\n",
        "    for i in range(out.size(1)):\n",
        "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
        "        gen = generator(out_column)\n",
        "        loss = criterion(gen, targets[:, i]) / normalize\n",
        "        total += loss.item()\n",
        "        #total += loss.data[0]\n",
        "        loss.backward()\n",
        "        out_grad.append(out_column.grad.data.clone())\n",
        "    out_grad = torch.stack(out_grad, dim=1)\n",
        "    out.backward(gradient=out_grad)\n",
        "    return total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiTlbMq2UgRY"
      },
      "source": [
        "def make_std_mask(src, tgt, pad):\n",
        "    src_mask = (src != pad).unsqueeze(-2)\n",
        "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "    return src_mask, tgt_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSF9AaKJUgRZ"
      },
      "source": [
        "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "                        \n",
        "        model_opt.step()\n",
        "        model_opt.optimizer.zero_grad()\n",
        "        if i % 10 == 1:\n",
        "            print(i, loss, model_opt._rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4kFYs7nUgRb"
      },
      "source": [
        "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
        "    model.test()\n",
        "    total = 0\n",
        "    for batch in valid_iter:\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJo5AZasUgRd"
      },
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_mask = src_mask\n",
        "        self.trg_mask = trg_mask\n",
        "        self.ntokens = ntokens\n",
        "    \n",
        "def data_gen(V, batch, nbatches):\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
        "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpU5b2sUgRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b44249e-74d7-43b6-c1a2-53316231ef1c"
      },
      "source": [
        "V = 11\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "model_opt = get_std_opt(model)\n",
        "for epoch in range(2):\n",
        "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2.80071884393692 6.987712429686844e-07\n",
            "11 2.69090473651886 4.192627457812107e-06\n",
            "1 2.500949651002884 7.686483672655528e-06\n",
            "11 2.2902756482362747 1.118033988749895e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2BcIoOUgRg"
      },
      "source": [
        "# 4. 実際例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_oq0kLUgRh"
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKynVliVXg6F"
      },
      "source": [
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtYwdHqUgRj"
      },
      "source": [
        "# Load words from IWSLT\n",
        "\n",
        "#!pip install torchtext spacy\n",
        "#!python -m spacy download en\n",
        "#!python -m spacy download de\n",
        "\n",
        "import spacy\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "import torchtext\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "#SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "SRC = torchtext.legacy.data.field(tokenize=tokenize_de, \n",
        "                                  lower=True, \n",
        "                                  pad_token=BLANK_WORD,\n",
        "                                  init_token=BOS_WORD, \n",
        "                                  eos_token=EOS_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "MAX_LEN = 100\n",
        "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "MIN_FREQ = 1\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8MTIJTWUgRl"
      },
      "source": [
        "# Detail. Batching seems to matter quite a bit. \n",
        "# This is temporary code for dynamic batching based on number of tokens.\n",
        "# This code should all go away once things get merged in this library.\n",
        "\n",
        "BATCH_SIZE = 4096\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
        "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
        "\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wamR3SPdUgRo"
      },
      "source": [
        "# Create the model an load it onto our GPU.\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SStCCZoiUgRp"
      },
      "source": [
        "\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NO9lsw2UgRt"
      },
      "source": [
        "\n",
        "OTHER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8BVm-hEUgRw"
      },
      "source": [
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field()\n",
        "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
        "\n",
        "MAX_LEN = 100\n",
        "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
        "                                    exts=('.en', '.fr'),\n",
        "                                    fields=(SRC, TGT), \n",
        "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "SRC.build_vocab(train.src, max_size=50000)\n",
        "TGT.build_vocab(train.trg, max_size=50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdZyOIzUgRx"
      },
      "source": [
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "print(pad_idx)\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
        "model_opt = get_opt(model)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cinuTkbtUgRz"
      },
      "source": [
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch(train_iter, model, criterion, model_opt)\n",
        "    valid_epoch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LadFBIEUgR3",
        "outputId": "ba9a812f-b5f6-4972-af91-215d91a223d1"
      },
      "source": [
        "print(pad_idx)\n",
        "print(len(SRC.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "50002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_Au0bHUgR7"
      },
      "source": [
        "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqKKIhoOUgR-"
      },
      "source": [
        "#weight = torch.ones(len(TGT.vocab))\n",
        "#weight[pad_idx] = 0\n",
        "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch(train_iter, model, criterion, model_opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35JC6i9QUgSB"
      },
      "source": [
        "1 10.825187489390373 6.987712429686844e-07\n",
        "101 9.447168171405792 3.56373333914029e-05\n",
        "201 7.142856806516647 7.057589553983712e-05\n",
        "301 6.237934365868568 0.00010551445768827134\n",
        "401 5.762486848048866 0.00014045301983670557\n",
        "501 5.415792358107865 0.00017539158198513977\n",
        "601 5.081815680023283 0.000210330144133574\n",
        "701 4.788327748770826 0.00024526870628200823\n",
        "801 4.381739928154275 0.0002802072684304424\n",
        "901 4.55433791608084 0.00031514583057887664\n",
        "1001 4.911875109748507 0.0003500843927273108\n",
        "1101 4.0579032292589545 0.0003850229548757451\n",
        "1201 4.2276234351193125 0.0004199615170241793\n",
        "1301 3.932735869428143 0.00045490007917261356\n",
        "1401 3.8179439397063106 0.0004898386413210477\n",
        "1501 3.3608515430241823 0.000524777203469482\n",
        "1601 3.832796103321016 0.0005597157656179162\n",
        "1701 2.907085266895592 0.0005946543277663504\n",
        "1801 3.5280659823838505 0.0006295928899147847\n",
        "1901 2.895841649500653 0.0006645314520632189\n",
        "2001 3.273784235585481 0.000699470014211653\n",
        "2101 3.181488689899197 0.0007344085763600873\n",
        "2201 3.4151616653980454 0.0007693471385085215\n",
        "2301 3.4343731447588652 0.0008042857006569557\n",
        "2401 3.0505455391539726 0.0008392242628053899\n",
        "2501 2.8089329147478566 0.0008741628249538242\n",
        "2601 2.7827929875456903 0.0009091013871022583\n",
        "2701 2.4428516102489084 0.0009440399492506926\n",
        "2801 2.4015486147254705 0.0009789785113991267\n",
        "2901 2.3568112018401735 0.001013917073547561\n",
        "3001 2.6349758653668687 0.0010488556356959952\n",
        "3101 2.5981983028614195 0.0010837941978444295\n",
        "3201 2.666826274838968 0.0011187327599928637\n",
        "3301 3.0092043554177508 0.0011536713221412978\n",
        "3401 2.4580375660589198 0.0011886098842897321\n",
        "3501 2.586465588421561 0.0012235484464381662\n",
        "3601 2.5663993963389657 0.0012584870085866006\n",
        "3701 2.9430236657499336 0.0012934255707350347\n",
        "3801 2.464644919440616 0.001328364132883469\n",
        "3901 2.7124062888276512 0.0013633026950319032\n",
        "4001 2.646443709731102 0.0013971932312809247\n",
        "4101 2.7294750874862075 0.001380057517579748\n",
        "4201 2.1295202329056337 0.0013635372009002666\n",
        "4301 2.596563663915731 0.001347596306985731\n",
        "4401 2.1265982036820787 0.0013322017384983986\n",
        "4501 2.3880532500334084 0.0013173229858148\n",
        "4601 2.6129120760888327 0.0013029318725783852\n",
        "4701 2.2873719420749694 0.001289002331178292\n",
        "4801 2.4949760700110346 0.0012755102040816328\n",
        "4901 2.496607314562425 0.001262433067573089\n",
        "5001 2.1889712483389303 0.0012497500749750088\n",
        "5101 1.8677761815488338 0.0012374418168536253\n",
        "5201 2.2992054556962103 0.0012254901960784316\n",
        "5301 2.664361578106707 0.0012138783159049418\n",
        "5401 2.705850490485318 0.0012025903795063202\n",
        "5501 2.581445264921058 0.0011916115995949978\n",
        "5601 2.2480602325085783 0.0011809281169581616\n",
        "5701 1.9289666265249252 0.0011705269268863989\n",
        "5801 2.4863578918157145 0.0011603958126073107\n",
        "5901 2.632946971571073 0.0011505232849492607\n",
        "6001 2.496141305891797 0.0011408985275576757\n",
        "6101 2.6422974687084206 0.0011315113470699342\n",
        "6201 2.448802186456305 0.0011223521277270118"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}