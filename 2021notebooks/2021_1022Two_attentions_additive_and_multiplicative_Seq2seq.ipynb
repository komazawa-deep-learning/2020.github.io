{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4d37c1",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638b905-c3bc-4660-ad57-827c90d566ba",
   "metadata": {
    "id": "e638b905-c3bc-4660-ad57-827c90d566ba"
   },
   "source": [
    "# 加算型注意 と 内積型注意 の実習\n",
    "\n",
    "- オリジナル [Sequence-to-sequence in Pytorch](https://github.com/b-etienne/Seq2seq-PyTorch)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://komazawa-deep-learning.github.io/assets/2015Bahdanau_attention.jpg\" width=\"33%\">\n",
    "<img src=\"https://komazawa-deep-learning.github.io/assets/2015Loung_fig3.svg\" width=\"33%\"><br/>\n",
    "</center>\n",
    "\n",
    "## 原著論文\n",
    "\n",
    "* [加算型注意 Bahdanau 2014 Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "* [内積型注意 Loung 2015 Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "* [Chan 2015 Listen, Attend and Spell](https://arxiv.org/abs/1508.01211)\n",
    "\n",
    "\n",
    "# Seq2seq モデル\n",
    "\n",
    "下図では，翻訳モデルで，ソース言語であるドイツ語から，ターゲット言語である英語への翻訳例である。\n",
    "赤色の単語を符号化器に入力し， 英語の単語を順次出力する(青の矢印)。\n",
    "すべての翻訳が青の h3 すなわち符号化器の最終時刻での中間層状態に依存していることに注意してください。\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1000/1*zsOlGqCVTsEiWm--1n6Bng.png\" width=\"49%\"><br/>\n",
    "ソース: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf\n",
    "</center>\n",
    "\n",
    "* まともに翻訳課題を例にとると，訓練に時間を要するので，今回は，おもちゃ問題で実行を確認します。\n",
    "* 具体的には，系列反転課題を取り上げます。\n",
    "* 数字の順唱課題，逆唱課題は WAIS 知能検査などにも取り上げられている短期記憶を測定する手法でもあります。\n",
    "* この課題を元に，注意の役割を考えるのが，今回の課題となります。\n",
    "* 具体的な課題は以下のとおりです\n",
    "\n",
    "4 文字 \"a\", \"b\", \"c\", \"d\" からランダムに文字を選んで，任意の長さの系列を作成します。\n",
    "このとき，符号化器に入力する文字列を，復号化器は反転することを学習するものとします。\n",
    "<!-- \n",
    "\n",
    "Sequence-to-sequence neural network with attention. \n",
    "You can play with a toy dataset to test different configurations.\n",
    "The toy dataset consists of batched (input, target) pairs, where the target is the reversed input.\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "You can tune the following parameters:\n",
    "\n",
    "* decoder type (with or without Attention)\n",
    "* encoder type (with or without downsampling, with or without preprocessing layers)\n",
    "* the encoder's hidden dimension\n",
    "* the number of recurrent layers in the encoder\n",
    "* the encoder dropout\n",
    "* the bidirectionality of the encoder\n",
    "* the decoder's hidden dimension\n",
    "* the number of recurrent layers in the decoder\n",
    "* the decoder dropout\n",
    "* the bidirectionality of the decoder\n",
    "* batch size\n",
    "* the type of attention used\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247d59a-5539-4729-a247-40b74945331a",
   "metadata": {
    "id": "1247d59a-5539-4729-a247-40b74945331a"
   },
   "source": [
    "# 1. データセットの作成\n",
    "\n",
    "PyTorch で用いるデータセット `torch.utils.data.Dataset` を自作する場合，最低限 3 つの関数をクラス内で定義します。\n",
    "1. `__init__()` 初期のための関数\n",
    "2. `__len__()` データセットのサイズを返す関数\n",
    "3. `__getitems__()` 数値を引数にとり，その数値に該当するデータを取り出して返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43e8613-4ea6-486a-ae92-b78389118c82",
   "metadata": {
    "id": "a43e8613-4ea6-486a-ae92-b78389118c82"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import editdistance\n",
    "from random import choice, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class ToyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    https://talbaumel.github.io/blog/attention/\n",
    "    \"\"\"\n",
    "    def __init__(self, min_length=5, max_length=20, type='train'):\n",
    "        self.SOS = \"<s>\"                       # 系列の始まりを表す特殊文字を定義 SOS は スタート(start) オブ(of) シーケンス(sequence) の頭文字\n",
    "        self.EOS = \"</s>\"                      # 系列の終わりを表す特殊文字を定義 EOS は エンド(end) オブ(of) シーケンス(sequnence) の頭文字\n",
    "        self.characters = list(\"abcd\")         # 課題に使用する文字を定義\n",
    "        self.int2char = list(self.characters)  # 文字列を数字に変換するためのリスト\n",
    "        self.char2int = {c: i+3 for i, c in enumerate(self.characters)} # 文字列を数字に変換するための辞書\n",
    "        #print(self.char2int)\n",
    "        self.VOCAB_SIZE = len(self.characters)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        if type == 'train':\n",
    "            self.set = [self._sample() for _ in range(3000)]\n",
    "        else:\n",
    "            self.set = [self._sample() for _ in range(300)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.set[item]\n",
    "\n",
    "    def _sample(self):\n",
    "        random_length = randrange(self.min_length, self.max_length)                      # ランダムサンプリングする文字列長を一つランダムに選ぶ\n",
    "        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # 選んだ文字列長分だけ文字をランダムサンプリングする\n",
    "        random_string = ''.join(random_char_list)                                        # 上で選んだ文字列を random_string として保存\n",
    "        a = np.array([self.char2int.get(x) for x in random_string])                      # 文字列を数値に変換\n",
    "        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2])          # 文字列を逆向きにする\n",
    "        x = np.zeros((random_length, self.VOCAB_SIZE)) # 文字列長 掛ける アルファベット文字数の行列を作って 0 で初期化した配列を x とする\n",
    "        x[np.arange(random_length), a-3] = 1           # 初期化した x に文字列に該当する要素を 1 に変換 ワンホット表現にしないといけませんので\n",
    "\n",
    "        return x, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c6cb77-eea1-48c7-ac28-1d0770e16094",
   "metadata": {
    "id": "f8c6cb77-eea1-48c7-ac28-1d0770e16094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データセットのサイズ: 3000\n",
      "データセットから，試しに一つデータを取り出してみる: \n",
      "(array([[0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.]]), array([5, 4, 5, 5, 4, 4, 4, 5, 5, 3, 5, 5, 4, 2]))\n",
      "データセットから一つランダムサンプリングしてみる: \n",
      "(array([[0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.]]), array([3, 3, 5, 4, 4, 3, 5, 4, 4, 4, 5, 3, 5, 2]))\n"
     ]
    }
   ],
   "source": [
    "dataset = ToyDataset(5, 15)\n",
    "print(f'データセットのサイズ: {dataset.__len__()}')\n",
    "print(f'データセットから，試しに一つデータを取り出してみる: \\n{dataset.__getitem__(5)}')\n",
    "print(f'データセットから一つランダムサンプリングしてみる: \\n{dataset._sample()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76b983-e6c2-4e4e-b7f1-3024c5a83cf8",
   "metadata": {
    "id": "4c76b983-e6c2-4e4e-b7f1-3024c5a83cf8"
   },
   "source": [
    "* 以下の 3 つのセルは下請け作業用ですので，単純に実行してください。\n",
    "* 定義だけなので，時間はかかりません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5deb11-fc1d-4ff3-babe-cdce4a096edc",
   "metadata": {
    "id": "1b5deb11-fc1d-4ff3-babe-cdce4a096edc"
   },
   "outputs": [],
   "source": [
    "# utils/misc.py\n",
    "#import librosa\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "def check_size(tensor, *args):\n",
    "    size = [a for a in args]\n",
    "    assert tensor.size() == torch.Size(size), tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a459a2c-3c92-4254-ae57-9efae09b4ae3",
   "metadata": {
    "id": "7a459a2c-3c92-4254-ae57-9efae09b4ae3"
   },
   "outputs": [],
   "source": [
    "# utils/data_generator.py\n",
    "from torch.utils import data\n",
    "from itertools import zip_longest\n",
    "\n",
    "def pad_tensor(vec, pad, value=0, dim=0):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = pad - vec.shape[0]\n",
    "\n",
    "    if len(vec.shape) == 2:\n",
    "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
    "    elif len(vec.shape) == 1:\n",
    "        zeros = torch.ones((pad_size,)) * value\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n",
    "\n",
    "\n",
    "\n",
    "def pad_collate(batch, values=(0, 0), dim=0):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        batch - list of (tensor, label)\n",
    "\n",
    "    reutrn:\n",
    "        xs - a tensor of all examples in 'batch' after padding\n",
    "        ys - a LongTensor of all labels in batch\n",
    "        ws - a tensor of sequence lengths\n",
    "    \"\"\"\n",
    "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch])\n",
    "    sequence_lengths, xids = sequence_lengths.sort(descending=True)\n",
    "    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n",
    "    target_lengths, yids = target_lengths.sort(descending=True)\n",
    "    # find longest sequence\n",
    "    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
    "    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n",
    "    # pad according to max_len\n",
    "    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n",
    "\n",
    "    # stack all\n",
    "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
    "    ys = torch.stack([x[1] for x in batch]).int()\n",
    "    xs = xs[xids]\n",
    "    ys = ys[yids]\n",
    "    return xs, ys, sequence_lengths.int(), target_lengths.int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde95ecb-dac0-4855-9ada-7c48f35939c3",
   "metadata": {
    "id": "dde95ecb-dac0-4855-9ada-7c48f35939c3"
   },
   "outputs": [],
   "source": [
    "# models/helpers.py\n",
    "import torch\n",
    "\n",
    "def mask_3d(inputs, seq_len, mask_value=0.):\n",
    "    batches = inputs.size()[0]\n",
    "    assert batches == len(seq_len)\n",
    "    max_idx = max(seq_len)\n",
    "    for n, idx in enumerate(seq_len):\n",
    "        if idx < max_idx.item():\n",
    "            if len(inputs.size()) == 3:\n",
    "                inputs[n, idx.int():, :] = mask_value\n",
    "            else:\n",
    "                assert len(inputs.size()) == 2, \"The size of inputs must be 2 or 3, received {}\".format(inputs.size())\n",
    "                inputs[n, idx.int():] = mask_value\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def skip_add_pyramid(x, seq_len, skip_add=\"add\"):\n",
    "    if len(x.size()) == 2:\n",
    "        x = x.unsqueeze(0)\n",
    "    x_len = x.size()[1] // 2\n",
    "    even = x[:, torch.arange(0, x_len*2-1, 2).long(), :]\n",
    "    odd = x[:, torch.arange(1, x_len*2, 2).long(), :]\n",
    "    if skip_add == \"add\":\n",
    "        return (even+odd) / 2, ((seq_len) / 2).int()\n",
    "    else:\n",
    "        return even, (seq_len / 2).int()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35e343-784b-434a-9fb3-39d0356d6599",
   "metadata": {
    "id": "db35e343-784b-434a-9fb3-39d0356d6599"
   },
   "source": [
    "# 2. 符号化器の定義\n",
    "\n",
    "* 符号化器は seq2seq モデルで，与えられた系列を「聞いて覚える」部分です。\n",
    "* 以下で定義するリカレントニューラルネットワークです。\n",
    "* 実習としては，SNN, GRU, LSTM などを選ぶことができます。\n",
    "* また，好きなものを選択可能) で構成されており， その前に畳み込み層や密な層を加えることができます。\n",
    "\n",
    "符号化器にデータを入力する前に `pack_padded_sequence` と `pad_packed_sequence` ヘルパーを使っています。\n",
    "データのバッチを使用しているので， バッチ内の各項目 (系列) は異なる長さを持ちますが，このバッチ内の全系列を，最長の系列の長さまで 0 で埋めることをしています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e95216-a4da-4e96-a191-bb039dcaa23f",
   "metadata": {
    "id": "89e95216-a4da-4e96-a191-bb039dcaa23f"
   },
   "outputs": [],
   "source": [
    "# models/encoders.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = config[\"n_channels\"]\n",
    "        self.hidden_size = config[\"encoder_hidden\"]\n",
    "        self.layers = config.get(\"encoder_layers\", 1)\n",
    "        self.dnn_layers = config.get(\"encoder_dnn_layers\", 0)\n",
    "        self.dropout = config.get(\"encoder_dropout\", 0.)\n",
    "        self.bi = config.get(\"bidirectional_encoder\", False)\n",
    "        if self.dnn_layers > 0:\n",
    "            for i in range(self.dnn_layers):\n",
    "                self.add_module('dnn_' + str(i), nn.Linear(\n",
    "                    in_features=self.input_size if i == 0 else self.hidden_size,\n",
    "                    out_features=self.hidden_size\n",
    "                ))\n",
    "        gru_input_dim = self.input_size if self.dnn_layers == 0 else self.hidden_size\n",
    "        self.rnn = nn.GRU(\n",
    "            gru_input_dim,\n",
    "            self.hidden_size,\n",
    "            self.layers,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=self.bi,\n",
    "            batch_first=True)\n",
    "        self.gpu = config.get(\"gpu\", True)\n",
    "\n",
    "    def run_dnn(self, x):\n",
    "        for i in range(self.dnn_layers):\n",
    "            x = F.relu(getattr(self, 'dnn_'+str(i))(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs, hidden, input_lengths):\n",
    "        if self.dnn_layers > 0:\n",
    "            inputs = self.run_dnn(inputs)\n",
    "        x = pack_padded_sequence(inputs, input_lengths, batch_first=True)\n",
    "        output, state = self.rnn(x, hidden)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.)\n",
    "\n",
    "        if self.bi:\n",
    "            output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]\n",
    "        return output, state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = Variable(torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "        return h0\n",
    "\n",
    "\n",
    "class EncoderPyRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderPyRNN, self).__init__()\n",
    "        self.input_size = config[\"n_channels\"]\n",
    "        self.hidden_size = config[\"encoder_hidden\"]\n",
    "        self.n_layers = config.get(\"encoder_layers\", 1)\n",
    "        self.dnn_layers = config.get(\"encoder_dnn_layers\", 0)\n",
    "        self.dropout = config.get(\"encoder_dropout\", 0.)\n",
    "        self.bi = config.get(\"bidirectional_encoder\", False)\n",
    "        self.skip_add = config.get(\"skip_add_pyramid_encoder\", \"add\")\n",
    "        self.gpu = config.get(\"gpu\", True)\n",
    "\n",
    "        if self.dnn_layers > 0:\n",
    "            for i in range(self.dnn_layers):\n",
    "                self.add_module('dnn_' + str(i), nn.Linear(\n",
    "                    in_features=self.input_size if i == 0 else self.hidden_size,\n",
    "                    out_features=self.hidden_size\n",
    "                ))\n",
    "        gru_input_dim = self.input_size if self.dnn_layers == 0 else self.hidden_size\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.add_module('pRNN_' + str(i), nn.GRU(\n",
    "                input_size=gru_input_dim if i == 0 else self.hidden_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                dropout=self.dropout,\n",
    "                bidirectional=self.bi,\n",
    "                batch_first=True))\n",
    "\n",
    "    def run_dnn(self, x):\n",
    "        for i in range(self.dnn_layers):\n",
    "            x = F.relu(getattr(self, 'dnn_'+str(i))(x))\n",
    "        return x\n",
    "\n",
    "    def run_pRNN(self, inputs, hidden, input_lengths):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input: (batch, seq_len, input_size)\n",
    "        :param hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for i in range(self.n_layers):\n",
    "            x = pack_padded_sequence(inputs, input_lengths, batch_first=True)\n",
    "            output, hidden = getattr(self, 'pRNN_'+str(i))(x, hidden)\n",
    "            output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.)\n",
    "            hidden = hidden\n",
    "\n",
    "            if self.bi:\n",
    "                output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]\n",
    "\n",
    "            if i < self.n_layers - 1:\n",
    "                inputs, input_lengths = skip_add_pyramid(output, input_lengths, self.skip_add)\n",
    "\n",
    "        return output, hidden, input_lengths\n",
    "\n",
    "    def forward(self, inputs, hidden, input_lengths):\n",
    "        if self.dnn_layers > 0:\n",
    "            inputs = self.run_dnn(inputs)\n",
    "\n",
    "        outputs, hidden, input_lengths = self.run_pRNN(inputs, hidden, input_lengths)\n",
    "\n",
    "        if self.bi:\n",
    "            hidden = torch.sum(hidden, 0)\n",
    "\n",
    "        return outputs, hidden, input_lengths\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = Variable(torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "        return h0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299c2cf-1774-43c1-9a42-d0acc94dc4ee",
   "metadata": {
    "id": "0299c2cf-1774-43c1-9a42-d0acc94dc4ee"
   },
   "source": [
    "# 3. 復号化器の定義\n",
    "\n",
    "* 復号化器は， 予測値を出力し，それをもとに損失値を計算します。\n",
    "* 符号化器からの入力を受け取って，出力に変換します。\n",
    "* このとき，以下のような 3 種類の復号化器を定義しています。\n",
    "    1. バニラ復号化器\n",
    "    2. Bahdanau 型注意付き復号化器\n",
    "    3. Luong 型注意付き復号化器\n",
    "    \n",
    "* 注意つき復号化器は， 復号化器の出力を入力とし， どの部分に注目して予測を出力するかを決定します\n",
    "* 注意なしのバニラモデルでは， 符号化器の最終時刻の隠れ層状態だけ使用して復号化を試みます。\n",
    "\n",
    "* 復号化器に対する入力は，以下のものがあります\n",
    "    1. 一時刻前の出力， (標的系列の次の文字，または前に出力された文字) 双方向 RNN を使ってる場合には，逆方向からも入力を受けます\n",
    "    2. 隠れ層 (中間層) の状態\n",
    "    3. 注意を使っているかどうかに応じたその他の引数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717abd7-9bd4-4496-87b9-542957bc2928",
   "metadata": {
    "id": "f717abd7-9bd4-4496-87b9-542957bc2928"
   },
   "source": [
    "## 3.1 リカレントネットワーク復号化器\n",
    "\n",
    "直下のセルでは以下の 3 つのクラスを定義しています\n",
    "\n",
    "1. Seq2seq モデルのプロトタイプである リカレントニューラルネットワークによる復号化器のプロトタイプ\n",
    "2. 加算型注意付き復号化器\n",
    "3. 内積型注意付き復号化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e75acdf-5bf2-4b70-ac1d-1857a0a34fb4",
   "metadata": {
    "id": "1e75acdf-5bf2-4b70-ac1d-1857a0a34fb4"
   },
   "outputs": [],
   "source": [
    "# models/decorders.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    復号化器のプロトタイプ\n",
    "    forward() 関数は，それぞれの実装ごとに定義するので，空の実装になっています\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.hidden_size = config[\"decoder_hidden\"]\n",
    "        embedding_dim = config.get(\"embedding_dim\", None)\n",
    "        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n",
    "        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embedding_dim+self.hidden_size if config['decoder'].lower() == 'bahdanau' else self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=config.get(\"decoder_layers\", 1),\n",
    "            dropout=config.get(\"decoder_dropout\", 0),\n",
    "            bidirectional=config.get(\"bidirectional_decoder\", False),\n",
    "            batch_first=True)\n",
    "        if config['decoder'] != \"RNN\":\n",
    "            self.attention = Attention(\n",
    "                self.batch_size,\n",
    "                self.hidden_size,\n",
    "                method=config.get(\"attention_score\", \"dot\"),\n",
    "                mlp=config.get(\"attention_mlp_pre\", False))\n",
    "\n",
    "        self.gpu = config.get(\"gpu\", False)\n",
    "        self.decoder_output_fn = F.log_softmax if config.get('loss', 'NLL') == 'NLL' else None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\" Must be overrided \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RNNDecoder(Decoder):\n",
    "    def __init__(self, config):\n",
    "        super(RNNDecoder, self).__init__(config)\n",
    "        self.output_size = config.get(\"n_classes\", 32)\n",
    "        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        input = kwargs[\"input\"]\n",
    "        hidden = kwargs[\"hidden\"]\n",
    "        # RNN (Eq 7 paper)\n",
    "        embedded = self.embedding(input).unsqueeze(0)\n",
    "        rnn_input = torch.cat((embedded, hidden.unsqueeze(0)), 2)  # NOTE : Tf concats `lambda inputs, attention: array_ops.concat([inputs, attention], -1)`.\n",
    "        # rnn_output, rnn_hidden = self.rnn(rnn_input.transpose(1, 0), hidden.unsqueeze(0))\n",
    "        rnn_output, rnn_hidden = self.rnn(embedded.transpose(1, 0), hidden.unsqueeze(0))\n",
    "        output = rnn_output.squeeze(1)\n",
    "        output = self.character_distribution(output)\n",
    "\n",
    "        if self.decoder_output_fn:\n",
    "            output = self.decoder_output_fn(output, -1)\n",
    "\n",
    "        return output, rnn_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class BahdanauDecoder(Decoder):\n",
    "    \"\"\" \n",
    "        Bahdanau 型の注意付き復号化器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BahdanauDecoder, self).__init__(config)\n",
    "        self.output_size = config.get(\"n_classes\", 32)\n",
    "        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param input: [B]\n",
    "        :param prev_context: [B, H]\n",
    "        :param prev_hidden: [B, H]\n",
    "        :param encoder_outputs: [B, T, H]\n",
    "        :return: output (B), context (B, H), prev_hidden (B, H), weights (B, T)\n",
    "        \"\"\"\n",
    "\n",
    "        input = kwargs[\"input\"]\n",
    "        prev_hidden = kwargs[\"prev_hidden\"]\n",
    "        encoder_outputs = kwargs[\"encoder_outputs\"]\n",
    "        seq_len = kwargs.get(\"seq_len\", None)\n",
    "\n",
    "        # 入力のチェック\n",
    "        assert input.size() == torch.Size([self.batch_size])\n",
    "        assert prev_hidden.size() == torch.Size([self.batch_size, self.hidden_size])\n",
    "\n",
    "        # 注意の重み\n",
    "        weights = self.attention.forward(prev_hidden, encoder_outputs, seq_len)  # B x T\n",
    "        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)           # [B x H]\n",
    "\n",
    "        # 埋め込み層\n",
    "        embedded = self.embedding(input).unsqueeze(0)\n",
    "        assert embedded.size() == torch.Size([1, self.batch_size, self.embedding_dim])\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context.unsqueeze(0)), 2)\n",
    "        outputs, hidden = self.rnn(rnn_input.transpose(1, 0), prev_hidden.unsqueeze(0)) # 1 x B x N, B x N\n",
    "\n",
    "        # output = self.proj(torch.cat((outputs.squeeze(0), context), 1))\n",
    "        output = self.character_distribution(outputs.squeeze(0))\n",
    "\n",
    "        if self.decoder_output_fn:\n",
    "            output = self.decoder_output_fn(output, -1)\n",
    "\n",
    "        if len(output.size()) == 3:\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "        return output, hidden.squeeze(0), weights\n",
    "\n",
    "\n",
    "class LuongDecoder(Decoder):\n",
    "    \"\"\"\n",
    "        Luong 型の注意付き復号化器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LuongDecoder, self).__init__(config)\n",
    "        self.output_size = config.get(\"n_classes\", 32)\n",
    "        self.character_distribution = nn.Linear(2*self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param input: [B]\n",
    "        :param prev_context: [B, H]\n",
    "        :param prev_hidden: [B, H]\n",
    "        :param encoder_outputs: [B, T, H]\n",
    "        :return: output (B, V), context (B, H), prev_hidden (B, H), weights (B, T)\n",
    "\n",
    "        https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n",
    "        TF says : Perform a step of attention-wrapped RNN.\n",
    "        - Step 1: Mix the `inputs` and previous step's `attention` output via\n",
    "          `cell_input_fn`.\n",
    "        - Step 2: Call the wrapped `cell` with this input and its previous state.\n",
    "        - Step 3: Score the cell's output with `attention_mechanism`.\n",
    "        - Step 4: Calculate the alignments by passing the score through the\n",
    "          `normalizer`.\n",
    "        - Step 5: Calculate the context vector as the inner product between the\n",
    "          alignments and the attention_mechanism's values (memory).\n",
    "        - Step 6: Calculate the attention output by concatenating the cell output\n",
    "          and context through the attention layer (a linear layer with\n",
    "          `attention_layer_size` outputs).\n",
    "        Args:\n",
    "          inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n",
    "          state: An instance of `AttentionWrapperState` containing\n",
    "            tensors from the previous time step.\n",
    "        Returns:\n",
    "          A tuple `(attention_or_cell_output, next_state)`, where:\n",
    "          - `attention_or_cell_output` depending on `output_attention`.\n",
    "          - `next_state` is an instance of `AttentionWrapperState`\n",
    "             containing the state calculated at this time step.\n",
    "        Raises:\n",
    "          TypeError: If `state` is not an instance of `AttentionWrapperState`.\n",
    "\n",
    "        \"\"\"\n",
    "        input = kwargs[\"input\"]\n",
    "        prev_hidden = kwargs[\"prev_hidden\"]\n",
    "        encoder_outputs = kwargs[\"encoder_outputs\"]\n",
    "        seq_len = kwargs.get(\"seq_len\", None)\n",
    "\n",
    "        # RNN 式(7)\n",
    "        embedded = self.embedding(input).unsqueeze(1) # [B, H]\n",
    "        prev_hidden = prev_hidden.unsqueeze(0)\n",
    "        # rnn_input = torch.cat((embedded, prev_context), -1) \n",
    "        # NOTE : Tf concats `lambda inputs, attention: array_ops.concat([inputs, attention], -1)`.\n",
    "        # rnn_output, hidden = self.rnn(rnn_input.transpose(1, 0), prev_hidden)\n",
    "        rnn_output, hidden = self.rnn(embedded, prev_hidden)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "\n",
    "        # Attention weights 式(6)\n",
    "        weights = self.attention.forward(rnn_output, encoder_outputs, seq_len) # B x T\n",
    "        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)  # [B x N]\n",
    "\n",
    "        # Projection 式 (8)\n",
    "        # Don't apply tanh on outputs, it fucks everything up\n",
    "        output = self.character_distribution(torch.cat((rnn_output, context), 1))\n",
    "\n",
    "        # Apply log softmax if loss is NLL\n",
    "        if self.decoder_output_fn:\n",
    "            output = self.decoder_output_fn(output, -1)\n",
    "\n",
    "        if len(output.size()) == 3:\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "        return output, hidden.squeeze(0), weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea46f04-a505-4bed-9e3e-003ac94884fa",
   "metadata": {
    "id": "cea46f04-a505-4bed-9e3e-003ac94884fa"
   },
   "source": [
    "## 3.2 注意の得点化\n",
    "\n",
    "* `AttentionDecoder` の心臓部には `Attention` モジュールがあります。\n",
    "* このモジュールにより， さまざまな注意スコアを計算することができます。\n",
    "* 内積型 (Luong) と 加算型 (Bahdanau) の 2 種類がある。\n",
    "\n",
    "* そこで テンソル間 (キーとクエリと呼ばれることもある) の類似性を計算するオプションを持つカスタム Attention モジュールを定義しています。 \n",
    "* スコアは正規化され， 類似性スコアのソフトマックス関数を用いて確率を定義します。 \n",
    "* このモジュールは， 重み計算でパディングをマスクするために， カスタムの mask_3d 関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29dec8fb-2c0a-4a9f-a74e-b1501a9409d4",
   "metadata": {
    "id": "29dec8fb-2c0a-4a9f-a74e-b1501a9409d4"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        last_hidden: (batch_size, hidden_size)\n",
    "        encoder_outputs: (batch_size, max_time, hidden_size)\n",
    "    Returns:\n",
    "        attention_weights: (batch_size, max_time)\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, method=\"dot\", mlp=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if method == 'dot':\n",
    "            pass\n",
    "        elif method == 'general':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        elif method == \"concat\":\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        elif method == 'bahdanau':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.mlp = mlp\n",
    "        if mlp:\n",
    "            self.phi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.psi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n",
    "        batch_size, seq_lens, _ = encoder_outputs.size()\n",
    "        if self.mlp:\n",
    "            last_hidden = self.phi(last_hidden)\n",
    "            encoder_outputs = self.psi(encoder_outputs)\n",
    "\n",
    "        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n",
    "        # attn_energies = Variable(torch.zeros(batch_size, seq_lens))  # B x S\n",
    "\n",
    "        if seq_len is not None:\n",
    "            attention_energies = mask_3d(attention_energies, seq_len, -float('inf'))\n",
    "\n",
    "        return F.softmax(attention_energies, -1)\n",
    "\n",
    "    def _score(self, last_hidden, encoder_outputs, method):\n",
    "        \"\"\"\n",
    "        Computes an attention score\n",
    "        :param last_hidden: (batch_size, hidden_dim)\n",
    "        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n",
    "        :param method: str (`dot`, `general`, `concat`)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # assert last_hidden.size() == torch.Size([batch_size, self.hidden_size]), last_hidden.size()\n",
    "        assert encoder_outputs.size()[-1] == self.hidden_size\n",
    "\n",
    "        if method == 'dot':\n",
    "            last_hidden = last_hidden.unsqueeze(-1)\n",
    "            return encoder_outputs.bmm(last_hidden).squeeze(-1)\n",
    "\n",
    "        elif method == 'general':\n",
    "            x = self.Wa(last_hidden)\n",
    "            x = x.unsqueeze(-1)\n",
    "            return encoder_outputs.bmm(x).squeeze(-1)\n",
    "\n",
    "        elif method == \"concat\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            #x = F.tanh(self.Wa(torch.cat((x, encoder_outputs), 1)))\n",
    "            x = torch.tanh(self.Wa(torch.cat((x, encoder_outputs), 1)))\n",
    "            return x.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        elif method == \"bahdanau\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            #out = F.tanh(self.Wa(x) + self.Ua(encoder_outputs))\n",
    "            out = torch.tanh(self.Wa(x) + self.Ua(encoder_outputs))\n",
    "            return out.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'score={}, mlp_preprocessing={}'.format(\n",
    "            self.method, self.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7775f1f7-a78d-46d1-b922-4d7888ac9339",
   "metadata": {
    "id": "7775f1f7-a78d-46d1-b922-4d7888ac9339"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence to sequence モデルのクラス\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.SOS = config.get(\"start_index\", 1),\n",
    "        self.vocab_size = config.get(\"n_classes\", 32)\n",
    "        self.batch_size = config.get(\"batch_size\", 1)\n",
    "        self.sampling_prob = config.get(\"sampling_prob\", 0.)\n",
    "        self.gpu = config.get(\"gpu\", False)\n",
    "\n",
    "        # Encoder\n",
    "        if config[\"encoder\"] == \"PyRNN\":\n",
    "            self._encoder_style = \"PyRNN\"\n",
    "            self.encoder = EncoderPyRNN(config)\n",
    "        else:\n",
    "            self._encoder_style = \"RNN\"\n",
    "            self.encoder = EncoderRNN(config)\n",
    "\n",
    "        # Decoder\n",
    "        self.use_attention = config[\"decoder\"] != \"RNN\"\n",
    "        if config[\"decoder\"] == \"Luong\":\n",
    "            self.decoder = LuongDecoder(config)\n",
    "        elif config[\"decoder\"] == \"Bahdanau\":\n",
    "            self.decoder = BahdanauDecoder(config)\n",
    "        else:\n",
    "            self.decoder = RNNDecoder(config)\n",
    "\n",
    "        if config.get('loss') == 'cross_entropy':\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "            config['loss'] = 'cross_entropy'\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.NLLLoss(ignore_index=0)\n",
    "            config['loss'] = 'NLL'\n",
    "        self.loss_type = config['loss']\n",
    "        #print(config)\n",
    "\n",
    "    def encode(self, x, x_len):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        init_state = self.encoder.init_hidden(batch_size)\n",
    "        if self._encoder_style == \"PyRNN\":\n",
    "            encoder_outputs, encoder_state, input_lengths = self.encoder.forward(x, init_state, x_len)\n",
    "        else:\n",
    "            encoder_outputs, encoder_state = self.encoder.forward(x, init_state, x_len)\n",
    "\n",
    "        assert encoder_outputs.size()[0] == self.batch_size, encoder_outputs.size()\n",
    "        assert encoder_outputs.size()[-1] == self.decoder.hidden_size\n",
    "\n",
    "        if self._encoder_style == \"PyRNN\":\n",
    "            return encoder_outputs, encoder_state.squeeze(0), input_lengths\n",
    "        return encoder_outputs, encoder_state.squeeze(0)\n",
    "\n",
    "    def decode(self, encoder_outputs, encoder_hidden, targets, targets_lengths, input_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_outputs: (B, T, H)\n",
    "            encoder_hidden: (B, H)\n",
    "            targets: (B, L)\n",
    "            targets_lengths: (B)\n",
    "            input_lengths: (B)\n",
    "        Vars:\n",
    "            decoder_input: (B)\n",
    "            decoder_context: (B, H)\n",
    "            hidden_state: (B, H)\n",
    "            attention_weights: (B, T)\n",
    "        Outputs:\n",
    "            alignments: (L, T, B)\n",
    "            logits: (B*L, V)\n",
    "            labels: (B*L)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        max_length = targets.size()[1]\n",
    "        # decoder_attns = torch.zeros(batch_size, MAX_LENGTH, MAX_LENGTH)\n",
    "        decoder_input = Variable(torch.LongTensor([self.SOS] * batch_size)).squeeze(-1)\n",
    "        decoder_context = encoder_outputs.transpose(1, 0)[-1]\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        alignments = Variable(torch.zeros(max_length, encoder_outputs.size(1), batch_size))\n",
    "        logits = Variable(torch.zeros(max_length, batch_size, self.decoder.output_size))\n",
    "\n",
    "        if self.gpu:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_context = decoder_context.cuda()\n",
    "            logits = logits.cuda()\n",
    "\n",
    "        for t in range(max_length):\n",
    "            # The decoder accepts, at each time step t :\n",
    "            # - an input, [B]\n",
    "            # - a context, [B, H]\n",
    "            # - an hidden state, [B, H]\n",
    "            # - encoder outputs, [B, T, H]\n",
    "\n",
    "            check_size(decoder_input, self.batch_size)\n",
    "            check_size(decoder_hidden, self.batch_size, self.decoder.hidden_size)\n",
    "            # The decoder outputs, at each time step t :\n",
    "            # - an output, [B]\n",
    "            # - a context, [B, H]\n",
    "            # - an hidden state, [B, H]\n",
    "            # - weights, [B, T]\n",
    "\n",
    "            if self.use_attention:\n",
    "                check_size(decoder_context, self.batch_size, self.decoder.hidden_size)\n",
    "                outputs, decoder_hidden, attention_weights = self.decoder.forward(\n",
    "                    input=decoder_input.long(),\n",
    "                    prev_hidden=decoder_hidden,\n",
    "                    encoder_outputs=encoder_outputs,\n",
    "                    seq_len=input_lengths)\n",
    "                alignments[t] = attention_weights.transpose(1, 0)\n",
    "            else:\n",
    "                outputs, hidden = self.decoder.forward(\n",
    "                    input=decoder_input.long(),\n",
    "                    hidden=decoder_hidden)\n",
    "\n",
    "            # print(outputs[0])\n",
    "            logits[t] = outputs\n",
    "\n",
    "            use_teacher_forcing = random.random() > self.sampling_prob\n",
    "\n",
    "            if use_teacher_forcing and self.training:\n",
    "                decoder_input = targets[:, t]\n",
    "\n",
    "            # SCHEDULED SAMPLING\n",
    "            # We use the target sequence at each time step which we feed in the decoder\n",
    "            else:\n",
    "                # TODO Instead of taking the direct one-hot prediction from the previous time step as the original paper\n",
    "                # does, we thought it is better to feed the distribution vector as it encodes more information about\n",
    "                # prediction from previous step and could reduce bias.\n",
    "                topv, topi = outputs.data.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "\n",
    "        labels = targets.contiguous().view(-1)\n",
    "\n",
    "        if self.loss_type == 'NLL': # ie softmax already on outputs\n",
    "            mask_value = -float('inf')\n",
    "            print(torch.sum(logits, dim=2))\n",
    "        else:\n",
    "            mask_value = 0\n",
    "\n",
    "        logits = mask_3d(logits.transpose(1, 0), targets_lengths, mask_value)\n",
    "        logits = logits.contiguous().view(-1, self.vocab_size)\n",
    "\n",
    "        return logits, labels.long(), alignments\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_loss(logits, labels):\n",
    "\n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        tag_pad_token = 0\n",
    "        mask = (labels > tag_pad_token).float()\n",
    "\n",
    "        # count how many tokens we have\n",
    "        nb_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "        logits = logits[range(logits.shape[0]), labels] * mask\n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = -torch.sum(logits) / nb_tokens\n",
    "\n",
    "        return ce_loss\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y, x_len, y_len = batch\n",
    "        if self.gpu:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            x_len = x_len.cuda()\n",
    "            y_len = y_len.cuda()\n",
    "\n",
    "        if self._encoder_style == \"PyRNN\":\n",
    "            encoder_out, encoder_state, x_len = self.encode(x, x_len)\n",
    "        else:\n",
    "            encoder_out, encoder_state = self.encode(x, x_len)\n",
    "        logits, labels, alignments = self.decode(encoder_out, encoder_state, y, y_len, x_len)\n",
    "        return logits, labels, alignments\n",
    "\n",
    "    def loss(self, batch):\n",
    "        logits, labels, alignments = self.step(batch)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        # loss2 = self.custom_loss(logits, labels)\n",
    "        return loss, logits, labels, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bcccc44-00c4-41aa-b8c5-eb0145917967",
   "metadata": {
    "id": "4bcccc44-00c4-41aa-b8c5-eb0145917967"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, state):\n",
    "    epoch, n_epochs, train_steps = state\n",
    "\n",
    "    losses = []\n",
    "    cers = []\n",
    "\n",
    "    # t = tqdm.tqdm(total=min(len(train_loader), train_steps))\n",
    "    t = tqdm.tqdm(train_loader)\n",
    "    model.train()\n",
    "\n",
    "    for batch in t:\n",
    "        t.set_description(f\"エポック {epoch+1:.0f}/{n_epochs:.0f} (train={model.training})\")\n",
    "        loss, _, _, _ = model.loss(batch)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad() # 勾配のリセット\n",
    "        loss.backward()       # 勾配の計算\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2) # 勾配爆発抑制のため\n",
    "        optimizer.step()      # 計算した勾配に基づいて，パラメータの更新\n",
    "\n",
    "        t.set_postfix(損失=f'{loss.item():05.3f}', 平均損失=f'{np.mean(losses):05.3f}')\n",
    "        t.update()\n",
    "\n",
    "    return model, optimizer\n",
    "    # print(\" 訓練終了: 損失:{np.mean(losses):05.3f} , cer={np.mean(cers)*100:03.1f}\")\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader):\n",
    "\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    t = tqdm.tqdm(eval_loader)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in t:\n",
    "            #t.set_description(\" 評価... (train={})\".format(model.training))\n",
    "            loss, logits, labels, alignments = model.loss(batch)\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            # acc = np.sum(np.argmax(preds, -1) == labels.detach().cpu().numpy()) / len(preds)\n",
    "            acc = 100 * editdistance.eval(np.argmax(preds, -1), labels.detach().cpu().numpy()) / len(preds)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "            #t.set_postfix(avg_acc='{:05.3f}'.format(np.mean(accs)), 平均損失='{:05.3f}'.format(np.mean(losses)))\n",
    "            t.set_postfix(平均精度=f'{np.mean(accs):05.3f}', 平均損失='{np.mean(losses):05.3f}')\n",
    "            t.update()\n",
    "        align = alignments.detach().cpu().numpy()[:, :, 0]\n",
    "\n",
    "    # Uncomment if you want to visualise weights\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.pcolormesh(align)\n",
    "    fig.savefig(\"att__.png\")\n",
    "    print(f\"  最終評価 : 平均損失値 {np.mean(losses):05.3f}, 平均精度 {np.mean(accs):03.1f}\")\n",
    "    return {'loss': np.mean(losses), 'cer': np.mean(accs)*100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fd263a3-3415-42c5-9c36-517ee4bfdf3a",
   "metadata": {
    "id": "6fd263a3-3415-42c5-9c36-517ee4bfdf3a"
   },
   "outputs": [],
   "source": [
    "# 各モデルのパラメータ，変更して実習を行います\n",
    "vanilla_setup = {\n",
    "    \"decoder\": \"RNN\",\n",
    "    \"encoder\": \"RNN\",\n",
    "    \"n_channels\": 4,\n",
    "    \"encoder_hidden\": 64,\n",
    "    \"encoder_layers\": 1,\n",
    "    \"encoder_dropout\": 0.0,\n",
    "    \"bidirectional_encoder\": False,\n",
    "    \"decoder_hidden\": 64,\n",
    "    \"decoder_layers\": 1,\n",
    "    \"decoder_dropout\": 0.1,\n",
    "    \"bidirectional_decoder\": False,\n",
    "    \"n_classes\": 7,\n",
    "    \"batch_size\": 4,\n",
    "    \"sampling_prob\": 0.0,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"attention_score\": \"general\",\n",
    "    \"attention_mlp_pre\": False,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"gpu\": False,\n",
    "    \"loss\": \"cross_entropy\"\n",
    "}\n",
    "\n",
    "bahdanau_setup = {\n",
    "    \"decoder\": \"Bahdanau\",\n",
    "    \"encoder\": \"RNN\",\n",
    "    \"n_channels\": 4,\n",
    "    \"encoder_hidden\": 64,\n",
    "    \"encoder_layers\": 1,\n",
    "    \"encoder_dropout\": 0.0,\n",
    "    \"bidirectional_encoder\": False,\n",
    "    \"decoder_hidden\": 64,\n",
    "    \"decoder_layers\": 1,\n",
    "    \"decoder_dropout\": 0.1,\n",
    "    \"bidirectional_decoder\": False,\n",
    "    \"n_classes\": 7,\n",
    "    \"batch_size\": 4,\n",
    "    \"sampling_prob\": 0.0,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"attention_score\": \"bahdanau\",\n",
    "    \"attention_mlp_pre\": False,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"gpu\": False,\n",
    "    \"loss\": \"cross_entropy\"\n",
    "}\n",
    "\n",
    "luong_setup = {\n",
    "    \"decoder\": \"Luong\",\n",
    "    \"encoder\": \"RNN\",\n",
    "    \"n_channels\": 4,\n",
    "    \"encoder_hidden\": 64,\n",
    "    \"encoder_layers\": 1,\n",
    "    \"encoder_dropout\": 0.0,\n",
    "    \"bidirectional_encoder\": False,\n",
    "    \"decoder_hidden\": 64,\n",
    "    \"decoder_layers\": 1,\n",
    "    \"decoder_dropout\": 0.1,\n",
    "    \"bidirectional_decoder\": False,\n",
    "    \"n_classes\": 7,\n",
    "    \"batch_size\": 4,\n",
    "    \"sampling_prob\": 0.0,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"attention_score\": \"general\",\n",
    "    \"attention_mlp_pre\": False,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"gpu\": False,\n",
    "    \"loss\": \"cross_entropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb28201-9de6-435b-a2be-132416afe214",
   "metadata": {
    "id": "8fb28201-9de6-435b-a2be-132416afe214"
   },
   "outputs": [],
   "source": [
    "def run(config=vanilla_setup, \n",
    "        n_epochs=20, \n",
    "        train_size=28000):\n",
    "\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    config[\"gpu\"] = torch.cuda.is_available()\n",
    "\n",
    "    dataset = ToyDataset(5, 15)\n",
    "    eval_dataset = ToyDataset(5, 15, type='eval')\n",
    "    #BATCHSIZE = 30\n",
    "    BATCHSIZE = config['batch_size']\n",
    "    \n",
    "    train_loader = data.DataLoader(dataset, batch_size=BATCHSIZE, shuffle=False, collate_fn=pad_collate, drop_last=True)\n",
    "    eval_loader = data.DataLoader(eval_dataset, batch_size=BATCHSIZE, shuffle=False, collate_fn=pad_collate,\n",
    "                                  drop_last=True)\n",
    "    config[\"batch_size\"] = BATCHSIZE\n",
    "\n",
    "    model = Seq2Seq(config)   # モデルの設定\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()  # GPU の設定\n",
    "\n",
    "    # 最適化手法の設定 ここでは Adam を指定しています。SGD でも RMSprop でもお好きなものを指定してください\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "    print('=== モデルの詳細===')\n",
    "    print(model)\n",
    "    for k, v in sorted(config.items(), key=lambda i: i[0]):\n",
    "        # print(\" (\" + k + \") : \" + str(v))\n",
    "        print(f'{k}: {str(v)}')\n",
    "\n",
    "    print(\"=== パラメータの初期化 ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            torch.nn.init.constant_(param, 0.0)\n",
    "        elif 'weight' in name:\n",
    "            torch.nn.init.xavier_normal_(param)\n",
    "\n",
    "    # 実際の訓練を n_epochs 回繰り返す\n",
    "    for epoch in range(n_epochs):\n",
    "        run_state = (epoch, n_epochs, train_size)\n",
    "\n",
    "        # Train needs to return model and optimizer, otherwise the model keeps restarting from zero at every epoch\n",
    "        model, optimizer = train(model, optimizer, train_loader, run_state)\n",
    "        evaluate(model, eval_loader)\n",
    "\n",
    "        # モデルの保存は実装していません\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94f7266-9a77-4355-89fe-7c7d8408b985",
   "metadata": {
    "id": "e94f7266-9a77-4355-89fe-7c7d8408b985"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== モデルの詳細===\n",
      "Seq2Seq(\n",
      "  (encoder): EncoderRNN(\n",
      "    (rnn): GRU(4, 64, batch_first=True)\n",
      "  )\n",
      "  (decoder): LuongDecoder(\n",
      "    (embedding): Embedding(7, 64, padding_idx=0)\n",
      "    (rnn): GRU(64, 64, batch_first=True, dropout=0.1)\n",
      "    (attention): Attention(\n",
      "      score=general, mlp_preprocessing=False\n",
      "      (Wa): Linear(in_features=64, out_features=64, bias=False)\n",
      "    )\n",
      "    (character_distribution): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "attention_mlp_pre: False\n",
      "attention_score: general\n",
      "batch_size: 4\n",
      "bidirectional_decoder: False\n",
      "bidirectional_encoder: False\n",
      "decoder: Luong\n",
      "decoder_dropout: 0.1\n",
      "decoder_hidden: 64\n",
      "decoder_layers: 1\n",
      "embedding_dim: 64\n",
      "encoder: RNN\n",
      "encoder_dropout: 0.0\n",
      "encoder_hidden: 64\n",
      "encoder_layers: 1\n",
      "gpu: False\n",
      "learning_rate: 0.001\n",
      "loss: cross_entropy\n",
      "n_channels: 4\n",
      "n_classes: 7\n",
      "sampling_prob: 0.0\n",
      "=== パラメータの初期化 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "エポック 1/2 (train=True): 100%|████████████████| 750/750 [00:17<00:00, 42.16it/s, 平均損失=0.676, 損失=0.108]?it/s]t/s]\n",
      "100%|██████████████████████| 75/75 [00:00<00:00, 120.15it/s, 平均損失={np.mean(losses):05.3f}, 平均精度=1.719], ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  最終評価 : 平均損失値 0.194, 平均精度 1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "エポック 2/2 (train=True): 100%|████████████████| 750/750 [00:18<00:00, 41.58it/s, 平均損失=0.090, 損失=0.032]?it/s]t/s]\n",
      "100%|██████████████████████| 75/75 [00:00<00:00, 127.27it/s, 平均損失={np.mean(losses):05.3f}, 平均精度=0.784], ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  最終評価 : 平均損失値 0.111, 平均精度 0.8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = vanilla_setup\n",
    "setup = bahdanau_setup\n",
    "setup = luong_setup\n",
    "\n",
    "run(config=setup, n_epochs=2, train_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35KyXNcxNPFQ",
   "metadata": {
    "id": "35KyXNcxNPFQ"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3c154-435b-416e-8b00-bc44640577f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "2021_1022Two_attentions_additive_and_multiplicative_Seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
