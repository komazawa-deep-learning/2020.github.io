{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_1105Sarsa_Q_learning_expected_sarsa.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPjzy7JLm67sUiZV5hf2KPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1105Sarsa_Q_learning_expected_sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4xBE-UFfaP8"
      },
      "source": [
        "# TD (時間差)学習, SARSA, 期待 SARSA, Q 学習 と Python 実装\n",
        "\n",
        "<!-- date: 2020-0519-->\n",
        "- title: Reinforcement learning: Temporal-Difference, SARSA, Q-Learning & Expected SARSA in python\n",
        "- author: Vaibhav Kumar\n",
        "- Date: May 9, 2019\n",
        "- Original: <https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e>\n",
        "\n",
        "<!-- \n",
        "# TD, SARSA, Q-Learning and Expected SARSA along with their python implementation and comparison\n",
        "-->\n",
        "\n",
        "<!--\n",
        "> If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-diff\n",
        "erence (TD) learning. — Andrew Barto and Richard S. Sutton\n",
        "-->\n",
        "\n",
        "> 強化学習の中心的で斬新なアイデアを一つ挙げるとすれば、それは間違いなく時間差（TD）学習であろう。<br/>\n",
        ">    -- アンドリュー・バルト， リチャード・S・サットン\n",
        "\n",
        "<!-- # Pre-requisites\n",
        "- Basics of Reinforcement learning\n",
        "- Markov chains, Markov Decision Process (MDPs)\n",
        "- Bellman equation\n",
        "- Value, policy functions and iterations\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeZ4ohUoPO8"
      },
      "source": [
        "## 0.1. 前提知識\n",
        "\n",
        "- 強化学習の基本\n",
        "- マルコフ連鎖, マルコフ決定プロセス(MDP)\n",
        "- ベルマン方程式\n",
        "- 価値，方策関数，反復\n",
        "\n",
        "<!-- # Model-dependent and model-free reinforcement learning\n",
        "Model-dependent RL algorithms (namely value and policy iterations) work with the help of a transition table. A transiti\n",
        "on table can be thought of as a life hack book which has all the knowledge the agent needs to be successful in the worl\n",
        "d it exists in. Naturally, writing such a book is very tedious and impossible in most cases which is why model dependen\n",
        "t learning algorithms have little practical use.\n",
        "\n",
        "Temporal Difference is a model-free reinforcement learning algorithm. This means that the agent learns through actual e\n",
        "xperience rather than through a readily available all-knowing-hack-book (transition table). This enables us to introduc\n",
        "e stochastic elements and large sequences of state-action pairs. The agent has no idea about the reward and transition \n",
        "systems. It does not know what will happen on taking an arbitrary action at an arbitrary state. The agent has to intera\n",
        "ct with “world” or “environment” and find out for itself.-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeURXFYXoRoZ"
      },
      "source": [
        "## 0.2 強化学習におけるモデル依存学習とモデル自由学習\n",
        "\n",
        "モデル依存 RLアルゴリズム（すなわち、値と方策反復）は，遷移表の助けを借りて動作します。\n",
        "遷移表は、エージェントが存在する世界で成功するために必要なすべての知識が書かれたライフハック本と考えることができます。\n",
        "当然のことながら、そのような本を書くのは非常に面倒で、ほとんどの場合不可能です。\n",
        "\n",
        "Temporal Difference はモデル自由型の強化学習アルゴリズムです。\n",
        "これは、エージェントがすぐに入手可能な全知全能のハックブック（遷移表）ではなく、実際の経験を通して学習することを意味し\n",
        "ます。\n",
        "これにより、確率的な要素と状態-行為ペアの大規模な系列を導入することが可能になります。\n",
        "エージェントは、報酬システムと遷移システムについては何も知りません。\n",
        "エージェントは、任意の状態で任意の行動をとった場合に何が起こるかを知りません。\n",
        "エージェントは、「世界」や「環境」と相互作用し、自分自身で見つけ出さなければなりません。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSE2US7Ml-9A"
      },
      "source": [
        "\n",
        "## 0.3 TD 学習 (時差学習)\n",
        "\n",
        "TD 時間差学習アルゴリズムは、エージェントが取る一つ一つの行動を通して学習することを可能にします。\n",
        "TD 学習では、エピソード（ゴールや終了状態に到達する）ごとではなく、タイムステップ（行動）ごとにエージェントの知識を更新します。\n",
        "\n",
        "$$\n",
        "\\text{新しい状態評価} \\leftarrow \\text{古い状態評価} + \\text{ステップサイズ}\\left[\\text{目標} - \\text{古い状態評価}\\right]\n",
        "$$\n",
        "\n",
        "<!-- The value Target-OldEstimate is called the target error. StepSize is usually denoted by α is also called the learn\n",
        "ing rate. Its value lies between 0 and 1.\n",
        "\n",
        "The equation above helps us achieve **Target** by making updates at every timestep. \n",
        "Target is the utility of a state. \n",
        "Higher utility means a better state for the agent to transition into. For the sake of brevity of this post, I have assu\n",
        "med the readers know about the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation). According to it, the\n",
        " utility of a state is the expected value of the discounted reward as follows:-->\n",
        "\n",
        "上式の 目標-古い状態評価 を目標誤差と呼びます。\n",
        "ステップサイズ は 通常 $\\alpha$ で表され、学習率と呼ばれます。$\\alpha$ の値は 0 から 1 の間になります。\n",
        "\n",
        "上式は、タイムステップごとに更新を行うことで、**Target** を達成するのに役立ちます。\n",
        "目標とは状態の効用(関数)です。\n",
        "効用が高ければ、エージェントが移行しやすい状態になることを意味します。\n",
        "ここでは [Bellman 方程式](https://en.wikipedia.org/wiki/Bellman_equation) を知っている と仮定しています。\n",
        "ベルマン方程式から，ある状態の効用は、以下の割引報酬の期待値です。 \n",
        "\n",
        "$$\n",
        "\\text{目標}=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r_{t+k+1}\\right]\n",
        "$$\n",
        "\n",
        "<!-- In layman terms, we are letting an agent run free into a world. The agent has no knowledge of the state, the rewar\n",
        "ds and transitions. It interacts with the environment (make random or informed actions) and learns new estimates (value\n",
        "s of state-action pairs) by updating it’s existing knowledge continuously after taking every action.\n",
        "\n",
        "The discussion till now shall give rise to several questions such as — What is an environment? How will the agent inter\n",
        "act with the environment? How will the agent choose actions i.e what action will the agent take in a particular state (\n",
        "policy)?\n",
        "\n",
        "This is where SARSA and Q-Learning come in. These are the two control policies that will guide our agent in an environm\n",
        "ent and enable it to learn interesting things. But before that, we shall discuss what is the environment.-->\n",
        "\n",
        "\n",
        "平たく言えば、我々はエージェントを世界に自由に走らせます。\n",
        "エージェントは、状態、報酬、遷移についての知識を持っていません。\n",
        "エージェントは環境と相互作用し（ランダムな行動や情報に基づいた行動をとる）、すべての行動をとった後に既存の知識を継続的\n",
        "に更新することで、 新たな推定値（状態と行動のペアの値）を学習します。\n",
        "\n",
        "これまでの議論では、次のようないくつかの疑問が出てきます。\n",
        "エージェントはどのように環境と相互作用するのか？\n",
        "エージェントはどのように行動を選択するのか、すなわち 特定の状態（ポリシー） でどのような行動をとるのか？\n",
        "\n",
        "これが SARSA と Q-学習 の出番です。\n",
        "これらは、環境の中でエージェントを誘導し、興味深いことを学ぶことを可能にする ２ つの 制御ポリシーです。\n",
        "これらを説明する前に、環境とは何かを議論しなければなりません。\n",
        "\n",
        "\n",
        "# 環境\n",
        "<!--\n",
        "An environment can be thought of as a mini-world where an agent can observe discrete states, take actions and observe r\n",
        "ewards by taking those actions. Think of a video game as an environment and yourself as the agent. In the game Doom, yo\n",
        "u as an agent will observe the states (screen frames) and take actions (press keys like Forward, backward, jump, shoot \n",
        "etc) and observe rewards. Killing an enemy would yield you pleasure (utility) and a positive reward while moving ahead \n",
        "won’t yield you much reward but you would still want to do that to get future rewards (find and then kill the enemy). C\n",
        "reating such environments can be tedious and hard ([a team of 7 people worked for more than a year to develop Doom](htt\n",
        "ps://en.wikipedia.org/wiki/Development_of_Doom)).-->\n",
        "\n",
        "環境は、エージェントが離散的な状態を観察し、行動をとり、その行動をとることで報酬を観察することができるミニ世界と考える\n",
        "ことができます。\n",
        "ビデオゲームは環境であり、自分自身がエージェントであると考えてください。\n",
        "ゲーム「ドゥーム」では、エージェントであるあなたは、状態（画面のフレーム）を観察し、アクション（前進、後退、ジャンプ、\n",
        "シュートなどのキーを押す）を行い、報酬を観察します。\n",
        "敵を殺せば喜び (効用) が得られ、前進している間はプラスの報酬が得られ、あまり報酬は得られませんが、将来の報酬(敵を見つけ\n",
        "て殺す) を得るために ゲームをしたいと思うでしょう。\n",
        "このような環境を作るのは面倒で大変な作業です ([7人のチームが1年以上かけて Doom を開発](https://en.wikipedia.org/wiki/Development_of_Doom))。\n",
        "\n",
        "<!-- \n",
        "OpenAI gym comes to the rescue! gym is a python library that has several in-built environments on which you can test va\n",
        "rious reinforcement learning algorithms. It has established itself as an academic standard to share, analyze and compar\n",
        "e results. Gym is very well [documented](https://gym.openai.com/docs/) and super easy to use. You must read the documen\n",
        "ts and familiarize yourself with it before proceeding further.\n",
        "\n",
        "For novel applications of reinforcement learning, you will have to create your own environments. It’s advised to always\n",
        " refer and write gym compatible environments and release them publicly so that everyone can use them. Reading the gym’s\n",
        " source code will help you do that. It is tedious but fun!-->\n",
        "\n",
        "\n",
        "OpenAI gym が登場したことは福音です。\n",
        "gym は 様々な強化学習アルゴリズムをテストできる環境が組み込まれている Python ライブラリ です。\n",
        "結果を共有したり、分析したり、比較したりするための学術的な標準環境としての地位を確立しています。\n",
        "Gym は [ドキュメント](https://gym.openai.com/docs/)が整備されていて、使いやすいです。\n",
        "ドキュメントを読んで慣れておく必要があります。\n",
        "\n",
        "強化学習の応用のためには、自分で環境を作る必要があります。\n",
        "常に gym 互換の環境を参考にして書いて、誰もが使えるように公開しておくことをお勧めします。\n",
        "Gym の ソースコードを読めば、公開ができるようになります。\n",
        "面倒だけど楽しい！ と思っている人にはおすすめです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2voyWwZ5mIkO"
      },
      "source": [
        "# 1. SARSA\n",
        "\n",
        "<!-- > SARSA is acronym for State-Action-Reward-State-Action-->\n",
        "\n",
        "> SARSA とは，State-Action-Reward-State-Action の省略形です\n",
        "\n",
        "<!-- SARSA is an on-policy TD control method. A policy is a state-action pair tuple. In python, you can think of it as \n",
        "a dictionary with keys as the state and values as the action. Policy maps the action to be taken at each state. An on-p\n",
        "olicy control method chooses the action for each state during learning by following a certain policy (mostly the one it\n",
        " is evaluating itself, like in policy iteration). Our aim is to estimate $QQ\\pi(s,a)$ for the current policy $\\pi$ and \n",
        "all state-action $(s-a)$ pairs. We do this using TD update rule applied at every timestep by letting the agent transiti\n",
        "on from one state-action pair to another state-action pair (unlike model dependent RL techniques where the agent transi\n",
        "tions from a state to another state).-->\n",
        "\n",
        "SARSA とははオンポリシーな 時間差制御方式 です。\n",
        "ポリシー (方策) とは 状態 と 動作(行動) とのペアのことです。\n",
        "python では 状態 を キー、動作(行動) を 値 とする 辞書 dict と考えることができます。\n",
        "ポリシー(方策) は各状態 で取るべき 動作（行動） をマッピングします。\n",
        "オンポリシー制御では、学習中にある ポリシー (大抵はポリシー反復 のように自分自身で評価しているもの) に従うことで、 状態\n",
        "ごとに 動作 (行動) を選択します。\n",
        "我々の目的は、現在の 方策 $pi$ と全ての 状態行動 $(s-a)$ のペアについて、 $Q \\pi(s,a)$ を推定することです。\n",
        "これは、ある 状態-動作(行動) の対 から 別の 状態-動作(行動)  のペア に エージェント を遷移させることで、 タイムステップ\n",
        " ごとに 適用される TD 更新規則 を用いて行う (状態 から 別の 状態に エージェントを遷移させる モデル依存型 強化学習技法と\n",
        "は異なります)。 \n",
        "\n",
        "<!-- **Q-value** You must be already familiar with the utility value of a state, Q-value is the same with the only diff\n",
        "erence of being defined over the state-action pair rather than just the state. It’s a mapping between state-action pair\n",
        " and a real number denoting its utility. Q-learning and SARSA are both policy control methods which work on evaluating \n",
        "the optimal Q-value for all action-state pairs.-->\n",
        "\n",
        "**Q-値** 状態の効用値についてはすでにお馴染みでしょうが、Q-値 も同じです。\n",
        "Q-値 は、状態と行動のペアとその効用を表す実数とのマッピングです。\n",
        "Q-学習 と SARSA とは、すべての 行動-状態 のペアに対して 最適な Q-値 を評価する 方策制御手法です。\n",
        "\n",
        "<!-- The update rule for SARSA is:-->\n",
        "\n",
        "SARSA の更新則は:\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\left[ R_{t+1}+\\gamma Q(S_{t+1},A_{t+1}-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "<!--\n",
        "![Source: Introduction to Reinforcement learning by Sutton and Barto — 6.7](https://cdn-images-1.medium.com/max/1280/1*3Ul422CbPyIDJI0XJVun3Q.png) \n",
        "-->\n",
        "\n",
        "<!-- \n",
        "If a state $S$ is terminal (goal state or end state) then, $Q(S,a)=0\\forall a\\in A$ where A is the set of all possible \n",
        "actions\n",
        "-->\n",
        "\n",
        "ある状態 $S$ が終了した場合 (ゴールに達したり，終了状態に陥った場合)， $Q(S,a)=0\\forall a\\in A$ となります。\n",
        "ここで，$A$ は全行動レパートリーを表します。\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*fYuXsaJoyCWuIZu49vx_GA.png\"><br/>\n",
        "Source: Introduction to Reinforcement learning by Sutton and Barto —Chapter 6\n",
        "\n",
        "<div style=\"background-color:lightgray\">\n",
        "\n",
        "- $Q(s,a)$ を全状態 $s$ と 全動作 $a$ について初期化，$Q(\\text{終了状態},\\cdot)=0$ とする\n",
        "- 各エピソードを繰り返す:\n",
        "    - $S$ を初期化する\n",
        "    - $S$ の中から Q 関数に従って $A$ を選ぶ\n",
        "    - $Q(S,A)\\leftarrow Q(S,A)+\\alpha\\left[R+\\gamma Q(S',S')-Q(S,A)\\right]$\n",
        "    - $S\\leftarrow S'$, $A\\leftarrow A'$ \n",
        "- $S$ が収束するまで繰り返す\n",
        "\n",
        "</div>\n",
        "\n",
        "## イプシロン ($\\epsilon$) 貪欲(欲張り) 方策\n",
        "\n",
        "<!-- Epsilon-greedy policy is this: -->\n",
        "イプシロン貪欲な方策とは以下のことを言います:\n",
        "\n",
        "<!-- \n",
        "1. Generate a random number $r\\in[0,1]$\n",
        "2. If $r>\\epsilon$ choose a random action\n",
        "3. Else choose an action derived from the $Q$ values (which yields the maximum utility)-->\n",
        "\n",
        "1. 0 から 1 の範囲 ($r\\in[0,1]$) の乱数 $r$ を一つ発生させます\n",
        "2. もし $r>\\epsilon$ であれあば，ランダムにある行動を選択します\n",
        "3. そうでなければ (すなわち $r\\le\\epsilon$) $Q$ 値 (最大効用をあたえる) 行動を選択します\n",
        "\n",
        "<!-- It shall become more clear after reading the python code.-->\n",
        "\n",
        "<!--It shall become more clear after reading the python code.-->\n",
        "以下に Python コードを示します:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qmVJT8qfmZR"
      },
      "source": [
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HvEc_6VgDVu"
      },
      "source": [
        "<!--\n",
        "source: https://gist.githubusercontent.com/TimeTraveller-San/40a7a2655743bf6230706d45d1201b49/raw/25ddaaf54e946d33576e27e6bab45a40f22864a9/epsilon_greedy.py -->\n",
        "\n",
        "$\\epsilon$ の値は **探索 = 知識利用 のジレンマ** を決定します。英語では exploration-exploitatoin dilemma と呼びます。綴が似ているために英語で覚えた方が良いでしょう。\n",
        "\n",
        "- $\\epsilon$ が大きければ、乱数 $r$ が $\\epsilon$ より大きくなることはほとんどなく、 ランダムな行動はほとんど起こらない (探索が少なく、知識利用 が多くなる)。\n",
        "- $\\epsilon$ が小さければ、乱数 $r$ は $\\epsilon$ よりも大きくなることが多いので、 エージェント は より 多くのランダム な 行動を選択することになる。\n",
        "\n",
        "このような確率的な性質を利用して、エージェントは環境をより探索することができるようになります。\n",
        "\n",
        "経験則として、$\\epsilon$ は通常 $0.9$ に選ばれます。\n",
        "ですが $\\epsilon$ は 環境タイプに応じて変化させることができます。\n",
        "いくつかのケースでは、より高い探索に続いてより高い知識利用 を可能にするために、時間の経過とともに $\\epsilon$ は 緩和 漸減 されます。\n",
        "\n",
        "以下 [OpenAI Gym Taxi-v3 環境](https://gym.openai.com/envs/Taxi-v3/) に適用された SARSA のシンプルな Python コードを示します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-2j4prnnW3I"
      },
      "source": [
        "## Taxi-v3 環境の説明\n",
        "\n",
        "Taxi-v3 課題は [Dietterich2000] で階層型強化学習の問題点を説明するために導入されました。\n",
        "4 つの場所（異なる文字でラベル付けされている）があり， エージェントの仕事は， ある場所で乗客を拾い， 別の場所で降ろすことです。\n",
        "送迎が成功すると +20点，時間がかかるごとに 1 点ずつ減ります。\n",
        "また、違法な乗降行為には 10 ポイントのペナルティがあります。<!-- \n",
        "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. \n",
        "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. \n",
        "You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfYVuu3gCGK"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\"\"\"\n",
        "方策学習 SARSA の python 実装\n",
        "Sutton and  Barto の著書にある SARSA アルゴリズムの python 実装。\n",
        "SARSAと呼ばれる理由は -(state, action, reward, state, action) だからです。\n",
        "SARSA と Q 学習 の唯一の違いは， SARSA が現在のポリシーに基づいて次の動作を決定するのに対し，\n",
        "Q 学習 は次の状態の効用が最大となる行動をとります。\n",
        "\"\"\"\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        total_reward = 0\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode reward: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYobcf66rh0x"
      },
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 3000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "#timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy3pRTdLiNHy"
      },
      "source": [
        "# 3. Q 学習\n",
        "\n",
        "Q-学習 は、方策によらない TD 制御です。\n",
        "SARSA とほぼ同じです。唯一の違いは、次の 動作(行動) $A$ を見つけるための 方策に従うのではなく、 貪欲に 動作(行動)  を選\n",
        "択することです。 \n",
        "SARSA と同様に Q値 を評価することを目的としており、更新則 は次のようになります:\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_{alpha} Q(S_{t+1},a)-Q(S_t,A_t)\\right]\n",
        "$$\n",
        "\n",
        "SARSA では、ある方策 に従って 行動 $A'$ を選択していました。\n",
        "これに対し、上式 Q 学習 では 行動 $A'$ ($a$) は、 $Q$ の最大値を取るだけの 欲張り(貪欲, グリーディ)な 方法で選択されま\n",
        "す。\n",
        "\n",
        "Q 学習のアルゴリズムを以下に示します:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png\" width=\"69%\"><br/>\n",
        "\n",
        "Source: [Introduction to Reinforcement learning by Sutton and Barto — Chapter 6](https://cdn-images-1.medium.com/max/1280/1*Rf_H0YXhSPPm-iyBY2Gnjg.png)\n",
        "</center>\n",
        "\n",
        "Python コードは以下になります:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Vu_7d_iR7r"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Q 学習はオフポリシー学習のpython実装です。\n",
        "Q 学習はオフポリシー学習のpython実装です。\n",
        "これは Sutton and Barto の著書 にある Q 学習アルゴリズムの python 実装です。\n",
        "SARSA と Q 学習の唯一の違いは、SARSA は現在のポリシーに基づいて次の行動を取るのに対し 現在の方針に基づいて次の行動をとるのに対し、Q 学習は次の状態で最大の効用を持つ行動をとることです。行動をとることです。\n",
        "\"\"\"\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"Reached goal!\")\n",
        "                else:\n",
        "                    print(\"Shit! dead x_x\")\n",
        "                time.sleep(3)\n",
        "                break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROXygIyTsDnw"
      },
      "source": [
        "%%time\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 10000\n",
        "max_steps = 2500\n",
        "n_tests = 2\n",
        "timestep_reward = qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = True)\n",
        "print(timestep_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6-TQZN2iwTq"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/9e56f9d09be7d50b795ef2f83be2ba72#file-qlearning-py\n",
        "\n",
        "# 3. 期待 SARSA\n",
        "\n",
        "<!-- Expected SARSA, as the name suggest takes the expectation (mean) of Q values for every possible action in the curr\n",
        "ent state. The target update rule shall make things more clear:-->\n",
        "\n",
        "期待 SARSA とは、その名が示すように、現在の状態で起こりうるすべての行動についての Q値 の期待値(平均値)を取ります。\n",
        "ターゲットの更新則を使うと、より明確になります。\n",
        "\n",
        "$$\n",
        "Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha\\left[R_{t+1}+\\gamma\\mathbb{E}\\left[Q(S_{t+1},A_{t+1}\\vert S_{t+1}\\right]-Q(S_t\n",
        ",A_t)\\right]\\\\ \\\\\n",
        "\\hspace{6em}\\leftarrow Q(S_t,A_t)+\\alpha\\left[\n",
        "R_{t+1}+\\gamma\\sum_{\\alpha}\\pi(a\\vert S_{t+1}) Q(S_{t+1},a) - Q(S_t,A_t)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Python コードを以下に示します:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPDLyhHQirtC"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\"\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action\n",
        "\n",
        "def expected_sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        total_reward = 0\n",
        "        s = env.reset()\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                expected_value = np.mean(Q[s_,:])\n",
        "                # print(Q[s,:], sum(Q[s,:]), len(Q[s,:]), expected_value)\n",
        "                Q[s, a] += alpha * (reward + (gamma * expected_value) - Q[s, a])\n",
        "            s = s_\n",
        "            if done:\n",
        "                if True:\n",
        "                    print(f\"This episode took {t} timesteps and reward {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode reward: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_W61s7usjdg"
      },
      "source": [
        "%%time\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.9\n",
        "episodes = 1000\n",
        "max_steps = 2500\n",
        "n_tests = 20\n",
        "timestep_reward = expected_sarsa(alpha, gamma, epsilon,\n",
        "                                 episodes, max_steps, n_tests,\n",
        "                                 render=False, test=True\n",
        "                                 )\n",
        "print(timestep_reward)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWhV9fnjcMM"
      },
      "source": [
        "source: https://gist.github.com/TimeTraveller-San/5bd93710e118633e0793dc5d0b92b19a#file-expectedsarsa-py\n",
        "\n",
        "# 比較\n",
        "\n",
        "以下のパラメータで 3 つのアルゴリズムを比較しました\n",
        "\n",
        "```python\n",
        "lpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 2000\n",
        "max_steps = 2500 # (max number of time steps possible in a single episode)\n",
        "```\n",
        "\n",
        "ここでは、上記の 3 つの方策制御方法の比較 をプロットします。\n",
        "\n",
        "## 収束\n",
        "\n",
        "以下のプロットが示すように、Q 学習 (緑) は SARSA(オレンジ) と 期待 SARSA(青) の両者より先に収束しました。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*0LpFOud2FUKciZ9jPd1ZBA.png\" width=\"49%\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — Convergence comparison\n",
        "</center>\n",
        "\n",
        "## 成績\n",
        "\n",
        "実装した 3 つのアルゴリズムでは、Q-学習 が最も良く、期待 SARSA が最も悪い性能のようです。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1280/1*IC0L25IzedYZ_TujMHpRKg.png\"><br/>\n",
        "SARSA, Q-learning & Expected SARSA — performance comparison!\n",
        "</center>\n",
        "\n",
        "# 結語\n",
        "\n",
        "<!-- Temporal Difference learning is the most important reinforcement learning concept. It’s further derivatives like D\n",
        "QN and double DQN (I may discuss them later in another post) have achieved groundbreaking results renowned in the field\n",
        " of AI. Google’s alpha go used DQN algorithm along with CNNs to defeat the go world champion. You are now equipped with\n",
        " the theoretical and practical knowledge of basic TD, go out and explore!\n",
        " -->\n",
        "\n",
        "時間差学習 は 最も重要な強化学習の概念です。 DQN や ダブルDQN のような、さらに派生したもの\n",
        "は、AI の分野で有名な画期的な結果を達成しています。\n",
        "Google の アルファ碁は、囲碁の世界チャンピオンを倒すために、CNN と DQN アルゴリズムを使用しました。\n"
      ]
    }
  ]
}