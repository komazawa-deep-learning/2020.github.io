{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1022transformer_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/_asakawa/study/2022komazawa-deep-learning_komazawa-deep-learning.github.io/2021notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d0lJuQO1Jb7w"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljNDua7gJb7y"
   },
   "source": [
    "\n",
    "# `nn.Transformer` と `TorchText` を用いた言語モデル\n",
    "\n",
    "このファイルは，PyTorch チュートリアルにあったファイルを和訳したものです。\n",
    "\n",
    "- 原典: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/dca13261bbb4e9809d1a3aa521d22dd7/transformer_tutorial.ipynb\n",
    "\n",
    "このチュートリアルでは、[nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) モジュールを使ったsequence-to-sequenceモデルの学習を行います。\n",
    "\n",
    "PyTorch 1.2 のリリースには [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 論文に基づいた標準的なトランスフォーマーモジュールが含まれています。\n",
    "リカレントニューラルネットワーク（RNN）と比較して、トランスフォーマーモデルは、多くの seq2seq 課題において，より並列化が可能でありながら， 優れた品質であることが証明されています。\n",
    "``nn.Transformer`` モジュールは [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) として実装されている注意機構に依存して，入出力間の大域的な依存関係を求めることができます。\n",
    "\n",
    "``nn.Transformer`` モジュールは高度にモジュール化されており、単一の成分 (例えば [`nn.TransformerEncoder`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)）を簡単に適応／合成することができます。\n",
    "\n",
    "<center>\n",
    "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\"><br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJM0cscZJb7z"
   },
   "source": [
    "# モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snYuXceCJb70"
   },
   "source": [
    "このチュートリアルでは，言語モデル課題で ``nn.TransformerEncoder`` モデルを学習します。\n",
    "言語モデルの課題は， 与えられた単語 (または単語列) が，ある単語の列に続く可能性について確率を割り当てることです。\n",
    "一連のトークンは，まず埋め込み層に渡され， 続いて単語の順序を考慮した位置符号化層に渡されます (詳細は次段落参照)。\n",
    "``nn.TransformerEncoder``は、[nn.TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)という複数の層で構成されています。\n",
    "また，`nn.TransformerEncoder` の **自己注意** 層は， 系列の前の位置にしか注目することができないため， 入力系列とともに， 正方形の注意マスクが必要となります。\n",
    "言語モデル課題では， 未来の位置にあるトークンはすべてマスクされます。\n",
    "出力された単語の確率分布を作成するために `nn.TransformerEncoder` モデルの出力は， 線形層を通過させた後，対数ソフトマックス関数を通過させます。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wNJG-k-xJb70"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dqZv88RJb71"
   },
   "source": [
    "`PositionalEncoding` モジュールは、系列内のトークンの相対的または絶対的な位置に関する何らかの情報を注入します。\n",
    "位置符号化器は埋め込み層と同じ次元なので， この 2 つを合計することができます。\n",
    "ここでは，異なる周波数の `サイン波` と `コサイン波`  関数を使用しています。\n",
    "\n",
    "---\n",
    "\n",
    "### (訳注) \n",
    "Transformer: Attention is all you need 原著論文中の 位置符号化器は以下のように定義されている:\n",
    "まず，マルチヘッド自己注意 (MHSA) は，クエリ，キー，バリューベクトルを学習すべきベクトルとして次式で定義される:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}\\left(Q,K,V\\right)=\\text{Concat}\\left(\\mathop{head}_1,\\ldots,\\mathop{head}_h\\right)W^O\n",
    "$$\n",
    "\n",
    "ここで，各ヘッドは, $\\text{head}_i =\\text{Attention}\\left(QW_i^Q,KW_i^K,VW_i^V\\right)$ である。\n",
    "\n",
    "それぞれの次元は以下のとおりである:\n",
    "<!--The projections are parameter matrices-->\n",
    "\n",
    "- $W_i^Q\\in\\mathbb{R}^{d_{\\mathop{model}}\\times d_k}$,\n",
    "- $W_i^K \\in\\mathbb{R}^{d_{\\mathop{model}}\\times d_k}$,\n",
    "- $W_i^V\\in\\mathbb{R}^{d_{\\mathop{model}}\\times d_v}$, \n",
    "- $W^O\\in\\mathbb{R}^{hd_v\\times d_{\\mathop{model}}}$. $h=8$\n",
    "- $d_k=d_v=\\frac{d_{\\mathop{model}}}{h}=64$\n",
    "\n",
    "$$\\text{FFN}(x)=\\max\\left(0,xW_1+b_1\\right)W_2+b_2$$\n",
    "\n",
    "<!--\n",
    "$$\\text{PE}_{(\\mathop{pos},2i)} = \\sin\\left(\\frac{\\mathop{pos}}{10000^{\\frac{2i}{d_{\\mathop{model}}}}}\\right)$$\n",
    "\n",
    "$$\\text{PE}_{(\\mathop{pos},2i+1)} = \\cos\\left(\\frac{\\mathop{pos}}{10000^{\\frac{2i}{d_{\\mathop{model}}}}}\\right)$$\n",
    "-->\n",
    "\n",
    "### (続 訳注) 位置符号器 Position encoders\n",
    "トランスフォーマーの入力には，上述の単語表現に加えて，位置符号器からの信号も重ね合わされる。\n",
    "位置 $i$ の信号は次式で周波数領域へと変換される:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{PE}_{(\\text{pos},2i)} &= \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\\\\\n",
    "\\text{PE}_{(\\text{pos},2i+1)} &= \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "位置符号器による位置表現は，$i$ 番目の位置情報をワンホット表現するのではなく，周波数領域に変換することで周期情報を表現する試みと見なし得る。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TThZpFfYJb71"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuEmvHXKQD6W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PE = PositionalEncoding(d_model=24, max_len=300)\n",
    "\n",
    "#PE(torch.rand(8))\n",
    "#print(torch.ones(4))\n",
    "X = PE(torch.Tensor(np.random.randn(24))).detach().numpy()\n",
    "plt.plot(range(len(X[0])), X[0])\n",
    "plt.plot(X[1][0])\n",
    "plt.plot(X[2][0])\n",
    "plt.plot(X[3][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kom4TsJJJb72"
   },
   "source": [
    "# データのロードとバッチ化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QotPIz0EJb72"
   },
   "source": [
    "このチュートリアルでは `torchtext` を用いて Wikitext-2  データセットを生成します。．vocab オブジェクトは訓練データセットに基づいて構築され， トークンをテンソルに数値化するために使用されます。\n",
    "Wikitext-2 では， 希少なトークンを  `<unk>` として表現します。\n",
    "1 次元ベクトルの系列データが与えられると， `batch_size` の列にデータを整列させる`batchify()` を行います。\n",
    "もしデータが `batch_size` の列に均等に分割されないならば，データはフィットするように切り詰められます。\n",
    "例えば、アルファベットをデータ (全長26) とし， `batch_size=4` とすると，アルファベットを 6 の長さの 4 つの列に分割することになります。\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "バッチングすることで、より並列処理が可能になります。\n",
    "しかし，バッチ処理を行うと， モデルは各列を独立して扱うことになります。\n",
    "例えば， 上例では `G` と `F` の依存関係は学習できません。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSoqvEpvJb73"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>']) \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOlDqewoJb73"
   },
   "source": [
    "## 入力配列とターゲット配列を生成する関数\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxe-ZjG7Jb73"
   },
   "source": [
    "`get_batch()` は，トランスフォーマモデルの入力とターゲット系列の対を生成します。\n",
    "ソースデータは長さ `bptt` のチャンクに分割されます。\n",
    "言語モデル課題では，モデルは次のような単語を `Target` として必要とする。\n",
    "例えば `bptt` の値が 2 の場合，`i` = 0 の時に以下の 2 つの変数が得られます。\n",
    "<!-- `get_batch()` generates a pair of input-target sequences for\n",
    "the transformer model. \n",
    "It subdivides the source data into chunks of length `bptt`. \n",
    "For the language modeling task, the model needs the following words as `Target`. \n",
    "For example, with a `bptt` value of 2, we’d get the following two Variables for `i` = 0: -->\n",
    "\n",
    "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/transformer_input_target1.png\">\n",
    "\n",
    "チャンクは 0 次元に沿っており， Transformer モデルの「S」次元と一致していることに留意する必要があります。\n",
    "バッチ次元「N」は次元 1 に沿っています。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d05UUlWIJb74"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g6gH3oeJb74"
   },
   "source": [
    "# インスタンスの初期化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu1IpUj8Jb74"
   },
   "source": [
    "モデルのハイパーパラメータは以下のように定義されています。\n",
    "語彙サイズは， vocab  オブジェクトの長さに等しい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DEL7y8MJJb74"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # 語彙数 size of vocabulary\n",
    "emsize = 200          # 埋め込み次元数 embedding dimension\n",
    "d_hid = 200           # nn.TransformerEncoderのフィードフォワードネットワークモデルの次元 dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2           # 層数  number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2             # 多頭注意 の頭の数 number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2         # ドロップアウト率 dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXZbTdpZJb74"
   },
   "source": [
    "# モデルの実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWEfVnwSJb75"
   },
   "source": [
    "[`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) を [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent) オプティマイザと組み合わせて使用しています。\n",
    "学習率は最初 5.0 に設定され， [`StepLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) のスケジュールに従います。\n",
    "学習時には [`nn.utils.clip_grad_norm`](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) を用いて， 勾配爆発を防ぎます。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FsDGxA9dJb75"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kKSqwqwJb75"
   },
   "source": [
    "エポックをループします。\n",
    "検証用の損失がこれまで見てきた中で最高であれば、モデルを保存します。\n",
    "各エポックの後、学習率を調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UqNRHLDJb75"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'検証損失 {val_loss:5.2f} | 検証錯綜度 {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCxTOSYbJb75"
   },
   "source": [
    "# テストデータセットを用いて，最良モデルを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SksAJFYMJb76",
    "outputId": "a6ff2468-55f3-469e-eba3-ca01fdcbc220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | テスト損失  5.50 | テストデータ錯綜度   244.58\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | テスト損失 {test_loss:5.2f} | '\n",
    "      f'テストデータ錯綜度 {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "2021_1022transformer_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
