{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ec295c",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_1028Olivetti_face_ResNet_from_Pytorch_source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cbe53-8b42-4fcc-947b-6e490e8b2b5a",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='https://komazawa-deep-learning.github.io/assets/ResNet_Fig2.svg' width=\"49%\"><br/>\n",
    "\n",
    "\n",
    "<img src='https://komazawa-deep-learning.github.io/assets/2015ResNet30.svg' width=\"88%\"><br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742",
   "metadata": {
    "id": "7bb12613-eaf9-4d3a-8298-aab1af02c742"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "isColab = 'google.colab' in str(IPython.get_ipython())\n",
    "if isColab:\n",
    "    !pip install japanize_matplotlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt    \n",
    "import japanize_matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35000912-ec57-4778-ae33-6c4610eb1d1e",
   "metadata": {
    "id": "35000912-ec57-4778-ae33-6c4610eb1d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オリジナル X サイズ:(400, 4096) X_train 訓練画像のサイズ: (320, 4096)\n",
      "オリジナル y サイズ:(400,) y_train 教師信号データのサイズ: (320,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# split_ratio = 0.2 としているので，訓練データ対テストデータが 8:2 になります\n",
    "split_ratio = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=split_ratio, \n",
    "                                                    stratify=y,\n",
    "                                                    random_state=0)\n",
    "print(f'オリジナル X サイズ:{X.shape}',\n",
    "      f'X_train 訓練画像のサイズ: {X_train.shape}')\n",
    "print(f'オリジナル y サイズ:{y.shape}',\n",
    "      f'y_train 教師信号データのサイズ: {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f15dd4d5-d65a-4f41-a1c1-bb577d2f7181",
    "outputId": "94d77087-c949-47f6-e7db-bbe952514fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# リソースの選択（CPU/GPU）\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 乱数シード固定（再現性の担保）\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "fix_seed(seed)\n",
    "\n",
    "# データローダーのサブプロセスの乱数の seed 固定\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "print(worker_init_fn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7",
   "metadata": {
    "id": "d2a9c691-d44c-48e5-b3c0-c9a652077de7"
   },
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "class _dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return feature, label\n",
    "\n",
    "\n",
    "X_ = torch.tensor(X_train).float().reshape(-1,1,64,64)\n",
    "#X_ = torch.reshape(torch.tensor(X_train).float(), (-1,1,64,64))\n",
    "y_ = torch.tensor(y_train).long()\n",
    "Xtest_ = torch.tensor(X_test).float().reshape(-1,1,64,64)\n",
    "ytest_ = torch.tensor(y_test).long()\n",
    "\n",
    "train_dataset = _dataset(X_, y_)\n",
    "test_dataset = _dataset(Xtest_, ytest_)\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=64,    # バッチサイズ\n",
    "                                               shuffle=True,     # データシャッフル\n",
    "                                               num_workers=0,    # 高速化\n",
    "                                               pin_memory=True,  # 高速化\n",
    "                                               worker_init_fn=worker_init_fn\n",
    "                                              )\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=64,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0,\n",
    "                                              pin_memory=True,\n",
    "                                              worker_init_fn=worker_init_fn\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35",
   "metadata": {
    "id": "8ca3f571-57e0-4132-9fc8-2c56b17a1e35"
   },
   "source": [
    "## `torch.nn.Conv2d` の引数\n",
    "\n",
    "* in_channels: int, 入力特徴数，(チャンネル数)\n",
    "* out_channels: int, 出力特徴数 (チャンネル数)\n",
    "* kernel_size: Union[int, Tuple[int, int]], カーネルサイズ，数字を 1 つだけ与えると 縦横とも同じサイズのカーネル幅になる\n",
    "* stride: Union[int, Tuple[int, int]] = 1,  ストライド，カーネルをスライドさせる幅，数字を 1 だけ与えると縦横とも同サイズの幅となる\n",
    "* padding: Union[str, int, Tuple[int, int]] = 0, 4 角に加える幅，数字を 1 つだけ与えると上下左右とも同サイズの幅にまる。\n",
    "W x H の画像に対して，横幅は，W_pad + W + W_pad となり，縦長は H_pad + H + H_pad となるので，H x W の入力データが (W + 2 W_pad) * (H + 2 H_pad) のサイズとなる\n",
    "* dilation: Union[int, Tuple[int, int]] = 1\n",
    "ダイレーションの幅，畳み込みカーネルの間隙を指定する\n",
    "* groups: int = 1, \n",
    "* bias: bool = True, \n",
    "バイアス項 `X @ w + b` にするときの `b` のこと\n",
    "* padding_mode: str = 'zeros', device=None, dtype=None) -> None        \n",
    "`padding` で指定した 4 角の拡張領域をどのような数値で埋めるかを指定する。デフォルトでは `zeros` すなわち 0 パディングとなる。\n",
    "そのほかに取りうる値は，`reflect`, `replicate`, `circular` である。\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\frac{H_{in} + 2 H_{\\text{pad}} - H_{\\text{dilation}}\\times \\left(H_{\\text{kernel}} -1\\right)+1}{H_{\\text{stride}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\frac{W_{in} + 2 W_{\\text{pad}} - W_{\\text{dilation}}\\times \\left(W_{\\text{kernel}} -1\\right)+1}{W_{\\text{stride}}}\n",
    "$$\n",
    "\n",
    "## `torch.nn.MacPool2d` の引数\n",
    "\n",
    "* kernel_size: Union[int, Tuple[int, ...]], \n",
    "* stride: Union[int, Tuple[int, ...], NoneType] = None, \n",
    "* padding: Union[int, Tuple[int, ...]] = 0, \n",
    "* dilation: Union[int, Tuple[int, ...]] = 1, \n",
    "* return_indices: bool = False, \n",
    "* ceil_mode: bool = False) -> None\n",
    "\n",
    "$$\n",
    "out(N_i,C_j,h,w)= \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \n",
    "\\text{input}\\left(N_i, C_j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c74c6ed-5286-4fd3-be20-855a211449ab",
   "metadata": {
    "id": "9c74c6ed-5286-4fd3-be20-855a211449ab"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_features, out_features, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_features, out_features,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=dilation,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation,\n",
    "                    )\n",
    "\n",
    "\n",
    "def conv1x1(in_features, out_features, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_features, out_features, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class ResNet_BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(in_features, out_features, stride=stride)\n",
    "        self.bn1 = norm_layer(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_features, out_features)\n",
    "        self.bn2 = norm_layer(out_features)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eefe4fe-822c-4627-8250-35a581d11779",
   "metadata": {
    "id": "2eefe4fe-822c-4627-8250-35a581d11779"
   },
   "outputs": [],
   "source": [
    "class ResNet_Bottleneck(nn.Module):\n",
    "    # ボトルネックは、ダウンサンプリングのストライドを 3x3 convolution(self.conv2) に置くのに対し、\n",
    "    # オリジナルの実装では最初の 1x1 convolution(self.conv1) にしています。\n",
    "    # [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) によると、\n",
    "    # ダウンサンプリングのストライドを 3x3 convolution(self.conv2) にしています。\n",
    "    # このバージョンは ResNet V1.5 としても知られており、精度が向上しています。\n",
    "    # (https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch)\n",
    "\n",
    "    #expansion: int = 4\n",
    "\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer= None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(out_featuress * (base_width / 64.0)) * groups\n",
    "        \n",
    "        self.expansion = 4\n",
    "        \n",
    "        self.conv1 = conv1x1(in_features, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, otu_features * self.expansion)\n",
    "        self.bn3 = norm_layer(out_features * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, inp):\n",
    "        identity = inp\n",
    "\n",
    "        out = self.conv1(inp)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 in_channels = 3,\n",
    "                 num_classes = 40, # 1000,\n",
    "                 zero_init_residual=False,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 #width_per_group=8,\n",
    "                 norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.in_features = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.in_features, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.in_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # 各残差ブランチの最後のバッチ正規化をゼロ初期化し，残差ブランチがゼロで始まり，各残差ブロックが恒等写像のように振る舞うようにします。\n",
    "        # https://arxiv.org/abs/1706.02677 によると，これによりモデルが 0.2~0.3 %改善されます。\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self,\n",
    "                    block,\n",
    "                    out_features,\n",
    "                    blocks,\n",
    "                    stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if stride != 1 or self.in_features != out_features * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_features, out_features * block.expansion, stride),\n",
    "                norm_layer(out_features * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_features, out_features, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.in_features = out_features * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.in_features,\n",
    "                    out_features,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model    \n",
    "\n",
    "    \n",
    "def resnet18(**kwargs):\n",
    "    \"\"\"ResNet-18 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet18\", ResNet_BasicBlock, [1, 1, 1, 1], **kwargs)\n",
    "    #return _resnet(\"resnet18\", ResNet_BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0",
   "metadata": {
    "id": "81bacbf6-9287-433d-8cc9-dfaaa6c637d0"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "in_channels=1\n",
    "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
    "#print(tmp_resnet_model)\n",
    "#summary(model, input_size=(in_channels,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876",
   "metadata": {
    "id": "5831e8d8-4f5b-4eb1-b407-aa570db9f876"
   },
   "source": [
    "$$\n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n",
    "$$\n",
    "\n",
    "where $x$ is the input, $y$ is the target, $w$ is the weight, and $N$ is the batch size. \n",
    "If `reduction` is not `none` (default `mean` ), then \n",
    "$$\n",
    "\\ell(x, y) = \\begin{cases}\n",
    "\\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n",
    "\\text{if reduction} = \\text{`mean';}\\\\\n",
    "\\sum_{n=1}^N l_n,  &\n",
    "\\text{if reduction} = \\text{`sum'.}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112",
   "metadata": {
    "id": "ec8d2413-2c88-4f68-bd08-1d4f8e923112"
   },
   "outputs": [],
   "source": [
    "# モデル\n",
    "in_channels=1\n",
    "model = resnet18(in_channels=in_channels, num_classes=40).to(device)\n",
    "\n",
    "#loss_f = nn.NLLLoss()\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optim_f = optim.Adam(model.parameters(), weight_decay=0.01)\n",
    "\n",
    "\n",
    "# 学習・評価\n",
    "def calc_loss(label, pred):\n",
    "    return loss_f(pred, label)\n",
    "\n",
    "def train_step(x, y):\n",
    "    model.train()\n",
    "    preds = model(x)\n",
    "    loss = calc_loss(y, preds)\n",
    "    optim_f.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_f.step()\n",
    "\n",
    "    return loss, preds\n",
    "\n",
    "def test_step(x, y):\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    loss = calc_loss(y, preds)\n",
    "    \n",
    "    return loss, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a",
   "metadata": {
    "id": "80ab3d50-0df9-47e2-aaea-181a12ef3b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エポック:  1, 訓練損失: 0.006 テスト損失: 3.312, 訓練精度:1.000 検証精度: 0.086\n",
      "エポック:  2, 訓練損失: 0.005 テスト損失: 3.075, 訓練精度:1.000 検証精度: 0.133\n",
      "エポック:  3, 訓練損失: 0.005 テスト損失: 2.915, 訓練精度:1.000 検証精度: 0.148\n",
      "エポック:  4, 訓練損失: 0.006 テスト損失: 2.762, 訓練精度:1.000 検証精度: 0.156\n",
      "エポック:  5, 訓練損失: 0.007 テスト損失: 2.637, 訓練精度:1.000 検証精度: 0.195\n",
      "エポック:  6, 訓練損失: 0.009 テスト損失: 2.410, 訓練精度:1.000 検証精度: 0.312\n",
      "エポック:  7, 訓練損失: 0.014 テスト損失: 2.165, 訓練精度:1.000 検証精度: 0.367\n",
      "エポック:  8, 訓練損失: 0.546 テスト損失: 3.038, 訓練精度:0.938 検証精度: 0.258\n",
      "エポック:  9, 訓練損失: 0.220 テスト損失: 1.927, 訓練精度:0.984 検証精度: 0.500\n",
      "エポック: 10, 訓練損失: 0.060 テスト損失: 0.419, 訓練精度:1.000 検証精度: 0.898\n",
      "エポック: 11, 訓練損失: 0.022 テスト損失: 0.251, 訓練精度:1.000 検証精度: 0.961\n",
      "エポック: 12, 訓練損失: 0.009 テスト損失: 0.167, 訓練精度:1.000 検証精度: 0.961\n",
      "エポック: 13, 訓練損失: 0.005 テスト損失: 0.142, 訓練精度:1.000 検証精度: 0.969\n",
      "エポック: 14, 訓練損失: 0.004 テスト損失: 0.149, 訓練精度:1.000 検証精度: 0.953\n",
      "エポック: 15, 訓練損失: 0.003 テスト損失: 0.198, 訓練精度:1.000 検証精度: 0.938\n",
      "エポック: 16, 訓練損失: 0.003 テスト損失: 0.266, 訓練精度:1.000 検証精度: 0.938\n",
      "エポック: 17, 訓練損失: 0.003 テスト損失: 0.341, 訓練精度:1.000 検証精度: 0.930\n",
      "エポック: 18, 訓練損失: 0.003 テスト損失: 0.430, 訓練精度:1.000 検証精度: 0.930\n",
      "エポック: 19, 訓練損失: 0.003 テスト損失: 0.516, 訓練精度:1.000 検証精度: 0.930\n",
      "エポック: 20, 訓練損失: 0.002 テスト損失: 0.617, 訓練精度:1.000 検証精度: 0.930\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "seed = 42\n",
    "fix_seed(seed)\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
    "for epoch in range(epochs):\n",
    "#for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, test_loss, train_acc, test_acc = 0., 0., 0., 0.\n",
    "    for (x, y) in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss, preds = train_step(x, y)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss, preds = test_step(x, y)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy_score(y.tolist(), preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f'エポック: {epoch + 1:2d},',\n",
    "          f'訓練損失: {train_loss:.3f}',\n",
    "          f'テスト損失: {test_loss:.3f},',\n",
    "          f'訓練精度:{train_acc:.3f}',\n",
    "          f'検証精度: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d4a59-c4f6-4910-b7ab-4f40947dbfd9",
   "metadata": {
    "id": "9c2d4a59-c4f6-4910-b7ab-4f40947dbfd9"
   },
   "outputs": [],
   "source": [
    "# 学習進行状況の描画\n",
    "plt.plot(train_losses, label='訓練損失')\n",
    "plt.plot(test_losses, label='テスト損失')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accs, label=\"テスト精度\")\n",
    "plt.plot(train_accs, label=\"訓練精度\")\n",
    "plt.legend()\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29be63-69a7-4bbf-9b51-5945f952052c",
   "metadata": {
    "id": "aa29be63-69a7-4bbf-9b51-5945f952052c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "2021_1028Olivetti_face_ResNet_from_Pytorch_source.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0348ff9565c04ed0b0b189c6c710a2a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "059444618a1c4466b113faf299278e79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "251bca57ec0d4c25a3387d76edb18d58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a621eb6c4dcb45f79ef1c57476d0b705",
        "IPY_MODEL_fc8b4717ae544fb385294158ef63fa88",
        "IPY_MODEL_4541674fed624bb2b7e449100c882907"
       ],
       "layout": "IPY_MODEL_059444618a1c4466b113faf299278e79"
      }
     },
     "282300e511114f478ff4b3f1d99e7c19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "308abc2d25404a3f97dc6815c7869665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "36532bd611ab4d7aacbe05b2f9b46808": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4413a9a8e3d64545aa55ec1d280dbd50",
       "style": "IPY_MODEL_eecb45fd070045dab78b546408591cdd",
       "value": "  0%"
      }
     },
     "43c27fef101b4fc29d410894e270fe81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4413a9a8e3d64545aa55ec1d280dbd50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4541674fed624bb2b7e449100c882907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8377656ad8e74732afd8867152929807",
       "style": "IPY_MODEL_f139679f7685457f9bfa2ab577eefa1e",
       "value": " 50/50 [04:12&lt;00:00,  5.06s/it]"
      }
     },
     "4a09da64132b479c856d0bcfcc575452": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "55426a5477454fbd894a1701e222918a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "64afe65a1dce4b728100fe2d5ab854a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "70deb402c6894ad998caff839808d914": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e4095c9c42474331ae0f49354f6e4321",
       "style": "IPY_MODEL_282300e511114f478ff4b3f1d99e7c19",
       "value": " 3/3 [00:14&lt;00:00,  5.00s/it]"
      }
     },
     "713c8b63819f484890241835719411cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a5b0a75ff20f477d90e5c7aec2f64c85",
        "IPY_MODEL_d7ad86e9ec5843a389fb4558fc7fcf68",
        "IPY_MODEL_70deb402c6894ad998caff839808d914"
       ],
       "layout": "IPY_MODEL_9ad44c901de340f99e476726fefe5f6d"
      }
     },
     "7395eb2c70fc491ebb8cf278fa6ebd43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_0348ff9565c04ed0b0b189c6c710a2a8",
       "max": 50,
       "style": "IPY_MODEL_d6b0d6a3e1c84328bfa5d6c4b6a9495d"
      }
     },
     "7917e0b2402d4729b5bd865cba3b9eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7b900fedc3524c2d9178dc097fc7cba3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cf823de6ecba48c1adc09dbcfa557a71",
       "style": "IPY_MODEL_7917e0b2402d4729b5bd865cba3b9eaf",
       "value": " 0/50 [00:00&lt;?, ?it/s]"
      }
     },
     "8377656ad8e74732afd8867152929807": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87d93c73bcd2494eaeed6a020b4028c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89881de4c9c14810b9752417d5067d1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9ad44c901de340f99e476726fefe5f6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5b0a75ff20f477d90e5c7aec2f64c85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f31fb4e35c314dbbb0ff266cf4b6b23d",
       "style": "IPY_MODEL_308abc2d25404a3f97dc6815c7869665",
       "value": "100%"
      }
     },
     "a621eb6c4dcb45f79ef1c57476d0b705": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_87d93c73bcd2494eaeed6a020b4028c0",
       "style": "IPY_MODEL_e4b3a41d451e4711b71459c911f720d2",
       "value": "100%"
      }
     },
     "cf823de6ecba48c1adc09dbcfa557a71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d6b0d6a3e1c84328bfa5d6c4b6a9495d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d7ad86e9ec5843a389fb4558fc7fcf68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_43c27fef101b4fc29d410894e270fe81",
       "max": 3,
       "style": "IPY_MODEL_55426a5477454fbd894a1701e222918a",
       "value": 3
      }
     },
     "e4095c9c42474331ae0f49354f6e4321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e4b3a41d451e4711b71459c911f720d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eecb45fd070045dab78b546408591cdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f139679f7685457f9bfa2ab577eefa1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f31fb4e35c314dbbb0ff266cf4b6b23d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f36f540ec4f44947b185f72afa9c4b2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_36532bd611ab4d7aacbe05b2f9b46808",
        "IPY_MODEL_7395eb2c70fc491ebb8cf278fa6ebd43",
        "IPY_MODEL_7b900fedc3524c2d9178dc097fc7cba3"
       ],
       "layout": "IPY_MODEL_4a09da64132b479c856d0bcfcc575452"
      }
     },
     "fc8b4717ae544fb385294158ef63fa88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_64afe65a1dce4b728100fe2d5ab854a7",
       "max": 50,
       "style": "IPY_MODEL_89881de4c9c14810b9752417d5067d1f",
       "value": 50
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
