{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "2021_0618CAM_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3de2b2c417e94d4daf92bb370cbb1c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6683d47519754dfcb025a7f5b8379f80",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1e6cb0a339cd4ced9f8a775e159da0cd",
              "IPY_MODEL_bef5f0a6067b484588f5e2d69f9e3482"
            ]
          }
        },
        "6683d47519754dfcb025a7f5b8379f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e6cb0a339cd4ced9f8a775e159da0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de6ecdc2902949f6bcf71aee2f63559a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5806f60f76a44b2c91bb9e74982b9b70"
          }
        },
        "bef5f0a6067b484588f5e2d69f9e3482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b433765c7974d96bc0cab335d3136da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 78.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6aa96718d19c4339b4b9285a5193e87c"
          }
        },
        "de6ecdc2902949f6bcf71aee2f63559a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5806f60f76a44b2c91bb9e74982b9b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b433765c7974d96bc0cab335d3136da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6aa96718d19c4339b4b9285a5193e87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0618CAM_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3de2b2c417e94d4daf92bb370cbb1c62",
            "6683d47519754dfcb025a7f5b8379f80",
            "1e6cb0a339cd4ced9f8a775e159da0cd",
            "bef5f0a6067b484588f5e2d69f9e3482",
            "de6ecdc2902949f6bcf71aee2f63559a",
            "5806f60f76a44b2c91bb9e74982b9b70",
            "1b433765c7974d96bc0cab335d3136da",
            "6aa96718d19c4339b4b9285a5193e87c"
          ]
        },
        "id": "ZQfuHUiSGy5p",
        "outputId": "c80da1c3-a656-4450-8fa4-57829a097431"
      },
      "source": [
        "# source: pytorchCAM.py\n",
        "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
        "\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pdb\n",
        "\n",
        "# input image\n",
        "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
        "IMG_URL = 'http://media.mlive.com/news_impact/photo/9933031-large.jpg'\n",
        "\n",
        "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
        "model_id = 2\n",
        "if model_id == 1:\n",
        "    net = models.squeezenet1_1(pretrained=True)\n",
        "    finalconv_name = 'features' # this is the last conv layer of the network\n",
        "elif model_id == 2:\n",
        "    net = models.resnet18(pretrained=True)\n",
        "    finalconv_name = 'layer4'\n",
        "elif model_id == 3:\n",
        "    net = models.densenet161(pretrained=True)\n",
        "    finalconv_name = 'features'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3de2b2c417e94d4daf92bb370cbb1c62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4-g2XSHGy5p"
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEZXC9nJGy5q"
      },
      "source": [
        "# hook the feature extractor\n",
        "features_blobs = []\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(net.parameters())\n",
        "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 256x256\n",
        "    size_upsample = (256, 256)\n",
        "    bz, nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqgrRwMaGy5q"
      },
      "source": [
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "preprocess = transforms.Compose([\n",
        "   transforms.Resize((224,224)),\n",
        "   transforms.ToTensor(),\n",
        "   normalize\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDM-6pGNGy5q"
      },
      "source": [
        "response = requests.get(IMG_URL)\n",
        "img_pil = Image.open(io.BytesIO(response.content))\n",
        "img_pil.save('test.jpg')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVAQ2V_zGy5r"
      },
      "source": [
        "import IPython\n",
        "IPython.display.Image(url=IMG_URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kltmcly4Gy5r"
      },
      "source": [
        "img_tensor = preprocess(img_pil)\n",
        "img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "logit = net(img_variable)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7rQgg1Gy5s"
      },
      "source": [
        "LABELS_ja_URL= 'https://gist.githubusercontent.com/PonDad/4dcb4b242b9358e524b4ddecbee385e9/raw/dda9454f74aa4fafee991ca8b848c9ab6ae0e732/imagenet_class_index.json'\n",
        "classes_ja = {int(i):val['ja'] for i, val in enumerate(requests.get(LABELS_ja_URL).json())}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Keq-fdiGy5s",
        "outputId": "5f316775-0e46-4193-8740-1097f64a76ae"
      },
      "source": [
        "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "# output the prediction\n",
        "for i in range(0, 5):\n",
        "    #print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes_ja[idx[i]]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.437 -> マウンテンバイク\n",
            "0.131 -> アルプス\n",
            "0.119 -> 自転車\n",
            "0.108 -> 一輪車\n",
            "0.049 -> 湖畔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3CFaMrNGy5s"
      },
      "source": [
        "classes = classes_ja"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw40-cu3Gy5s"
      },
      "source": [
        "# generate class activation mapping for the top1 prediction\n",
        "candidate = 0\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[candidate]])\n",
        "\n",
        "# render the CAM and output\n",
        "print('CAM の出力 {0}: {1}'. format(candidate,  classes[idx[candidate]]))\n",
        "\n",
        "img = cv2.imread('test.jpg')\n",
        "height, width, _ = img.shape\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.3 + img * 0.5\n",
        "cv2.imwrite('CAM.jpg', result)\n",
        "IPython.display.Image('CAM.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l589ZfCdGy5t"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as pil_img\n",
        "%matplotlib inline"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biK3Jsc6Gy5t"
      },
      "source": [
        "for candidate in range(5):\n",
        "    # generate class activation mapping for the top1 prediction\n",
        "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[candidate]])\n",
        "\n",
        "    # render the CAM and output\n",
        "    print('CAM の出力 {0}: {1}'. format(candidate,  classes[idx[candidate]]))\n",
        "\n",
        "    img = cv2.imread('test.jpg')\n",
        "    height, width, _ = img.shape\n",
        "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "    result = heatmap * 0.3 + img * 0.5\n",
        "    cv2.imwrite('CAM.jpg', result)\n",
        "    #IPython.display.Image('CAM.jpg')\n",
        "    plt.axis(False); plt.imshow(pil_img.open('CAM.jpg'))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk6TcmAxGy5t",
        "outputId": "c4c48a6e-b3e3-4858-dfbd-07eb3db3dd06"
      },
      "source": [
        "img_tensor = preprocess(img_pil)\n",
        "img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "logit = net(img_variable)\n",
        "\n",
        "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "# output the prediction\n",
        "for i in range(0, 5):\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes_ja[idx[i]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.327 -> ダルメシアン\n",
            "0.064 -> ラブラドル・レトリーバー犬\n",
            "0.042 -> 蟻\n",
            "0.033 -> ヒップ\n",
            "0.032 -> イングリッシュフォックスハウンド\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0i_icUGy5u"
      },
      "source": [
        "# generate class activation mapping for the top1 prediction\n",
        "candidate = 1\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[candidate]])\n",
        "\n",
        "# render the CAM and output\n",
        "#print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
        "print('output CAM.jpg for the top1 prediction: %s'%classes_ja[idx[candidate]])\n",
        "\n",
        "img = cv2.imread('test.jpg')\n",
        "height, width, _ = img.shape\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.3 + img * 0.5\n",
        "cv2.imwrite('CAM.jpg', result)\n",
        "IPython.display.Image('CAM.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xcositxGy5u"
      },
      "source": [
        "# まとめて関数化\n",
        "def CAM5(img_url, n=5):\n",
        "    response = requests.get(img_url)\n",
        "    img_pil = Image.open(io.BytesIO(response.content))\n",
        "    img_pil.save('test.jpg')\n",
        "    \n",
        "    img_tensor = preprocess(img_pil)\n",
        "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "    logit = net(img_variable)\n",
        "\n",
        "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "    probs, idx = h_x.sort(0, True)\n",
        "    probs = probs.numpy()\n",
        "    idx = idx.numpy()\n",
        "\n",
        "    for cand in range(n):\n",
        "        # generate class activation mapping for the top1 prediction\n",
        "        CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[cand]])\n",
        "\n",
        "        # render the CAM and output\n",
        "        print('CAM の出力 {0}: {1}'. format(candidate,  classes[idx[cand]]))\n",
        "\n",
        "        img = cv2.imread('test.jpg')\n",
        "        height, width, _ = img.shape\n",
        "        heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "        result = heatmap * 0.3 + img * 0.5\n",
        "        cv2.imwrite('CAM.jpg', result)\n",
        "        #IPython.display.Image('CAM.jpg')\n",
        "        plt.axis(False); plt.imshow(pil_img.open('CAM.jpg'))\n",
        "        plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z1HAKgtGy5v"
      },
      "source": [
        "url_img = 'https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/master/assets/2012AlexNetResult.jpg'\n",
        "CAM5(url_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ZT_Ig9Gy5v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}