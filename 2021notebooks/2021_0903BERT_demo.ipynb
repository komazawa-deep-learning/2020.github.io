{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/komazawa-deep-learning/komazawa-deep-learning.github.io/blob/master/2021notebooks/2021_0903BERT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Wh5P8nVwasf"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "isColab = True if platform.system() == 'Linux' else False\n",
    "\n",
    "if isColab:\n",
    "    # MeCab, fugashi, ipadic のインストール\n",
    "    !apt install aptitude swig > /dev/null 2>&1\n",
    "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1 \n",
    "    !pip install mecab-python3 > /dev/null 2>&1 \n",
    "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1 \n",
    "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1 \n",
    "\n",
    "    import subprocess\n",
    "    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n",
    "\n",
    "    !pip install fugashi[unidic] > /dev/null 2>&1 \n",
    "    !python -m unidic download > /dev/null 2>&1 \n",
    "    !pip install ipadic > /dev/null 2>&1\n",
    "    \n",
    "    # huggingface の transformers をインストール\n",
    "    !pip  instal transformers > /dev/null 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmJpattUEaMN"
   },
   "source": [
    "# ごく簡単な BERT の使い方\n",
    "\n",
    "- date: 2021_0903\n",
    "- author: 浅川伸一\n",
    "- 参考サイト: https://huggingface.co/bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAJrbOJsARF8"
   },
   "outputs": [],
   "source": [
    "# MeCab\b, fugashi, ipadic のインストール\n",
    "!apt install aptitude swig\n",
    "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "!pip install mecab-python3\n",
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
    "\n",
    "import subprocess\n",
    "\n",
    "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                           shell=True).communicate()[0]).decode('utf-8')\n",
    "\n",
    "!pip install fugashi[unidic]\n",
    "!python -m unidic download\n",
    "!pip install ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vaceAxAx_1Gj"
   },
   "outputs": [],
   "source": [
    "# allennlp の transformers のインストール\n",
    "#!pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xv4l4qKe_8m0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 07:00:55.096411: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 学習済 BERT 日本語モデルのインストール special thanks to 東北大学 乾研究室\n",
    "unmasker = pipeline('fill-mask', model='cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gVfY0zl7AAo7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本日 は と なり, スコア: 0.751 \n",
      "本日 は 大相撲 なり, スコア: 0.037 \n",
      "本日 は 開催 なり, スコア: 0.026 \n",
      "本日 は 廃止 なり, スコア: 0.018 \n",
      "本日 は に なり, スコア: 0.013 \n"
     ]
    }
   ],
   "source": [
    "# ここからは，実演です\n",
    "masked_sentences = [\n",
    "                    ['本日 は [MASK] なり'],\n",
    "                    #['ニューラルネットワーク は 神経 心理学 の [MASK] です.'],\n",
    "                    # ['札幌 クラーク 病院 は 神経 心理学 の [MASK] です.'],\n",
    "                    # ['東京女子大学 は [MASK] な 大学 です.'],\n",
    "                    # ['[MASK] 大学 は リベラル な 大学 です.'],\n",
    "                    ]\n",
    "for sentence in masked_sentences:\n",
    "    for x in unmasker(sentence):\n",
    "        print(f\"{x['sequence']}, スコア: {x['score']:.3f} \")\n",
    "\n",
    "#for x in unmasker(\"ニューラルネットワーク は 神経 心理学 の [MASK] です.\"):\n",
    "#    print(f\"{x['sequence']}, スコア:{x['score']:.3f} \")\n",
    "#print(unmasker(\"札幌 クラーク 病院 は 神経 心理学 の [MASK] です.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "67NdljJyGbOu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 108, 32, 9, 4, 297, 143, 3]\n",
      "[2, 108, 32, 9, 4, 297, 143, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('本日は[MASK]なり.'))\n",
    "print(tokenizer.encode('本日 は [MASK] なり.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ccntkNQAJze"
   },
   "outputs": [],
   "source": [
    "tokenize\n",
    "#unmasker(['CCAP は 神経 心理学 に とって [MASK] な 存在 です.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoIg7ncqCQT9"
   },
   "outputs": [],
   "source": [
    "# 以下は BERT モデルの内部を確認する作業\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk7FGql8Sz-z"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# 東北大学 乾研究室の設定を確認\n",
    "config_japanese = BertConfig.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "print(config_japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PeysNYSbTNN3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 分かち書きをするための tokenizer を呼び出して利用するため\n",
    "import torch \n",
    "tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL7sPczpTnlN"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(f'''\n",
    "    青葉山で{tokenizer.mask_token}の研究をしています。\n",
    "''', return_tensors='pt')\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovv09j-_3uLA"
   },
   "outputs": [],
   "source": [
    "＃help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpFK2jqw3ytb"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)\n",
    "print(tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "#print(tokenizer.eos_token, tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHC4e5oZ4GgA"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode('本日は晴天なり.', return_tensors='pt'))                # 平文，分かち書きしない場合 -> 独自の構文解析\n",
    "print(tokenizer.encode(['本日','は','晴天','なり', '.'], return_tensors='pt')) # 文字列で構成されたリストを渡した場合 -> 構文解析はしない\n",
    "print(tokenizer.encode('本日 は 晴天 なり .', return_tensors='pt'))            # 空白で分かち書きを区切った場合 -> 分かち書きを信用しない\n",
    "print(tokenizer.encode('本日 は 晴天 なり', return_tensors='pt'))              # 文末にピリオド\n",
    "#print(tokenizer.encode('私は誰', return_tensors='pt'))\n",
    "#print(tokenizer.encode('本日は曇天なり'))\n",
    "#inputs = tokenizer('無職転生と鬼滅の刃ではどちらが人気があるのだろうか？', '次の文章をセンテンスbとする', padding='max_length', return_tensors='pt')\n",
    "#inputs = tokenizer('無職転生と鬼滅の刃ではどちらが人気があるのだろうか？', '次の文章をセンテンスbとする')\n",
    "#print(inputs.keys())\n",
    "#for k in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "#    print(f'key:{k} -> inputs[k]:{inputs[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U54X8O_qHTFR"
   },
   "source": [
    "[2, 108, 32, 9, 4, 297, 143, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvW7TFtLFPJU"
   },
   "outputs": [],
   "source": [
    "# 分かち書きしたものを，もとに戻す確認作業\n",
    "for id in  [   2,  108,   32,    9, 4798,  372,  297,  143,    3]:\n",
    "    print(id, tokenizer.ids_to_tokens[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeO01i1y5tuU"
   },
   "outputs": [],
   "source": [
    "# 「めかぶ」との違いを明確にするために，めかぶの解析結果を示します。\n",
    "import MeCab\n",
    "print(MeCab.Tagger().parse('本日は曇天なり'))\n",
    "\n",
    "#help(tokenizer.encode)\n",
    "#type(inputs)\n",
    "#inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyqDaAtE6DlM"
   },
   "outputs": [],
   "source": [
    "# 日本語\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qScqzVIR_Sm2"
   },
   "outputs": [],
   "source": [
    "#model.pooler.dense\n",
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAtfNT2CAjpF"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/transformer.png\"><br/>\n",
    "トランスフォーマーモデル (Source: https://arxiv.org/abs/1706.03762)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66e3nEB__gNg"
   },
   "outputs": [],
   "source": [
    "#tokenizer.ids_token\n",
    "print(tokenizer.encode('今宵は月夜です', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKrha7wgD1vs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMa+EIGTsZJ4vfRSX2UPE6K",
   "include_colab_link": true,
   "name": "2021_0903BERT_demo.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
