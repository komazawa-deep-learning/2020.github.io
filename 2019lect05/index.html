<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Shin Asakawa">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>畳込みニューラルネットワーク(2) 第5回 - 2020駒澤大学心理学特講IIIA</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet">
  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet">
  <link href="../css/specific.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(2) \u7b2c5\u56de";
    var mkdocs_page_input_path = "2019lect05.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> 2020駒澤大学心理学特講IIIA</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00check_meet/">第 0 回 事前確認</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect00guidance/">第 0 回 ガイダンス</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect01/">第 1 回 05 月 08 日</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect02/">第 2 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect03/">第 3 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect04/">第 4 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect05/">第 5 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect06/">第 6 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect07/">第 7 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect08/">第 8 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../lect09/">第 9 回</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">付録</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../colaboratory_intro/">Colab 事始め</a>
                </li>
                <li class="">
                    
    <a class="" href="../colaboratory_faq/">Colaboratory FAQ</a>
                </li>
                <li class="">
                    
    <a class="" href="../eco/">エコシステム</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_numpy_intro_ja/">Python の基礎</a>
                </li>
                <li class="">
                    
    <a class="" href="../python_modules/">Python modules</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">2020駒澤大学心理学特講IIIA</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>畳込みニューラルネットワーク(2) 第5回</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="2">畳込みニューラルネットワーク(2)<a class="headerlink" href="#2" title="Permanent link">&para;</a></h1>
<h2 id="_1">前回の復習<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>ディープラーニング</strong> 深層学習, deep learning とは <strong>信用割当問題</strong> credit assignment problem を回避するために <strong>畳込み</strong> convolution 演算(処理)を用いて多層にしたニューラルネットワークのこと</li>
<li>畳込み演算の概念</li>
</ul>
<p>
<script type="math/tex; mode=display">
\left[\mbox{畳込み}\left(\ge1\right) \rightarrow \mbox{プーリング}\left(\ge0\right)\right]
\times \left(\ge1\right)\rightarrow\mbox{全結合層}\left(\ge1\right)
</script>
</p>
<ul>
<li>カーネルサイズ</li>
<li>プーリング</li>
<li>ストライド</li>
<li>パディング</li>
<li>ハイパーパラメータとしてのカーネル(特徴)サイズ，ストライド，パディング</li>
</ul>
<hr />
<h1 id="_2">本日のお品書き，あるいは目標，ねらい<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<ul>
<li>ニューラルネットワーク，機械学習の分野で頻繁に用いられている性能向上のための技法を紹介</li>
<li>この授業の目標は深層学習の心理学的な意味付けを考えることであるので，紹介する上記の技法は無関係のように思われる</li>
<li>だがそうではないことを理解することが目的</li>
</ul>
<hr />
<h1 id="_3">本日の実習<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_face_detection_demo.ipynb">顔領域の検出</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019Google_Demo_Mask_R_CNN_Image_Segmentation_Demo.ipynb">TensorFlow Hub による領域分割</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019google_DeepLab_Demo.ipynb">Deeplab のデモによる画像の意味分割</a></li>
<li><a href="https://github.com/ShinAsakawa/2019komazawa/blob/master/notebooks/2019keras_kmnist_demo.ipynb">kminst による CNN 実習</a></li>
<li>'*.ipynb' を提出</li>
</ul>
<hr />
<h1 id="_4">アレックスネット以降の流れ<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h1>
<ul>
<li>2013年 ZF ネット</li>
<li>2014年 GoogLeNet (インセプション)，VGG，Very Deep,  GAN, VAE</li>
<li>2015年 残渣ネット (ResNet) この時点で，人間超え</li>
<li>2016年 YOLO, SSD, Segmentation (Semantics/Instance)</li>
<li>2017年 Mask R-CNN</li>
<li>2018年 モバイルネット</li>
</ul>
<p><a href="../assets/2018asakawa_jdlaGtestChapt7.pdf">G 検定公式テキスト7章より</a></p>
<p><center>
<img src='../assets/2012Zeiler_Deconvolution.png' style='width:49%'></br>
<br>Zeiller 2012 より
</center></p>
<h1 id="_5">高精度化，高速化への努力<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h1>
<ul>
<li>確率的勾配降下法，オンライン学習，バッチ学習</li>
<li>データ拡張</li>
<li>正則化</li>
<li>ドロップアウト</li>
<li>非線形活性化関数</li>
<li>最適化手法</li>
</ul>
<h1 id="pholosophical-consideration">Pholosophical consideration<a class="headerlink" href="#pholosophical-consideration" title="Permanent link">&para;</a></h1>
<ul>
<li>Epistemology 思念的，観念的</li>
<li>Empirical Episitemology 実証的 = psychology</li>
<li>Constructive epipstemology 構成論的 = computer vision, neural networks</li>
</ul>
<h1 id="_6">生理学，視覚心理学との対応<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h1>
<ul>
<li>Julesz</li>
<li>Julesz (1981) Textons, the elements of texture perception, and their interactions, Nature</li>
</ul>
<p><center>
<img src="../assets/1981Julesz-texton-Fig2.svg" style="width:84%"></br>
Julesz (1981) Fig. 2 より
</center></p>
<ul>
<li>Marr<ul>
<li>Computational approach: Vision (1908) </li>
</ul>
</li>
<li>Poggio<ul>
<li>Poggio, Torre, and Koch (1985) Computational vision and regularization theory</li>
</ul>
</li>
</ul>
<h2 id="one-algorithm-hypothesis">一つのアルゴリズム仮説 One algorithm hypothesis<a class="headerlink" href="#one-algorithm-hypothesis" title="Permanent link">&para;</a></h2>
<ul>
<li>Metin and Frost (1989) Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus</li>
<li>Roe et al. (1992) Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex</li>
</ul>
<h3 id="hubel-and-wiesel-blackmore">生理学との対応 (Hubel and Wiesel のネコとサル, Blackmore のネコ, ヴァンエッセン)<a class="headerlink" href="#hubel-and-wiesel-blackmore" title="Permanent link">&para;</a></h3>
<ul>
<li>層間の結合の仕方, アーキテクチャ</li>
<li>forward/backward 役割，機能，実現方法</li>
<li>側抑制 lateral inhibition (これについては多層化して回避できる可能性あり)</li>
<li>
<p>shape from X は正しかったのか？ ただし発達心理学におけるシェイプバアスは言語発達において重要な意味を持つはず。だからと言って乳幼児はそのように強制(脅迫？)，矯正されて育つわけではないだろう。</p>
<ul>
<li>Ritter (2017) Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study</li>
<li>Landau, Smith, Jones (1992) Syntactic Context and the Shape Bias in Childrens and Adults Lexical Learning</li>
<li>
<p>Yamins (2016) Using goal-driven deep learning models to understand sensory cortex</p>
</li>
<li>
<p>Julez のアプローチは視覚研究者 Haar, SIFT, DoG などのアルゴリズム開発者と対応</p>
</li>
<li>Poggio (1985) Computational Vision and Regularization Theory</li>
</ul>
</li>
</ul>
<p><center>
<img src='../assets/2016Yamins_Fig1.svg' style='width:94%'>
<img src='../assets/2016Yamins_Fig2.svg' style='width:94%'>
<img src='../assets/2016Yamins_Fig3.svg' style='width:94%'></p>
<p><img src='../assets/2014Yamins_Fig2.svg' style='width:74%'>
<img src='../assets/2014Yamins_FigS2.svg' style='width:74%'>
</center></p>
<p><center>
<img src='../assets/2019Inception_screenshot.png' style='width:84%'><br>
映画インセプションのスクリーンショット。Special thanks to <a href="https://www.netflix.com/watch/70131314?trackId=14170286&amp;tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT">Netflix</a><br></p>
<p><a href="https://www.netflix.com/watch/70131314?trackId=14170286&amp;tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT">https://www.netflix.com/watch/70131314?trackId=14170286&amp;tctx=3%2C0%2C9a10a321-9c1f-4396-b5df-00b5b84e6917-23965358%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_86910893X3XX1558568676167%2C3d0e40f0-b286-48eb-afb3-2c7c501c86fc_ROOT</a></p>
<p>『インセプション』（原題: Inception）は、クリストファー・ノーラン監督・脚本・製作による2010年のアメリカのSFアクション映画。第83回アカデミー賞では作品賞、脚本賞、撮影賞、視覚効果賞、美術賞、作曲賞、音響編集賞、録音賞の8部門にノミネートされ、撮影賞、視覚効果賞、音響編集賞、録音賞を受賞した。全米脚本家組合賞ではオリジナル脚本賞を受賞した。<a href="https://ja.wikipedia.org/wiki/%E3%82%A4%E3%83%B3%E3%82%BB%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3">日本語ウィキペデイアより</a></p>
<p><center>
<img src='../assets/Inception3.svg' style="width:94%"></br>
<img src='../assets/2015GoogLeNet_Inception.svg' style="width:74%"></br>
Inception モジュール
</center></p>
<p></center>
<center>
<img src='../assets/2014Cadieu_Fig3.svg' style='width:74%'>
</center></p>
<ol>
<li>Edge detection</li>
<li>Spatio-temporal interpolation and approximation</li>
<li>Computation of optical flow</li>
<li>Computation of lightness and albedo</li>
<li>Shape from contours</li>
<li>Shape from texture</li>
<li>Shape from shading</li>
<li>Binocular stereo matching</li>
<li>Structure from motion</li>
<li>Structure from stereo</li>
<li>Surface reconstruction</li>
<li>Computation of surface colour</li>
</ol>
<!--
The regularization of the ill-posed problem of finding $z$ from
the 'data' $y$

\begin{equation}
Az=y \;\;\;\;\;\;\;\;\;\;(1)
\end{equation}

requires the choice of norms $||\cdot||$ and of a stabilizing functional
$|Pz|$.  In standard regularization theory, $A$ is a linear operator, the
norms are quadratic and $P$ is linear.  Two methods that can be applied
are: (1) among $z$ that satisfy $|Az-y|<\epsilon$ find $z$ that minimizes
$\epsilon$ depends on the estimated measurement errors and is zero if the
data are noiseless

\begin{equation}
|Pz|^{2} \;\;\;\;\;\;\;\;\;\;(2)
\end{equation}

(2) find $z$ minimizes

\begin{equation}
|Az-y|^2+\lambda|Pz|^2 \;\;\;\;\;\;\;\;\;\;(3)
\end{equation}

where $\lambda$ is a so-call regualarization parameter.
-->

<ul>
<li>Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex by Qianli Liao and Tomaso Poggio は注目すべき？ ResNet の解釈
&lt;!--</li>
<li>Hinton, Deep Learning, (Rumelhart backprop also) は Sutton の Bitter lesson の具現化である。end-to-end 一気通貫学習は，特徴抽出(特徴分析)，表現学習(内部表象)，分類器(意思決定)を含む。
--&gt;</li>
</ul>
<h2 id="_7">用語の整理<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<ul>
<li>SGD 確率的勾配降下法 stochastic gradient descent method<ul>
<li>勾配消失問題 gradient decent problem</li>
<li>責任割当問題 credit assignment problem</li>
</ul>
</li>
<li>非線形変換 non lineaar transformation<ul>
<li>ReLU 整流線形ユニット Recutified Linear Unit</li>
<li>ソフトマックス関数 softmax function</li>
<li>交差エントロピー cross entropy</li>
</ul>
</li>
<li>オーバーフィッティング 過学習 overfitting の回避法<ul>
<li>正則化 regularization</li>
<li>ドロップアウト dropout</li>
</ul>
</li>
<li>データ拡張 data augumentation</li>
<li>penultimate layer</li>
<li>{バッチ, 層, 重み} 正規化</li>
<li>one algorithm hypothesis</li>
</ul>
<!--
Roe et. al (1992) Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex
-->

<!-- How does cortex that normally processes inputs from one sensory
modality respond when provided with input from a different modality? We
have addressed such a question with an experimental preparation in which
retinal input is routed to the auditory pathway in ferrets. Following
neonatal surgical manipulations, a specific population of retinal ganglion
cells is induced to innervate the auditory thalamus and provides visual
input to cells in auditory cortex (Sur et al., 1988).  We have now examined
in detail the visual response properties of single cells in primary
auditory cortex (A 1) of these rewired animals and compared the responses
to those in primary visual cortex (V1) of normal animals. Cells in A 1 of
rewired animals differed from cells in normal V1: they exhibited larger
receptive field sizes and poorer visual responsivity, and responded with
longer latencies to electrical stimulation of their inputs. However,
striking similarities were also found. Like cells in normal V1, A 1 cells
in rewired animals exhibited orientation and direction selectivity and had
simple and complex receptive field organizations. Furthermore, the degree
of orientation and directional selectivity as well as the proportions of
simple, complex, and nonoriented cells found in A1 and V1 were very
similar. These results have significant implications for possible
commonalities in intracortical processing circuits between sensory
cortices, and for the role of inputs in specifying intracortical circuitry.

あるモダリティからの入力を通常処理する皮質は、異なるモダリティからの入力を
与えられたときにどのように反応するのだろうか？網膜入力が西洋イタチ，フェレッ
トの聴覚経路にルーティングされる実験でそのような状況を作り出した。新生児外
科手術に続いて、網膜神経節細胞の特定の集団が聴覚視床を神経支配するように誘
導し、聴覚皮質の細胞に視覚的な入力を提供した（Sur et al 1988）。今回、これ
らの再配線された動物の一次聴覚皮質（A1）における単細胞の視覚反応特性を詳細
に調べ、正常な動物の一次視覚皮質（V1）におけるそれらとの反応を比較した。再
配線された動物の A1 細胞は、正常な V1 細胞とは異なっていた：それらはより大
きい受容野の大きさと劣った視覚的反応性を示し、入力電気刺激に対してより長い
潜時で反応した。だが、驚くほどの類似点も見つかった。正常な V1 の細胞と同様、
再配線された動物の A1 細胞は、方向選択性と方位選択性を示し、単純型，複雑型
の受容野組織を有していた。さらに、方位選択性および方向選択性、ならびに A1
および V1 に見られる単純、複雑、および無配向のセルの割合は非常に類似してい
た。これらの結果は、皮質内処理回路における知覚皮質間の可能な共通性、および
皮質内回路の指定における入力の役割に対して重要な意味を持つ。
-->

<!--
- Metin and Frost (1988) Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus
-->

<!--
These experiments investigate the capacity of thalamic and cortical
structures in a sensory system to proces..  information of a modality
normally associated with another system. Retinal ganglion ceUs in newborn
Syrian hamsters were made to project permanently to the main thalamic
somatosensory (ventrobasal) nucleus. When the animals were adults, single
unit recordings were made in the somatosensory cortices, the principal
targets of the ventrobasal nucleus. The somatosensory neurons responded to
visual stimulation of distinct receptive fields, and their response
properties resembled, in several characteristic features, those of normal
visual cortical neurons. In the visual cortex of normal animals and the
somatosensory cortex of operated animals, the same functional categories of
neurons occurred in similar proportions, and the neurons' selectivity for
the orientation or direction of movement of visual stimuli was
comparable. These results suggest that thalamic nuclei or cortical areas at
corresponding levels in the visual and somatosensory pathways perform
similar transformations on their inputs.

実験で視床と皮質の能力を調べた。モダリティの情報を処理するための感覚システムの構造
通常は他のシステムと関連付けられています。
新生児シリアンハムスターの網膜神経節細胞は、主に視床の体性感覚（腹側基底核）核に永久的に突出するように作られた。
動物が成体のときは、腹側基底核の主な標的である体性感覚皮質において単一単位の記録が行われた。
体性感覚ニューロンは、異なる受容野の視覚刺激に応答し、そしてそれらの応答特性は、いくつかの特徴的な特徴において、正常な視覚皮質ニューロンのそれらに似ていた。
正常な動物の視覚皮質および手術された動物の体性感覚皮質において、同じ機能範疇のニューロンが同様の割合で生じ、そしてニューロンの選択性は
視覚刺激の運動の方向または方向は
同程度の。
これらの結果は、視経路および体性感覚経路における対応するレベルの視床核または皮質領域がそれらの入力に対して同様の変換を実行することを示唆している。
-->

<h2 id="_8">正規化，正則化，標準化，白色化，二重中心化<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>白色化については平井有三先生のパターン認識が参考文献で良いかな</p>
</li>
<li>
<p><a href="https://maristie.com/blog/differences-between-normalization-standardization-and-regularization/">Differences between normalization, standardization and regularization</a></p>
</li>
</ul>
<h2 id="tensorflow-hub"><a href="https://www.tensorflow.org/hub">TensorFlow HUB</a><a class="headerlink" href="#tensorflow-hub" title="Permanent link">&para;</a></h2>
<h2 id="inception-resnet-r-cnn-regional-convolutional-neural-networks">インセプション Inception，残渣ネット ResNet，領域 R-CNN (Regional Convolutional Neural Networks)<a class="headerlink" href="#inception-resnet-r-cnn-regional-convolutional-neural-networks" title="Permanent link">&para;</a></h2>
<ul>
<li>what and where routes</li>
<li>心理学的対応物(？)</li>
<li>/2015documents/2014Cadieu_Deep_Neural_Networks_Rival_the_Representation_of_Primate_IT_Cortex_for_Core_Visual_Object_Recognition.pdf</li>
<li>/2019documents/2019NasrViswanathanNieder_Number_detectors_spontaneously_emerge_in_a_deep_neural_network_designed_for_visual_object_recognition.pdf</li>
<li>/2018documents/2018Marcus_Deep_Learning_A_Critical_Appraisal.pdf</li>
<li>転移学習</li>
</ul>
<h3 id="notebooks">Notebooks<a class="headerlink" href="#notebooks" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/text_classification_with_tf_hub_on_kaggle.ipynb">colab/text_classification_with_tf_hub_on_kaggle.ipynb</a>
Shows how to solve a problem on Kaggle with TF-Hub.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb">colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb</a>
Explores text semantic similarity with the <a href="https://tfhub.dev/google/universal-sentence-encoder/2">Universal Encoder Module</a>.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_generative_image_module.ipynb">colab/tf_hub_generative_image_module.ipynb</a>
Explores a generative image module.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb">colab/action_recognition_with_tf_hub.ipynb</a>
Explores action recognition from video.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_delf_module.ipynb">colab/tf_hub_delf_module.ipynb</a>
Exemplifies use of the <a href="https://tfhub.dev/google/delf/1">DELF Module</a> for landmark recognition and matching.</li>
<li><a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">colab/object_detection.ipynb</a> 
Explores object detection with the use of the  <a href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb">Faster R-CNN module trained on Open Images v4</a>.</li>
</ul>
<hr />
<!--
<center>
<img src='https://cdn-images-1.medium.com/max/1280/1*sS_WZM4GLS88XlnDLKcZ-g.png' style='width:94%'><br>
from [A guide to Face Detection in Python](https://towardsdatascience.com/a-guide-to-face-detection-in-python-3eab0f6b9fc1)
</center>

---

-->

<ul>
<li>
<p><a href="https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb">The Complete Beginner’s Guide to Deep Learning: Convolutional Neural Networks and Image Classification</a>, Anne Bonner Feb. 02</p>
</li>
<li>
<p>畳込みニューラルネットワーク (Convlutional Neural Networks:CNN) とは画像認識におけるゲームチェンジャー(以後，画像認識，ビデオ分類，自動運転，ドローン，ゲームなどへの応用多数)</p>
</li>
<li><a href="http://image-net.org/challenges/LSVRC/">イメージネット画像コンテスト</a>では，分類 (classification) 課題と位置 (locallization) 課題とからなる。</li>
<li>コンテストは 2010 年から Li Fei-Fei さん中心となって <a href="https://www.mturk.com/">AMT</a> で画像のアノテーションを行って 画像を2012 年の優勝チームが CNN を使った。通称<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">アレックスネット</a></li>
<li><a href="http://cs231n.stanford.edu/index.html">スタンフォード大学の授業 CS231n: Convolutional Neural Networks for Visual Recognition</a>. スライド](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture05.pdf)</li>
</ul>
<h2 id="_9">さらなる情報<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h2>
<ul>
<li>Math? <a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu</li>
<li>C.-C. Jay Kuo <a href="https://arxiv.org/abs/1609.04112">Understanding Convolutional Neural Networks With a Mathematical Model</a>.</li>
<li><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">the absolute basics of activation functions, you can find that here</a>!</li>
<li>Artificial neural networks? <a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">You can learn about them here</a>!)</li>
</ul>
<!--
#### ReLU layer
The **ReLU** (rectified linear unit) layer is another step to our convolution layer. 
You’re applying an activation function onto your feature maps to increase non-linearity in the network. 
This is because images themselves are highly non-linear! 
It removes negative values from an activation map by setting them to zero.

Convolution is a linear operation with things like element wise matrix
multiplication and addition. 
The real-world data we want our CNN to learn will be non-linear. 
We can account for that with an operation like ReLU. 
You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy.
-->

<!--
Want to dig deeper? Try Kaiming He, et al. [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).

If you need a little more info about [the absolute basics of activation functions, you can find that here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!
-->

<!--
Here’s how our little buddy is looking after a ReLU activation function
turns all of the negative pixel values black



wzxhzdk:0


<center>
<img src="../assets/output2.jpg" style="width:84%">
</center>


#### Pooling
The last thing you want is for your network to look for one specific feature in an exact shade in an exact location. 
That’s useless for a good CNN! 
You want images that are flipped, rotated, squashed, and so on. 
You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location. 
No matter what the lighting or the number of spots,or whether that leopard is fast asleep or crushing prey. 
You want **spatial variance**! You want flexibility. 
That’s what pooling is all about.

Pooling progressively reduces the size of the input representation. 
It makes it possible to detect objects in an image no matter where they’re located. 
Pooling helps to reduce the number of required parameters and the amount of computation required. 
It also helps control **overfitting**.

Overfitting can be kind of like when you memorize super specific details before a test without understanding the information. 
When you memorize details, you can do a great job with your flashcards at home.
You’ll fail a real test, though, if you’re presented with new information.

(Another example: if all of the dogs in your training data have spots and dark eyes, your network will believe that for an image to be classified as a dog, it must have spots and dark eyes. 
If you test your data with that same training data, it will do an amazing job of
classifying dogs correctly! But if your outputs are only “dog” and “cat,” and your network is presented with new images containing, say, a Rottweiler and a Husky, it will probably wind up classifying both the Rottweiler and the Husky as cats. You can see the problem!)


Without variance, your network will be useless with images that don’t exactly match the training data. 
**Always, always, always keep your training and testing data separate**! 
If you test with the data you trained on, your network has the information memorized! 
It will do a terrible job when it’s introduced to any new data.

#### Overfitting is not cool.
So for this step, you take the **feature map**, apply a **pooling layer**, and the result is the **pooled feature map**.

The most common example of pooling is **max pooling**. 
In max pooling, the input image is partitioned into a set of areas that don’t overlap. 
The outputs of each area are the maximum value in each area. 
This makes a smaller size with fewer parameters.

Max pooling is all about grabbing the maximum value at each spot in the image. 
This gets rid of 75% of the information that is not the feature. 
By taking the maximum value of the pixels, you’re accounting for distortion. 
If the feature rotates a little to the left or right or whatever, the pooled feature will be the same. You’re reducing the size and parameters. 
This is great because it means that the model won’t overfit on that information.

You could use **average pooling or sum pooling**, but they aren’t common choices. 
Max pooling tends to perform better than both in practice. 
In max pooling, you’re taking the largest pixel value. 
In average pooling, you take the average of all the pixel values at that spot in the image. 
(Actually, there’s a trend now towards using smaller filters or discarding pooling layers entirely. 
This is in response to an aggressive reduction in representation size.)

__Want to look a little more at why you might want to choose max pooling
and why you might prefer a stride of two pixels? Check out Dominik
Scherer et. al, [Evaluation of Pooling Operations in Convolutional
Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf).__

If you go [here](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) you can check out a really interesting 2D visualization of a convolutional layer. 
Draw a number in the box on the left-hand side of the screen and then really go through the output. 
You can see the  convolved and pooled layers as well as the guesses. 
Try hovering over a single pixel so you can see where the filter was applied.


So now we have an input image, an applied convolutional layer, and an applied pooling layer.

Let’s visualize the output of the pooling layer!

We were here:

<center>
<img src="../assets/output3.jpg" style="width:94%">
</center>

The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps. 
It does this by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.

See how the image has changed size?

<center>
<img src="../assets/output4.jpg" style="width:94%">
</center>

Cool, right?

#### Flattening

This is a pretty simple step. You flatten the pooled feature map into a sequential column of numbers (a long vector). 
This allows that information to become the input layer of an artificial neural network for further processing.


#### Fully connected layer
At this step, we add an **artificial neural network** to our convolutional neural network. 
(Not sure about artificial neural networks? [You can learn about them here](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)!)
-->

<!--
The main purpose of the artificial neural network is to combine our features into more attributes. 
These will predict the classes with greater accuracy. This combines features and attributes that can predict classes better.

At this step, the error is calculated and then backpropagated. 
The weights and feature detectors are adjusted to help optimize the performance of the model. 
Then the process happens again and again and again. 
This is how our network trains on the data! 

How do the output neurons work when there’s more than one?

First, we have to understand what weights to apply to the synapses that connect to the output. 
We want to know which of the previous neurons are important for the output.

If, for example, you have two output classes, one for a cat and one for a dog, a neuron that reads “0” is absolutely uncertain that the feature belongs to a cat. A neuron that reads “1 is absolutely certain that the feature belongs to a cat. 
In the final fully connected layer, the neurons will read values between 0 and 1. 
This signifies different levels of certainty. 
A value of 0.9 would signify a certainty of 90%. 
The cat neurons that are certain when a feature is identified know that the image is a cat. 
They say the mathematical equivalent of, “These are my neurons! I should be triggered!” If this happens many times, the network learns that when certain features fire up, the image is a cat.


Through lots of iterations, the cat neuron learns that when certain features fire up, the image is a cat. 
The dog (for example) neuron learns that when certain other features fire up, the image is a dog. 
The dog neuron learns that for example again, the “big wet nose” neuron and the “floppy ear” neuron contribute with a great deal of certainty to the dog neuron.
It gives greater weight to the “big wet nose” neuron and the “floppy ear” neuron. 
The dog neuron learns to more or less ignore the “whiskers” neuron and the “cat-iris” neuron. 
The cat neuron learns to give greater weight to neurons like “whiskers” and “cat-iris.”
(Okay, there aren’t actually “big wet nose” or “whiskers” neurons. 
But the detected features do have distinctive features of the specific class.)


Once the network has been trained, you can pass in an image and the neural network will be able to determine the image class probability for that image with a great deal of certainty.

The fully connected layer is a traditional Multi-Layer Perceptron. 
It uses a classifier in the output layer. 
The classifier is usually a softmax activation function. 
Fully connected means every neuron in the previous layer connects to every neuron in the next layer. 
What’s the purpose of this layer? To use the features from the output of the previous layer to classify the input image based on the training data.

Once your network is up and running you can see, for example, that you have a 95% probability that your image is a dog and a 5% probability that your image is a cat.


Why do these numbers add up to 1.0? (0.95 + 0.05)

There isn’t anything that says that these two outputs are connected to each other. 
What is it that makes them relate to each other? 
Essentially, they wouldn’t, but they do when we introduce the **softmax function**.
This brings the values between 0 and 1 and makes them add up to 1 (100%). 
(You can read all about this on Wikipedia.) 
The softmax function takes a vector of scores and squashes it to a vector of values between 0 and 1 that add up to 1.

After you apply a softmax function, you can apply the loss function.
Cross entropy often goes hand in hand with softmax. 
We want to minimize the loss function so we can maximize the performance of our
network.

At the beginning of backpropagation, your output values would be tiny.
That’s why you might choose cross entropy loss. 
The gradient would be very low and it would be hard for the neural network to start adjusting in the right direction. 
Using cross entropy helps the network assess even a tiny error and get to the optimal state faster.


#### Want more? Check out
-->

<ul>
<li><a href="https://www.youtube.com/watch?v=mlaLLQofmR8">video by Geoffrey Hinton</a> on the softmax function</li>
<li><a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/">A Friendly Introduction to Cross Entropy Loss</a> by Rob DiPietro</li>
<li><a href="https://peterroelants.github.io/posts/cross-entropy-softmax/">How to Implement a Neural Network Intermezzo 2</a> by Peter Roelants</li>
<li>
<p><a href="https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb">画像分類の基礎</a>)</p>
</li>
<li>
<p><a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Introduction to Convolutional Neural Networks</a> by Jianxin Wu </p>
</li>
<li>Yann LeCun’s original article, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-Based Learning Applied to Document Recognition</a></li>
<li><a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The Nine Deep Learning Papers You Need to Know About</a> (Understanding CNNs part 3) by Adit Deshpande</li>
</ul>
<!--
from keras import backend as K

# with a Sequential model
get_3rd_layer_output = K.function([model.layers[0].input],
                                  [model.layers[3].output])
layer_output = get_3rd_layer_output([x])[0]
```


- Similarly, you could build a Theano and TensorFlow function directly.

- Note that if your model has a different behavior in training and testing phase (e.g. if it uses Dropout, BatchNormalization, etc.), you will need to pass the learning phase flag to your function:


```python
get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],
                                  [model.layers[3].output])

# output in test mode = 0
layer_output = get_3rd_layer_output([x, 0])[0]

# output in train mode = 1
layer_output = get_3rd_layer_output([x, 1])[0]
````

Alternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:
-->

<!--
# [TensorFlow Hub is a library for reusable machine learning modules](https://www.tensorflow.org/hub)

- [Hub with Keras](https://www.tensorflow.org/tutorials/images/hub_with_keras)
-->

<!--
- 李 飛飛さんの TED talk
<center>
<iframe width="480" height="300" src="https://www.youtube.com/embed/40riCqvRoMs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></br>
**李 飛飛さんの TED talk**
</center>

<center>
<iframe width="300" height="160" src="https://www.youtube.com/embed/BDKHFq7X4m0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></br>

<iframe width="300" height="160" src="https://www.youtube.com/embed/XlnbNFW2tX8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
-->

<!--
<center>
<iframe width="950" height="533" src="https://www.youtube.com/embed/NkEEQUQkjCQ?list=PL1w8k37X_6L-Y5F0ebFCETbSMf6uONc4j" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
**Ng's coursera video, Brain and Neural Networks**
</center>

以下は Ng の deeplearning.ai couse week1 の python video のセリフ

Hello. This lesson is going to introduce you to the Python programming language.
Now, don't worry, the purpose of this course is
not to teach you to become a proficient Python programmer,
although that might happen.
Instead, we want to teach you how to write and execute
Python scripts to perform basic data analytics tasks.
Now, this course could, of course,
use proprietary tools like Microsoft Excel or Tablo to teach you
these skills but rather than learning how
to perform those analytic skills and those analytics tasks,
you instead would have learned how to use those tools.
If you really want to be able to understand how algorithms
work and how to apply them to different types of data in different types of situations,
you need to learn how to do some basic programming.
That's why we're using Python.
It's a very simple programming language that has a lot of power and the ability to
operate on many different types of data both small data on your own laptop or desktop,
as well as scaling to very large data sets that might be analyzed in the cloud.
Now, this particular lesson is about introducing you to the basics of Python.
Specifically, you're going to learn about Python keywords.
The fact that there's a built in help system that Python
provides mechanisms to document your code via comments.
You'll learn about the built in Python math operators and
basic data types and these allow you to treat Python as a little calculator.
And you'll also learn about creating and using Python variables.
So, let's change over to the notebook for this particular lesson.
Python is an open source,
high level, general purpose programming language.
It's been around for several decades.
It's in what is known as the C family of languages.
And what that means is, if you learn Python and can understand and read Python,
you'll be able to look at a program written in C or Java or C
plus plus or any other language it's in that family of programming languages,
and you'll be able to understand basically what's going on.
That's one of the other benefits of using Python,
is that you can understand some of these other languages.
Python makes it very easy to do
complex tasks and Python is meant to be able to be read easily.
Python has a number of modules that extend
the language and you'll see many of those in this course.
This is both a good and bad thing.
It's good because it enables you to do lots of things
very easily by simply importing a module into your program.
The problem is that in order for
your program then to work these modules have to be installed.
This used to be a bigger problem than it is now.
Your course server has all of these modules already installed and
there are standard ways to ensure these modules are now installed wherever you go.
If you're working in a company,
often the IT department will help make sure those are installed.
One thing I want to mention, any time you see the word optional in a notebook,
that particular section is optional,
you do not have to read it,
but it might be useful for you, particularly,
if you want to understand
a more nuanced or detailed version of what you're actually trying to learn.
In this case, it is a history of the Python programming language and it talks,
in particular, about the fact that there is now something called Python three.
You see that we are in a Python three notebook right here.
And there's differences between Python three and Python two.
So, if you want to understand that a little bit better you might
read this particular section.
We've now left that section and we now come into a required section.
The rest of the notebook will introduce you to these basic concepts.
First of all, you'll learn about what you need to
keep in mind when you're writing a good Python code.
First of all, in Python,
white space is important.
If you are indenting code,
you need to make sure that you're using the same amount of indentation.
The standard is four spaces.
You want to create new names, they should be descriptive.
Somebody should be able to look at it and know what you're trying to
do with that particular variable or function.
You also don't want lines of code to stretch too long.
We typically try to keep them less than 80 characters.
And you'll learn how to indicate that the line continues on the next line.
And of course good code should be thoroughly documented.
And you'll learn here in this notebook how to do that.
One important point I want to make is that Python identifiers which
are variables or function names have to be uniquely defined.
And it's important to do that because if you think about it,
if you enter a room and there's a lot of people in the room,
and two people have the same name say,
Joe, and you say,
"Hello Joe," they won't know to which person you're referring.
The same thing happens in Python.
When you're trying to write a Python script and you have something named Joe,
say a variable you're trying to use,
and there's a function defined in a module you're trying to use with the same name,
we get what's called a scope conflict.
And the interpreter doesn't know which Joe you're referring to.
So, we want to avoid those conflicts.
One of the important things is we don't want to use
any of the existing Python three keywords.
Never name something one of these names.
There's also other names that are used in Python like string and list,
or things that might be in one of the Python modules you want to use.
You should avoid those names.
And we typically do that by coming up with a descriptive name.
The rest of this particular notebook talks about how to
use documentation in particular comments to
indicate what you're trying to do and the different types of comments.
You'll also learn about the built in help,
function which gives you a ability in a notebook to
learn how something might actually operate and to get more details.
There are some students exercises and again I want you to try those out.
So, for instance you might try help.
And if I execute this code so,
you'll get a display of the built in documentation for the dictionary.
We'll learn more about dictionaries in a later course or a later lesson in this course.
Python has a number of built-in math operators.
Things to do grouping, exponential's, multiplication, division.
Two things here that might be a little different is integer division
which is to say five integer division two would be two.
And then, of course, the remainder is,
or what remains after you have divided five by two is an integer.
There's also addition and subtraction.
We also have a number of built-in functions that can
provide additional support for things.
There's also the math library that has additional functions like trig functions,
log functions, and some special functions.
This is part of the standard Python library,
so it's always available.
But there's additional libraries so we could bring in to do more things,
and these are things like SciPy and NumPy.
And we'll see those in later lessons.
The rest of this notebook will walk you through using those,
there's a few more student exercises and you should try those
out and you can see some interesting things.
One last thing I want to make a note of is if you see something like this,
we don't use it a lot in this course but sometimes you'll see it in people's notebooks.
This underscore is referring to the previous cells result.
So, this is saying, take two and multiply it by this number right here,
the result of the previous cell.
You can do this with a single underscore to refer now to
the previous cell and two underscores to refer to two cells ago.
Again, I don't think it's a great thing to do because it's better to use variables.
This might be confusing but you might see how other people might use it at some point.
And then lastly, we import from the math library the square root,
the exponential and the cosine function and we use those in our codes.
So, hopefully this has given you a quick introduction to the Python programming language.
Definitely play with this notebook. Try things out.
See how you can use these techniques and
ideas to start building your own simple math operations.
You can treat this if you will as a simple calculator and you can
learn how to build on these to make more complex things.
If you have any questions about this,
please reach out to us in the course forums. And good luck. 
-->
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright (c) 2020</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
